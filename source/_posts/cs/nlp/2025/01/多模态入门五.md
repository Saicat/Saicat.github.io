---
title: 多模态入门(五)--InternVL系列
tags:
  - 多模态
  - CV
  - NLP
  - transformer
  - 预训练
  - 无监督学习
  - SFT
categories:
  - CS
  - 多模态
abbrlink: 52c8a4f9
date: 2025-01-22 19:50:10
---

【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  

***  

最近InternVL2.5和Mini-InternVL-2相继发布。看了下，发现Intern模型在MLLM领域的相关工作还挺多的。  

前面的文章已经讲解过InternLM-1/2/2.5，而在多模态入门(三)中，也学习了InternVL-1/1.5，这里就再继续学习下InternLM-XComposer系列、Mini-InternVL系列以及InternVL-2/2.5。  

重点的内容应该是在InternVL-2.5部分，披露的细节比较多，模型的效果也最好。  

# InternLM-XComposer  

论文：《InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition》  

时间：2023年9月  

机构：上海AI Lab  

InternLM-XComposer是基于InternLM开发的一个多模态模型。它的一个特点就是不仅能生成纯文本的结果，还能在这个基础上（通过搜索）配图，给出图文交错的文章。  

## 训练  

1、模型结构  

模型结构上，依然是包含三大常规部分：  
- vision encoder：使用EVA-CLIP，标准CLIP ViT的一个变体  
- perceive sampler：用于把原始的257个token减少到64个，来和LLM对齐；初始化参数来自BLIP2；所有输入都会resize到224×224，patch size=14  
- LLM：用InternLM初始化  

2、训练  

基于这个参数初始化后，训练包括预训练和SFT两个阶段。两个阶段的冻结和训练参数情况如下：  

{% asset_img ixc_train.png 多模态入门 %}  

可以看到整个过程中vision encoder都是保持冻结的，而perceive sampler则是全程保持可训练。  

文中对冻结哪部分参数做了消融实验：  

{% asset_img ixc_freeze_ablation.png 多模态入门 %}  

从实验结果看，冻结perceive sampler会对最终模型的效果带来比较大的损失，因此最终选择在预训练和微调都保持perceive sampler参数可训练。  

3、数据  

预训练数据如下，总共有1.1B张图片（相当于70.4B token），77B文本token的数据：  

{% asset_img ixc_ptm_data.png 多模态入门 %}  

- 包含了训练InternLM所用的文本数据，帮助模型在多模态训练过程中保持语言能力  
- in-house concept data：这部分私有数据是从web数据中挖掘的，包含了大量visual concept的优质数据，以及相关的detail explanation  

而SFT阶段包含了两大类数据，multi-task training和instruction-tuning：  

{% asset_img ixc_sft_data.png 多模态入门 %}  

multi-task training的数据都构造成多轮对话的格式。训练这些数据时，有一个特别的点是使用LoRA来微调，这样可以帮助模型保持原LLM的语言能力。  

## Interleaved Image-Text Composition  

前面说到InternLM-XComposer可以生成图文交错的文章，其方法主要分成三步：  
- （1）生成纯文本结果  
- （2）找到文本结果中可以/需要插入图像的位置，并生成对应的caption  
- （3）用上一步的caption进行图像检索，获得候选，并选择最符合的图像，获得最终结果  

{% asset_img ixc_inter_gen.png 多模态入门 %}  

第（2）步中，训练数据使用真实的图文交错文章，并利用GPT-4获取caption；第（3）步中，图像的检索使用CLIP。  

# InternLM-XComposer-2  

论文：《InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Models》  

时间：2024年1月  

InternLM-XComposer2 7B模型下载：https://github.com/InternLM/InternLM-XComposer  

## 模型  

InternLM-XComposer2认为，之前的多模态对齐方法主要有两大类：  
- 把图像token和文本token视为完全同等的存在，一视同仁地处理；这种方法忽视了各个模态之间的固有区别  
- 把图像tokne和文本tokne视为完全不同的存在，分别进行处理；这种方法导致对齐的成本大大增加  

InternLM-XComposer2里则是提出了另外一种方法，partial LoRA，来进行LLM和vision encoder的对齐：  

{% asset_img ixc2_plora.png 多模态入门 %}  

训练时模型的设置：  

- LLM：基于InternLM-2-7B-ChatSFT初始化  
- Vision encoder：基于OpenAI CLIP ViT-L-14-336初始化，并把分辨率提升到490x490以提升效果  
- Partial LoRA：rank256  

另外预训练中还使用了layer-wise LR，LR随层数递减，衰减系数为0.9。  

## 训练  

1、预训练  

预训练中，LLM的参数保持冻结，训练vision encoder和p-lora。  

预训练的数据包含三个层级的目标：  
- general semantic alignment：一般的通用对齐，让模型学习理解图像的基本内容，比如能知道一张爱因斯坦的照片包含“人类”这个概念；所用数据包括各种caption数据  
- world knowledge alignment：包含知识，比如能够知道图像上的人是爱因斯坦，是物理学家；这里使用InternLM-XComposer中构建的概念数据集  
- vision capability enhancement：高级的能力，比如OCR、grounding以及结构化信息的识别（图表）等  

2、SFT  

和InternLM-XComposer类似，几个细节是：  
- 也使用decay factor=0.9的layer-wise LR策略  
- 学习率是预训练的20%  

# InternLM-XComposer-2.5  

论文：《InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output》  

时间：2024年7月  

InternLM-XComposer-2.5，简称IXC-2.5，相比上一代，效果上主要有三个升级：  
- 更高分辨率：复用IXC-2的ViT，分辨率从490x490提升到560x560  
- 细粒度视频理解：把视频视为由几十帧到几百帧构成的超大图像  
- 多轮多图像对话  

IXC-2.5整体framework如下：  

{% asset_img ixc2.5_framework.png 多模态入门 %}  

预训练和微调的大致流程和IXC-2是差不多的，只是数据上进行了一些更新。  

在IXC-2.5的基础上，研究人员构建了两个应用：网页生成和文章撰写。  

网页生成就是根据输入的网页图片和命令，自动构建网页。训练这个能力最重要的任务就是搞数据。这里的数据来自于Stack v2，进行了一些清洗之后获得了250k的网页数据，然后用这些数据进行LoRA训练。  

而文章撰写能力则是在SFT的基础上，增加了DPO，以提升生成的效果。  

# InternVL-2  

时间：2024年7月  

InternVL2没有技术报告，只有一篇blog，https://internvl.github.io/blog/2024-07-02-InternVL-2.0/。  

InternVL-2是在InternVL-1.5上的优化版本，主要有3点优化：  
- 多阶段的训练策略  
- 更精细的训练数据  
- 支持多模态的输出  

{% asset_img internvl2_model.png 多模态入门 %}  

# InternVL-2.5  

论文：《Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling》  

时间：2024年12月  

InternVL-2.5相比InternVL-1.5和InternVL-2在效果上有了比较大的提升。  

## Architecture  

大的架构上，InternVL-2.5依然保持和InternVL-1.5相同的设计，使用“ViT-MLP-LLM”的范式：  

{% asset_img ivl2.5_archi.png 多模态入门 %}  

1、vision encoder  

InternVL-2.5中使用了两个版本的vision encoder，大小分别为6B和0.3B。  

在InternVL-1中首次使用了6B的ViT模型，InternViT-6B-224px；不过那时候的效果还没有那么好，为了提升vision encoder的效果，这里进一步对这个6B模型进行训练，优化点包括：  
- 使用更大的分辨率（224-->448）/动态分辨率  
- 进行增量预训练（连接LLM后）  

获得了InternViT-6B之后，通过蒸馏得到InternViT-300M-448px-Distill。和6B模型不同，300M模型使用standard layernorm，并且没有使用QK-Norm。  

为了降低蒸馏成本，300M模型并不是从零初始化，而是借用了CLIP-ViT-Large-336px的参数。虽然二者的架构并不完全相同，不过效果总比随机初始化要好。  

蒸馏之后，还做了一个增量预训练，在更多样化的数据上训练300M模型，得到 InternViT-300M-448px-V2.5。  

还有一个细节是，InternVL-2.5使用pixel unshuffle operation降低了输入token的数量，因此448x448的输入图像最后会表达成256个token。  

前面这些操作中间产生了好几个版本的模型，都在下面的表格里：  

{% asset_img ivl2.5_vits.png 多模态入门 %}  

2、LLM  

InternVL早期版本使用的LLM有很多来源，而InternVL-2.5中主要是使用InternLM-2.5和Qwen-2.5，这两个更新的LLM：  

{% asset_img ivl2.5_models.png 多模态入门 %}  

## 训练  

1、Dynamic High-Resolution  

动态分辨率在InternVL-1.5的基础上进一步优化了。主要有这几个步骤：  
- 首先计算输入图像的原始宽高比，并从一系列预定义的宽高比中选择最接近的一个；这一步的目的是要把图像切分成多个448x448的子图，子图的数量被预定义在n_min和n_max之间，那么预定的宽高比其实就由预定义的子图的样式决定；输入图像的宽高比和预定义的宽高比的差异计算为W/H的绝对值差  
- 如果出现多个相同的候选宽高比，优先选择图像大小差异小的  
- 按照选择预定宽高，resize原图，并把图像切分成448x448的子图  

如果原始图像被切分了（如果原始图像太小就不需要切分），那么还会生成一张缩略图，贴在子图列表后面。这样的动态分辨率方法也可以扩展到多图或者视频输入：  

{% asset_img ivl2.5_datatype.png 多模态入门 %}  

- 多图：把子图的数量按比例分配给各张图片  
- 视频：每帧图片都直接resize到448x448  

实际使用中，对于多图数据集、高分辨率数据集以及图表信息数据等，n_max设得比较大，比如24/36；而对于低分辨率的数据集，则设得比较低，比如6/12。  

2、training pipeline  

训练分成3个stage：  
- stage1（对齐）：只训练MLP，使用的LR也比较大，能够让MLP快速学习对齐；另外这个阶段就开始使用动态分辨率了  
- stage1.5（预训练）：这个阶段训练MLP和ViT；所使用的LR相对低一些，防止灾难性遗忘  
- stage2（SFT）：全部参数进行训练，这一阶段的数据质量最为重要，即使是少量的异常样本比如几千条不好的数据，都已经足以影响模型的生成效果了；这个阶段超参策略相对简单，对所有参数都保持相同的LR  

各个阶段的训练设置：  

{% asset_img ivl2.5_trainconfig.png 多模态入门 %}  

那么其中的stage1.5是需要拿出来说一下的。研究人员实验发现，使用较小的LLM在stage1.5训练了ViT之后，把ViT连接到更大的LLM上，就可以保持比较好的效果了，因此也就引出一个被称为progressive scaling strategy的方法：在小LLM上训练好ViT，再连接大的LLM，就可以直接只使用stage1（对齐） + stage2（SFT）的训练，从而减少很多训练量，降低成本。  

{% asset_img ivl2.5_train_pipeline.png 多模态入门 %}  

3、训练优化  

训练中，额外还使用了两个优化策略。  

（1）JPEG compression  

为了避免过拟合，随机使用quality level在75到100之间的设置进行JPEG图像压缩。  

需要注意的是，这个方法不用在视频数据中，因为要保持每帧都有相同的质量。  

（2）Loss reweighting  

一般来说NTP的训练是对整个训练batch中的token进行平均，这样每个训练token对最终loss的贡献和影响是一样大的。这种情况下可能会导致训练结果受长度较长的数据影响更大。而如果以样本为单位进行平均，那么每条训练样本对loss的贡献是一样的，这又会导致短的样本在loss中的权重更大。两种情况的公式表达如下，x为样本的token数量：  

$$\mathcal{L}=\frac{w_i}{\sum_jw_j}\cdot\mathcal{L}_i,\quad w_i=\begin{cases}\frac{1}{x^0},&\text{for token averaging}\\\frac{1}{x^1},&\text{for sample averaging},&\end{cases}$$  

为了平衡这两种方案，InternVL-2.5的做法是使用  

$$w_{i}=\frac{1}{x^{0.5}}$$  

## 数据  

首先，不同类型的数据（单图、多图、视频、文本）有不同的增强、采样设置：  

{% asset_img ivl2.5_data_config.png 多模态入门 %}  

此外，还有以下几个操作。 

1、data packing  

data packing是大模型训练的常规做法了，这样可以减少padding，提升训练效率。对于多模态数据的packing，有两个要考虑的因素：（a）sequence长度（b）image tile number for ViT。  

考虑到这两个因素，整个packing策略分为四步：  

（1）select：这一步不用还不用考虑packing的事，而从数据池采样一些单条的数据；对于过长的单条数据，会把它们切分成更小的item；“过长”有两个指标，只要命中一个就是过长：要么总的token数据量超过L_max，或者image tile的数量超过T_max；切分后的item保证长度≤L_max，image tile数量≤T_max；切分后的item都会被视作independent的样本  

（2）search：这一步从buffer中搜索一个sample，和当前已有的sample拼接在一起；这里要保证拼接后的结果依然满足不过长的标准，也就是总token数和image tile数都不超过最大值；如果buffer中有多条sample都符合要求，那么选择最长、image tile最多的那个；出于这个考虑，维护buffer的时候可以按长度和image tile从大到小排序，方便使用二分检索快速搜索  

（3）pack：从上一步搜索的sample拼接到一起，注意每个独立sample在attention中只能看到自己的token，不能看到其他拼接样本的token；基于此，一条独立样本中的多个item的positional index也独立计数的  

（4）maintain：如果当前的packed sample达到长度或者image tile上限，那么就会输出用于训练；否则就会把他放回到buffer list中；为了避免buffer耗费太多空间，buffer有一个最大size，如果buffer已经达到最大size，那么就把当前最长的样本输出用于训练，释放空间  

2、数据过滤  

有一个观察，LLM对噪音数据的敏感程度明显高于vision encoder，即使只有少量几千个异常样本也会影响最终模型的效果。这些噪音数据中，影响最大的就是具有重复模式的数据，这些数据可能会让模型输出陷入重复循环的样式，严重影响用户体验，并且也使得test-time scaling的策略没法进行。  

{% asset_img ivl2.5_samples.png 多模态入门 %}  

那么一个最直接的方法就是进行数据过滤，pipeline如下图：  

{% asset_img ivl2.5_filter.png 多模态入门 %}  

对于纯文本数据，有三个策略：  
- LLM-Based质量打分：首先把数据分成不同的领域，包括code、math、general、stem等，然后用LLM加上各个领域对应的prompt给每个样本打分，分数从0~10，然后删除低于阈值（e.g. 7）的数据  
- 重复检测：也是利用LLM + prompt检测重复样本，然后后面会有人工review，决定阈值再删除对应数据  
- 规则：包括长度异常、过长的zero sequence、重复行过多的样本  

对于多模态数据，鉴于现有的MLLM在打分方面的能力还不是很行，因此专注在重复检测和规则两种策略来过滤。几个细节：  
- 重复检测的时候略过了高质量的学术数据集  
- 规则检验找出来的异常数据会人工review再确认是否合理  

3、data mixture  

（1）预训练  

在预训练阶段所包含的数据还是很多的：  

{% asset_img ivl2.5_ptm_data.png 多模态入门 %}  

值得注意的是，所有数据都被构建成对话格式，对于那些本身不是对话格式的数据，会构建问题把它们转成对话格式。这个阶段的数据尽可能多地包含各种内容，提高模型的泛化能力，而没有特别考虑数据的质量，高低质量的数据都会用（因为只有MLP和ViT会被训练）。  

研究人员认为，理想的状况是在预训练阶段把SFT(stage 2)要用的数据都包含进去，即SFT数据应该是预训练数据的子集，这样让模型能够充分学习。不过实践中，由于stage1.5的成本比较高，所以往往只能包含部分stage 2的数据，那些持续新增的SFT数据就没法放进预训练中了。  

（2）SFT  

从InternVL-1.5的5.1M，InternVL-2的7.3M，到InternVL-2.5的16.3M，SFT数据量有了比较大的提升。InternVL-2.5所包含的SFT数据如下：  

{% asset_img ivl2.5_sft_data.png 多模态入门 %}  

# Mini-InternVL  

论文：《Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance》  

时间：2024年10月  

Mini-InternVL相比前面InternVL各个版本，在技术上没有太多改变，可以认为是一次小型化的实践。Mini-InternVL系列有1B、2B、4B三个规模。如文章标题，Mini-InternVL用5%的参数量实现了大模型90%的效果。（至于10%的损失作为小型化的代价是否是可以接受的就见仁见智了）  

Mini-InternVL的效果：  

{% asset_img miniivl_perf.png 多模态入门 %}  

1、框架和训练  

Mini-InternVL使用CLIP-ViT-L-336px初始化一个300M的ViT，并用6B的ViT做teacher进行蒸馏，获得InternViT-300M。  

Mini-InternVL的训练分成两个阶段：  
- 对齐：对齐阶段只训练MLP  
- instruction tuning：全参微调  

这里有点意外的是在对齐阶段之后，没有对ViT或LLM进行大规模的预训练。整体框架和训练如下图所示：  

{% asset_img miniivl_archi.png 多模态入门 %}  

2、数据  

数据上，Mini-InternVL对各种任务都设计了对话数据格式，各个任务有特殊的token用于标识重要内容，比如bounding box、类别候选等，如下图：  

{% asset_img miniivl_format.png 多模态入门 %}  

# Mini-InternVL-2  

时间：2024年12月

Mini-InternVL2没有技术报告，有一篇blog，https://internvl.github.io/blog/2024-10-21-Mini-InternVL-2.0/。从blog看，只有两个训练阶段不冻结的可参数参数多了些：  

{% asset_img miniivl2_model.png 多模态入门 %}  

但是blog里report的结果又和一代是相同的，这就有点奇怪了。  

***  

博客：[http://www.linsight.cn/](http://www.linsight.cn/)  
知乎：[Linsight](https://www.zhihu.com/people/us4ever)  
微信公众号：Linsight  
![](/images/qrcode.jpg)
博主微信号(添加请注明来意)：  
![](/images/wechat.png)  

***  

【推荐文章】  
- MoE：  
[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  
[DeepSeek-V2和MLA](https://www.linsight.cn/83c49df0.html)  
[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  
[成本10w刀的JetMoE](https://www.linsight.cn/f3acf042.html)  
[MoE的top-p routing](https://www.linsight.cn/224c42da.html)  
[对MoE模型的一些观察](https://www.linsight.cn/5e1d14b3.html)  
[从dense到MoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  
[MoE路由--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  
- 端侧模型：  
[苹果智能系统模型--AFM](https://www.linsight.cn/1e34e252.html)  
[MiniCPM](https://www.linsight.cn/376db710.html)  
[适合移动设备的语言模型--MobileLLM](https://www.linsight.cn/5ac36d34.html)  
[phi系列模型](https://www.linsight.cn/fe13b56f.html)  
[Gemma2](https://www.linsight.cn/cf3f1f81.html)  
[苹果的OpenELM](https://www.linsight.cn/f845f3e4.html)  
[bilibili的index-1.9B](https://www.linsight.cn/770b63e1.html)  
- 预训练：  
[代码大模型(一)--业界现状](https://www.linsight.cn/a0b50049.html)  
[代码大模型(二)--OpenCoder](https://www.linsight.cn/7856bcc1.html)  
[LLM高效预训练(一)](https://www.linsight.cn/dcb57672.html)  
[LLM高效预训练(二)](https://www.linsight.cn/1e2e35a7.html)  
[Llama3.1--预训练要点一览](https://www.linsight.cn/7d7294cb.html)  
[Qwen2技术报告](https://www.linsight.cn/a8f8b641.html)  
[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  
[InternLM系列模型](https://www.linsight.cn/7f3d361.html)  
[GLM4报告的一些技术点](https://www.linsight.cn/a5206abd.html)  
[从Yuan2.0到Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  
[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  
- 数据：  
[训练数据合成(一)](https://www.linsight.cn/85132189.html)  
[训练数据合成(二)](https://www.linsight.cn/2a22baeb.html)  
[训练数据合成(三)](https://www.linsight.cn/e259c7b2.html)  
[LLM预训练数据策略(一)](https://www.linsight.cn/2c2cdc34.html)  
[预训练数据处理--长度分解](https://www.linsight.cn/210dbccd.html)  
- 长上下文：  
[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  
[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  
[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  
- 推理加速：  
[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  
[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  
- 对齐：  
[Llama3.1--post-training要点一览](https://www.linsight.cn/93328a2a.html)  
[模型平均 -- model soup](https://www.linsight.cn/bb8fcf21.html)  
[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  
[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  
[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  
[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  
- Transformer：  
[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  
[LLM的重复生成和ICL](https://www.linsight.cn/7381cae3.html)  
[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  
[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  
[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  
[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  
[RoPE的远距离衰减](https://www.linsight.cn/f0902f1a.html)  
- 项目应用：  
[一个模型支持智能助手系统](https://www.linsight.cn/9c593ccd.html)  
- CV：  
[CV入门--关于Vision Transformer](https://www.linsight.cn/a11e2633.html)  
[CV入门--无监督学习](https://www.linsight.cn/ae81a87b.html)  
- 多模态：  
[多模态入门(一)--CLIP](https://www.linsight.cn/3069051d.html)  
[多模态入门(二)--Flamingo,LLaVA系列和BLIP系列](https://www.linsight.cn/569d722c.html)  
[多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列](https://www.linsight.cn/f16505b3.html)  
[多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B](https://www.linsight.cn/e00debee.html)  
- 大模型算法题：  
[(1)](http://www.linsight.cn/3345028a.html)、
[(2)](http://www.linsight.cn/ad0bba9d.html)、
[(3)](http://www.linsight.cn/1736008.html)、
[(4)](http://www.linsight.cn/1736008.html)、
[(5)](http://www.linsight.cn/336f2f3e.html)、
[(6)](http://www.linsight.cn/7c04944d.html)、
[(7)](https://www.linsight.cn/dd614e12.html)、
[(8)](https://www.linsight.cn/e287b9c3.html)、
[(9)](https://www.linsight.cn/fb9c8882.html)  

# Reference  

【1】InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Models  
【2】InternVL2博客：https://internvl.github.io/blog/2024-07-02-InternVL-2.0/  
【3】Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance  
【4】Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling  
【5】Mini-InternVL2博客：https://internvl.github.io/blog/2024-10-21-Mini-InternVL-2.0/  
【6】InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition  
【7】InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output  