---
title: 大模型算法题(7)
tags:
  - NLP
  - LLM
  - 算法题
categories:
  - CS
  - NLP
  - LLM
abbrlink: dd614e12
date: 2024-06-12 22:13:46
---

【往期文章】  

[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  
[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  
[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  
[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  
[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  
[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  
[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  
[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  
[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  
[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  
[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  
[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  
[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  
[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  
[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  
[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  
[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  
[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  
[大模型算法题(3)](http://www.linsight.cn/1736008.html)  
[大模型算法题(4)](http://www.linsight.cn/1736008.html)  
[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  
[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  

***  

【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  

本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~  

如有错漏，欢迎指正~

***  

# 1.MoE模型训练中，如果不对专家的路由进行适当干预，可能会遇到什么问题，有什么解决方法？  

MoE使用多个并行的expert，每次推理只选择其中的一小部分expert使用。  

如果让模型完全自行学习，有可能出现routing collapse的问题，也就是模型倾向于总是选择那几个常用的专家。  

而这些常用的专家由于使用得更多，训练得更好，又会提升被路由到的概率，导致大部分模型参数几乎没有被用上。  

一般可以通过增加一个负载平衡的loss来缓解。负载平衡loss有不同的设计和计算方式，但是大致的思路都是迫使模型均匀地使用不同的专家，如果出现某些专家被选中的概率过高，就会进行惩罚。  

# 2.Bert的预训练方式是MLM，通过[Mask] token对部分输入进行掩盖，要求模型预测。为什么要使用[Mask] token而不直接修改attention mask矩阵？  

直接修改attention mask矩阵也可以让模型看不到对应位置的输入，但是相比使用[Mask] token缺少了位置编码的信息。  

另外使用[Mask] token掩盖要预测的值这种做法在实现上相对方便，只需要对输入数据进行处理即可，而不需要修改modeling的内容，更加通用。  


# 3.为什么大模型训练的时候需要warmup？  

在训练前期，刚随机初始化的模型参数离收敛值很远，此时的loss会比较大，梯度也会很大。如果直接使用固定的较大learning rate，模型容易过拟合到早期step见过的这些数据。  

依照ResNet的实验结果，如果一开始模型就跑偏了，那后面再怎么训练，收敛效果都会比较差，说明早期太大的学习率导致模型过早收敛到不太好的局部最优了。  

另外，模型刚开始训练的时候，大学习率带来的大更新值，会导致模型参数的震荡会很大，使得模型学到的参数很不稳定，这也不利于训练。  

使用少数step让模型进行热身，可以很大程度规避这些问题。  

# 4.Roberta在Bert的基础上做了什么优化？  

1. Bert在MLM训练中，提早把要mask的token处理好，在训练时训了多个epoch。这样在不同的epoch中，同一条sample总是使用相同的mask进行训练。Roberta使用了dynamic mask，即每条数据在训练前才随机决定进行mask的位置，这样不同的epoch之间同一条样本也有不同的mask结果，提升了数据多样性。  

2. Bert使用了MLM和NSP两个任务，而Roberta通过实验发现NSP的作用不大，因此直接取消了NSP任务。  

3. Roberta增大了batch size，提高训练效率，以获取更好的训练结果。  

4. Roberta增大了训练数据和训练step数，实验表明模型继续训练还能进一步收敛。  

5. Bert使用WordPiece分词，而Roberta使用BBPE，增大了词表。  

6. 再后来，Google发布了WWM全词mask，改进了mask方式。Roberta-WWM也成了最广泛使用的版本。  

# 5.LoRA的参数是怎么初始化的？  

LoRA包含一个降维矩阵A，和一个升维矩阵B。矩阵A用随机高斯分布初始化，而矩阵B用初始化为0。这样可以使得训练开始的时候，LoRA的参数不产生效果，模型能够保持增加LoRA前的输出。但是A、B矩阵不能同时为0，这样会有对称性问题，降低了模型的表达能力。  

{% asset_img lora.png lora %}  

***  

读到这了，来一发点赞收藏关注吧~

博客：[http://www.linsight.cn/](http://www.linsight.cn/)  
知乎：[Linsight](https://www.zhihu.com/people/us4ever)  
微信公众号：Linsight  
![](/images/qrcode.jpg)  

***  

【往期文章】  

[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  
[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  
[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  
[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  
[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  
[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  
[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  
[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  
[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  
[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  
[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  
[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  
[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  
[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  
[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  
[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  
[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  
[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  
[大模型算法题(3)](http://www.linsight.cn/1736008.html)  
[大模型算法题(4)](http://www.linsight.cn/1736008.html)  
[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  
[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  