---
title: 大模型算法题(4)
abbrlink: '1736008'
date: 2024-04-20 16:56:45
tags:
categories:
---

![](/images/cover.png)  

往期文章

[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)

[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)

[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)

[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  

[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)

[大模型算法题(1)](http://www.linsight.cn/3345028a.html)

***  

【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  

本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错漏，欢迎指正~

***  

# 1.为什么Transformer用layernorm而不是batchnorm  

首先，NLP数据中由于每条样本可能不一样长，会使用padding，如果对padding部分进行normalization，对效果有负面影响。直观来说，batchnorm会对同一个特征以batch为组进行归一化，而对于文本数据，同一个位置的token很可能是没有关联的两个token，对这样一组数据进行归一化没有什么实际意义。《PowerNorm: Rethinking Batch Normalization in Transformers》论文的实验也表明，在NLP数据使用batchnorm，均值和方差相对layernorm会更加震荡，因此效果欠佳。  

# 2.transformer中，encdoer和decoder是怎么进行交互的？  

decoder部分的输入，在每层中，先进行一次self-attention；之后用encoder的输出作为attention计算中的K、V，decoder的输入作为Q，进行cross-attention。  

{% asset_img transformer.png transformer %}  

# 3.PyTorch中，Tensor的view()和reshape()两个方法有什么区别？  

1.功能上：view()与reshape()方法都可以用来改变tensor的形状，但是使用条件不同，view()能做的是reshape的子集。  

2.view()方法需要tensor满足连续性，操作后返回一个引用，返回值是视图，没有改变原储存空间的值，多个视图共享同一个物理储存空间的内容。  

3.reshape()方法不需要tensor一定满足连续性。如果tensor不满足连续性的要求，则会使用新的储存空间并返回。如果满足连续性需求，则功能和view()一致。  

4.连续性：比如一个二维张量，如果按行优先展开成一维的结果，和物理储存顺序是一致的，就是连续的。可以用is_contiguous()来判断一个张量是否连续，如果不连续，可以用contiguous()得到一份新空间中的连续副本。  

# 4.RLHF中，PPO需要哪几个模型，分别是什么作用？  

一般来说，PPO需要使用4个模型。  

1.Actor模型：由SFT初始化，就是进行强化学习的主模型，是我们想要最终获得的模型；它不断产生action并被Critic模型所评价，计算loss进行训练。  

2.Reference模型：一般也是从SFT模型初始化，RLHF中Reference模型并不更新参数，只是作为Actor模型的参考使用；通过约束Actor模型和Reference模型的KL penalty等，可以防止Actor模型被训得跑得太偏。  

3.Reward模型：提前训练好的，对SFT模型进行打分的模型，RLHF中参数是冻结的。  

4.Critic模型：一般由Reward模型进行初始化，参数可训练，用于预测Actor模型生成的token的收益。  

# 5.GPT类模型训练过程中，消耗显存的主要有哪些部分？分别是多少？哪部分占用最多？假设模型有L层，词表大小为V，hidden size为H，batch size为B，训练窗口长度为S，使用Adam优化器混合精度训练（需要存一阶和二阶动量），注意力头数为N。  

训练过程中，显存消耗主要有模型参数、梯度、optimizer状态值和中间激活值。  

1.模型参数Φ：词表部分VH，每层参数12H^2+13H，总共有Φ=VH+L(12H^2+13H)，如果是半精度就是2Φ  

2.梯度：每个参数对应有一个梯度，总量为Φ，如果是半精度就是2Φ  

3.optimizer状态值：每个参数有一个对应梯度，每个参数又对应优化器一个一阶动量和二阶动量。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有(Φ+Φ)*2+(Φ+Φ+2Φ)*4=20Φ，除去参数和梯度，优化器占部分16Φ  

4.激活值：保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。总共是34BSH+5BNS^2，如果都是半精度，就乘以2。  

模型参数、梯度和优化器状态和输入长度无关，是固定值，而激活值随着长度增加，以平方速度增长。
以GPT3（175B）为例，H=12288，L=96，N=96。模型参数量显存越为350G。以B=1计算，如果S=1024，激活值约为90G；如果S=8192，激活值约为3420G。  

***  

读到这了，来一发点赞收藏关注吧~

博客：[http://www.linsight.cn/](http://www.linsight.cn/)  
知乎：[Linsight](https://www.zhihu.com/people/us4ever)  
微信公众号：Linsight  
![](/images/qrcode.jpg)  

***  

往期文章

[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)

[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)

[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)

[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  

[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)

[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  
