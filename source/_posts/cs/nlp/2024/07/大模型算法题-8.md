---
title: 大模型算法题(8)
tags:
  - NLP
  - LLM
  - 算法题
categories:
  - CS
  - NLP
  - LLM
abbrlink: e287b9c3
date: 2024-07-18 20:11:05
---

【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  

本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~  

如有错漏，欢迎指正~

***  

## 1.激活函数GeLU是怎么设计的，有什么优点？  

GeLU函数在2016年在论文《Gaussian Error Linear Units (GELUs)》中提出。  

之前比较常用的激活函数ReLU具有计算快，有激活值的时候不容易出现梯度消失等优点，但是ReLU是个分段函数，存在不可导的断点，可能会对模型效果有所影响。此外，ReLU是确定性的激活函数，一般会加入随机正则项（如dropout，随机将部分输出置0）以提高模型的泛化能力。  

而GeLU被设计为一种自带随机正则属性的激活函数，即输入是否置0，取决于当前的输入和其他输入的对比：  

GeLU(x) = x*P(X≤x)=x*Φ(x)  

其中Φ(x)是标准正态分布的累积分布函数。GeLU精确值的计算比较复杂，因此论文给出了近似解，图像如图。  

{% asset_img 1.png 图 %}  

GeLU相比ReLU更为光滑，在处理负数的时候也不会直接输出0，减少了梯度消失和神经元死亡的情况，实际使用中效果也较好。  

## 2.Gshard和Switch Transformer对MoE模型都提出expert capacity的概念，即每个expert在一个batch中最多能处理的token数。设定不同的expert capacity，对模型会有什么影响？  

expert capacity模型容量限定了一个batch中最多能处理的token数。如果一个token被分配到已经满载的expert，则这个token不会被处理，而是通过残差链接透传给下一层，这种情况称为overflow。overflow会对最终效果有损害，因为有些token没有被完整计算。  

比如一个batch里有N个token，每层有E个expert，每个token被分配到2个expert，假设我们设置了expert capacity = 2N/E。只有当所有token完美平衡分配才不会出现overflow。显然，expert capacity设置得越大，越能兼容分配不平衡的情况，overflow的情况越少；但是分配不平衡的情况会对模型的计算效率有损害，因为部分expert被频繁使用，而另外一部分可能使用很少，造成计算资源浪费。  

下图展示了不同expert capacity下的情况  

{% asset_img 2.png 图 %}  

# 3.重计算recomputation是什么，有什么用？  
通常来说，使用更大的模型或者更大的batch size可以获得更好的训练效果。但是更多的激活值也带来了显存不足的问题。正常来说，为了进行反向传播，我们需要保存模型中每个节点的激活值，比如使用一个4层的模型，每层的和输出大小都为8，那么一共就要保存32个激活值。如果设备的显存只够储存24个值，这时候就会出现OOM。一个解决方法就是，我们只保存第1和第3层的激活值，那么就只需要保存16个值。在计算第4层的反向传播时，先拿第3层的激活值重新计算第4层的前向结果，再进行正常反向传播；在计算第2层的的反向传播时，先拿第1层的激活值重新计算第2层的前向结果，以此类推。这里重新计算第4层、第2层的前向结果的做法，就是重计算recomputation。  

重计算可以节省显存，使得同样的设备可以训练更大的模型，但是由于需要多次计算前向结果，训练速度就会比较慢。因此重计算是用时间换空间的方法。  



# 4.MoE模型中，使用更小的expert有什么优势？  

更小的expert在同样总参数量和激活参数的情况下，可以提供更多的排列组合，使得每个expert在训练过程中能达到更高程度的专家化。比如原来每层有16个专家，每个专家大小为128，每次激活两个，那么组合情况总共有120种不同的组合；如果使用64个专家，每个专家大小为32，每次激活8个，那么就有4,426,165,368种不同的组合。  

另外，专家数量不够多的情况下，一个专家就可能要负责学习多个领域的内容。以学习高中知识为例，在只有两个专家的时候，只能一个专家学习理科知识，另一个学习文科知识；当我们有8个专家的时候，不同专家就可以分别学习语文、英语、历史、地理、物理、生物、化学、数学知识。显然后者所学知识的专业化程度更高。  

# 5.MoE模型中，使用的expert数量多少对模型有什么影响？  

（1）专家的数量越多，同样的输入，给每个专家分配的平均token越少。这在训练时可能导致batch size过小，影响训练效果。  

（2）专家的数量越多，训练和推理时所需的设备越多，多个设备进行通讯的成本会变高。  

（3）专家的数量如果比较少，提供的组合数就比较少；专家数量多的话可以提供更多可能的组合。  

（4）专家的数量多，模型的总参数量更大，理论上模型能具有更大的容量，上限更高。  

***  

读到这了，来一发点赞收藏关注吧~

博客：[http://www.linsight.cn/](http://www.linsight.cn/)  
知乎：[Linsight](https://www.zhihu.com/people/us4ever)  
微信公众号：Linsight  
![](/images/qrcode.jpg)  

***  

【往期文章】  
[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  
[DeepSeek-V2和MLA](https://www.linsight.cn/83c49df0.html)  
[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  
[成本10w刀的JetMoE](https://www.linsight.cn/f3acf042.html)  
[MoE的top-p routing](https://www.linsight.cn/224c42da.html)  
[对MoE模型的一些观察](https://www.linsight.cn/5e1d14b3.html)  
[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  
[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  
[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  
[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  
[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  
[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  
[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  
[LLM的重复生成和ICL](https://www.linsight.cn/7381cae3.html)  
[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  
[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  
[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  
[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  
[Qwen2技术报告](https://www.linsight.cn/a8f8b641.html)  
[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  
[MiniCPM](https://www.linsight.cn/376db710.html)  
[GLM4报告的一些技术点](https://www.linsight.cn/a5206abd.html)  
[Gemma2](https://www.linsight.cn/cf3f1f81.html)  
[苹果的OpenELM](https://www.linsight.cn/f845f3e4.html)  
[从Yuan2.0到Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  
[bilibili的index-1.9B](https://www.linsight.cn/770b63e1.html)  
[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  
[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  
[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  
[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  
[RoPE的远距离衰减](https://www.linsight.cn/f0902f1a.html)  
[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  
[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  
[大模型算法题(3)](http://www.linsight.cn/1736008.html)  
[大模型算法题(4)](http://www.linsight.cn/1736008.html)  
[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  
[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  
[大模型算法题(7)](https://www.linsight.cn/dd614e12.html)  