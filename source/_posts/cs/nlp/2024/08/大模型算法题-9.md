---
title: 大模型算法题(9)
tags:
  - NLP
  - LLM
  - 算法题
categories:
  - CS
  - NLP
  - LLM
abbrlink: fb9c8882
date: 2024-08-06 21:28:26
---

【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  

本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~  

如有错漏，欢迎指正~

***  

# 1.大模型训练过程中有什么可以缓解显存不足的办法？  

（1）模型结构：使用LoRA、adaptor等训练  

（2）注意力计算底层优化：flash attention、paged attention、ring attention等  

（3）训练框架：使用混合精度训练，使用ZeRO、recomputation、cpu-offload等  

（4）训练策略：梯度累积  

# 2.为什么MoE模型的训练相比dense模型，更容易受到精度不足的影响？  

模型训练中经常使用的float32/float16/bfloat16等，都存在舍入误差，并且表达的数值越大，舍入误差越大。而MoE模型中，gating function大量使用了exponential计算，而exponential会把输入数据中的误差放大很多倍，从而使得输出结果大大偏离。因此有些工作会限制在gating function使用更高精度的表达，并通过一些前置手段压缩数值的大小，从而缓解误差在多个MoE层累计的情况。  

# 3.模型蒸馏中，为什么要使用温度T？  

经典的模型蒸馏过程中，student model会学习teacher model的softmax输出，在teacher model的输出进入softmax前，会除以一个温度T>1。  

经过较好训练的teacher model一般会以较高的置信度给出最终结果，而我们使用蒸馏而不从groud truth（即one-hot label）直接学习的原因，是因为除了置信度最高的结果之外，其他类别的分值也能够提供有价值的信息。比如在水果图片三分类任务，共有三个类别 [菠萝，榴莲，甘蔗]，teacher model 对一张图片给出 [0.7, 0.2, 0.1] 的结果，除了正确答案0.7以外，0.2和0.1这两个分支提供了一个信息：榴莲和菠萝的相似程度大于甘蔗和菠萝，这是one-hot label提供不了的。  

而除以温度T，能够让teacher model给出的label变得更加soft，信息熵更大，从而可以让student model更好地学习。  

# 4.大模型有什么加速推理的方法？  

1.针对训练好的模型：模型蒸馏、模型量化、模型剪枝等  

2.模型结构设计：MoE  

3.训练策略：early-exit  

4.解码策略：投机解码  

# 5.为什么LLAMA1/2的intermiedia size/hidden size相比Bert的减小了，从4下降到8/3？  

LLAMA使用SwiGLU FFN，相比Bert的FFN，多了一个线性变化矩阵。为了保持模型的参数量不变，因此把intermediate size从4倍hidden size降为8/3倍。  

***  

博客：[http://www.linsight.cn/](http://www.linsight.cn/)  
知乎：[Linsight](https://www.zhihu.com/people/us4ever)  
微信公众号：Linsight  
![](/images/qrcode.jpg)  

***  

【推荐文章】  
- MoE：  
[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  
[DeepSeek-V2和MLA](https://www.linsight.cn/83c49df0.html)  
[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  
[成本10w刀的JetMoE](https://www.linsight.cn/f3acf042.html)  
[MoE的top-p routing](https://www.linsight.cn/224c42da.html)  
[对MoE模型的一些观察](https://www.linsight.cn/5e1d14b3.html)  
[从dense到MoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  
[MoE路由--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  
- 端侧模型：  
[苹果智能系统模型--AFM](https://www.linsight.cn/1e34e252.html)  
[适合移动设备的语言模型--MobileLLM](https://www.linsight.cn/5ac36d34.html)  
- 预训练：  
[Llama3.1--预训练要点一览](https://www.linsight.cn/7d7294cb.html)  
[Qwen2技术报告](https://www.linsight.cn/a8f8b641.html)  
[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  
[MiniCPM](https://www.linsight.cn/376db710.html)  
[GLM4报告的一些技术点](https://www.linsight.cn/a5206abd.html)  
[Gemma2](https://www.linsight.cn/cf3f1f81.html)  
[苹果的OpenELM](https://www.linsight.cn/f845f3e4.html)  
[从Yuan2.0到Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  
[bilibili的index-1.9B](https://www.linsight.cn/770b63e1.html)  
[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  
- 数据：  
[预训练数据处理--长度分解](https://www.linsight.cn/210dbccd.html)  
- 长上下文：  
[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  
[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  
[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  
- 推理加速：  
[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  
[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  
- 对齐：  
[Llama3.1--post-training要点一览](https://www.linsight.cn/93328a2a.html)  
[模型平均 -- model soup](https://www.linsight.cn/bb8fcf21.html)  
[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  
[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  
[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  
[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  
- Transformer：  
[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  
[LLM的重复生成和ICL](https://www.linsight.cn/7381cae3.html)  
[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  
[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  
[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  
[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  
[RoPE的远距离衰减](https://www.linsight.cn/f0902f1a.html)  
- 大模型算法题：  
[(1)](http://www.linsight.cn/3345028a.html)、
[(2)](http://www.linsight.cn/ad0bba9d.html)、
[(3)](http://www.linsight.cn/1736008.html)、
[(4)](http://www.linsight.cn/1736008.html)、
[(5)](http://www.linsight.cn/336f2f3e.html)、
[(6)](http://www.linsight.cn/7c04944d.html)、
[(7)](https://www.linsight.cn/dd614e12.html)、
[(8)](https://www.linsight.cn/e287b9c3.html)  
