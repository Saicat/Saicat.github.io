---
title: 大模型算法题(2)
tags:
  - NLP
  - LLM
  - 算法题
categories:
  - CS
  - NLP
  - LLM
abbrlink: ad0bba9d
date: 2024-03-24 11:24:47
---

![](/images/cover.png)  

往期文章

[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)

[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)

[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)

[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  

[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)

[大模型算法题(1)](http://www.linsight.cn/3345028a.html)

***  

【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  

本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错误，欢迎指正~

***  

# 1、在Bert中，词向量token embedding和(绝对)位置编码position encoding为什么可以直接相加？  

1、两个向量相加，理论上其效果等价于维度拼接concat+线性变换，而相加的操作在实现上更为方便。  

2、高维空间中(如768维)，两个随机向量近似为正交关系。模型在高维度有能力区分出所有组合的情况。假设共有2万个词向量，500个位置，则模型需要在768维空间区分1000万个点，即使768维每个维度只能取1和-1也具备足够的区分能力。  

3、词向量和位置编码可以认为都是一个one-hot向量经过一层线性变换层得到的。两个向量相加等价于把它们的one-hot编码拼接后进行线性变换。  

4、没有使用相乘则是出于工程考虑。相加相比相乘结果更为稳定，方便训练。  

# 2、LoRA和全参数训练在计算量和显存上相比如何？为什么LoRA能提升大模型训练效率？  

1、计算量上：LoRA训练时，在主干模型的（部分）全连接层增加了LoRA旁路，前向和后向的计算量都在主干模型的基础上，增加了旁路部分的计算，因此相比全参数训练，略有增加。  

2、显存上：训练时，显存主要有①模型参数②梯度③中间激活值④优化器参数四个部分。模型参数/梯度/激活值相比全参数训练也略微增加；而优化器则不需要再存储原模型参数的部分，只需要存储LoRA旁路部分，这部分节省较多显存。  

3、使用LoRA能提升训练效率主要是因为（1）优化器部分的显存需要减少了，可以增大batch（2）优化器参数减少了，分布式训练中多卡之间的通信量减少了（3）（optional）主干模型由于不用更新，可以进一步量化到int8/int4等。  

# 3、为什么模型需要normalization（batchnorm/layernorm等）？  

1、输入数据包含多个特征，特征之间有不同的量纲和范围（如身高180和年龄18岁），通过normalization进行归一化再经过模型进行线性/非线性组合，能够防止部分特征占据主导，部分特征被忽略。  

2、batchnorm论文认为：模型一般有多层，前一层的输出是后一层的输入，而训练中前一层的参数更新会导致后一层的输入数据分布变化导致ICS（internal covariate shift），这样后面的层就不得不频繁剧烈更新适应分布变化，导致分布偏移进入激活函数饱和区而出现梯度消失，另外分布变化也是对i.i.d.条件的破坏。使用normalization可以保持分布的稳定，减小方差，使模型训练可以正常进行。  

3.《How Does Batch Normalization Help Optimization?》设计了实验测量使用batchnorm前后的ICS，发现batchnorm实际上并没有缓解ICS，甚至有所增加。而batchnorm能优化模型训练的原因更多是使得损失函数平面更加光滑，而便于梯度下降收敛。  

# 4、Transformer中pre-norm和post-norm各有什么优缺点?  

1.原始的Transformer用的是post-norm，它在残差之后进行归一化（add & norm），对参数正则化的效果更强，模型更为鲁棒；post-norm对每个通路都进行了归一化，使得梯度在回传的时候容易出现消失。  

2.Pre-norm相对于post-norm，残差部分存在不经过归一化的通路，因此能够缓解梯度消失，能够训练更深的网络。但是模型的等效“深度”受到影响，L+1层网络近似于一个L层的宽网络。  

3.也就是说，在层数较少，post-norm和pre-norm都能正常收敛的情况下，post-norm的效果更好一些；但是pre-norm更适合用于训练更深的网络。  

# 5、对于使用Multi-Head Attention的模型，假设hidden size=D，注意力头数量为h，每个头维度为d（假设有D=d×h），输入上下文长度为s，batch size=1，计算self-attention模块各个部分的计算量（Float Operations）。  

1.QKV线性变换：6 × s × D^2（矩阵乘法，每个位置有加法和乘法两个运算，因此每个位置需要2D次计算）  

2.QK内积：h × 2 × d × s^2（h组矩阵分别计算）  

3.scaling：h × s^2  

4.softmax：h × 3 × s^2（softmax是按列进行的，每列要计算s个exp，s个exp结果的求和，以及s次除法）  

5.reduction（权重矩阵乘以V）：h × 2 × d × s^2  

***  

读到这了，来一发点赞收藏关注吧~

博客：[http://www.linsight.cn/](http://www.linsight.cn/)  
知乎：[Linsight](https://www.zhihu.com/people/us4ever)  
微信公众号：Linsight  
![](/images/qrcode.jpg)  

***  

往期文章

[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)

[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)

[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)

[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  

[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)

[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  
