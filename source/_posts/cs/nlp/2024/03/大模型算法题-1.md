---
title: 大模型算法题(1)
tags:
  - NLP
  - LLM
  - 算法题
categories:
  - CS
  - NLP
  - LLM
abbrlink: 3345028a
date: 2024-03-17 10:46:09
---

![](/images/cover.png)  

往期回顾

[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  

[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  

[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  

[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  

***

【本文已在同名微信公众号/知乎/个人博客同步上线】  

本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错误，欢迎指正~

# 1、在Transformer模型中，为什么scaled dot-product attention在计算QK内积之后要除以根号d？  

简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。如果不对attention值进行scaling，也可以通过在参数初始化时将方差除以根号d ，同样可以起到预防softmax饱和的效果。

# 2、Transformer自注意力计算中，为什么Q和K要使用不同的权重矩阵进行线性变换投影，为什么不使用同一个变换矩阵，或者不进行变换？  

1、如果Q和K一样，则矩阵乘积的结果是一个对称矩阵，这样减弱了模型的表达能力。  

2、如果Q和K一样，乘积结果的对称矩阵中，对角线的值会比较大，导致每个位置过分关注自己。  

3、使用不同的投影矩阵，参数增多，可以增强模型表达能力。  

# 3、Transformer模型中，注意力计算后面使用了两个FFN层，为什么第一个FFN层先把维度提升，第二个FFN层再把维度降回原大小？  

1、提升维度：类似SVM kernel，通过提升维度可以识别一些在低维无法识别的特征。  

2、提升维度：更大的可训练参数，提升模型的容量。  

3、降回原维度：方便多层注意力层和残差模块进行拼接，而无需进行额外的处理。  

# 4、MQA(Multi-Query Attention)和GQA(Grouped-Query Attention)相比MHA(Multi-Head Attention)，计算量变化如何，主要带来了什么优化？  

1、MQA和GQA虽然可训练参数量比MHA少，但是计算量和MHA相比变化不大，主要在生成KV时有少量降低。  

2、Decoder-only的大模型由于causal attention的存在，使用了KV缓存加速推理。MQA和GQA能减少KV头的数量，节省了缓存，使得在输入长度较长时也能把KV放进缓存。  

# 5、为什么现在主流的LLM模型基本都是Decoder-only的结构？单向注意力模型为什么效果比双向注意力效果好？  

1、双向Attention在多层模型训练中容易退化成低秩矩阵，限制了模型容量；而Decoder-only模型使用了下三角注意力矩阵，使得训练过程中矩阵是满秩，建模能力更强。  

2、单向注意力模型相比双向注意力模型在训练的时候难度更大，能迫使模型学到更多信息。  

3、Causal Attention天然具有位置编码的功能，而双向Attention即使交换两个token的位置也基本不影响表示，对语序区分能力较弱。  

4、工程上，单向模型支持KV Cache等，对于对话场景效率友好。  

5、轨迹依赖，基模型训练成本高，业界倾向于沿着已经成功的模型继续开发。  

***  

读到这了，来一发点赞收藏关注吧~

博客：[http://www.linsight.cn/](http://www.linsight.cn/)  
知乎：[Linsight](https://www.zhihu.com/people/us4ever)  
微信公众号：Linsight  
![](/images/qrcode.jpg)  

***

往期回顾

[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  

[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  

[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  

[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  