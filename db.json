{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","path":"js/comments.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","path":"js/pjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/config.js","path":"js/config.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","path":"js/schedule.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","path":"css/noscript.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","path":"js/third-party/addtoany.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","path":"js/third-party/tags/wavedrom.js","modified":1,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/images/qrcode.jpg","path":"images/qrcode.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","path":"images/avatar/20180303210737_XsJVr.jpeg","modified":1,"renderable":0},{"_id":"source/images/avatar/Picasso_Elephant.png","path":"images/avatar/Picasso_Elephant.png","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","path":"images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","path":"images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","path":"images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar/shadow.png","path":"images/avatar/shadow.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-2ywymm.png","path":"images/background/wallhaven-2ywymm.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-gpxpg3.png","path":"images/background/wallhaven-gpxpg3.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-x636oz.png","path":"images/background/wallhaven-x636oz.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-p97q73.png","path":"images/background/wallhaven-p97q73.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/about.txt","path":"images/favicon/favicon_io/about.txt","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","path":"images/favicon/favicon_io/android-chrome-192x192.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","path":"images/favicon/favicon_io/android-chrome-512x512.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","path":"images/favicon/favicon_io/apple-touch-icon.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","path":"images/favicon/favicon_io/favicon-16x16.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","path":"images/favicon/favicon_io/favicon-32x32.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon.ico","path":"images/favicon/favicon_io/favicon.ico","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/site.webmanifest","path":"images/favicon/favicon_io/site.webmanifest","modified":1,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"b178e7a22149338022f2f4b4b8bf6a6a817a2e8c","modified":1709016856180},{"_id":"source/_data/styles.styl","hash":"f4bb55ef0972c829e3382d1bae1786b3ab5d54ef","modified":1707045638288},{"_id":"source/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1706872557451},{"_id":"source/categories/index.md","hash":"f5c920fbc09ea3d8edf250de7e31bcc6b3e765ae","modified":1706698717077},{"_id":"source/_posts/.DS_Store","hash":"b2c6a8666301f8bbaa0654dee1208496dfae29cb","modified":1708958488163},{"_id":"source/about/index.md","hash":"9294d008cc673abc2eaf740f101ebac560029267","modified":1706698701349},{"_id":"source/tags/index.md","hash":"e995ed2b8452b1906600b3853b920f13423098b7","modified":1706698644396},{"_id":"source/images/.DS_Store","hash":"60fbc9bc8c2d88510803a394d229a0442cae1cb2","modified":1709016856181},{"_id":"source/_posts/cs/nlp/.DS_Store","hash":"8777c3becdebb5e87f2ceefe04bb188cebc2465b","modified":1708756131402},{"_id":"source/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1707548301740},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1707030615190},{"_id":"source/images/avatar/.DS_Store","hash":"c3fa37607ceb3f7ba411cf4203d2a333f773d921","modified":1707118756919},{"_id":"source/_posts/cs/.DS_Store","hash":"398e9f875fb2374f4869b59af671deb79d4d9859","modified":1708756131404},{"_id":"source/images/background/.DS_Store","hash":"88f5d31d0db89adcf679f2a7fefc8947139a1c1f","modified":1709026807046},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1707048498957},{"_id":"source/images/favicon/.DS_Store","hash":"83ddccadffca5384db3dfc167728b7c7cacd9a87","modified":1707796842439},{"_id":"source/_posts/cs/nlp/2024/.DS_Store","hash":"729ceb96338faa524b921bc3383fc6f180a2c3c0","modified":1708756131404},{"_id":"source/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题.md","hash":"3beb71a267023be73b968196758eb78253360c1d","modified":1709366427734},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/02/.DS_Store","hash":"f704a4f0ddae8889a3964762e63d2ee1a965c331","modified":1709197037670},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE.md","hash":"1dae1211b15ab91c1b9f991209aa0a16fc6dbceb","modified":1709015970683},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1709188879237},{"_id":"node_modules/hexo-theme-next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1706697684353},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/.DS_Store","hash":"2e33b8d145af72ec31cbad8c19fc2528fc2a909f","modified":1709014663934},{"_id":"node_modules/hexo-theme-next/package.json","hash":"4b48877b223ec717e708540a2df03d64983c02ab","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/.DS_Store","hash":"bef639ede199b5b3aea85285a9d03c82257c52aa","modified":1709026831096},{"_id":"node_modules/hexo-theme-next/source/.DS_Store","hash":"20f7a9b9a682cc55305492b2e240489f6bf832e6","modified":1709026838425},{"_id":"node_modules/hexo-theme-next/_vendors.yml","hash":"4f6046ceb1470be9ff334ede20b73871c951d845","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/_config.yml","hash":"255c963c680da5da34c259c560dd8211b75188ca","modified":1708604632809},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1708758408339},{"_id":"node_modules/hexo-theme-next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/languages/ar.yml","hash":"7d0f39e8684284a04bb9808521c87fecda8bd131","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/languages/de.yml","hash":"79b37df731c29665dee6cd7c90d278e1edfb6e24","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/languages/bn.yml","hash":"564bed75da6e05b11dce6164508f97a15e2fb6c2","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/en.yml","hash":"ba0fd79a2b1d8db01a034180556061745965ff05","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/eo.yml","hash":"e34bb33ae827bf2f0727088599a73bc64bdad1b0","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/README.md","hash":"d6820f46d03a93bd6dc8b10f49f58aec82ad2b06","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/languages/it.yml","hash":"16d716ecfd748def2f6486ef5a82d0ab7ceb4890","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fa.yml","hash":"f3ffc444599f4ac92d62e9ed00a1490ebc277d70","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/es.yml","hash":"dffc63ef42e1266b88e0acf08994fd17a9908d53","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/id.yml","hash":"929df147f4f17d638b07de5fe52ca13e2549ab1c","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fr.yml","hash":"8ac44e58f71a38b7697a2f7f98a6971ed818cb5b","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ko.yml","hash":"d345a303310c8a5f4836c3683f3580f861ebd1b4","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/nl.yml","hash":"3cb3687696635ec71b4ca40c5fc43b56acc8843e","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ja.yml","hash":"543222bfc516aab6c33e8534f807972ecb8943a9","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/pt-BR.yml","hash":"76b8576ce228d540a16b1f0af5af2cce20923194","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/si.yml","hash":"2d712eedf3f60d04d36c3108cf5a12e2a52e875c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/ru.yml","hash":"c6d8de0ff7d8148d09993257cfd3b7aca755696c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/tk.yml","hash":"511726054873f6f8d7ce0d2e803f6731de0ddbe7","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/vi.yml","hash":"7ebcba5e1128784195e4681dffc9d34c4e873fec","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/th.yml","hash":"6829e998b39f8f143e20b276bb1f62d95a29de58","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/zh-HK.yml","hash":"88ea50eeb9097ab4a87a44981a102d8594feb064","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/tr.yml","hash":"a57e4ed089b893a95f5e1ecff17ce625165f4d46","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/zh-CN.yml","hash":"741d7efe0262c9cdc2c648014b55599665d90f6b","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/pt.yml","hash":"70de366e10ea584ba039d40d6b35ac97f93454ad","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_layout.njk","hash":"fc0a45112f2dcfc2642404e8934ea32a793c3bd7","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/.DS_Store","hash":"91040d8017183ac4f7319d2d695edb07ec1b09c8","modified":1707031368282},{"_id":"node_modules/hexo-theme-next/languages/uk.yml","hash":"ff537047b4b4c3ca9a7b64fa7f428a9942751eeb","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/layout/page.njk","hash":"b0660b2af0ac7d3fda14ca4d9f2c9e79ef06c6f9","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/scripts/events/index.js","hash":"bd9ea82376cd87df611ea3ae077875c7c595a3df","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/languages/zh-TW.yml","hash":"4695c87d6b81b3a23d16ad6513d9eaa925f8d8ad","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/layout/post.njk","hash":"0bfce9f133f501a9a4837257e3b862b3bbca15be","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/scripts/filters/post.js","hash":"fdc8a0af90035e89c3fcb754a0eb189b8951a2bc","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/helpers/engine.js","hash":"d292b78485e8e8055712b0ed6de7cf559c5fbdcd","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/scripts/filters/locals.js","hash":"9eb5310664759931287dd28ea39165dfb67f12ed","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/scripts/helpers/navigation.js","hash":"78107021101553c3d23e89290f7530b60cf4aa86","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/helpers/font.js","hash":"3394185a7f0393c16ce52c8028f90da3e9239c55","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/scripts/filters/minify.js","hash":"447db39d17775b2bd18d8af9c9d65b7b8449f751","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-url.js","hash":"6281d47c1de98eb38f3aa0f6df29bbb19d412173","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-paginator.js","hash":"e86c764b546e4fbb87970cabc4135a56f9ef9fe1","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-config.js","hash":"ead37e9167b682f1fa34b5401c3050e18c7ee4a3","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1706697684337},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-vendors.js","hash":"957241c28796ff352de7f4cffba7bb289b043586","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/index.js","hash":"1f6aba7820f1fb58b61969485148db21846e1aa9","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/tags/group-pictures.js","hash":"9ed799c329abf830f623689d7e136991256a24ca","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/tags/mermaid.js","hash":"4fb01ca650fa8b256b8d48f50dc1b18350bd3d6d","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/wavedrom.js","hash":"b44dfeeb58b41945d469141787f3dbce4b117d08","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1706697684330},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1706697684350},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","hash":"921a58577f411cf4eb5cfd66db0a241f8f88578c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_colors.styl","hash":"3c6798c10cc220d83481cb3f3782e78558cee789","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_mixins.styl","hash":"83647a6207333b9609ba90b0946b3fa9548e6381","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"12a6631617695504d5cf2a94b57d87bd331bef6f","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","hash":"dadc81256afb127b77eac6763d5ee0ec9c77f0a3","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/README.md","hash":"12a3e96581964a22b474cc739675d52ef93ff932","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/layout/_third-party/index.njk","hash":"dfd7cdd6ba89f8c3deabc27726c7a350cadafd11","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/addtoany.njk","hash":"ef64c6bfb8540cd874701236b9be47db2496e98e","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_partials/footer.njk","hash":"d77ec95cfee58b17807763dc2adb7946829cb316","modified":1706757600094},{"_id":"node_modules/hexo-theme-next/docs/ru/README.md","hash":"29c89a41b371f893e56c87ea61adabc444ec58cc","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/layout/_partials/.DS_Store","hash":"88e4de27e19e826f7296d295124581534c0c2c8b","modified":1707031373963},{"_id":"node_modules/hexo-theme-next/layout/_partials/comments.njk","hash":"d0c470b0f6690aa217e9ada848c5e2e73fb27c6f","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_partials/pagination.njk","hash":"bc719473ed5948ab6859449d60b8d36cfc1542b4","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_scripts/vendors.njk","hash":"be80b9fe415a9a09d74c28e230995fd292dfc123","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_macro/post.njk","hash":"cbe208445e4d1df82ebd1761e1eaced3eab77fb3","modified":1706698899947},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/config.js","hash":"9ec51eb61f7fee612ffc5252f489003a0fa301fc","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/highlight.js","hash":"6aec7b2c38c50989a23bfaa0d560e75c7f553e12","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/utils.js","hash":"6853e5433e3eaa19ea43fa20b08d956ba4cec4ac","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/layout/_macro/post-collapse.njk","hash":"abda600685ee972e1f6b7a2dcc56f13e2daa6263","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_macro/sidebar.njk","hash":"547c62ab14d9e05d2d9116db9048a677fbe1fb6d","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/navigation.js","hash":"dd3562686d95a50375e6fd32e717ccb0d99c1e3d","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/vendors.js","hash":"464db1e7182e5b9cdbd32e8b5368d5e683b1d9c7","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/common.js","hash":"19a402a225c31edffc50f202a14e0d582d3db23e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/changyan.js","hash":"5798cfc8f63665031dd3e01debed051628cec319","modified":1706697684338},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqusjs.js","hash":"a600a98e7436edeb31e291abca359885567df3c9","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/layout/_scripts/index.njk","hash":"6668878a0f9a1166c6a879755f54a08d942da870","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/layout/_partials/widgets.njk","hash":"e7f988ecddb2159313699a00827a45eca5622bd4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Mist.styl","hash":"a1418c9dc8c0f1a0ad4ded0f4627c45bf0db1a10","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1706697684332},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Muse.styl","hash":"e3be898f5ebcf435a26542653a9297ff2c71aeb0","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/.DS_Store","hash":"71c6bca6ae43dd79b3d75183550713e9ad0f9f8e","modified":1709197037676},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Pisces.styl","hash":"48f4f277946a168d0db1ea02804e85c22ca2c7db","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/source/css/_variables/base.styl","hash":"c4fc4e862d09221265ab1466085f057be2ad2e4d","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/cloudflare.njk","hash":"a5b8297c2c383124dd6a56e256ecc0c0dcf489be","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/index.njk","hash":"f900306497b133e8b098bd9f4b96b93d1d96c185","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"9dc00fcb0a05899f048eace9f9160b78956655d5","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/source/css/.DS_Store","hash":"6b12ac4edf32b2194ccd6b95c3f5930b07c7d56b","modified":1709026838423},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/matomo.njk","hash":"4e89648a8ec8194c5823064cbca39c938a799006","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/plausible.njk","hash":"ef9f2bb7110507f1c4336800af9157d5fa9765bd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/umami.njk","hash":"3343750682fbd8535e50f8129be3003ad26015b4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/algolia-search.njk","hash":"24ed76e0c72a25ac152820c750a05826a706b6f4","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/katex.njk","hash":"1ebf658690468ea197bdd0416eb7cfa4bd0b083a","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"a4bc501da0f22f7e420f0ca47e83988ce90b1368","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/wavedrom.njk","hash":"02202bf563fb5eedde2ccad4d6c5b9109d30a703","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head-unique.njk","hash":"8da52a144060db1a0a088ccb2e6cc8376d1fce70","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/brand.njk","hash":"dd9c4c03e99dfde0dfb8edefcb2c933f2f560efc","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head.njk","hash":"5388b157bba4a40b9312f4a45c6678974ccf0837","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu-item.njk","hash":"41a8b0cc16f60fa085cb719d07216d86b6bc4bf8","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu.njk","hash":"ee6fc2f111572d3eeab0a2fecbb2d6b3e37ab26b","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/sub-menu.njk","hash":"06480d8ec5f0b87eafd47f082f07968d7282dd5c","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/breadcrumb.njk","hash":"89825e75cc45e9709fa6ba89883669eedaff6f46","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/schedule.njk","hash":"0f4bc8e257da60f77c0c1738607b2bde55810684","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-copyright.njk","hash":"bfff923526d6800218f08dba6ce0bbf5c17755fd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-related.njk","hash":"e0986db00a0201dd3c60570f964829c84ba5bc68","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-reward.njk","hash":"e8b8a7c41e9ec612d0c0c73419529d55d1c16256","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-followme.njk","hash":"c1e33b4889f75acc490af3c8bde0ec56c518ff41","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-share.njk","hash":"16696990e4ce65fc8db18c4635082a5d5d06ff07","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/index.njk","hash":"8f6f256ab3b351ffc80f1f3f1d9834e9a7cfac31","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1706697684335},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1706697684333},{"_id":"node_modules/hexo-theme-next/layout/_partials/sidebar/site-overview.njk","hash":"78a1a8cac44de7e963ab4cd51c988442eb3e789a","modified":1707031409664},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_layout.styl","hash":"fa4fd8f76464e214fb7318f325b13c2b62f4b478","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_header.styl","hash":"dafc6d23c80d6fe3e55a7711e94210d2479b629a","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_posts-expand.styl","hash":"485d23ccb42c0d0c8ead7ea8930dd3e06d79a285","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_header.styl","hash":"3fbfab591f280e2e7f3b0265901c93bc4bd137ed","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_menu.styl","hash":"fb550935d374e0bdf1097fce187337dc05cad3e1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_menu.styl","hash":"82cda756f5b7092df2eee6641b9786df71623bdb","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_layout.styl","hash":"6569a6640f79d247a8235b3914772c0e2f99ead2","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sidebar.styl","hash":"547c0b5cd5e7ea10d21863d13a6b16579a49396c","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_header.styl","hash":"ac2dc0ce9c775a83ef7132ae957b54539366ac9c","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_menu.styl","hash":"72dc825c50357402c342d62ab60fc0c478ab6bc1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sidebar.styl","hash":"91dbf3ca5c3a613d4e30618c120da535bf2d0336","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_layout.styl","hash":"26a0cba1eee5de45a45a5e14e17707f905390512","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/pagination.styl","hash":"f4228c759db4a650c8d38745c2edd1dc83c45687","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/index.styl","hash":"2298e521253b3bf376a2412271bc2a7d305051f3","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Gemini/index.styl","hash":"9dfe853c901bdc52fc950bacdf15484dbb9bf140","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/mobile.styl","hash":"1dbf2c339adcd27026c3a2ded32ee91ce08cea26","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/toggles.styl","hash":"782ee1fc5e669d3ddbfeb82b73ad7fe561f1a4fb","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top.styl","hash":"7664491542046df9a3887cf40a06e00c0b4086a9","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f634f94828620e88c3f5a8db56f7944f6ba232b0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/fold.styl","hash":"42a0b65491ad85438596b3fe0b7f23973e4cef34","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/index.styl","hash":"22cd37bd5df9972d5074710896aba4424ad5161c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/index.styl","hash":"138f78147bc6bd6005f329ada34dc79b7625542d","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7f8a7345e6537a62cd9e9a94c8f7065b541d9b04","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/wavedrom.styl","hash":"af113411ad9cca7674177be36af8dd399680834d","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/tabs.styl","hash":"33dd6ad015dde65fd46f34961655442e8e82b52e","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/note.styl","hash":"98d4c20aff0f0fcfe1824017fb06ab21ef0d218e","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/categories.styl","hash":"b6e2eb1550a7845cb2adf86081a4ab6c7bde1e68","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/index.styl","hash":"54d12e2c5d9982f7b9e5b23be5133954a8514e9d","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/disqusjs.styl","hash":"877a537d5b95beb048142e4fdee6f17e6ef9c7bb","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/base.styl","hash":"d0a7c99095f490b0d2ed6b1be43d435960798cec","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/search.styl","hash":"e72799ce3f9b79753e365b2f8c8ef6c310668d4a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/index.styl","hash":"6e0d0796ef7fbbb62ffdfb448753a850de82c74f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/menu.styl","hash":"bbbc40b03cb299d2a6a568f329b2ce98e1cdc430","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/index.styl","hash":"098d4bd034e986fcf7e443eac4fc2193935461b7","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/footer/index.styl","hash":"4e967702cf4c637132346bc74ec8854426f1a68c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-body.styl","hash":"56d5b7ff73f466c9ae54f7204ae899281295d749","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-meta.styl","hash":"a851e9d5aefcd027c95eeb323860b6da70f202d1","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-collapse.styl","hash":"7369928305330c73ae0b3f063a681a8384d8fde4","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-footer.styl","hash":"11497388f124bfbb4001495a67d3629a9f618405","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-followme.styl","hash":"1ecfd64507954810b07a9d21fb5305b5378feda0","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-header.styl","hash":"1191f1bfa5c43e54be8e5b3cc0d802984e161747","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-reward.styl","hash":"04cf4a69537fc14d3b8904f965d283356853847f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/index.styl","hash":"da5e88f8debd5ac8d7af5c6ba6240df66104955f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-widgets.styl","hash":"ebfba158a0a4af3d1dabcacbc58986664de52140","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b05908f04ef95f2d91e6eba89b12411c378d050f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"0847400d8579b0a2dd1bf662c78954c10adf2680","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"46eece42510c2c89bb9209afb0262ad76a4b0b36","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"c6a27beb3f741211a14576026f3b4cfc44cc6407","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"c2e354a565c8c1b32bd0ceacc972b17982758b67","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitalk.styl","hash":"8f094c4ac17e2ab45569b12d157747f9c7333c12","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"24752d145c6fb8f5344dca9c7b9640839c02e009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"9a7c71560fbdc936ad4e736fe15063ea3e8a644b","modified":1706697684374},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1707048511782},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1707048415396},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1709199016415},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1708957811757},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1709262766062},{"_id":"source/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1707118741657},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1708958930811},{"_id":"source/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1707045207190},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1709198077742},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1709197025669},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1709206562025},{"_id":"source/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1706779539112},{"_id":"source/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1707045618160},{"_id":"source/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1707045245660},{"_id":"source/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1706875075740},{"_id":"public/baidusitemap.xml","hash":"65f2a94b6c190b27bb71d462e31e657f9d2dfee6","modified":1709366528917},{"_id":"public/sitemap.xml","hash":"634efd9643ddb0dc0764d805a0f3919467519583","modified":1709366528917},{"_id":"public/sitemap.txt","hash":"9adbcfd46bce8edeaefa5fb26eae1af8834cbe6c","modified":1709366528917},{"_id":"public/search.xml","hash":"aeca86396674ba760f6f6e2a76bf2d33208f2c79","modified":1709366528917},{"_id":"public/about/index.html","hash":"cef2b67b2f84261f6b5985e046557926652a839a","modified":1709366528917},{"_id":"public/tags/index.html","hash":"456f213e746669e838b9980e9fcabcec5911bd65","modified":1709366528917},{"_id":"public/c4da56c0.html","hash":"d05c562df143ec1f6a42dbae05550470f07d656b","modified":1709366528917},{"_id":"public/archives/index.html","hash":"f3db4df99faaccf087d56d0284fa5f00ea3dab9e","modified":1709366528917},{"_id":"public/archives/2024/index.html","hash":"34ba2879a6ea1910b03e28d16a454aa1cf71ff3d","modified":1709366528917},{"_id":"public/archives/2024/02/index.html","hash":"d7857c76d483036e405c1cb78b419aa8579af36a","modified":1709366528917},{"_id":"public/index.html","hash":"87e6af73c879f53b624b6243d22f980babb80875","modified":1709366528917},{"_id":"public/a051710f.html","hash":"c55faa882ebadc6c5a506a15be8a3bc486718cfb","modified":1709366528917},{"_id":"public/categories/CS/index.html","hash":"5023baaae5072c4d9a521b54037f5a208ac1cf62","modified":1709366528917},{"_id":"public/categories/index.html","hash":"cae92f67a872781ff7f7ad5094367bfd496818f2","modified":1709366528917},{"_id":"public/categories/CS/NLP/index.html","hash":"1ad68ae30f58a24495cc9aee637f0f7a381f5d36","modified":1709366528917},{"_id":"public/categories/CS/NLP/LLM/index.html","hash":"e5b0310125fbc5863c78d726b77a32d1c3516b81","modified":1709366528917},{"_id":"public/tags/NLP/index.html","hash":"d9a79c412edc6e86177ccc903352d167edab5075","modified":1709366528917},{"_id":"public/tags/LLM/index.html","hash":"47c8f18194ccc3faf6d260d224497b4e6c11b9fd","modified":1709366528917},{"_id":"public/tags/transformer/index.html","hash":"b5af995af056a2587d59886651d0bf69bc4d4c71","modified":1709366528917},{"_id":"public/tags/长上下文/index.html","hash":"4213c2d9e7fcfc3d51745e06e25e814ba6540eba","modified":1709366528917},{"_id":"public/tags/窗口外推/index.html","hash":"689fa9d3417d097b15119a097b933759c6583ba0","modified":1709366528917},{"_id":"public/tags/positional-encoding/index.html","hash":"a81a04083adab2afd3e1ff8992d852b4c55117bf","modified":1709366528917},{"_id":"public/tags/RoPE/index.html","hash":"acae5d30c3020eb0743d8713acc0635ae9d875fb","modified":1709366528917},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1709366528917},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1709366528917},{"_id":"public/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1709366528917},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1709366528917},{"_id":"public/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1709366528917},{"_id":"public/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1709366528917},{"_id":"public/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1709366528917},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1709366528917},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1709366528917},{"_id":"public/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1709366528917},{"_id":"public/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1709366528917},{"_id":"public/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1709366528917},{"_id":"public/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1709366528917},{"_id":"public/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1709366528917},{"_id":"public/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1709366528917},{"_id":"public/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1709366528917},{"_id":"public/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1709366528917},{"_id":"public/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1709366528917},{"_id":"public/a051710f/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1709366528917},{"_id":"public/c4da56c0/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1709366528917},{"_id":"public/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1709366528917},{"_id":"public/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1709366528917},{"_id":"public/c4da56c0/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1709366528917},{"_id":"public/c4da56c0/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1709366528917},{"_id":"public/a051710f/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1709366528917},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1709366528917},{"_id":"public/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1709366528917},{"_id":"public/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1709366528917},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1709366528917},{"_id":"public/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1709366528917},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1709366528917},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1709366528917},{"_id":"public/css/noscript.css","hash":"4cd5301e478e0e0d4b176740ec314087ec5cb707","modified":1709366528917},{"_id":"public/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1709366528917},{"_id":"public/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1709366528917},{"_id":"public/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1709366528917},{"_id":"public/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1709366528917},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1709366528917},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1709366528917},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1709366528917},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1709366528917},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1709366528917},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1709366528917},{"_id":"public/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1709366528917},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1709366528917},{"_id":"public/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1709366528917},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1709366528917},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1709366528917},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1709366528917},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1709366528917},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1709366528917},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1709366528917},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1709366528917},{"_id":"public/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1709366528917},{"_id":"public/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1709366528917},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1709366528917},{"_id":"public/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1709366528917},{"_id":"public/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1709366528917},{"_id":"public/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1709366528917},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1709366528917},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1709366528917},{"_id":"public/css/main.css","hash":"6aea217c0462e6970606601a9fe39183cf15614c","modified":1709366528917},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1709366528917},{"_id":"public/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1709366528917},{"_id":"public/a051710f/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1709366528917},{"_id":"public/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1709366528917},{"_id":"public/c4da56c0/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1709366528917},{"_id":"public/c4da56c0/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1709366528917},{"_id":"public/c4da56c0/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1709366528917},{"_id":"public/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1709366528917},{"_id":"public/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1709366528917},{"_id":"public/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1709366528917},{"_id":"public/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1709366528917}],"Category":[{"name":"CS","_id":"clt9sp3ky0003fs4kgnev70s8"},{"name":"NLP","parent":"clt9sp3ky0003fs4kgnev70s8","_id":"clt9sp3kz0007fs4k5g3ogrbq"},{"name":"LLM","parent":"clt9sp3kz0007fs4k5g3ogrbq","_id":"clt9sp3kz0009fs4kdp7g5s70"}],"Data":[{"_id":"styles","data":".post-toc .nav .nav-child {\n  display: block;\n}\n.post-toc ol {\n  font-size: 13px;\n}\nbody {\n  background: url(\"/images/background/wallhaven-p97q73.png\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-size: cover;\n  background-position: 50% 50%;\n}\n:root {\n  --content-bg-color: rgba(32,32,32,0.816);\n}\n"}],"Page":[{"title":"about","date":"2024-01-31T10:57:44.000Z","type":"about","comments":0,"_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2024-01-31 18:57:44\ntype: \"about\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:21.349Z","path":"about/index.html","layout":"page","_id":"clt9sp3kv0000fs4kd6xj21uw","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"categories","date":"2024-01-31T10:57:57.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2024-01-31 18:57:57\ntype: \"categories\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:37.077Z","path":"categories/index.html","layout":"page","_id":"clt9sp3kx0002fs4k83jtesmi","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"tags","date":"2024-01-31T10:50:02.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2024-01-31 18:50:02\ntype: \"tags\"\ncomments: false\n---\n","updated":"2024-01-31T10:57:24.396Z","path":"tags/index.html","layout":"page","_id":"clt9sp3kz0005fs4k6xbsel80","content":"\n","length":0,"excerpt":"","more":"\n"}],"Post":[{"title":"LLM长上下文的问题","abbrlink":"c4da56c0","date":"2024-02-28T07:19:28.000Z","_content":"\n最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。  \n\n跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：[博客](http://www.linsight.cn/a051710f.html) [知乎](https://zhuanlan.zhihu.com/p/684072868) [微信公众号](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n# 关于长上下文  \n\n2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k tokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。  \n\n（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）  \n\n差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。\n\n今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型  \n\n<center>\n\n| 模型 | 支持长度 |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi | 192k |\n| Claude2 | 200k |\n\n</center>\n\n大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。  \n\n为什么要那么长？  \n\n# 长上下文的需求  \n\n取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都>1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。  \n\n最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。  \n\n上面这个场景对应的是大模型的<big><u>**工具化**</u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。  \n\n另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented generation），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。  \n\n除了工具化的应用场景，还有一些<big><u>**个性化**</u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。  \n\n实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。  \n\n上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。\n\n# 模型怎么支持长上下文\n\n看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？  \n\n如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u>**位置编码**</u>，模型不能很好地处理。  \n\n## 直接训练\n\n既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？  \n\n这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。  \n\n1.训练数据  \n\n直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。  \n\n当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention mask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention mask，效果也挺好。  \n\n总的来说，就是【连续长文本】>【多个中等文本拼接】（也可用）  \n\n2.资源消耗  \n\n来简单看一下transformer在训练中所消耗的资源。  \n\n假设模型有 $l$ 层，词表大小为 $V$ ，hidden size为 $h$ ，batch size为 $b$ ，训练窗口长度为 $s$ ，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。  \n\n(1) 参数量\n\n模型总参数量 $\\Phi$  = 词向量参数量 + $l$ * decoder层参数量 = $Vh + l(12h^2 + 13h)$  \n\n可以看到参数量和窗口长度 $s$ 无关，模型确定了就是一个固定值。  \n\n(2) 计算量  \n\n一次前向计算量 = 输出分类头logits计算 + $l$ * 每层计算量 $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\n看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n可以看到，总计算量随着输入长度的增长是平方的。在 $s << h$ 的时候，基本还可以认为是线性的。目前大部分模型的 $h$ 是在1k到1w这个范围，基本上可以认为 $s$ 和 $sh$ 在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系  \n\n(3) 显存  \n\n训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。\n\n训练中，每个参数（$\\Phi$）有一个对应梯度（$\\Phi$），每个参数又对应优化器一个一阶动量和二阶动量（$2\\Phi$）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有 $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ 的参数占用。\n\n{% asset_img mix_precision_fp16.png 混合精度训练 %}  \n\n这部分跟输入长度没有直接关系。\n\n另外一个需要占用显存的部分是中间激活值。  \n\n保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。  \n\n对于attention层，输入时先要对 $x$ 做 $Q、K、V$ 投影，需要保存 $x$ 的中间值；计算权重的时候有 $Q、K$ 矩阵的相乘，需要保存 $Q、K$ 矩阵的值；做softmax的时候输入有 $QK^T$ 要保存；以此类推，则需要保存的所有中间激活值为 $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$ 。对于 $l$ 层的模型，就再乘以 $l$ 。  \n\n可以看到中间激活值随着 $s$ 增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch size，或者提升gradient accumulation的值，无论如何，都会增加<big><u>**训练成本**</u></big>。  \n\n小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。  \n\n现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。  \n\n而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。  \n\n## 线性插值 Position Interpolation\n\n23年6月，Meta在[《EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION》](https://arxiv.org/pdf/2306.15595.pdf)中就提出了针对RoPE的线性插值方法PI（Position Interpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。\n\n{% asset_img meta_pi.png PI效果 %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。\n\n看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。  \n\n论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差 $\\left|m-n \\right|$ 不太大的时候（<2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦 $\\left|m-n \\right|$ 超过这个区间，还是有可能出现很大的值。\n\n{% asset_img meta_rope_ext.png RoPE外推 %}  \n\n看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention score。而右边的图使用了插值的方式，就相对稳定。\n\n（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）\n\n而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。  \n\n{% asset_img meta_pi_nosft.png PI效果 %}  \n\n插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。\n\n{% asset_img meta_pi_explanation.png PI效果 %}  \n\n这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。  \n\n由于三角函数光滑的特性，我们可以重新定义attention score的计算，使得结果不要出现异常大的值，也就是 $\\tilde{a}(s)=a(Ls/L^{\\prime})$ ，$L$ 是原长度（也就是2048），$L^{\\prime}$ 是我们想要增大的长度（8k/16k/32k等）。\n\n更具体来说，就是对RoPE做一点修改  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n相当于位置 $m$ 的分辨率从1下降成了 ${L}/{L'}$。  \n\n（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）  \n\n然后使用几万到几十万条样本进行预训练，就可以了。\n\n（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）\n\n## NTK-Aware Interpolation \n\n线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware Interpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u>**非线性插值**</u>的方法，NTK-Aware Scaled RoPE。CodeLlama就是用这种方法把长度推广到1M。  \n\nNTK，就是Neural Tangent Kernel，神经正切核。具体是什么，让GLM4帮忙解答一下  \n\n>Neural Tangent Kernel (NTK) 是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK 的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。  \n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是 Neural Tangent Kernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。  \n具体来说，NTK 使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。  \nNTK 的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。\n\n这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？  \n\n它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。  \n\n回顾一下在RoPE中，对位置 $m$ 的输入向量进行“旋转”的矩阵长这样  \n\n{% asset_img rope_matrix.png RoPE旋转矩阵 %}  \n\n它把输入向量的元素划分成2个2个一组，共有 $d/2$ 组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于 $\\theta_j=10000^{-2j/d}$ ，可以看到， $j$ 越小越靠前的组旋转越快，$j$ 越大的旋转越慢。这里 $base=10000$ ， $base$ 越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。  \n\n不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。  \n\n怎么实现“高频外推，低频内插”？  \n\n先看回讲[RoPE](https://www.zhihu.com/people/us4ever)的时候，对于2维情况，有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n推广到高维的情况，则有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ ，$s=m-n$ 。  \n\n在这个公式下，线性插值相当于把  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n变成了  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $\\alpha=L'/L>1$ ，相当于把 $s$ 压缩了。  \n\n而NTK-Aware Scaled RoPE则是对 $\\theta_j$ 进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n相当于 $\\theta$ 乘了一个系数 $\\alpha^{\\frac{-2j}{d-2}}$ ，当 $j$ 比较小的时候， $\\alpha^{\\frac{-2j}{d-2}}$ 接近1，相当于直接进行了外推，而当 $j$ 比较大的时候（注意 $j$ 的取值是从0到 $d - 1$），$\\alpha^{\\frac{-2j}{d-2}}$ 就接近 $\\alpha^{-1}$ ，这就和线性插值趋近了。\n\n引用来自[知乎一篇文章](https://zhuanlan.zhihu.com/p/645770522)的一个视角来理解NTK-Aware Interpolation  \n\n>有意思的解释一下，RoPE 的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的 RoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动 1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE 缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware RoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5 倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24 小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k 秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。  \n\n另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下[原文](https://kexue.fm/archives/9675)，也很巧妙。  \n\n在YaRN的[论文](https://arxiv.org/pdf/2309.00071.pdf)中，对NTK的优缺点作了点评  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那 $\\alpha=L'/L$ 就要选得比8更大一些，比如16。\n\n## NTK-by-parts\n\nNTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。  \n\n对于分量 $j$ ，RoPE嵌入的波长  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$ 代表旋转一周所需的长度。当 $j$ 比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。  \n\n这里观察到，当 $j$ 比较大时，波长就可能比 $L$ 要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如 $sin$ 只转了1/4圈，那值全都集中在0~1之间，-1~0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当 $j$ 比较小时，模型只能访问到相对位置信息。  \n\n此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是  \n\n- 如果维度 $j$ 的波长 $\\lambda_j$ 远小于上下文长度 ，就不插值只外推  \n- 如果波长 $\\lambda_j\\geq$ 上下文长度，就只插值不外推  \n- 中间的部分就同时存在两种，类似NTK-aware interpolation  \n\n引入一个比例 $r(j)=\\frac{L}{\\lambda_j}$ 来表示波长和上下文长度的关系。另外还需要两个阈值 $\\beta_1、\\beta_2$ 来区分以上三种情况。如果 $r(j)<\\beta_1$ ，就认为波长大，如果 $r(j)\\geq \\beta_2$ ，就认为波长小。方便起见，定义一个斜坡函数  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts插值可以定义为对 $\\theta_j$ 的一个操作  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n这里有两个超参 $\\beta_1、\\beta_2$ 要定，文中根据实验给出的推荐值是 $\\beta_1=1，\\beta_2=32$ ，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。   \n\n## Dynamically NTK Scaled RoPE  \n\n无论是线性插值还是NTK-Aware Interpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention score暴增的风险。另一方面，在解码过程中，当已解码的长度 $l$ 还没有达到训练长度 $L$ 时，就使用 $\\alpha$ 来修改base，也可能带来一些损失。Dynamically NTK Scaled RoPE是在NTK插值的基础上，把固定的系数改成动态的系数。  \n\n具体来说，就是  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n这样随着解码长度 $l$ 的增长，当 $l>L$ 之后 $\\alpha$ 从1逐渐增大， $l\\leq L$ 时则不需要改动。  \n\n有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。  \n\n## YaRN  \n\n上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。  \n\n当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度 $t>1$ 来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 $\\sqrt{t}$ 来扩展RoPE的长度。这样可以不必修改注意力的代码。  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\n通过对Llama 1和Llama 2的实验，文章提出了建议值$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。  \n\nYaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention score进行调整。  \n\nYaRN在微调以及无微调的情况下，效果都比上面的几种都要好。\n\n## logn  \n\nlogn指的是对attention计算中的缩放因子 $\\sqrt{d}$ 进行通过logn进行改进的一个方法，苏剑林在[博客](https://zhuanlan.zhihu.com/p/678755776)中进行了分析。大致的思路和YaRN中的缩放颇有些相似。  \n\n简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention score公式  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n可以看到，当 $L'>L$ 时，其效果和YaRN中的放缩是类似的。\n\n## 其他\n\n在扩展推理长度上，还有很多其他有效的工作，比如各种window attention，streaming LLM，LongLoRA，Focus Transformer等，还有数据、评测等更方面的分析，待逐个梳理。\n\n# 小结  \n\n较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降  \n\n- 推理时用到了没训练过的位置编码  \n- 推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏  \n\n这两个问题分别可以从位置编码和attention score的放缩来缓解。  \n\n线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。  \n\n# Reference  \n【1】分析transformer模型的参数量、计算量、中间激活、KV cache https://zhuanlan.zhihu.com/p/624740065  \n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n【3】Transformer升级之路：10、RoPE是一种β进制编码 https://kexue.fm/archives/9675  \n【4】YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n【5】详解基于调整RoPE旋转角度的大模型长度外推方法 https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符 https://cloud.tencent.com/developer/article/2330611  \n【8】Transformer升级之路：8、长度外推性与位置鲁棒性 https://spaces.ac.cn/archives/9444  \n【9】RoPE外推优化——支持192K上下文长度 https://zhuanlan.zhihu.com/p/678755776\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLM长上下文的问题.md","raw":"---\ntitle: LLM长上下文的问题\nabbrlink: c4da56c0\ndate: 2024-02-28 15:19:28\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 长上下文\n  - 窗口外推\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。  \n\n跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：[博客](http://www.linsight.cn/a051710f.html) [知乎](https://zhuanlan.zhihu.com/p/684072868) [微信公众号](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n# 关于长上下文  \n\n2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k tokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。  \n\n（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）  \n\n差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。\n\n今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型  \n\n<center>\n\n| 模型 | 支持长度 |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi | 192k |\n| Claude2 | 200k |\n\n</center>\n\n大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。  \n\n为什么要那么长？  \n\n# 长上下文的需求  \n\n取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都>1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。  \n\n最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。  \n\n上面这个场景对应的是大模型的<big><u>**工具化**</u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。  \n\n另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented generation），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。  \n\n除了工具化的应用场景，还有一些<big><u>**个性化**</u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。  \n\n实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。  \n\n上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。\n\n# 模型怎么支持长上下文\n\n看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？  \n\n如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u>**位置编码**</u>，模型不能很好地处理。  \n\n## 直接训练\n\n既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？  \n\n这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。  \n\n1.训练数据  \n\n直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。  \n\n当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention mask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention mask，效果也挺好。  \n\n总的来说，就是【连续长文本】>【多个中等文本拼接】（也可用）  \n\n2.资源消耗  \n\n来简单看一下transformer在训练中所消耗的资源。  \n\n假设模型有 $l$ 层，词表大小为 $V$ ，hidden size为 $h$ ，batch size为 $b$ ，训练窗口长度为 $s$ ，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。  \n\n(1) 参数量\n\n模型总参数量 $\\Phi$  = 词向量参数量 + $l$ * decoder层参数量 = $Vh + l(12h^2 + 13h)$  \n\n可以看到参数量和窗口长度 $s$ 无关，模型确定了就是一个固定值。  \n\n(2) 计算量  \n\n一次前向计算量 = 输出分类头logits计算 + $l$ * 每层计算量 $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\n看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n可以看到，总计算量随着输入长度的增长是平方的。在 $s << h$ 的时候，基本还可以认为是线性的。目前大部分模型的 $h$ 是在1k到1w这个范围，基本上可以认为 $s$ 和 $sh$ 在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系  \n\n(3) 显存  \n\n训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。\n\n训练中，每个参数（$\\Phi$）有一个对应梯度（$\\Phi$），每个参数又对应优化器一个一阶动量和二阶动量（$2\\Phi$）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有 $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ 的参数占用。\n\n{% asset_img mix_precision_fp16.png 混合精度训练 %}  \n\n这部分跟输入长度没有直接关系。\n\n另外一个需要占用显存的部分是中间激活值。  \n\n保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。  \n\n对于attention层，输入时先要对 $x$ 做 $Q、K、V$ 投影，需要保存 $x$ 的中间值；计算权重的时候有 $Q、K$ 矩阵的相乘，需要保存 $Q、K$ 矩阵的值；做softmax的时候输入有 $QK^T$ 要保存；以此类推，则需要保存的所有中间激活值为 $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$ 。对于 $l$ 层的模型，就再乘以 $l$ 。  \n\n可以看到中间激活值随着 $s$ 增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch size，或者提升gradient accumulation的值，无论如何，都会增加<big><u>**训练成本**</u></big>。  \n\n小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。  \n\n现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。  \n\n而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。  \n\n## 线性插值 Position Interpolation\n\n23年6月，Meta在[《EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION》](https://arxiv.org/pdf/2306.15595.pdf)中就提出了针对RoPE的线性插值方法PI（Position Interpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。\n\n{% asset_img meta_pi.png PI效果 %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。\n\n看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。  \n\n论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差 $\\left|m-n \\right|$ 不太大的时候（<2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦 $\\left|m-n \\right|$ 超过这个区间，还是有可能出现很大的值。\n\n{% asset_img meta_rope_ext.png RoPE外推 %}  \n\n看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention score。而右边的图使用了插值的方式，就相对稳定。\n\n（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）\n\n而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。  \n\n{% asset_img meta_pi_nosft.png PI效果 %}  \n\n插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。\n\n{% asset_img meta_pi_explanation.png PI效果 %}  \n\n这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。  \n\n由于三角函数光滑的特性，我们可以重新定义attention score的计算，使得结果不要出现异常大的值，也就是 $\\tilde{a}(s)=a(Ls/L^{\\prime})$ ，$L$ 是原长度（也就是2048），$L^{\\prime}$ 是我们想要增大的长度（8k/16k/32k等）。\n\n更具体来说，就是对RoPE做一点修改  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n相当于位置 $m$ 的分辨率从1下降成了 ${L}/{L'}$。  \n\n（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）  \n\n然后使用几万到几十万条样本进行预训练，就可以了。\n\n（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）\n\n## NTK-Aware Interpolation \n\n线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware Interpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u>**非线性插值**</u>的方法，NTK-Aware Scaled RoPE。CodeLlama就是用这种方法把长度推广到1M。  \n\nNTK，就是Neural Tangent Kernel，神经正切核。具体是什么，让GLM4帮忙解答一下  \n\n>Neural Tangent Kernel (NTK) 是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK 的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。  \n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是 Neural Tangent Kernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。  \n具体来说，NTK 使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。  \nNTK 的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。\n\n这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？  \n\n它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。  \n\n回顾一下在RoPE中，对位置 $m$ 的输入向量进行“旋转”的矩阵长这样  \n\n{% asset_img rope_matrix.png RoPE旋转矩阵 %}  \n\n它把输入向量的元素划分成2个2个一组，共有 $d/2$ 组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于 $\\theta_j=10000^{-2j/d}$ ，可以看到， $j$ 越小越靠前的组旋转越快，$j$ 越大的旋转越慢。这里 $base=10000$ ， $base$ 越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。  \n\n不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。  \n\n怎么实现“高频外推，低频内插”？  \n\n先看回讲[RoPE](https://www.zhihu.com/people/us4ever)的时候，对于2维情况，有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n推广到高维的情况，则有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ ，$s=m-n$ 。  \n\n在这个公式下，线性插值相当于把  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n变成了  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $\\alpha=L'/L>1$ ，相当于把 $s$ 压缩了。  \n\n而NTK-Aware Scaled RoPE则是对 $\\theta_j$ 进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n相当于 $\\theta$ 乘了一个系数 $\\alpha^{\\frac{-2j}{d-2}}$ ，当 $j$ 比较小的时候， $\\alpha^{\\frac{-2j}{d-2}}$ 接近1，相当于直接进行了外推，而当 $j$ 比较大的时候（注意 $j$ 的取值是从0到 $d - 1$），$\\alpha^{\\frac{-2j}{d-2}}$ 就接近 $\\alpha^{-1}$ ，这就和线性插值趋近了。\n\n引用来自[知乎一篇文章](https://zhuanlan.zhihu.com/p/645770522)的一个视角来理解NTK-Aware Interpolation  \n\n>有意思的解释一下，RoPE 的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的 RoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动 1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE 缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware RoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5 倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24 小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k 秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。  \n\n另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下[原文](https://kexue.fm/archives/9675)，也很巧妙。  \n\n在YaRN的[论文](https://arxiv.org/pdf/2309.00071.pdf)中，对NTK的优缺点作了点评  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那 $\\alpha=L'/L$ 就要选得比8更大一些，比如16。\n\n## NTK-by-parts\n\nNTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。  \n\n对于分量 $j$ ，RoPE嵌入的波长  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$ 代表旋转一周所需的长度。当 $j$ 比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。  \n\n这里观察到，当 $j$ 比较大时，波长就可能比 $L$ 要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如 $sin$ 只转了1/4圈，那值全都集中在0~1之间，-1~0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当 $j$ 比较小时，模型只能访问到相对位置信息。  \n\n此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是  \n\n- 如果维度 $j$ 的波长 $\\lambda_j$ 远小于上下文长度 ，就不插值只外推  \n- 如果波长 $\\lambda_j\\geq$ 上下文长度，就只插值不外推  \n- 中间的部分就同时存在两种，类似NTK-aware interpolation  \n\n引入一个比例 $r(j)=\\frac{L}{\\lambda_j}$ 来表示波长和上下文长度的关系。另外还需要两个阈值 $\\beta_1、\\beta_2$ 来区分以上三种情况。如果 $r(j)<\\beta_1$ ，就认为波长大，如果 $r(j)\\geq \\beta_2$ ，就认为波长小。方便起见，定义一个斜坡函数  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts插值可以定义为对 $\\theta_j$ 的一个操作  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n这里有两个超参 $\\beta_1、\\beta_2$ 要定，文中根据实验给出的推荐值是 $\\beta_1=1，\\beta_2=32$ ，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。   \n\n## Dynamically NTK Scaled RoPE  \n\n无论是线性插值还是NTK-Aware Interpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention score暴增的风险。另一方面，在解码过程中，当已解码的长度 $l$ 还没有达到训练长度 $L$ 时，就使用 $\\alpha$ 来修改base，也可能带来一些损失。Dynamically NTK Scaled RoPE是在NTK插值的基础上，把固定的系数改成动态的系数。  \n\n具体来说，就是  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n这样随着解码长度 $l$ 的增长，当 $l>L$ 之后 $\\alpha$ 从1逐渐增大， $l\\leq L$ 时则不需要改动。  \n\n有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。  \n\n## YaRN  \n\n上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。  \n\n当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度 $t>1$ 来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 $\\sqrt{t}$ 来扩展RoPE的长度。这样可以不必修改注意力的代码。  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\n通过对Llama 1和Llama 2的实验，文章提出了建议值$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。  \n\nYaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention score进行调整。  \n\nYaRN在微调以及无微调的情况下，效果都比上面的几种都要好。\n\n## logn  \n\nlogn指的是对attention计算中的缩放因子 $\\sqrt{d}$ 进行通过logn进行改进的一个方法，苏剑林在[博客](https://zhuanlan.zhihu.com/p/678755776)中进行了分析。大致的思路和YaRN中的缩放颇有些相似。  \n\n简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention score公式  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n可以看到，当 $L'>L$ 时，其效果和YaRN中的放缩是类似的。\n\n## 其他\n\n在扩展推理长度上，还有很多其他有效的工作，比如各种window attention，streaming LLM，LongLoRA，Focus Transformer等，还有数据、评测等更方面的分析，待逐个梳理。\n\n# 小结  \n\n较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降  \n\n- 推理时用到了没训练过的位置编码  \n- 推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏  \n\n这两个问题分别可以从位置编码和attention score的放缩来缓解。  \n\n线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。  \n\n# Reference  \n【1】分析transformer模型的参数量、计算量、中间激活、KV cache https://zhuanlan.zhihu.com/p/624740065  \n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n【3】Transformer升级之路：10、RoPE是一种β进制编码 https://kexue.fm/archives/9675  \n【4】YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n【5】详解基于调整RoPE旋转角度的大模型长度外推方法 https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符 https://cloud.tencent.com/developer/article/2330611  \n【8】Transformer升级之路：8、长度外推性与位置鲁棒性 https://spaces.ac.cn/archives/9444  \n【9】RoPE外推优化——支持192K上下文长度 https://zhuanlan.zhihu.com/p/678755776\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLM长上下文的问题","published":1,"updated":"2024-03-02T08:00:27.734Z","comments":1,"layout":"post","photos":[],"_id":"clt9sp3kw0001fs4kaeor5ce8","content":"<p>最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。</p>\n<p>跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：<a href=\"http://www.linsight.cn/a051710f.html\">博客</a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\">知乎</a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\">微信公众号</a></p>\n<h1 id=\"关于长上下文\">关于长上下文</h1>\n<p>2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k\ntokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。</p>\n<p>（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）</p>\n<p>差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。</p>\n<p>今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\">模型</th>\n<th style=\"text-align: center;\">支持长度</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。</p>\n<p>为什么要那么长？</p>\n<h1 id=\"长上下文的需求\">长上下文的需求</h1>\n<p>取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都&gt;1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。</p>\n<p>最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。</p>\n<p>上面这个场景对应的是大模型的<big><u><strong>工具化</strong></u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。</p>\n<p>另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented\ngeneration），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。</p>\n<p>除了工具化的应用场景，还有一些<big><u><strong>个性化</strong></u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。</p>\n<p>实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。</p>\n<p>上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。</p>\n<h1 id=\"模型怎么支持长上下文\">模型怎么支持长上下文</h1>\n<p>看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？</p>\n<p>如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u><strong>位置编码</strong></u>，模型不能很好地处理。</p>\n<h2 id=\"直接训练\">直接训练</h2>\n<p>既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？</p>\n<p>这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。</p>\n<p>1.训练数据</p>\n<p>直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。</p>\n<p>当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention\nmask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention\nmask，效果也挺好。</p>\n<p>总的来说，就是【连续长文本】&gt;【多个中等文本拼接】（也可用）</p>\n<p>2.资源消耗</p>\n<p>来简单看一下transformer在训练中所消耗的资源。</p>\n<p>假设模型有 <span class=\"math inline\">\\(l\\)</span> 层，词表大小为\n<span class=\"math inline\">\\(V\\)</span> ，hidden size为 <span class=\"math inline\">\\(h\\)</span> ，batch size为 <span class=\"math inline\">\\(b\\)</span> ，训练窗口长度为 <span class=\"math inline\">\\(s\\)</span>\n，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。</p>\n<ol type=\"1\">\n<li>参数量</li>\n</ol>\n<p>模型总参数量 <span class=\"math inline\">\\(\\Phi\\)</span> = 词向量参数量\n+ <span class=\"math inline\">\\(l\\)</span> * decoder层参数量 = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p>可以看到参数量和窗口长度 <span class=\"math inline\">\\(s\\)</span>\n无关，模型确定了就是一个固定值。</p>\n<ol start=\"2\" type=\"1\">\n<li>计算量</li>\n</ol>\n<p>一次前向计算量 = 输出分类头logits计算 + <span class=\"math inline\">\\(l\\)</span> * 每层计算量 <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，总计算量随着输入长度的增长是平方的。在 <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n的时候，基本还可以认为是线性的。目前大部分模型的 <span class=\"math inline\">\\(h\\)</span> 是在1k到1w这个范围，基本上可以认为\n<span class=\"math inline\">\\(s\\)</span> 和 <span class=\"math inline\">\\(sh\\)</span>\n在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系</p>\n<ol start=\"3\" type=\"1\">\n<li>显存</li>\n</ol>\n<p>训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。</p>\n<p>训练中，每个参数（<span class=\"math inline\">\\(\\Phi\\)</span>）有一个对应梯度（<span class=\"math inline\">\\(\\Phi\\)</span>），每个参数又对应优化器一个一阶动量和二阶动量（<span class=\"math inline\">\\(2\\Phi\\)</span>）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n的参数占用。</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"混合精度训练\">\n<p>这部分跟输入长度没有直接关系。</p>\n<p>另外一个需要占用显存的部分是中间激活值。</p>\n<p>保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。</p>\n<p>对于attention层，输入时先要对 <span class=\"math inline\">\\(x\\)</span>\n做 <span class=\"math inline\">\\(Q、K、V\\)</span> 投影，需要保存 <span class=\"math inline\">\\(x\\)</span> 的中间值；计算权重的时候有 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的相乘，需要保存 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的值；做softmax的时候输入有\n<span class=\"math inline\">\\(QK^T\\)</span>\n要保存；以此类推，则需要保存的所有中间激活值为 <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> 。对于\n<span class=\"math inline\">\\(l\\)</span> 层的模型，就再乘以 <span class=\"math inline\">\\(l\\)</span> 。</p>\n<p>可以看到中间激活值随着 <span class=\"math inline\">\\(s\\)</span>\n增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch\nsize，或者提升gradient\naccumulation的值，无论如何，都会增加<big><u><strong>训练成本</strong></u></big>。</p>\n<p>小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。</p>\n<p>现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。</p>\n<p>而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。</p>\n<h2 id=\"线性插值-position-interpolation\">线性插值 Position\nInterpolation</h2>\n<p>23年6月，Meta在<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">《EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION》</a>中就提出了针对RoPE的线性插值方法PI（Position\nInterpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI效果\">\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。</p>\n<p>看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。</p>\n<p>论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n不太大的时候（&lt;2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n超过这个区间，还是有可能出现很大的值。</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE外推\">\n<p>看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention\nscore。而右边的图使用了插值的方式，就相对稳定。</p>\n<p>（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）</p>\n<p>而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI效果\">\n<p>插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI效果\">\n<p>这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。</p>\n<p>由于三角函数光滑的特性，我们可以重新定义attention\nscore的计算，使得结果不要出现异常大的值，也就是 <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> ，<span class=\"math inline\">\\(L\\)</span> 是原长度（也就是2048），<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n是我们想要增大的长度（8k/16k/32k等）。</p>\n<p>更具体来说，就是对RoPE做一点修改</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于位置 <span class=\"math inline\">\\(m\\)</span> 的分辨率从1下降成了\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span>。</p>\n<p>（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）</p>\n<p>然后使用几万到几十万条样本进行预训练，就可以了。</p>\n<p>（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）</p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware\nInterpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u><strong>非线性插值</strong></u>的方法，NTK-Aware\nScaled RoPE。CodeLlama就是用这种方法把长度推广到1M。</p>\n<p>NTK，就是Neural Tangent\nKernel，神经正切核。具体是什么，让GLM4帮忙解答一下</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\n是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK\n的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。<br>\n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是\nNeural Tangent\nKernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。<br>\n具体来说，NTK\n使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。<br>\nNTK\n的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。</p>\n</blockquote>\n<p>这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？</p>\n<p>它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。</p>\n<p>回顾一下在RoPE中，对位置 <span class=\"math inline\">\\(m\\)</span>\n的输入向量进行“旋转”的矩阵长这样</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE旋转矩阵\">\n<p>它把输入向量的元素划分成2个2个一组，共有 <span class=\"math inline\">\\(d/2\\)</span>\n组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> ，可以看到，\n<span class=\"math inline\">\\(j\\)</span> 越小越靠前的组旋转越快，<span class=\"math inline\">\\(j\\)</span> 越大的旋转越慢。这里 <span class=\"math inline\">\\(base=10000\\)</span> ， <span class=\"math inline\">\\(base\\)</span>\n越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。</p>\n<p>不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。</p>\n<p>怎么实现“高频外推，低频内插”？</p>\n<p>先看回讲<a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>的时候，对于2维情况，有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>推广到高维的情况，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n，<span class=\"math inline\">\\(s=m-n\\)</span> 。</p>\n<p>在这个公式下，线性插值相当于把</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>变成了</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n，相当于把 <span class=\"math inline\">\\(s\\)</span> 压缩了。</p>\n<p>而NTK-Aware Scaled RoPE则是对 <span class=\"math inline\">\\(\\theta_j\\)</span>\n进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于 <span class=\"math inline\">\\(\\theta\\)</span> 乘了一个系数 <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> ，当 <span class=\"math inline\">\\(j\\)</span> 比较小的时候， <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n接近1，相当于直接进行了外推，而当 <span class=\"math inline\">\\(j\\)</span>\n比较大的时候（注意 <span class=\"math inline\">\\(j\\)</span> 的取值是从0到\n<span class=\"math inline\">\\(d - 1\\)</span>），<span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> 就接近 <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> ，这就和线性插值趋近了。</p>\n<p>引用来自<a href=\"https://zhuanlan.zhihu.com/p/645770522\">知乎一篇文章</a>的一个视角来理解NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>有意思的解释一下，RoPE\n的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的\nRoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动\n1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE\n缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware\nRoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5\n倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24\n小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k\n秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。</p>\n</blockquote>\n<p>另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下<a href=\"https://kexue.fm/archives/9675\">原文</a>，也很巧妙。</p>\n<p>在YaRN的<a href=\"https://arxiv.org/pdf/2309.00071.pdf\">论文</a>中，对NTK的优缺点作了点评</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n就要选得比8更大一些，比如16。</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。</p>\n<p>对于分量 <span class=\"math inline\">\\(j\\)</span> ，RoPE嵌入的波长</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n代表旋转一周所需的长度。当 <span class=\"math inline\">\\(j\\)</span>\n比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。</p>\n<p>这里观察到，当 <span class=\"math inline\">\\(j\\)</span>\n比较大时，波长就可能比 <span class=\"math inline\">\\(L\\)</span>\n要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如\n<span class=\"math inline\">\\(sin\\)</span>\n只转了1/4圈，那值全都集中在0<sub>1之间，-1</sub>0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当\n<span class=\"math inline\">\\(j\\)</span>\n比较小时，模型只能访问到相对位置信息。</p>\n<p>此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是</p>\n<ul>\n<li>如果维度 <span class=\"math inline\">\\(j\\)</span> 的波长 <span class=\"math inline\">\\(\\lambda_j\\)</span> 远小于上下文长度\n，就不插值只外推<br>\n</li>\n<li>如果波长 <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n上下文长度，就只插值不外推<br>\n</li>\n<li>中间的部分就同时存在两种，类似NTK-aware interpolation</li>\n</ul>\n<p>引入一个比例 <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n来表示波长和上下文长度的关系。另外还需要两个阈值 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span> 来区分以上三种情况。如果\n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n，就认为波长大，如果 <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> ，就认为波长小。方便起见，定义一个斜坡函数</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts插值可以定义为对 <span class=\"math inline\">\\(\\theta_j\\)</span> 的一个操作</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这里有两个超参 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span>\n要定，文中根据实验给出的推荐值是 <span class=\"math inline\">\\(\\beta_1=1，\\beta_2=32\\)</span>\n，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>无论是线性插值还是NTK-Aware\nInterpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention\nscore暴增的风险。另一方面，在解码过程中，当已解码的长度 <span class=\"math inline\">\\(l\\)</span> 还没有达到训练长度 <span class=\"math inline\">\\(L\\)</span> 时，就使用 <span class=\"math inline\">\\(\\alpha\\)</span>\n来修改base，也可能带来一些损失。Dynamically NTK Scaled\nRoPE是在NTK插值的基础上，把固定的系数改成动态的系数。</p>\n<p>具体来说，就是</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这样随着解码长度 <span class=\"math inline\">\\(l\\)</span> 的增长，当\n<span class=\"math inline\">\\(l&gt;L\\)</span> 之后 <span class=\"math inline\">\\(\\alpha\\)</span> 从1逐渐增大， <span class=\"math inline\">\\(l\\leq L\\)</span> 时则不需要改动。</p>\n<p>有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。</p>\n<p>当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度\n<span class=\"math inline\">\\(t&gt;1\\)</span>\n来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\n来扩展RoPE的长度。这样可以不必修改注意力的代码。</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>通过对Llama 1和Llama 2的实验，文章提出了建议值<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。</p>\n<p>YaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention\nscore进行调整。</p>\n<p>YaRN在微调以及无微调的情况下，效果都比上面的几种都要好。</p>\n<h2 id=\"logn\">logn</h2>\n<p>logn指的是对attention计算中的缩放因子 <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\n进行通过logn进行改进的一个方法，苏剑林在<a href=\"https://zhuanlan.zhihu.com/p/678755776\">博客</a>中进行了分析。大致的思路和YaRN中的缩放颇有些相似。</p>\n<p>简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention\nscore公式</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，当 <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\n时，其效果和YaRN中的放缩是类似的。</p>\n<h2 id=\"其他\">其他</h2>\n<p>在扩展推理长度上，还有很多其他有效的工作，比如各种window\nattention，streaming LLM，LongLoRA，Focus\nTransformer等，还有数据、评测等更方面的分析，待逐个梳理。</p>\n<h1 id=\"小结\">小结</h1>\n<p>较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降</p>\n<ul>\n<li>推理时用到了没训练过的位置编码<br>\n</li>\n<li>推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏</li>\n</ul>\n<p>这两个问题分别可以从位置编码和attention score的放缩来缓解。</p>\n<p>线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】分析transformer模型的参数量、计算量、中间激活、KV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n【3】Transformer升级之路：10、RoPE是一种β进制编码\nhttps://kexue.fm/archives/9675<br>\n【4】YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n【5】详解基于调整RoPE旋转角度的大模型长度外推方法\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符\nhttps://cloud.tencent.com/developer/article/2330611<br>\n【8】Transformer升级之路：8、长度外推性与位置鲁棒性\nhttps://spaces.ac.cn/archives/9444<br>\n【9】RoPE外推优化——支持192K上下文长度\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":12630,"excerpt":"","more":"<p>最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。</p>\n<p>跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：<a href=\"http://www.linsight.cn/a051710f.html\">博客</a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\">知乎</a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\">微信公众号</a></p>\n<h1 id=\"关于长上下文\">关于长上下文</h1>\n<p>2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k\ntokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。</p>\n<p>（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）</p>\n<p>差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。</p>\n<p>今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\">模型</th>\n<th style=\"text-align: center;\">支持长度</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。</p>\n<p>为什么要那么长？</p>\n<h1 id=\"长上下文的需求\">长上下文的需求</h1>\n<p>取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都&gt;1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。</p>\n<p>最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。</p>\n<p>上面这个场景对应的是大模型的<big><u><strong>工具化</strong></u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。</p>\n<p>另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented\ngeneration），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。</p>\n<p>除了工具化的应用场景，还有一些<big><u><strong>个性化</strong></u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。</p>\n<p>实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。</p>\n<p>上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。</p>\n<h1 id=\"模型怎么支持长上下文\">模型怎么支持长上下文</h1>\n<p>看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？</p>\n<p>如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u><strong>位置编码</strong></u>，模型不能很好地处理。</p>\n<h2 id=\"直接训练\">直接训练</h2>\n<p>既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？</p>\n<p>这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。</p>\n<p>1.训练数据</p>\n<p>直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。</p>\n<p>当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention\nmask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention\nmask，效果也挺好。</p>\n<p>总的来说，就是【连续长文本】&gt;【多个中等文本拼接】（也可用）</p>\n<p>2.资源消耗</p>\n<p>来简单看一下transformer在训练中所消耗的资源。</p>\n<p>假设模型有 <span class=\"math inline\">\\(l\\)</span> 层，词表大小为\n<span class=\"math inline\">\\(V\\)</span> ，hidden size为 <span class=\"math inline\">\\(h\\)</span> ，batch size为 <span class=\"math inline\">\\(b\\)</span> ，训练窗口长度为 <span class=\"math inline\">\\(s\\)</span>\n，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。</p>\n<ol type=\"1\">\n<li>参数量</li>\n</ol>\n<p>模型总参数量 <span class=\"math inline\">\\(\\Phi\\)</span> = 词向量参数量\n+ <span class=\"math inline\">\\(l\\)</span> * decoder层参数量 = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p>可以看到参数量和窗口长度 <span class=\"math inline\">\\(s\\)</span>\n无关，模型确定了就是一个固定值。</p>\n<ol start=\"2\" type=\"1\">\n<li>计算量</li>\n</ol>\n<p>一次前向计算量 = 输出分类头logits计算 + <span class=\"math inline\">\\(l\\)</span> * 每层计算量 <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，总计算量随着输入长度的增长是平方的。在 <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n的时候，基本还可以认为是线性的。目前大部分模型的 <span class=\"math inline\">\\(h\\)</span> 是在1k到1w这个范围，基本上可以认为\n<span class=\"math inline\">\\(s\\)</span> 和 <span class=\"math inline\">\\(sh\\)</span>\n在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系</p>\n<ol start=\"3\" type=\"1\">\n<li>显存</li>\n</ol>\n<p>训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。</p>\n<p>训练中，每个参数（<span class=\"math inline\">\\(\\Phi\\)</span>）有一个对应梯度（<span class=\"math inline\">\\(\\Phi\\)</span>），每个参数又对应优化器一个一阶动量和二阶动量（<span class=\"math inline\">\\(2\\Phi\\)</span>）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n的参数占用。</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"混合精度训练\">\n<p>这部分跟输入长度没有直接关系。</p>\n<p>另外一个需要占用显存的部分是中间激活值。</p>\n<p>保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。</p>\n<p>对于attention层，输入时先要对 <span class=\"math inline\">\\(x\\)</span>\n做 <span class=\"math inline\">\\(Q、K、V\\)</span> 投影，需要保存 <span class=\"math inline\">\\(x\\)</span> 的中间值；计算权重的时候有 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的相乘，需要保存 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的值；做softmax的时候输入有\n<span class=\"math inline\">\\(QK^T\\)</span>\n要保存；以此类推，则需要保存的所有中间激活值为 <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> 。对于\n<span class=\"math inline\">\\(l\\)</span> 层的模型，就再乘以 <span class=\"math inline\">\\(l\\)</span> 。</p>\n<p>可以看到中间激活值随着 <span class=\"math inline\">\\(s\\)</span>\n增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch\nsize，或者提升gradient\naccumulation的值，无论如何，都会增加<big><u><strong>训练成本</strong></u></big>。</p>\n<p>小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。</p>\n<p>现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。</p>\n<p>而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。</p>\n<h2 id=\"线性插值-position-interpolation\">线性插值 Position\nInterpolation</h2>\n<p>23年6月，Meta在<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">《EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION》</a>中就提出了针对RoPE的线性插值方法PI（Position\nInterpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI效果\">\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。</p>\n<p>看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。</p>\n<p>论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n不太大的时候（&lt;2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n超过这个区间，还是有可能出现很大的值。</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE外推\">\n<p>看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention\nscore。而右边的图使用了插值的方式，就相对稳定。</p>\n<p>（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）</p>\n<p>而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI效果\">\n<p>插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI效果\">\n<p>这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。</p>\n<p>由于三角函数光滑的特性，我们可以重新定义attention\nscore的计算，使得结果不要出现异常大的值，也就是 <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> ，<span class=\"math inline\">\\(L\\)</span> 是原长度（也就是2048），<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n是我们想要增大的长度（8k/16k/32k等）。</p>\n<p>更具体来说，就是对RoPE做一点修改</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于位置 <span class=\"math inline\">\\(m\\)</span> 的分辨率从1下降成了\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span>。</p>\n<p>（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）</p>\n<p>然后使用几万到几十万条样本进行预训练，就可以了。</p>\n<p>（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）</p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware\nInterpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u><strong>非线性插值</strong></u>的方法，NTK-Aware\nScaled RoPE。CodeLlama就是用这种方法把长度推广到1M。</p>\n<p>NTK，就是Neural Tangent\nKernel，神经正切核。具体是什么，让GLM4帮忙解答一下</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\n是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK\n的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。<br>\n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是\nNeural Tangent\nKernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。<br>\n具体来说，NTK\n使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。<br>\nNTK\n的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。</p>\n</blockquote>\n<p>这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？</p>\n<p>它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。</p>\n<p>回顾一下在RoPE中，对位置 <span class=\"math inline\">\\(m\\)</span>\n的输入向量进行“旋转”的矩阵长这样</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE旋转矩阵\">\n<p>它把输入向量的元素划分成2个2个一组，共有 <span class=\"math inline\">\\(d/2\\)</span>\n组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> ，可以看到，\n<span class=\"math inline\">\\(j\\)</span> 越小越靠前的组旋转越快，<span class=\"math inline\">\\(j\\)</span> 越大的旋转越慢。这里 <span class=\"math inline\">\\(base=10000\\)</span> ， <span class=\"math inline\">\\(base\\)</span>\n越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。</p>\n<p>不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。</p>\n<p>怎么实现“高频外推，低频内插”？</p>\n<p>先看回讲<a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>的时候，对于2维情况，有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>推广到高维的情况，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n，<span class=\"math inline\">\\(s=m-n\\)</span> 。</p>\n<p>在这个公式下，线性插值相当于把</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>变成了</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n，相当于把 <span class=\"math inline\">\\(s\\)</span> 压缩了。</p>\n<p>而NTK-Aware Scaled RoPE则是对 <span class=\"math inline\">\\(\\theta_j\\)</span>\n进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于 <span class=\"math inline\">\\(\\theta\\)</span> 乘了一个系数 <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> ，当 <span class=\"math inline\">\\(j\\)</span> 比较小的时候， <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n接近1，相当于直接进行了外推，而当 <span class=\"math inline\">\\(j\\)</span>\n比较大的时候（注意 <span class=\"math inline\">\\(j\\)</span> 的取值是从0到\n<span class=\"math inline\">\\(d - 1\\)</span>），<span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> 就接近 <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> ，这就和线性插值趋近了。</p>\n<p>引用来自<a href=\"https://zhuanlan.zhihu.com/p/645770522\">知乎一篇文章</a>的一个视角来理解NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>有意思的解释一下，RoPE\n的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的\nRoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动\n1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE\n缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware\nRoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5\n倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24\n小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k\n秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。</p>\n</blockquote>\n<p>另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下<a href=\"https://kexue.fm/archives/9675\">原文</a>，也很巧妙。</p>\n<p>在YaRN的<a href=\"https://arxiv.org/pdf/2309.00071.pdf\">论文</a>中，对NTK的优缺点作了点评</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n就要选得比8更大一些，比如16。</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。</p>\n<p>对于分量 <span class=\"math inline\">\\(j\\)</span> ，RoPE嵌入的波长</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n代表旋转一周所需的长度。当 <span class=\"math inline\">\\(j\\)</span>\n比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。</p>\n<p>这里观察到，当 <span class=\"math inline\">\\(j\\)</span>\n比较大时，波长就可能比 <span class=\"math inline\">\\(L\\)</span>\n要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如\n<span class=\"math inline\">\\(sin\\)</span>\n只转了1/4圈，那值全都集中在0<sub>1之间，-1</sub>0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当\n<span class=\"math inline\">\\(j\\)</span>\n比较小时，模型只能访问到相对位置信息。</p>\n<p>此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是</p>\n<ul>\n<li>如果维度 <span class=\"math inline\">\\(j\\)</span> 的波长 <span class=\"math inline\">\\(\\lambda_j\\)</span> 远小于上下文长度\n，就不插值只外推<br>\n</li>\n<li>如果波长 <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n上下文长度，就只插值不外推<br>\n</li>\n<li>中间的部分就同时存在两种，类似NTK-aware interpolation</li>\n</ul>\n<p>引入一个比例 <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n来表示波长和上下文长度的关系。另外还需要两个阈值 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span> 来区分以上三种情况。如果\n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n，就认为波长大，如果 <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> ，就认为波长小。方便起见，定义一个斜坡函数</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts插值可以定义为对 <span class=\"math inline\">\\(\\theta_j\\)</span> 的一个操作</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这里有两个超参 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span>\n要定，文中根据实验给出的推荐值是 <span class=\"math inline\">\\(\\beta_1=1，\\beta_2=32\\)</span>\n，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>无论是线性插值还是NTK-Aware\nInterpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention\nscore暴增的风险。另一方面，在解码过程中，当已解码的长度 <span class=\"math inline\">\\(l\\)</span> 还没有达到训练长度 <span class=\"math inline\">\\(L\\)</span> 时，就使用 <span class=\"math inline\">\\(\\alpha\\)</span>\n来修改base，也可能带来一些损失。Dynamically NTK Scaled\nRoPE是在NTK插值的基础上，把固定的系数改成动态的系数。</p>\n<p>具体来说，就是</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这样随着解码长度 <span class=\"math inline\">\\(l\\)</span> 的增长，当\n<span class=\"math inline\">\\(l&gt;L\\)</span> 之后 <span class=\"math inline\">\\(\\alpha\\)</span> 从1逐渐增大， <span class=\"math inline\">\\(l\\leq L\\)</span> 时则不需要改动。</p>\n<p>有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。</p>\n<p>当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度\n<span class=\"math inline\">\\(t&gt;1\\)</span>\n来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\n来扩展RoPE的长度。这样可以不必修改注意力的代码。</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>通过对Llama 1和Llama 2的实验，文章提出了建议值<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。</p>\n<p>YaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention\nscore进行调整。</p>\n<p>YaRN在微调以及无微调的情况下，效果都比上面的几种都要好。</p>\n<h2 id=\"logn\">logn</h2>\n<p>logn指的是对attention计算中的缩放因子 <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\n进行通过logn进行改进的一个方法，苏剑林在<a href=\"https://zhuanlan.zhihu.com/p/678755776\">博客</a>中进行了分析。大致的思路和YaRN中的缩放颇有些相似。</p>\n<p>简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention\nscore公式</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，当 <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\n时，其效果和YaRN中的放缩是类似的。</p>\n<h2 id=\"其他\">其他</h2>\n<p>在扩展推理长度上，还有很多其他有效的工作，比如各种window\nattention，streaming LLM，LongLoRA，Focus\nTransformer等，还有数据、评测等更方面的分析，待逐个梳理。</p>\n<h1 id=\"小结\">小结</h1>\n<p>较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降</p>\n<ul>\n<li>推理时用到了没训练过的位置编码<br>\n</li>\n<li>推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏</li>\n</ul>\n<p>这两个问题分别可以从位置编码和attention score的放缩来缓解。</p>\n<p>线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】分析transformer模型的参数量、计算量、中间激活、KV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n【3】Transformer升级之路：10、RoPE是一种β进制编码\nhttps://kexue.fm/archives/9675<br>\n【4】YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n【5】详解基于调整RoPE旋转角度的大模型长度外推方法\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符\nhttps://cloud.tencent.com/developer/article/2330611<br>\n【8】Transformer升级之路：8、长度外推性与位置鲁棒性\nhttps://spaces.ac.cn/archives/9444<br>\n【9】RoPE外推优化——支持192K上下文长度\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"理解LLM位置编码:RoPE","abbrlink":"a051710f","date":"2024-02-21T13:18:13.000Z","mathjax":true,"_content":"\n最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。\n\n# 关于RoPE\n\nRoPE（Rotary Position Embedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u>**以绝对位置编码形式实现的相对位置编码**</u></big>，兼顾了模型性能和效率。\n\n2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。\n\n苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。  \n\n# 以绝对位置编码的方式实现相对位置编码\n\n前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？\n\n先说原因：  \n\n在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。  \n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。  \n而使用相对位置编码则<u>**更容易外推**</u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。  \n但是传统相对位置编码的实现相对<u>**复杂**</u>，有些也会有<u>**计算效率低**</u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u>**线性注意力**</u>计算法模型中。  \n总结来说，就是绝对位置编码<u>**好实现**</u>，<u>**效率高**</u>，<u>**适用线性注意力**</u>，而相对位置编码<u>**易外推**</u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。  \n\n下面简单回顾一下绝对位置编码和相对位置编码。  \n\n（对位置编码比较熟悉的朋友可以直接跳到第3节。）  \n\n## 绝对位置编码\n\n先回顾一下带绝对位置编码的self-attention。  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$ 和 $x_j$ 分别是位置 $i$ 和 $j$ 的输入，$p$ 是对应位置的位置编码向量。  \n\n这里的位置编码$p$可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量 $x$ 和位置向量 $p$ 相加即可，相比attention中的softmax计算，element-wise addition操作的计算量非常小，是可以忽略不计的。\n\n大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把 $x + p$ 变成  $x * p$ 这样，效果上也是大差不差。\n\n## 相对位置编码\n\n在绝对位置编码中，可以在输入阶段就把 $x$ 和 $p$ 直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。  \n\n比如“我”这个词放在位置1时，形成一个 $e_1 = x_我 + p_1$ 这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量 $e_8 = x_我 + p_8$ 。两个向量 $e_1$ 和 $e_8$ 虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u>**耦合**</u>在一起共同构成了一个完整的输入。  \n\n直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。\n\n扯远了，现在回来看一下相对位置编码。把公式（1）中的 $q_{i}k_{j}^{T}$展开来  \n\n$$\\begin{align*}q_1k_j^\\top&=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n和位置相关的有 $p_iW_\\mathbb{Q}$ 和 $W_K^\\top p_j^\\top$ 两项。  \n\n### Google式\n\n在最早引入相对位置编码的Google的论文《Self-Attention with Relative Position Representations》中，把第一项 $p_iW_\\mathbb{Q}$ 去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置 $j$），把第二项 $W_K^\\top p_j^\\top$ 改成和位置 $i$、$j$ 都相关的位置向量 $R_{ij}^K$，于是在这个使用相对位置编码的attention计算中，<u>**不再是直接计算input projection的内积来获取权重**</u>，而变成  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ 是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n其中 $p_\\mathrm{K}$ 就是可训练的向量或者三角函数向量。 \n\n为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系**理应**更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到>256的长度。\n\n本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle的方法把 $p_{j}W_{\\mathrm{V}}$ 也改成了包含相对位置信息的向量\n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$ 和 $R_{ij}^K$ 相似，都是一个相对位置向量 + clip操作。\n\n### XLNET式\n\nXLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。  \n\n在公式（2）的基础上继续展开  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n把绝对位置相关的几个参数改成相对位置相关的参数，变成：\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n把 $p_i$ 变成了两个可训练的向量 $u$ 和 $\\nu$ ，把 $p_j$ 变成相对位置向量 $R_{i-j}^\\top$ 。  \n\n实际实现上可以把 $u$ 和 $\\nu$ 后面跟着的矩阵省掉了，去掉这个线性变化不影响 $u$ 和 $\\nu$ 的训练，变成\n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\n此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\n可以看到，Google式和XLNET式的相对位置编码在权重 $\\mathrm{a_{i,j}}$ 的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置 $i$ 、 $j$ 都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。\n\n当然，也有简单一点的实现，比如T5的方法。  \n\n### T5式\n\n公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置 $i$ 和位置 $j$ 的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\n（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）\n\n## 对比\n\n看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。  \n\n公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法\n\n从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。  \n\n总之在实现方式上和计算效率上，绝对位置编码具有一些优势。\n\n而在输入输出窗口外推方面，相对位置编码有着天然的优势。\n\n另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear attention方案中去，这个以后再展开讲（又挖了个坑）。  \n\n# RoPE的设计思路\n\n## 保持attention计算形式\n\n回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。\n\n先说设计思路：  \n\n首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端 = 内积 + softmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。  \n\n也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n其中 $q_m$ 是在位置 $m$ 的query向量，$k_n$ 是在位置 $n$ 的key向量，$f_q$ 和 $f_k$ 是分别针对这query和key向量的操作函数。  \n\n我们的任务就是要找到一组 $f_q$ 、 $f_k$ 和 $g$ ，使得公式（11）恒成立。  \n\n当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？  \n\n## 借用复数寻找组合\n\n式（11）中， $g$ 的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。\n\n这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量  \n\n{% asset_img complex_number.png 282 401 复数平面 %}\n\n现在考虑query和key向量都是2维的情况，那么可以代入复数的操作  \n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）  \n\n那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n其中 $\\boldsymbol{k}_n^*$ 是 $\\boldsymbol{k}_n$ 的共轭复数。  \n\n（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）\n\n共轭复数是这样的关系  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n先证明一下这个组合的正确性，是不是真的满足公式（11）。  \n\n（也可以先跳过证明，选择先相信这个组合）  \n\n回顾一下欧拉公式  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n因为现在我们讨论的是2维的情况，那2维向量 $q_m$ 可以用一个复数来表示  \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n那从复数角度来看，就有\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）  \n\n类似地，有\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n和\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n则有  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n用了三角函数和差公式\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n再看 $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n证毕。\n\n## “旋转”位置编码\n\n发现式（17）可以写成这样\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n同样地  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n如果从向量视角来看，则有  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n看式（22）和（23），可以看到等号右边都有  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n这正是一个二维平面的旋转矩阵。 $f_q$ 、 $f_k$ 的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。  \n\n这也是为什么叫做“旋转”位置编码。\n\n## 从2维推广到高维\n\n我们现在已经确认，对于2维的情况，经过 $f_q$ 、 $f_k$ 和 $g$ 这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？  \n\n答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$ 是的输入向量的维度，由于是两个两个一组，所以一共有 $d/2$ 组小旋转矩阵，这 $d/2$ 组矩阵为了区分，设计使用了不同的 $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n那么在实际操作的时候，给位置 $m$ 和位置 $n$ 的输入向量分别乘以 $R_m$ 和 $R_n$，再进行self-attention，就能获得仅使用相对位置信息编码的效果。  \n\n另外 $\\theta$ 是怎么来的呢？这里是参考了Google最初在《Attention is All You Need》中提出的，这里就先不展开了，可以看看论文原文。\n\n## 高效率实现\n\n式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\n只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。  \n\n另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。\n\n## 远程衰减的特性\n\n至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。  \n\n直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。\n\n回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个 $\\theta$ 的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。  \n\n证明过程这里就偷偷懒略过了，具体可以看[Roformer的论文](https://arxiv.org/abs/2104.09864)或者[苏神的博客](https://spaces.ac.cn/archives/8265)。  \n\n当 $d = 128$ 时，画出来的图像如下\n\n{% asset_img remote_attenuation.png 775 457 远程衰减 %}  \n\n# 小结  \n\n总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。\n\n# Reference\n【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130  \n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265  \n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n【4】十分钟读懂旋转编码（RoPE） https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLM位置编码RoPE.md","raw":"---\ntitle: 理解LLM位置编码:RoPE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - positional encoding\n  - RoPE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: a051710f\ndate: 2024-02-21 21:18:13\nmathjax: true\n---\n\n最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。\n\n# 关于RoPE\n\nRoPE（Rotary Position Embedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u>**以绝对位置编码形式实现的相对位置编码**</u></big>，兼顾了模型性能和效率。\n\n2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。\n\n苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。  \n\n# 以绝对位置编码的方式实现相对位置编码\n\n前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？\n\n先说原因：  \n\n在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。  \n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。  \n而使用相对位置编码则<u>**更容易外推**</u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。  \n但是传统相对位置编码的实现相对<u>**复杂**</u>，有些也会有<u>**计算效率低**</u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u>**线性注意力**</u>计算法模型中。  \n总结来说，就是绝对位置编码<u>**好实现**</u>，<u>**效率高**</u>，<u>**适用线性注意力**</u>，而相对位置编码<u>**易外推**</u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。  \n\n下面简单回顾一下绝对位置编码和相对位置编码。  \n\n（对位置编码比较熟悉的朋友可以直接跳到第3节。）  \n\n## 绝对位置编码\n\n先回顾一下带绝对位置编码的self-attention。  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$ 和 $x_j$ 分别是位置 $i$ 和 $j$ 的输入，$p$ 是对应位置的位置编码向量。  \n\n这里的位置编码$p$可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量 $x$ 和位置向量 $p$ 相加即可，相比attention中的softmax计算，element-wise addition操作的计算量非常小，是可以忽略不计的。\n\n大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把 $x + p$ 变成  $x * p$ 这样，效果上也是大差不差。\n\n## 相对位置编码\n\n在绝对位置编码中，可以在输入阶段就把 $x$ 和 $p$ 直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。  \n\n比如“我”这个词放在位置1时，形成一个 $e_1 = x_我 + p_1$ 这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量 $e_8 = x_我 + p_8$ 。两个向量 $e_1$ 和 $e_8$ 虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u>**耦合**</u>在一起共同构成了一个完整的输入。  \n\n直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。\n\n扯远了，现在回来看一下相对位置编码。把公式（1）中的 $q_{i}k_{j}^{T}$展开来  \n\n$$\\begin{align*}q_1k_j^\\top&=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n和位置相关的有 $p_iW_\\mathbb{Q}$ 和 $W_K^\\top p_j^\\top$ 两项。  \n\n### Google式\n\n在最早引入相对位置编码的Google的论文《Self-Attention with Relative Position Representations》中，把第一项 $p_iW_\\mathbb{Q}$ 去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置 $j$），把第二项 $W_K^\\top p_j^\\top$ 改成和位置 $i$、$j$ 都相关的位置向量 $R_{ij}^K$，于是在这个使用相对位置编码的attention计算中，<u>**不再是直接计算input projection的内积来获取权重**</u>，而变成  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ 是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n其中 $p_\\mathrm{K}$ 就是可训练的向量或者三角函数向量。 \n\n为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系**理应**更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到>256的长度。\n\n本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle的方法把 $p_{j}W_{\\mathrm{V}}$ 也改成了包含相对位置信息的向量\n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$ 和 $R_{ij}^K$ 相似，都是一个相对位置向量 + clip操作。\n\n### XLNET式\n\nXLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。  \n\n在公式（2）的基础上继续展开  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n把绝对位置相关的几个参数改成相对位置相关的参数，变成：\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n把 $p_i$ 变成了两个可训练的向量 $u$ 和 $\\nu$ ，把 $p_j$ 变成相对位置向量 $R_{i-j}^\\top$ 。  \n\n实际实现上可以把 $u$ 和 $\\nu$ 后面跟着的矩阵省掉了，去掉这个线性变化不影响 $u$ 和 $\\nu$ 的训练，变成\n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\n此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\n可以看到，Google式和XLNET式的相对位置编码在权重 $\\mathrm{a_{i,j}}$ 的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置 $i$ 、 $j$ 都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。\n\n当然，也有简单一点的实现，比如T5的方法。  \n\n### T5式\n\n公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置 $i$ 和位置 $j$ 的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\n（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）\n\n## 对比\n\n看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。  \n\n公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法\n\n从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。  \n\n总之在实现方式上和计算效率上，绝对位置编码具有一些优势。\n\n而在输入输出窗口外推方面，相对位置编码有着天然的优势。\n\n另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear attention方案中去，这个以后再展开讲（又挖了个坑）。  \n\n# RoPE的设计思路\n\n## 保持attention计算形式\n\n回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。\n\n先说设计思路：  \n\n首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端 = 内积 + softmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。  \n\n也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n其中 $q_m$ 是在位置 $m$ 的query向量，$k_n$ 是在位置 $n$ 的key向量，$f_q$ 和 $f_k$ 是分别针对这query和key向量的操作函数。  \n\n我们的任务就是要找到一组 $f_q$ 、 $f_k$ 和 $g$ ，使得公式（11）恒成立。  \n\n当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？  \n\n## 借用复数寻找组合\n\n式（11）中， $g$ 的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。\n\n这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量  \n\n{% asset_img complex_number.png 282 401 复数平面 %}\n\n现在考虑query和key向量都是2维的情况，那么可以代入复数的操作  \n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）  \n\n那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n其中 $\\boldsymbol{k}_n^*$ 是 $\\boldsymbol{k}_n$ 的共轭复数。  \n\n（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）\n\n共轭复数是这样的关系  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n先证明一下这个组合的正确性，是不是真的满足公式（11）。  \n\n（也可以先跳过证明，选择先相信这个组合）  \n\n回顾一下欧拉公式  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n因为现在我们讨论的是2维的情况，那2维向量 $q_m$ 可以用一个复数来表示  \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n那从复数角度来看，就有\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）  \n\n类似地，有\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n和\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n则有  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n用了三角函数和差公式\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n再看 $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n证毕。\n\n## “旋转”位置编码\n\n发现式（17）可以写成这样\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n同样地  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n如果从向量视角来看，则有  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n看式（22）和（23），可以看到等号右边都有  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n这正是一个二维平面的旋转矩阵。 $f_q$ 、 $f_k$ 的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。  \n\n这也是为什么叫做“旋转”位置编码。\n\n## 从2维推广到高维\n\n我们现在已经确认，对于2维的情况，经过 $f_q$ 、 $f_k$ 和 $g$ 这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？  \n\n答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$ 是的输入向量的维度，由于是两个两个一组，所以一共有 $d/2$ 组小旋转矩阵，这 $d/2$ 组矩阵为了区分，设计使用了不同的 $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n那么在实际操作的时候，给位置 $m$ 和位置 $n$ 的输入向量分别乘以 $R_m$ 和 $R_n$，再进行self-attention，就能获得仅使用相对位置信息编码的效果。  \n\n另外 $\\theta$ 是怎么来的呢？这里是参考了Google最初在《Attention is All You Need》中提出的，这里就先不展开了，可以看看论文原文。\n\n## 高效率实现\n\n式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\n只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。  \n\n另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。\n\n## 远程衰减的特性\n\n至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。  \n\n直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。\n\n回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个 $\\theta$ 的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。  \n\n证明过程这里就偷偷懒略过了，具体可以看[Roformer的论文](https://arxiv.org/abs/2104.09864)或者[苏神的博客](https://spaces.ac.cn/archives/8265)。  \n\n当 $d = 128$ 时，画出来的图像如下\n\n{% asset_img remote_attenuation.png 775 457 远程衰减 %}  \n\n# 小结  \n\n总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。\n\n# Reference\n【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130  \n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265  \n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n【4】十分钟读懂旋转编码（RoPE） https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLM位置编码RoPE","published":1,"updated":"2024-02-27T06:39:30.683Z","comments":1,"layout":"post","photos":[],"_id":"clt9sp3l0000kfs4ker9hhx8i","content":"<p>最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。</p>\n<h1 id=\"关于rope\">关于RoPE</h1>\n<p>RoPE（Rotary Position\nEmbedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u><strong>以绝对位置编码形式实现的相对位置编码</strong></u></big>，兼顾了模型性能和效率。</p>\n<p>2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。</p>\n<p>苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。</p>\n<h1 id=\"以绝对位置编码的方式实现相对位置编码\">以绝对位置编码的方式实现相对位置编码</h1>\n<p>前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？</p>\n<p>先说原因：</p>\n<p>在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。<br>\n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。<br>\n而使用相对位置编码则<u><strong>更容易外推</strong></u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。<br>\n但是传统相对位置编码的实现相对<u><strong>复杂</strong></u>，有些也会有<u><strong>计算效率低</strong></u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u><strong>线性注意力</strong></u>计算法模型中。<br>\n总结来说，就是绝对位置编码<u><strong>好实现</strong></u>，<u><strong>效率高</strong></u>，<u><strong>适用线性注意力</strong></u>，而相对位置编码<u><strong>易外推</strong></u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。</p>\n<p>下面简单回顾一下绝对位置编码和相对位置编码。</p>\n<p>（对位置编码比较熟悉的朋友可以直接跳到第3节。）</p>\n<h2 id=\"绝对位置编码\">绝对位置编码</h2>\n<p>先回顾一下带绝对位置编码的self-attention。</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span> 和 <span class=\"math inline\">\\(x_j\\)</span> 分别是位置 <span class=\"math inline\">\\(i\\)</span> 和 <span class=\"math inline\">\\(j\\)</span> 的输入，<span class=\"math inline\">\\(p\\)</span> 是对应位置的位置编码向量。</p>\n<p>这里的位置编码<span class=\"math inline\">\\(p\\)</span>可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量\n<span class=\"math inline\">\\(x\\)</span> 和位置向量 <span class=\"math inline\">\\(p\\)</span>\n相加即可，相比attention中的softmax计算，element-wise\naddition操作的计算量非常小，是可以忽略不计的。</p>\n<p>大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把\n<span class=\"math inline\">\\(x + p\\)</span> 变成 <span class=\"math inline\">\\(x * p\\)</span> 这样，效果上也是大差不差。</p>\n<h2 id=\"相对位置编码\">相对位置编码</h2>\n<p>在绝对位置编码中，可以在输入阶段就把 <span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(p\\)</span>\n直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。</p>\n<p>比如“我”这个词放在位置1时，形成一个 <span class=\"math inline\">\\(e_1 =\nx_我 + p_1\\)</span>\n这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量\n<span class=\"math inline\">\\(e_8 = x_我 + p_8\\)</span> 。两个向量 <span class=\"math inline\">\\(e_1\\)</span> 和 <span class=\"math inline\">\\(e_8\\)</span>\n虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u><strong>耦合</strong></u>在一起共同构成了一个完整的输入。</p>\n<p>直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。</p>\n<p>扯远了，现在回来看一下相对位置编码。把公式（1）中的 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span>展开来</p>\n<p><span class=\"math display\">\\[\\begin{align*}q_1k_j^\\top&amp;=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p>和位置相关的有 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n和 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 两项。</p>\n<h3 id=\"google式\">Google式</h3>\n<p>在最早引入相对位置编码的Google的论文《Self-Attention with Relative\nPosition Representations》中，把第一项 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置\n<span class=\"math inline\">\\(j\\)</span>），把第二项 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 改成和位置 <span class=\"math inline\">\\(i\\)</span>、<span class=\"math inline\">\\(j\\)</span>\n都相关的位置向量 <span class=\"math inline\">\\(R_{ij}^K\\)</span>，于是在这个使用相对位置编码的attention计算中，<u><strong>不再是直接计算input\nprojection的内积来获取权重</strong></u>，而变成</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\n是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n就是可训练的向量或者三角函数向量。</p>\n<p>为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系<strong>理应</strong>更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到&gt;256的长度。</p>\n<p>本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google的方法把 <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n也改成了包含相对位置信息的向量</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> 和\n<span class=\"math inline\">\\(R_{ij}^K\\)</span> 相似，都是一个相对位置向量\n+ clip操作。</p>\n<h3 id=\"xlnet式\">XLNET式</h3>\n<p>XLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。</p>\n<p>在公式（2）的基础上继续展开</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p>把绝对位置相关的几个参数改成相对位置相关的参数，变成：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p>把 <span class=\"math inline\">\\(p_i\\)</span> 变成了两个可训练的向量\n<span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> ，把 <span class=\"math inline\">\\(p_j\\)</span> 变成相对位置向量 <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> 。</p>\n<p>实际实现上可以把 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span>\n后面跟着的矩阵省掉了，去掉这个线性变化不影响 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> 的训练，变成</p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>可以看到，Google式和XLNET式的相对位置编码在权重 <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置\n<span class=\"math inline\">\\(i\\)</span> 、 <span class=\"math inline\">\\(j\\)</span>\n都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。</p>\n<p>当然，也有简单一点的实现，比如T5的方法。</p>\n<h3 id=\"t5式\">T5式</h3>\n<p>公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置\n<span class=\"math inline\">\\(i\\)</span> 和位置 <span class=\"math inline\">\\(j\\)</span>\n的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）</p>\n<h2 id=\"对比\">对比</h2>\n<p>看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。</p>\n<p>公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法</p>\n<p>从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。</p>\n<p>总之在实现方式上和计算效率上，绝对位置编码具有一些优势。</p>\n<p>而在输入输出窗口外推方面，相对位置编码有着天然的优势。</p>\n<p>另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear\nattention方案中去，这个以后再展开讲（又挖了个坑）。</p>\n<h1 id=\"rope的设计思路\">RoPE的设计思路</h1>\n<h2 id=\"保持attention计算形式\">保持attention计算形式</h2>\n<p>回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。</p>\n<p>先说设计思路：</p>\n<p>首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端\n= 内积 +\nsoftmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。</p>\n<p>也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是</p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(q_m\\)</span> 是在位置 <span class=\"math inline\">\\(m\\)</span> 的query向量，<span class=\"math inline\">\\(k_n\\)</span> 是在位置 <span class=\"math inline\">\\(n\\)</span> 的key向量，<span class=\"math inline\">\\(f_q\\)</span> 和 <span class=\"math inline\">\\(f_k\\)</span>\n是分别针对这query和key向量的操作函数。</p>\n<p>我们的任务就是要找到一组 <span class=\"math inline\">\\(f_q\\)</span> 、\n<span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span> ，使得公式（11）恒成立。</p>\n<p>当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？</p>\n<h2 id=\"借用复数寻找组合\">借用复数寻找组合</h2>\n<p>式（11）中， <span class=\"math inline\">\\(g\\)</span>\n的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。</p>\n<p>这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"复数平面\">\n<p>现在考虑query和key向量都是2维的情况，那么可以代入复数的操作<br>\n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）</p>\n<p>那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span> 是 <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> 的共轭复数。</p>\n<p>（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）</p>\n<p>共轭复数是这样的关系</p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>先证明一下这个组合的正确性，是不是真的满足公式（11）。</p>\n<p>（也可以先跳过证明，选择先相信这个组合）</p>\n<p>回顾一下欧拉公式</p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>因为现在我们讨论的是2维的情况，那2维向量 <span class=\"math inline\">\\(q_m\\)</span> 可以用一个复数来表示</p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p>那从复数角度来看，就有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）</p>\n<p>类似地，有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p>和</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p>则有<br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p>用了三角函数和差公式 <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p>再看 <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p>证毕。</p>\n<h2 id=\"旋转位置编码\">“旋转”位置编码</h2>\n<p>发现式（17）可以写成这样</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p>同样地</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p>如果从向量视角来看，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>看式（22）和（23），可以看到等号右边都有</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p>这正是一个二维平面的旋转矩阵。 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span>\n的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。</p>\n<p>这也是为什么叫做“旋转”位置编码。</p>\n<h2 id=\"从2维推广到高维\">从2维推广到高维</h2>\n<p>我们现在已经确认，对于2维的情况，经过 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span>\n这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？</p>\n<p>答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n是的输入向量的维度，由于是两个两个一组，所以一共有 <span class=\"math inline\">\\(d/2\\)</span> 组小旋转矩阵，这 <span class=\"math inline\">\\(d/2\\)</span> 组矩阵为了区分，设计使用了不同的\n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p>那么在实际操作的时候，给位置 <span class=\"math inline\">\\(m\\)</span>\n和位置 <span class=\"math inline\">\\(n\\)</span> 的输入向量分别乘以 <span class=\"math inline\">\\(R_m\\)</span> 和 <span class=\"math inline\">\\(R_n\\)</span>，再进行self-attention，就能获得仅使用相对位置信息编码的效果。</p>\n<p>另外 <span class=\"math inline\">\\(\\theta\\)</span>\n是怎么来的呢？这里是参考了Google最初在《Attention is All You\nNeed》中提出的，这里就先不展开了，可以看看论文原文。</p>\n<h2 id=\"高效率实现\">高效率实现</h2>\n<p>式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。</p>\n<p>另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。</p>\n<h2 id=\"远程衰减的特性\">远程衰减的特性</h2>\n<p>至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。</p>\n<p>直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。</p>\n<p>回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个\n<span class=\"math inline\">\\(\\theta\\)</span>\n的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。</p>\n<p>证明过程这里就偷偷懒略过了，具体可以看<a href=\"https://arxiv.org/abs/2104.09864\">Roformer的论文</a>或者<a href=\"https://spaces.ac.cn/archives/8265\">苏神的博客</a>。</p>\n<p>当 <span class=\"math inline\">\\(d = 128\\)</span>\n时，画出来的图像如下</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"远程衰减\">\n<h1 id=\"小结\">小结</h1>\n<p>总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130<br>\n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265<br>\n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n【4】十分钟读懂旋转编码（RoPE）\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":14264,"excerpt":"","more":"<p>最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。</p>\n<h1 id=\"关于rope\">关于RoPE</h1>\n<p>RoPE（Rotary Position\nEmbedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u><strong>以绝对位置编码形式实现的相对位置编码</strong></u></big>，兼顾了模型性能和效率。</p>\n<p>2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。</p>\n<p>苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。</p>\n<h1 id=\"以绝对位置编码的方式实现相对位置编码\">以绝对位置编码的方式实现相对位置编码</h1>\n<p>前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？</p>\n<p>先说原因：</p>\n<p>在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。<br>\n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。<br>\n而使用相对位置编码则<u><strong>更容易外推</strong></u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。<br>\n但是传统相对位置编码的实现相对<u><strong>复杂</strong></u>，有些也会有<u><strong>计算效率低</strong></u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u><strong>线性注意力</strong></u>计算法模型中。<br>\n总结来说，就是绝对位置编码<u><strong>好实现</strong></u>，<u><strong>效率高</strong></u>，<u><strong>适用线性注意力</strong></u>，而相对位置编码<u><strong>易外推</strong></u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。</p>\n<p>下面简单回顾一下绝对位置编码和相对位置编码。</p>\n<p>（对位置编码比较熟悉的朋友可以直接跳到第3节。）</p>\n<h2 id=\"绝对位置编码\">绝对位置编码</h2>\n<p>先回顾一下带绝对位置编码的self-attention。</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span> 和 <span class=\"math inline\">\\(x_j\\)</span> 分别是位置 <span class=\"math inline\">\\(i\\)</span> 和 <span class=\"math inline\">\\(j\\)</span> 的输入，<span class=\"math inline\">\\(p\\)</span> 是对应位置的位置编码向量。</p>\n<p>这里的位置编码<span class=\"math inline\">\\(p\\)</span>可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量\n<span class=\"math inline\">\\(x\\)</span> 和位置向量 <span class=\"math inline\">\\(p\\)</span>\n相加即可，相比attention中的softmax计算，element-wise\naddition操作的计算量非常小，是可以忽略不计的。</p>\n<p>大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把\n<span class=\"math inline\">\\(x + p\\)</span> 变成 <span class=\"math inline\">\\(x * p\\)</span> 这样，效果上也是大差不差。</p>\n<h2 id=\"相对位置编码\">相对位置编码</h2>\n<p>在绝对位置编码中，可以在输入阶段就把 <span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(p\\)</span>\n直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。</p>\n<p>比如“我”这个词放在位置1时，形成一个 <span class=\"math inline\">\\(e_1 =\nx_我 + p_1\\)</span>\n这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量\n<span class=\"math inline\">\\(e_8 = x_我 + p_8\\)</span> 。两个向量 <span class=\"math inline\">\\(e_1\\)</span> 和 <span class=\"math inline\">\\(e_8\\)</span>\n虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u><strong>耦合</strong></u>在一起共同构成了一个完整的输入。</p>\n<p>直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。</p>\n<p>扯远了，现在回来看一下相对位置编码。把公式（1）中的 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span>展开来</p>\n<p><span class=\"math display\">\\[\\begin{align*}q_1k_j^\\top&amp;=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p>和位置相关的有 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n和 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 两项。</p>\n<h3 id=\"google式\">Google式</h3>\n<p>在最早引入相对位置编码的Google的论文《Self-Attention with Relative\nPosition Representations》中，把第一项 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置\n<span class=\"math inline\">\\(j\\)</span>），把第二项 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 改成和位置 <span class=\"math inline\">\\(i\\)</span>、<span class=\"math inline\">\\(j\\)</span>\n都相关的位置向量 <span class=\"math inline\">\\(R_{ij}^K\\)</span>，于是在这个使用相对位置编码的attention计算中，<u><strong>不再是直接计算input\nprojection的内积来获取权重</strong></u>，而变成</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\n是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n就是可训练的向量或者三角函数向量。</p>\n<p>为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系<strong>理应</strong>更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到&gt;256的长度。</p>\n<p>本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google的方法把 <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n也改成了包含相对位置信息的向量</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> 和\n<span class=\"math inline\">\\(R_{ij}^K\\)</span> 相似，都是一个相对位置向量\n+ clip操作。</p>\n<h3 id=\"xlnet式\">XLNET式</h3>\n<p>XLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。</p>\n<p>在公式（2）的基础上继续展开</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p>把绝对位置相关的几个参数改成相对位置相关的参数，变成：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p>把 <span class=\"math inline\">\\(p_i\\)</span> 变成了两个可训练的向量\n<span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> ，把 <span class=\"math inline\">\\(p_j\\)</span> 变成相对位置向量 <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> 。</p>\n<p>实际实现上可以把 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span>\n后面跟着的矩阵省掉了，去掉这个线性变化不影响 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> 的训练，变成</p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>可以看到，Google式和XLNET式的相对位置编码在权重 <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置\n<span class=\"math inline\">\\(i\\)</span> 、 <span class=\"math inline\">\\(j\\)</span>\n都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。</p>\n<p>当然，也有简单一点的实现，比如T5的方法。</p>\n<h3 id=\"t5式\">T5式</h3>\n<p>公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置\n<span class=\"math inline\">\\(i\\)</span> 和位置 <span class=\"math inline\">\\(j\\)</span>\n的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）</p>\n<h2 id=\"对比\">对比</h2>\n<p>看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。</p>\n<p>公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法</p>\n<p>从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。</p>\n<p>总之在实现方式上和计算效率上，绝对位置编码具有一些优势。</p>\n<p>而在输入输出窗口外推方面，相对位置编码有着天然的优势。</p>\n<p>另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear\nattention方案中去，这个以后再展开讲（又挖了个坑）。</p>\n<h1 id=\"rope的设计思路\">RoPE的设计思路</h1>\n<h2 id=\"保持attention计算形式\">保持attention计算形式</h2>\n<p>回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。</p>\n<p>先说设计思路：</p>\n<p>首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端\n= 内积 +\nsoftmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。</p>\n<p>也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是</p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(q_m\\)</span> 是在位置 <span class=\"math inline\">\\(m\\)</span> 的query向量，<span class=\"math inline\">\\(k_n\\)</span> 是在位置 <span class=\"math inline\">\\(n\\)</span> 的key向量，<span class=\"math inline\">\\(f_q\\)</span> 和 <span class=\"math inline\">\\(f_k\\)</span>\n是分别针对这query和key向量的操作函数。</p>\n<p>我们的任务就是要找到一组 <span class=\"math inline\">\\(f_q\\)</span> 、\n<span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span> ，使得公式（11）恒成立。</p>\n<p>当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？</p>\n<h2 id=\"借用复数寻找组合\">借用复数寻找组合</h2>\n<p>式（11）中， <span class=\"math inline\">\\(g\\)</span>\n的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。</p>\n<p>这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"复数平面\">\n<p>现在考虑query和key向量都是2维的情况，那么可以代入复数的操作<br>\n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）</p>\n<p>那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span> 是 <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> 的共轭复数。</p>\n<p>（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）</p>\n<p>共轭复数是这样的关系</p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>先证明一下这个组合的正确性，是不是真的满足公式（11）。</p>\n<p>（也可以先跳过证明，选择先相信这个组合）</p>\n<p>回顾一下欧拉公式</p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>因为现在我们讨论的是2维的情况，那2维向量 <span class=\"math inline\">\\(q_m\\)</span> 可以用一个复数来表示</p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p>那从复数角度来看，就有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）</p>\n<p>类似地，有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p>和</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p>则有<br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p>用了三角函数和差公式 <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p>再看 <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p>证毕。</p>\n<h2 id=\"旋转位置编码\">“旋转”位置编码</h2>\n<p>发现式（17）可以写成这样</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p>同样地</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p>如果从向量视角来看，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>看式（22）和（23），可以看到等号右边都有</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p>这正是一个二维平面的旋转矩阵。 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span>\n的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。</p>\n<p>这也是为什么叫做“旋转”位置编码。</p>\n<h2 id=\"从2维推广到高维\">从2维推广到高维</h2>\n<p>我们现在已经确认，对于2维的情况，经过 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span>\n这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？</p>\n<p>答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n是的输入向量的维度，由于是两个两个一组，所以一共有 <span class=\"math inline\">\\(d/2\\)</span> 组小旋转矩阵，这 <span class=\"math inline\">\\(d/2\\)</span> 组矩阵为了区分，设计使用了不同的\n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p>那么在实际操作的时候，给位置 <span class=\"math inline\">\\(m\\)</span>\n和位置 <span class=\"math inline\">\\(n\\)</span> 的输入向量分别乘以 <span class=\"math inline\">\\(R_m\\)</span> 和 <span class=\"math inline\">\\(R_n\\)</span>，再进行self-attention，就能获得仅使用相对位置信息编码的效果。</p>\n<p>另外 <span class=\"math inline\">\\(\\theta\\)</span>\n是怎么来的呢？这里是参考了Google最初在《Attention is All You\nNeed》中提出的，这里就先不展开了，可以看看论文原文。</p>\n<h2 id=\"高效率实现\">高效率实现</h2>\n<p>式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。</p>\n<p>另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。</p>\n<h2 id=\"远程衰减的特性\">远程衰减的特性</h2>\n<p>至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。</p>\n<p>直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。</p>\n<p>回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个\n<span class=\"math inline\">\\(\\theta\\)</span>\n的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。</p>\n<p>证明过程这里就偷偷懒略过了，具体可以看<a href=\"https://arxiv.org/abs/2104.09864\">Roformer的论文</a>或者<a href=\"https://spaces.ac.cn/archives/8265\">苏神的博客</a>。</p>\n<p>当 <span class=\"math inline\">\\(d = 128\\)</span>\n时，画出来的图像如下</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"远程衰减\">\n<h1 id=\"小结\">小结</h1>\n<p>总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130<br>\n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265<br>\n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n【4】十分钟读懂旋转编码（RoPE）\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"}],"PostAsset":[{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi.png","post":"clt9sp3kw0001fs4kaeor5ce8","slug":"meta_pi.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_explanation.png","post":"clt9sp3kw0001fs4kaeor5ce8","slug":"meta_pi_explanation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_nosft.png","post":"clt9sp3kw0001fs4kaeor5ce8","slug":"meta_pi_nosft.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_rope_ext.png","post":"clt9sp3kw0001fs4kaeor5ce8","slug":"meta_rope_ext.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/mix_precision_fp16.png","post":"clt9sp3kw0001fs4kaeor5ce8","slug":"mix_precision_fp16.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/rope_matrix.png","post":"clt9sp3kw0001fs4kaeor5ce8","slug":"rope_matrix.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/complex_number.png","post":"clt9sp3l0000kfs4ker9hhx8i","slug":"complex_number.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/remote_attenuation.png","post":"clt9sp3l0000kfs4ker9hhx8i","slug":"remote_attenuation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/rope.png","post":"clt9sp3l0000kfs4ker9hhx8i","slug":"rope.png","modified":1,"renderable":0}],"PostCategory":[{"post_id":"clt9sp3kw0001fs4kaeor5ce8","category_id":"clt9sp3ky0003fs4kgnev70s8","_id":"clt9sp3l0000cfs4k1mre6xfc"},{"post_id":"clt9sp3kw0001fs4kaeor5ce8","category_id":"clt9sp3kz0007fs4k5g3ogrbq","_id":"clt9sp3l0000dfs4kd4ql8jq2"},{"post_id":"clt9sp3kw0001fs4kaeor5ce8","category_id":"clt9sp3kz0009fs4kdp7g5s70","_id":"clt9sp3l0000ffs4k4yon6lwy"},{"post_id":"clt9sp3l0000kfs4ker9hhx8i","category_id":"clt9sp3ky0003fs4kgnev70s8","_id":"clt9sp3l0000mfs4kgnb43ht6"},{"post_id":"clt9sp3l0000kfs4ker9hhx8i","category_id":"clt9sp3kz0007fs4k5g3ogrbq","_id":"clt9sp3l0000ofs4kemue0lxn"},{"post_id":"clt9sp3l0000kfs4ker9hhx8i","category_id":"clt9sp3kz0009fs4kdp7g5s70","_id":"clt9sp3l0000pfs4khzhfegw7"}],"PostTag":[{"post_id":"clt9sp3kw0001fs4kaeor5ce8","tag_id":"clt9sp3ky0004fs4k3mm7bdeh","_id":"clt9sp3l0000efs4kcqxpe2ea"},{"post_id":"clt9sp3kw0001fs4kaeor5ce8","tag_id":"clt9sp3kz0006fs4kg9mzfld9","_id":"clt9sp3l0000gfs4k1nel9bid"},{"post_id":"clt9sp3kw0001fs4kaeor5ce8","tag_id":"clt9sp3kz0008fs4ke6n7hv8t","_id":"clt9sp3l0000hfs4kgk5qax16"},{"post_id":"clt9sp3kw0001fs4kaeor5ce8","tag_id":"clt9sp3kz000afs4kexfwhjt1","_id":"clt9sp3l0000ifs4k87h71y8q"},{"post_id":"clt9sp3kw0001fs4kaeor5ce8","tag_id":"clt9sp3kz000bfs4kho5ja1xe","_id":"clt9sp3l0000jfs4kgeux321a"},{"post_id":"clt9sp3l0000kfs4ker9hhx8i","tag_id":"clt9sp3ky0004fs4k3mm7bdeh","_id":"clt9sp3l0000qfs4k7zjpbyjb"},{"post_id":"clt9sp3l0000kfs4ker9hhx8i","tag_id":"clt9sp3kz0006fs4kg9mzfld9","_id":"clt9sp3l0000rfs4k1g1s6jl7"},{"post_id":"clt9sp3l0000kfs4ker9hhx8i","tag_id":"clt9sp3kz0008fs4ke6n7hv8t","_id":"clt9sp3l0000sfs4kgs4a7osn"},{"post_id":"clt9sp3l0000kfs4ker9hhx8i","tag_id":"clt9sp3l0000lfs4ke0nq7rei","_id":"clt9sp3l0000tfs4k8gvo3o5x"},{"post_id":"clt9sp3l0000kfs4ker9hhx8i","tag_id":"clt9sp3l0000nfs4kf0mm8t3x","_id":"clt9sp3l0000ufs4k6j3yh2a1"}],"Tag":[{"name":"NLP","_id":"clt9sp3ky0004fs4k3mm7bdeh"},{"name":"LLM","_id":"clt9sp3kz0006fs4kg9mzfld9"},{"name":"transformer","_id":"clt9sp3kz0008fs4ke6n7hv8t"},{"name":"长上下文","_id":"clt9sp3kz000afs4kexfwhjt1"},{"name":"窗口外推","_id":"clt9sp3kz000bfs4kho5ja1xe"},{"name":"positional encoding","_id":"clt9sp3l0000lfs4ke0nq7rei"},{"name":"RoPE","_id":"clt9sp3l0000nfs4kf0mm8t3x"}]}}