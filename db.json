{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","path":"css/noscript.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/config.js","path":"js/config.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","path":"js/comments.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","path":"js/pjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","path":"js/schedule.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","path":"js/third-party/addtoany.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","path":"js/third-party/tags/wavedrom.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":1,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/images/cover.png","path":"images/cover.png","modified":1,"renderable":0},{"_id":"source/images/qrcode.jpg","path":"images/qrcode.jpg","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-gpxpg3.png","path":"images/background/wallhaven-gpxpg3.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-2ywymm.png","path":"images/background/wallhaven-2ywymm.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-x636oz.png","path":"images/background/wallhaven-x636oz.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-p97q73.png","path":"images/background/wallhaven-p97q73.png","modified":1,"renderable":0},{"_id":"source/images/avatar/Picasso_Elephant.png","path":"images/avatar/Picasso_Elephant.png","modified":1,"renderable":0},{"_id":"source/images/avatar/shadow.png","path":"images/avatar/shadow.png","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","path":"images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","path":"images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","path":"images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","modified":1,"renderable":0},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","path":"images/avatar/20180303210737_XsJVr.jpeg","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/about.txt","path":"images/favicon/favicon_io/about.txt","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","path":"images/favicon/favicon_io/android-chrome-192x192.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","path":"images/favicon/favicon_io/android-chrome-512x512.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","path":"images/favicon/favicon_io/apple-touch-icon.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","path":"images/favicon/favicon_io/favicon-16x16.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/site.webmanifest","path":"images/favicon/favicon_io/site.webmanifest","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","path":"images/favicon/favicon_io/favicon-32x32.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon.ico","path":"images/favicon/favicon_io/favicon.ico","modified":1,"renderable":0}],"Cache":[{"_id":"node_modules/hexo-theme-next/_vendors.yml","hash":"4f6046ceb1470be9ff334ede20b73871c951d845","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/.DS_Store","hash":"1d67a44d93a3429d76ad084bde035dc4f20e3100","modified":1715072325708},{"_id":"node_modules/hexo-theme-next/package.json","hash":"4b48877b223ec717e708540a2df03d64983c02ab","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/README.md","hash":"d6820f46d03a93bd6dc8b10f49f58aec82ad2b06","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/source/.DS_Store","hash":"8b400bb5f7b29cca6335dd6ab550d517d0597767","modified":1715072325717},{"_id":"node_modules/hexo-theme-next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/_config.yml","hash":"255c963c680da5da34c259c560dd8211b75188ca","modified":1708604632809},{"_id":"node_modules/hexo-theme-next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/languages/bn.yml","hash":"564bed75da6e05b11dce6164508f97a15e2fb6c2","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ar.yml","hash":"7d0f39e8684284a04bb9808521c87fecda8bd131","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/languages/es.yml","hash":"dffc63ef42e1266b88e0acf08994fd17a9908d53","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/de.yml","hash":"79b37df731c29665dee6cd7c90d278e1edfb6e24","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fr.yml","hash":"8ac44e58f71a38b7697a2f7f98a6971ed818cb5b","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fa.yml","hash":"f3ffc444599f4ac92d62e9ed00a1490ebc277d70","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/eo.yml","hash":"e34bb33ae827bf2f0727088599a73bc64bdad1b0","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/id.yml","hash":"929df147f4f17d638b07de5fe52ca13e2549ab1c","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/it.yml","hash":"16d716ecfd748def2f6486ef5a82d0ab7ceb4890","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/scripts/.DS_Store","hash":"f185aa14c1236b351528993f4337fe74ba8c2af2","modified":1715072325671},{"_id":"node_modules/hexo-theme-next/languages/en.yml","hash":"ba0fd79a2b1d8db01a034180556061745965ff05","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ko.yml","hash":"d345a303310c8a5f4836c3683f3580f861ebd1b4","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ja.yml","hash":"543222bfc516aab6c33e8534f807972ecb8943a9","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/nl.yml","hash":"3cb3687696635ec71b4ca40c5fc43b56acc8843e","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/pt-BR.yml","hash":"76b8576ce228d540a16b1f0af5af2cce20923194","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/pt.yml","hash":"70de366e10ea584ba039d40d6b35ac97f93454ad","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/si.yml","hash":"2d712eedf3f60d04d36c3108cf5a12e2a52e875c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/th.yml","hash":"6829e998b39f8f143e20b276bb1f62d95a29de58","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/uk.yml","hash":"ff537047b4b4c3ca9a7b64fa7f428a9942751eeb","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/tk.yml","hash":"511726054873f6f8d7ce0d2e803f6731de0ddbe7","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/ru.yml","hash":"c6d8de0ff7d8148d09993257cfd3b7aca755696c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/zh-CN.yml","hash":"741d7efe0262c9cdc2c648014b55599665d90f6b","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/tr.yml","hash":"a57e4ed089b893a95f5e1ecff17ce625165f4d46","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/zh-HK.yml","hash":"88ea50eeb9097ab4a87a44981a102d8594feb064","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/layout/_layout.njk","hash":"fc0a45112f2dcfc2642404e8934ea32a793c3bd7","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/languages/vi.yml","hash":"7ebcba5e1128784195e4681dffc9d34c4e873fec","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/page.njk","hash":"b0660b2af0ac7d3fda14ca4d9f2c9e79ef06c6f9","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/.DS_Store","hash":"a2cb6a68d26ca70f79d97ee70364cdebc56f5b79","modified":1715072325706},{"_id":"node_modules/hexo-theme-next/scripts/events/index.js","hash":"bd9ea82376cd87df611ea3ae077875c7c595a3df","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/layout/post.njk","hash":"0bfce9f133f501a9a4837257e3b862b3bbca15be","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/languages/zh-TW.yml","hash":"4695c87d6b81b3a23d16ad6513d9eaa925f8d8ad","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/scripts/filters/post.js","hash":"fdc8a0af90035e89c3fcb754a0eb189b8951a2bc","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-config.js","hash":"ead37e9167b682f1fa34b5401c3050e18c7ee4a3","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/filters/minify.js","hash":"447db39d17775b2bd18d8af9c9d65b7b8449f751","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/helpers/font.js","hash":"3394185a7f0393c16ce52c8028f90da3e9239c55","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/helpers/navigation.js","hash":"78107021101553c3d23e89290f7530b60cf4aa86","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/filters/locals.js","hash":"9eb5310664759931287dd28ea39165dfb67f12ed","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-paginator.js","hash":"e86c764b546e4fbb87970cabc4135a56f9ef9fe1","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-vendors.js","hash":"957241c28796ff352de7f4cffba7bb289b043586","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/engine.js","hash":"d292b78485e8e8055712b0ed6de7cf559c5fbdcd","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/group-pictures.js","hash":"9ed799c329abf830f623689d7e136991256a24ca","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1706697684337},{"_id":"node_modules/hexo-theme-next/scripts/tags/index.js","hash":"1f6aba7820f1fb58b61969485148db21846e1aa9","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/tags/mermaid.js","hash":"4fb01ca650fa8b256b8d48f50dc1b18350bd3d6d","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-url.js","hash":"6281d47c1de98eb38f3aa0f6df29bbb19d412173","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/tags/wavedrom.js","hash":"b44dfeeb58b41945d469141787f3dbce4b117d08","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","hash":"dadc81256afb127b77eac6763d5ee0ec9c77f0a3","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_colors.styl","hash":"3c6798c10cc220d83481cb3f3782e78558cee789","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","hash":"921a58577f411cf4eb5cfd66db0a241f8f88578c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/css/.DS_Store","hash":"31813a1741cf49d75d4d2d99255403d0d3bb7935","modified":1715072325704},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/css/_mixins.styl","hash":"83647a6207333b9609ba90b0946b3fa9548e6381","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1706697684330},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/README.md","hash":"12a3e96581964a22b474cc739675d52ef93ff932","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1706697684350},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"12a6631617695504d5cf2a94b57d87bd331bef6f","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/docs/ru/README.md","hash":"29c89a41b371f893e56c87ea61adabc444ec58cc","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/layout/_third-party/addtoany.njk","hash":"ef64c6bfb8540cd874701236b9be47db2496e98e","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/index.njk","hash":"dfd7cdd6ba89f8c3deabc27726c7a350cadafd11","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/comments.njk","hash":"d0c470b0f6690aa217e9ada848c5e2e73fb27c6f","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_partials/pagination.njk","hash":"bc719473ed5948ab6859449d60b8d36cfc1542b4","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/footer.njk","hash":"d77ec95cfee58b17807763dc2adb7946829cb316","modified":1706757600094},{"_id":"node_modules/hexo-theme-next/layout/_macro/post-collapse.njk","hash":"abda600685ee972e1f6b7a2dcc56f13e2daa6263","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/widgets.njk","hash":"e7f988ecddb2159313699a00827a45eca5622bd4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_macro/sidebar.njk","hash":"547c62ab14d9e05d2d9116db9048a677fbe1fb6d","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/.DS_Store","hash":"168617768f812b394ad2e34bbffa6ca6fb8b2f98","modified":1715072325705},{"_id":"node_modules/hexo-theme-next/layout/_scripts/index.njk","hash":"6668878a0f9a1166c6a879755f54a08d942da870","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/config.js","hash":"9ec51eb61f7fee612ffc5252f489003a0fa301fc","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/highlight.js","hash":"6aec7b2c38c50989a23bfaa0d560e75c7f553e12","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/navigation.js","hash":"dd3562686d95a50375e6fd32e717ccb0d99c1e3d","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/utils.js","hash":"6853e5433e3eaa19ea43fa20b08d956ba4cec4ac","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/layout/_third-party/.DS_Store","hash":"10b75eb755ab25235244a5b6e64dafa853e092f5","modified":1715072325673},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/vendors.js","hash":"464db1e7182e5b9cdbd32e8b5368d5e683b1d9c7","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/changyan.js","hash":"5798cfc8f63665031dd3e01debed051628cec319","modified":1706697684338},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/common.js","hash":"19a402a225c31edffc50f202a14e0d582d3db23e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqusjs.js","hash":"a600a98e7436edeb31e291abca359885567df3c9","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/layout/_macro/post.njk","hash":"cbe208445e4d1df82ebd1761e1eaced3eab77fb3","modified":1706698899947},{"_id":"node_modules/hexo-theme-next/layout/_scripts/vendors.njk","hash":"be80b9fe415a9a09d74c28e230995fd292dfc123","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Mist.styl","hash":"a1418c9dc8c0f1a0ad4ded0f4627c45bf0db1a10","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Muse.styl","hash":"e3be898f5ebcf435a26542653a9297ff2c71aeb0","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/.DS_Store","hash":"cba17e35154c352959c43a31374ed836c96990a4","modified":1715072325721},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/css/_variables/base.styl","hash":"c4fc4e862d09221265ab1466085f057be2ad2e4d","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1706697684332},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Pisces.styl","hash":"48f4f277946a168d0db1ea02804e85c22ca2c7db","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/js/third-party/.DS_Store","hash":"b7a4a8e8ee3bbf1c3d47bc7f5afa02917522698e","modified":1715072325672},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/index.njk","hash":"f900306497b133e8b098bd9f4b96b93d1d96c185","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/matomo.njk","hash":"4e89648a8ec8194c5823064cbca39c938a799006","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/cloudflare.njk","hash":"a5b8297c2c383124dd6a56e256ecc0c0dcf489be","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"9dc00fcb0a05899f048eace9f9160b78956655d5","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/plausible.njk","hash":"ef9f2bb7110507f1c4336800af9157d5fa9765bd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/umami.njk","hash":"3343750682fbd8535e50f8129be3003ad26015b4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/algolia-search.njk","hash":"24ed76e0c72a25ac152820c750a05826a706b6f4","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/katex.njk","hash":"1ebf658690468ea197bdd0416eb7cfa4bd0b083a","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"a4bc501da0f22f7e420f0ca47e83988ce90b1368","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head.njk","hash":"5388b157bba4a40b9312f4a45c6678974ccf0837","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/wavedrom.njk","hash":"02202bf563fb5eedde2ccad4d6c5b9109d30a703","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu.njk","hash":"ee6fc2f111572d3eeab0a2fecbb2d6b3e37ab26b","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/brand.njk","hash":"dd9c4c03e99dfde0dfb8edefcb2c933f2f560efc","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu-item.njk","hash":"41a8b0cc16f60fa085cb719d07216d86b6bc4bf8","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/sub-menu.njk","hash":"06480d8ec5f0b87eafd47f082f07968d7282dd5c","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/breadcrumb.njk","hash":"89825e75cc45e9709fa6ba89883669eedaff6f46","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head-unique.njk","hash":"8da52a144060db1a0a088ccb2e6cc8376d1fce70","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/schedule.njk","hash":"0f4bc8e257da60f77c0c1738607b2bde55810684","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/index.njk","hash":"8f6f256ab3b351ffc80f1f3f1d9834e9a7cfac31","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-followme.njk","hash":"c1e33b4889f75acc490af3c8bde0ec56c518ff41","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-copyright.njk","hash":"bfff923526d6800218f08dba6ce0bbf5c17755fd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-related.njk","hash":"e0986db00a0201dd3c60570f964829c84ba5bc68","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-share.njk","hash":"16696990e4ce65fc8db18c4635082a5d5d06ff07","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_layout.styl","hash":"fa4fd8f76464e214fb7318f325b13c2b62f4b478","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_header.styl","hash":"dafc6d23c80d6fe3e55a7711e94210d2479b629a","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_menu.styl","hash":"fb550935d374e0bdf1097fce187337dc05cad3e1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_posts-expand.styl","hash":"485d23ccb42c0d0c8ead7ea8930dd3e06d79a285","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_header.styl","hash":"3fbfab591f280e2e7f3b0265901c93bc4bd137ed","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_menu.styl","hash":"82cda756f5b7092df2eee6641b9786df71623bdb","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_layout.styl","hash":"6569a6640f79d247a8235b3914772c0e2f99ead2","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sidebar.styl","hash":"547c0b5cd5e7ea10d21863d13a6b16579a49396c","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/layout/_partials/sidebar/site-overview.njk","hash":"78a1a8cac44de7e963ab4cd51c988442eb3e789a","modified":1707031409664},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-reward.njk","hash":"e8b8a7c41e9ec612d0c0c73419529d55d1c16256","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_layout.styl","hash":"26a0cba1eee5de45a45a5e14e17707f905390512","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_menu.styl","hash":"72dc825c50357402c342d62ab60fc0c478ab6bc1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sidebar.styl","hash":"91dbf3ca5c3a613d4e30618c120da535bf2d0336","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top.styl","hash":"7664491542046df9a3887cf40a06e00c0b4086a9","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/base.styl","hash":"d0a7c99095f490b0d2ed6b1be43d435960798cec","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/index.styl","hash":"2298e521253b3bf376a2412271bc2a7d305051f3","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Gemini/index.styl","hash":"9dfe853c901bdc52fc950bacdf15484dbb9bf140","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/pagination.styl","hash":"f4228c759db4a650c8d38745c2edd1dc83c45687","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_header.styl","hash":"ac2dc0ce9c775a83ef7132ae957b54539366ac9c","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1706697684335},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/mobile.styl","hash":"1dbf2c339adcd27026c3a2ded32ee91ce08cea26","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/toggles.styl","hash":"782ee1fc5e669d3ddbfeb82b73ad7fe561f1a4fb","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/.DS_Store","hash":"7dc4c0e066c7f2249dc291afac6065e647136cc8","modified":1715072325686},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1706697684333},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/categories.styl","hash":"b6e2eb1550a7845cb2adf86081a4ab6c7bde1e68","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/disqusjs.styl","hash":"877a537d5b95beb048142e4fdee6f17e6ef9c7bb","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/index.styl","hash":"54d12e2c5d9982f7b9e5b23be5133954a8514e9d","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f634f94828620e88c3f5a8db56f7944f6ba232b0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/search.styl","hash":"e72799ce3f9b79753e365b2f8c8ef6c310668d4a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitalk.styl","hash":"8f094c4ac17e2ab45569b12d157747f9c7333c12","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/fold.styl","hash":"42a0b65491ad85438596b3fe0b7f23973e4cef34","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/index.styl","hash":"138f78147bc6bd6005f329ada34dc79b7625542d","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/index.styl","hash":"22cd37bd5df9972d5074710896aba4424ad5161c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7f8a7345e6537a62cd9e9a94c8f7065b541d9b04","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/tabs.styl","hash":"33dd6ad015dde65fd46f34961655442e8e82b52e","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/wavedrom.styl","hash":"af113411ad9cca7674177be36af8dd399680834d","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/footer/index.styl","hash":"4e967702cf4c637132346bc74ec8854426f1a68c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/index.styl","hash":"6e0d0796ef7fbbb62ffdfb448753a850de82c74f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/note.styl","hash":"98d4c20aff0f0fcfe1824017fb06ab21ef0d218e","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-meta.styl","hash":"a851e9d5aefcd027c95eeb323860b6da70f202d1","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/menu.styl","hash":"bbbc40b03cb299d2a6a568f329b2ce98e1cdc430","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/index.styl","hash":"da5e88f8debd5ac8d7af5c6ba6240df66104955f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b05908f04ef95f2d91e6eba89b12411c378d050f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"0847400d8579b0a2dd1bf662c78954c10adf2680","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"46eece42510c2c89bb9209afb0262ad76a4b0b36","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"24752d145c6fb8f5344dca9c7b9640839c02e009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"c6a27beb3f741211a14576026f3b4cfc44cc6407","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"c2e354a565c8c1b32bd0ceacc972b17982758b67","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"9a7c71560fbdc936ad4e736fe15063ea3e8a644b","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/index.styl","hash":"098d4bd034e986fcf7e443eac4fc2193935461b7","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-body.styl","hash":"56d5b7ff73f466c9ae54f7204ae899281295d749","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-followme.styl","hash":"1ecfd64507954810b07a9d21fb5305b5378feda0","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-header.styl","hash":"1191f1bfa5c43e54be8e5b3cc0d802984e161747","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-footer.styl","hash":"11497388f124bfbb4001495a67d3629a9f618405","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-reward.styl","hash":"04cf4a69537fc14d3b8904f965d283356853847f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-widgets.styl","hash":"ebfba158a0a4af3d1dabcacbc58986664de52140","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-collapse.styl","hash":"7369928305330c73ae0b3f063a681a8384d8fde4","modified":1706697684373},{"_id":"source/_posts/.DS_Store","hash":"bfaf2a95124f5039dd32af60f62265454b5b5ac0","modified":1718249984334},{"_id":"source/.DS_Store","hash":"6afae9b06b7081901115a5c8633b527d0d0880b4","modified":1718249984333},{"_id":"source/_data/styles.styl","hash":"f4bb55ef0972c829e3382d1bae1786b3ab5d54ef","modified":1707045638288},{"_id":"source/about/index.md","hash":"9294d008cc673abc2eaf740f101ebac560029267","modified":1706698701349},{"_id":"source/_posts/cs/.DS_Store","hash":"526610f7d7657b4c34e9c00bce8fa37e0ea829b6","modified":1718249984336},{"_id":"source/tags/index.md","hash":"e995ed2b8452b1906600b3853b920f13423098b7","modified":1706698644396},{"_id":"source/categories/index.md","hash":"f5c920fbc09ea3d8edf250de7e31bcc6b3e765ae","modified":1706698717077},{"_id":"source/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1706872557451},{"_id":"source/_posts/cs/nlp/.DS_Store","hash":"06ef8762208e6814779bf78615579b8ce810d475","modified":1718249990857},{"_id":"source/images/cover.png","hash":"2f2aa6173619dd38425673ba110b50b9156d4d10","modified":1710684380714},{"_id":"source/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1707548301740},{"_id":"source/images/background/.DS_Store","hash":"88f5d31d0db89adcf679f2a7fefc8947139a1c1f","modified":1709026807046},{"_id":"source/images/.DS_Store","hash":"e08e12a8507734ef2fe42991af2aa57b0860cc60","modified":1715072325720},{"_id":"source/images/avatar/.DS_Store","hash":"c3fa37607ceb3f7ba411cf4203d2a333f773d921","modified":1707118756919},{"_id":"source/images/favicon/.DS_Store","hash":"83ddccadffca5384db3dfc167728b7c7cacd9a87","modified":1707796842439},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1707030615190},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1707048498957},{"_id":"source/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/02/.DS_Store","hash":"ab55fc2d73f2238323a1a5d7b8362d5620868275","modified":1715072325702},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE.md","hash":"dbe2c4a5a96fe27437a53c4fa9054bb19e05f28a","modified":1712299467271},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题.md","hash":"dd73e47b1cdb864f26d5780ac4fe08603bcc9b3c","modified":1710314618942},{"_id":"source/_posts/cs/nlp/2024/04/大模型算法题-3.md","hash":"b56b2454ac63f79df92591824dda52efb8084e00","modified":1715323861150},{"_id":"source/_posts/cs/nlp/2024/04/从实现看normalization-到底干了什么.md","hash":"0855c9b52c67a72d809c3f53240f4b62bb99de79","modified":1715323869591},{"_id":"source/_posts/cs/nlp/2024/04/大模型算法题-4.md","hash":"419c28f934cfe340a4ca4751668fe1512a26ab57","modified":1715323846826},{"_id":"source/_posts/cs/nlp/2024/04/.DS_Store","hash":"327e321d722e88824dd22a5f789f1eaa20bc42e9","modified":1715351302735},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM.md","hash":"30dfb09aaa495ff30731cdc6d9d1b9a9afcfa7d6","modified":1719202196529},{"_id":"source/_posts/cs/nlp/2024/06/RoPE的远距离衰减.md","hash":"e95970adeaae06307b574bc40ebb9f41df24e05e","modified":1719370756930},{"_id":"source/_posts/cs/nlp/2024/06/.DS_Store","hash":"cc8d50b790377630beda1bed5c9f871cc2e0a6df","modified":1719383816017},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力.md","hash":"3250729eaff9712b7a659248c72e7735f60a4060","modified":1718529415156},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象.md","hash":"fdf118d5039194c8e5ca193612ccd6b3fa0df0b3","modified":1718717773750},{"_id":"source/_posts/cs/nlp/2024/06/大模型偏好对齐-IPO.md","hash":"83e10e034aefd8cd0cff53397ee2e57b005d252d","modified":1717685520000},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE.md","hash":"1d269c5349c126573508a86948dac1b422dfa95a","modified":1719406155240},{"_id":"source/_posts/cs/nlp/2024/06/大模型算法题-7.md","hash":"78da9544c79d8aa1e4e8f1f9d0e315c405650917","modified":1718202061594},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA.md","hash":"da0dfbe26455cbd782985690ed8503cb54ad40d9","modified":1718439564061},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型.md","hash":"63e30d0920f45051f9532e8bb37c608f029117f6","modified":1717591310292},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO.md","hash":"7c876990c33a7d7d43fe3e58ed0972d85cd74611","modified":1716985993225},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO.md","hash":"77ba73314fb67af0807703ad0dc91db8724b84f3","modified":1717158416345},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO.md","hash":"39da16332aee228e964f83135764b8857bc81b04","modified":1717300939947},{"_id":"source/_posts/cs/nlp/2024/05/.DS_Store","hash":"2d2a0a1f4a33aaa8460628a46d05494760cd6cc7","modified":1717751195197},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础.md","hash":"5a20cdecb741dac615b82bf8f603fa13b2136d70","modified":1716608290437},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大.md","hash":"9a46f37e067d004004a3e36f4f4d7cee608ad22b","modified":1715591178036},{"_id":"source/_posts/cs/nlp/2024/05/大模型算法题-5.md","hash":"ca99e9196eaf8743885528f6e6d6404bd8cf34ee","modified":1715323819009},{"_id":"source/_posts/cs/nlp/2024/.DS_Store","hash":"e36c337f1c4083b76b69bbfacf27a5e6c3efe40e","modified":1719383815410},{"_id":"source/_posts/cs/nlp/2024/05/大模型算法题-6.md","hash":"b76808355cc79f0f22debf8717a9002404c4e988","modified":1715686009973},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力.md","hash":"dc51b82a33b114c119bf88dab64733912bf050f2","modified":1715323820731},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节.md","hash":"1871881f3afdaf9b2930d31a03d62079ca4ff9db","modified":1711713217115},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention.md","hash":"31d3d5cbbde092ba1a855319647b24e32ffda8ef","modified":1710934710908},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization.md","hash":"39dbb9473a0afe9654512f34be923e0210d5e1a0","modified":1712471774847},{"_id":"source/_posts/cs/nlp/2024/03/.DS_Store","hash":"de21b2ce8789af4981bcb8ca056033d4b22b7716","modified":1715072325687},{"_id":"source/_posts/cs/nlp/2024/03/大模型算法题-1.md","hash":"f25140aae947e215b4bf597973bf28a52bce575a","modified":1710685009511},{"_id":"source/_posts/cs/nlp/2024/03/大模型算法题-2.md","hash":"d8b936ccac17c2d991f3894335231adb87aaabc9","modified":1711253769176},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们.md","hash":"9af08ff8d7274b005502eba5fb0461ea1a0729d9","modified":1710646109054},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA.md","hash":"bdd2ea9b7abe82b13ace07dff7be5b2a757db7f2","modified":1714231402122},{"_id":"source/_posts/cs/nlp/2024/04/从实现看normalization-到底干了什么/.DS_Store","hash":"96243fa0e625d8d7395157516c6299723b4ce769","modified":1712575638552},{"_id":"source/_posts/cs/nlp/2024/04/从实现看normalization-到底干了什么/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1711118594120},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1708758408339},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/.DS_Store","hash":"2e33b8d145af72ec31cbad8c19fc2528fc2a909f","modified":1709014663934},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1709188879237},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/.DS_Store","hash":"048e5b4d3acc0214cfb4641afefad1f2c57af2b1","modified":1718528197082},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/.DS_Store","hash":"5353920a8962599ceb60f96de4bf901e325f55de","modified":1719106582974},{"_id":"source/_posts/cs/nlp/2024/06/RoPE的远距离衰减/1.png","hash":"cb86a4ce4589b590aa1b220dbdf32f9db559a1b8","modified":1719315883922},{"_id":"source/_posts/cs/nlp/2024/06/RoPE的远距离衰减/2.png","hash":"8c8c02a78f8e874162574caa267c32f469665110","modified":1719316049637},{"_id":"source/_posts/cs/nlp/2024/06/RoPE的远距离衰减/4.png","hash":"491537b72fcab5676e6322c47d9ae7e1054f3387","modified":1719317189348},{"_id":"source/_posts/cs/nlp/2024/06/RoPE的远距离衰减/3.png","hash":"8bff531083b17baa6de7d44ecdf464c29d175a31","modified":1719316336855},{"_id":"source/_posts/cs/nlp/2024/06/RoPE的远距离衰减/5.png","hash":"982701399546708e62d974938e35f17a6aa5abb5","modified":1719317449536},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/.DS_Store","hash":"3c56377d253e8212100ccda316b258f134ef0f2c","modified":1718770249764},{"_id":"source/_posts/cs/nlp/2024/06/大模型偏好对齐-IPO/.DS_Store","hash":"714de020e255973703c8f95f272f7aea6b76cdb1","modified":1717420106364},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/.DS_Store","hash":"f19c5b5f3d05fd26a1348a12500b67efda4b3802","modified":1718367083334},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/.DS_Store","hash":"ae0d2ae0ce4e9a23c90d2154831259d19e4e1685","modified":1717593293091},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/.DS_Store","hash":"ef25e9af6860a6c162c9290b57f27b77606c64e9","modified":1717053157303},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/.DS_Store","hash":"cee7f0dd4ed83bbcfac552e3b170580975b1bf69","modified":1717420098368},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/contingency_table.png","hash":"94b25e2d4803d9802d3c5455aed84911fe506089","modified":1717233039090},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/reward_accuracy_compare.png","hash":"0b5526d61a1cbb3188e8e53ea858b1d9d1953660","modified":1717259353701},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/.DS_Store","hash":"ff49d0cafe604c7a0b14ed0b6d60dfd2cb5edac7","modified":1717219419238},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/.DS_Store","hash":"c5b11ad717ef8750b621f8ccfb2e430289158592","modified":1719391077349},{"_id":"source/_posts/cs/nlp/2024/05/大模型算法题-5/.DS_Store","hash":"b4bd95f9044124fd6715a9d1baf9ee4858fdc14b","modified":1714914202734},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/.DS_Store","hash":"816910ea2f357470b3d5c3908437853f6d744a13","modified":1716541217062},{"_id":"source/_posts/cs/nlp/2024/05/大模型算法题-5/yarn.png","hash":"ee124a0823b429842082acebe78a7162915cc11c","modified":1714809224728},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/.DS_Store","hash":"a1143c0efa86d8f8e5cb1ddbbb0932d0adb854d9","modified":1715070961565},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/formula.png","hash":"65fe200098b51d1712b6c38d039aa8be22d38e82","modified":1716453725490},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/.DS_Store","hash":"24df341533adcab1c2da9cc93f748f972ed4e580","modified":1715591105150},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_attention_entropy.png","hash":"be91b6a49cfe30dd51ed4f8eb258eb4715a70e37","modified":1715176570143},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_attention_logits_explode.png","hash":"3cc54ee973126c1ca3bcd85d75039e490efc9acf","modified":1715175378314},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_middle_k.png","hash":"869d4b687714409a5a4b89c85dc5ee2c1f0c2c86","modified":1715240601309},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_starting_tokens_num.png","hash":"6a2c841a3d3fd354f57757aa7e663e83585545d6","modified":1715180059874},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/.DS_Store","hash":"f036d46924a246fd06315b601bca6cc759d95300","modified":1710600789234},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1710560488146},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1710250364698},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/.DS_Store","hash":"5d08a407b858db4f8a5bad5168cbb23224622856","modified":1714982537175},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1710321816411},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1711118594120},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1711247018739},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/bn_ln_gn_in.png","hash":"9783c818f5e0eaea33d169718476bbe8874cf945","modified":1711120826525},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/lossfunc_surface.jpeg","hash":"c78a8df335da0d507963fb73a62fe2c3d145c91f","modified":1711005649925},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/rmsnorm.png","hash":"55bbcb42145011f7b5adf90cc613e22e2b94f060","modified":1711165884464},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/realformer_attention.png","hash":"e9a92e5c07c8ca6873ea70671ad54eb2f1a13332","modified":1711206279560},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/.DS_Store","hash":"81b22bc167ab6b1514c4698d3c49fcf0836924dc","modified":1710670459475},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/.DS_Store","hash":"9d409e9dac238eb07cc6841f8e8d05eed83df842","modified":1710056957220},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1709973970557},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1709723125449},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1709780575011},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1709716888116},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1709716894560},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1709781776387},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1709821278308},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1709716876496},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/.DS_Store","hash":"f40652099a252049ed00339c05066bc05635ded0","modified":1714093902405},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/qwen1.5_moe_tps.png","hash":"5478a6583a6c6fb68f1bc9429c103e84fe39efaf","modified":1713691310250},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/softplus.png","hash":"bdc66c39227441390f2241b4f26c0b1fbab331d9","modified":1713279943448},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_round_error.png","hash":"0172ba008837b3490a3e456306aa72be65636d90","modified":1713862612528},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/xiaomi_moe.jpg","hash":"d898ba33f1ee70efa136dbff3cb38983b461524f","modified":1711814228139},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1707048415396},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇.md","hash":"9769c51d515f2aee1cd86930faf7ddb9cc80f525","modified":1715323896425},{"_id":"source/_posts/cs/nlp/2024/04/从实现看normalization-到底干了什么/cv_layernorm.jpeg","hash":"f0874ecc4b9d8da8bf3bff0e13a6313ed19a7b15","modified":1712494304517},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1708957811757},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1709262766062},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1709199016415},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/eng_data.png","hash":"3d16e16eb4a57436ff9d2c0b64e7042944dbfab8","modified":1718524164949},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp1_param.png","hash":"5f940193d379f18bb36ac38f2bfa5d1aaef65b15","modified":1718508399845},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/2_stage.png","hash":"ca7a6d52aa5dbf3afa1e00de6548386ad0ea738b","modified":1718955041160},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/batch_size_2.png","hash":"d2f8c192c90acd0ccba2ec814bd36fc2e646e6af","modified":1718800118846},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/cos_lr.png","hash":"951b0fc36a8b0e7b9c58a39345950546f090fb3a","modified":1718801929626},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/5.png","hash":"8fa2b214c490a5262951301bbd4d0510d5199b3c","modified":1718713696291},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/ditto_1.png","hash":"bfe3f56dee05d35b5b816a356ec7688d81c33b92","modified":1718706809378},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/ditto_2.png","hash":"abd8e2173d83ff20cc0adc636163a97f7fc46de2","modified":1718707274921},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/model_param.png","hash":"a053d5204998fb025425706c16409d01f6589548","modified":1719386283837},{"_id":"source/_posts/cs/nlp/2024/06/大模型偏好对齐-IPO/curve.png","hash":"f81e68f050653f66ca087c2f13a222aca426384a","modified":1717420081841},{"_id":"source/_posts/cs/nlp/2024/06/大模型算法题-7/lora.png","hash":"c044aa39cff915b64ee576c558756cb1093e5075","modified":1718201952288},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/speed.png","hash":"402da175e2e365db3cc0e6c377b89df5b8615b6c","modified":1718362366610},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/gate_dist.png","hash":"912b7bf9c26345d2c1513050ac484efde65580b2","modified":1717555982403},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/lr_exp.png","hash":"32b3dda778f6dd42388ec8ad6f8782738068f54f","modified":1717576006119},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/gradient.png","hash":"0b72939cfd4770fc21727bb203efb9c5dd2491d1","modified":1716907479268},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/result_3.png","hash":"df5861c846176c90bd9a90bd5836919ef023b13e","modified":1716985497709},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/dpo_correlation.png","hash":"2c1dafd42b7ffa3318395a4934df87692ba5fd62","modified":1717259082807},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/reward_accuracy.png","hash":"7fb3d4dd64e3013534bd77eb1f2def23ec57c8cd","modified":1717257132254},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/simpo_contingency.png","hash":"773700b1d041aba8b3912deee9c8bc7886fec099","modified":1717259285268},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/simpo_hyperparameters.png","hash":"f976162c0882c49b43b503beb1384b447e2d5d00","modified":1717246443198},{"_id":"source/_posts/cs/nlp/2024/05/大模型算法题-5/bfloat16.jpeg","hash":"8678b705b0d6e0b7deb230bf28f2d92ce0d42088","modified":1714809868152},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/acce_draft_model_param.png","hash":"2e5b1852eaf4745f3d9bfc9b0fcccbd37621bb93","modified":1715691438739},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_choose_gamma.png","hash":"65be032bf276290ca97b7d983bc4e1e2deaa95fc","modified":1715673837432},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/add_money.jpg","hash":"0b00f9f1dd128e5601f0c7502dd2cf9233898f0f","modified":1714982519785},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_speed_and_op_table.png","hash":"416238792292bff7178830267d53941da202eadc","modified":1716471229174},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_ppl_200m.png","hash":"7b232b7bf3c7238836f71b4b99e492d9f6c285f0","modified":1715241848803},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/stremingllm_kv_cache.png","hash":"11b09e96662feb7cc246e60e1b21d7ffceb47ae6","modified":1714393259685},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1710561804292},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1710255091449},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1707048511782},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/model.png","hash":"631efc6d4e92da128d7a10a4dc6af307ee4ddcbf","modified":1711459921302},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/third_party.png","hash":"673fe2b2cad3b1f40c0fcfd190c0034d8dbe7f31","modified":1711615706510},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/ellipse_2.png","hash":"ee20ecce8c3470d17b1a4bd43df811d16269ffd3","modified":1711006599335},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/prmsnorm.png","hash":"d5826342f665f4cb04fdbb2e3d83e0b2607355c9","modified":1711166590963},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/postnorm_prenorm.png","hash":"d8830735e89c73ca416baabf4a195d7891d9f0ed","modified":1711167201092},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/sigmoid.png","hash":"f1ced5f06861a2e0296050aff17eebfe3d023a6f","modified":1711246985230},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1709637863252},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1709971938995},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1709983486925},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_long_perf_2.png","hash":"2b6be0099f1eeb0ee2d2056eae7cf541e146b636","modified":1713699160099},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_train_efficiency.png","hash":"c117407ada2adab8d97250268e2eafa533bb9083","modified":1713699487306},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/qwen1.5_moe_params.png","hash":"93d14a2645969b08a4fb80a31aa75fd8e5201ff8","modified":1713691207069},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/qwen1.5_moe_perf.png","hash":"b79ad1a909081fd0537bd9d44cfac2dc2133de6c","modified":1713691095074},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/modular_connectionist.png","hash":"b5865cf34faba075b4f2316c2cae0559dac2d883","modified":1711981604894},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe_load_function.png","hash":"644684f21f85d565328d98334d41bbe019acbbfc","modified":1712050094734},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_more_add_bias.png","hash":"f0de5347918e4928dbcbc59a897c3f3227c3d30f","modified":1713856613597},{"_id":"source/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1707118741657},{"_id":"source/_posts/cs/nlp/2024/04/从实现看normalization-到底干了什么/cv_batchnorm.png","hash":"d9e8d897c36125fddcf1cbcfa5c237a37158a939","modified":1712503708413},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1708958930811},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/mistral_8_7b_perf.png","hash":"f90d9ff5b14326b0eef2a0026b3f5940e0d42f0a","modified":1713700127874},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/exp_model.png","hash":"39ea7fc867afbeeaa1e764513ca21019f4076cc3","modified":1718958829569},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/layers.png","hash":"fc52ceb86ba2af1a3898f7afc18ab138d63c529d","modified":1718955962053},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/learning_rate.png","hash":"e53cbb5a0d395e77bdc09779eb003134b7bbbe18","modified":1718801481150},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/data.png","hash":"86cde89044da233481994e8728ccb256146b3ae6","modified":1718956268617},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/moe_result.png","hash":"e31fb8b893615de6f8a46c79d4cc7b813c7e9509","modified":1718958779005},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/tokenizer.png","hash":"c4e4116baccb9bdf88048b5e38827937ad48c045","modified":1718955432935},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_exp2.png","hash":"86256736d0e7b22c1f57778d3973202e4ac00f69","modified":1718874313485},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/diff_dense.png","hash":"541feaefbc8790820c45a5bf0317e69c69087c24","modified":1717576882837},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/100B.png","hash":"c9cc7f93992b0288d219a5926ba764860ed40c76","modified":1717509019859},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/structure.png","hash":"b71c6bfe51757237c56124d801bf0409c5d34b19","modified":1717506299219},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/margin_dist.png","hash":"cf82f48158e4ba3e503cbe25cc811910804489cf","modified":1717257492586},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/alpha.png","hash":"4e3ad45447f5757d2cfcdf9d9555351233456c3b","modified":1717157185206},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/summarization.png","hash":"4934f3e42f20bc3a44ad07267e91a14c4c005543","modified":1717155934841},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_expected_token_num.png","hash":"52be2409ca9513de2e5ce10a0e77d8aa98dfc328","modified":1715672443581},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_walltime.png","hash":"b645987bec587e743021bc330de277416ef36d5e","modified":1715674129472},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_speed_and_op.png","hash":"9b5e3a6c9276309e7aa5a8848d52ef361e62bb36","modified":1715674286748},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_quality.png","hash":"fab44d68fc27f7bb2c06f758e537b9b249be0699","modified":1714913630381},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_structure.png","hash":"35f958d9ba50460689727c7038bf3a344000fa52","modified":1714288831777},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_downstream.png","hash":"192160ca6972003a61678e2f7f2467f5bbd94451","modified":1715242314359},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_process.png","hash":"259ea90040b7a5b98a27afcb992a6bee31707ab9","modified":1714289830786},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1710560038203},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/9B.png","hash":"7de19972e48b1f43c501d60a1c43a28c46b198e8","modified":1711618166808},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/long_context_result.png","hash":"daea6f734d64bdf5d24c6e17a640f23b1bd35b5f","modified":1711531324567},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1710516051198},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/bn_algo.png","hash":"56f1ab55c0e94814e6e37c30421012ed82098d62","modified":1711028736211},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/deepnorm.png","hash":"4726d8a40d1d0db397005408295e1ba54809a7e4","modified":1711207769375},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/deepnorm_result.png","hash":"138cafc159f1e2e02455c540b4754f7cbb7f521d","modified":1711208592246},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/ics_define.png","hash":"6bf3240ef78bad2cf76897a29c05428f4c195fba","modified":1711116459766},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1709975039443},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_long_perf_1.png","hash":"bc9c40bde860e78882965a25056a848aa4a89c77","modified":1713699132210},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_model_param.png","hash":"7b838274937cf45d73e59ac1fb5c2034e46586ae","modified":1713683464457},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_expert_specialization.png","hash":"64947872486dd083a7076bfdfe67cb0626208579","modified":1712805838590},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_less_activated_expert.png","hash":"ac7ad86dabe94564bdb17399231c6ddc7da83962","modified":1712806182926},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_compare_gpt3.png","hash":"311b21079599473054378e339885e0b87719e63e","modified":1712848303496},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe.png","hash":"2a2ee095b8cc0727daa2cdd0c63891e4a470eae8","modified":1712042970123},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/mistral_8_7b_active_perf.png","hash":"0c67d935657e62b9e8eeabc6403c269e09016626","modified":1713700314192},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe_137b.png","hash":"fc11cfc87b2994956a9adbee76621fe5d964dc30","modified":1713447317172},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_z_loss_result.png","hash":"3edd8e9eb069c9557c98fd21600c02b3a1978cc5","modified":1713858286214},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_remove_multiplications.png","hash":"52bc94bfe726dfc832534dde409154efe0ce7b0f","modified":1713856140610},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_more_add_noise.png","hash":"12d13a9ee6c28553626e469ed18d022e0176a873","modified":1713856976557},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/vanilla_moe.png","hash":"7da0a6e9d529256107b5f6b287737ac47513a797","modified":1711962655018},{"_id":"source/_posts/cs/nlp/2024/04/大模型算法题-4/transformer.png","hash":"9dddf171ca51f2ed1218baa9b84f4b98e9b911cf","modified":1713605151549},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/downstream_dataset_num.png","hash":"ac409ad0cd39c968f3d59a0ab7d4e75f9922d682","modified":1718524405587},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/train_loss.png","hash":"ac1261a27009436fae1bf50412017e8f25ea7d38","modified":1718956787257},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_update.png","hash":"bdee7c642152f421d099748d5afd6570b7c99a5a","modified":1718875127763},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/data1.png","hash":"050a2ebcc9784a20ca811c99215b137c8257c0c7","modified":1719386642602},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/structure.png","hash":"61969133bdb9770d5139d27374eb3e3f4cc44d0e","modified":1719383798123},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/lr_result.png","hash":"96674fe834c8223d29136a4c8d36abf28cb3c195","modified":1717576132058},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/result_4.png","hash":"6f554d1b6911c3db211a7e57e886e62f03b46ffd","modified":1716985505197},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/intro.png","hash":"3ff9cce772cd825ec8b88591d576a5f52982d679","modified":1717231737607},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/ablation.png","hash":"4c73c83eb527141e17d1109b6c2cff3488de6259","modified":1717250998001},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/benchmark.png","hash":"83084a5f64006000898d5252b3f8afccb635b3f0","modified":1717246623701},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_t5_result.png","hash":"83b63aafceeb8f2bc3f89ee6a1e3caec7987a1c9","modified":1715674742164},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_ppl.png","hash":"fccca1509ebab2e89a3ceaad0dfeedc700de2691","modified":1714841526104},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_dataset_dist.png","hash":"e5754afcb70a45c0d11ee5db43c724350ec64257","modified":1714913484177},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_lost.png","hash":"b9d73b8022266af17789ab049c7adda621729cc9","modified":1714913979224},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_booksum.png","hash":"276dfa014d35f7d3375fbb7cee6eed21127f9955","modified":1715160163895},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_ablation.png","hash":"babe57024a1c2212405220124dfd376a8a2bcfb6","modified":1715242576296},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_language_modeling.png","hash":"a49a7cc02694e7017c2a20dc0666046108b3c4c5","modified":1715159444656},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_passkey.png","hash":"811c5c677f7616b6625a0b86f2004f6d3ebeefe9","modified":1715160050601},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/xl_vanilla_sw.png","hash":"3259a751066a0083ef249a0412f43fb582e6544c","modified":1715138182854},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/stremingllm_attention_sink.png","hash":"933f34f3bc1d04e2b36f3305ac9fe2acd8bc9939","modified":1714383008964},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1710516401027},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/base_model_eval.png","hash":"9b4e65d246865683f8e3348d74bfad03c937b65f","modified":1711616539832},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/perf.png","hash":"3c068a423dcd32cac7f1630bd69fbe5a4c6789af","modified":1711614561095},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/sft.png","hash":"ea9aea143af836012f44d21956ab5455487e9bfb","modified":1711615463552},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/bs_bn.png","hash":"aa28241d75f914603b9f7f67cc54db4e61bac668","modified":1711113379148},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/rmsnorm_eff.png","hash":"350a7a2703eef1ee9357609ae5820bfc30835681","modified":1711166117196},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1709982190361},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1709970717456},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1709982952107},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1709802508932},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1709638849226},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/cover.jpeg","hash":"6226a5276377816b37a20572a8b725af3ddf5760","modified":1714102101607},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_vs_open_models.png","hash":"ba0348d3fe68a27f8c6435c4c3a6d08d9c8869c6","modified":1713698848988},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_2b_less_expert.png","hash":"e03a00a194efd33890517b4ad642bca5566cf9df","modified":1713688653303},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_vs_closed_models.png","hash":"6211c05b7d69194f2d820622525273110467a0d5","modified":1713698929131},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_ablation.png","hash":"108dc8d66ae0e370e7969403efd85178b9a8523a","modified":1712805107241},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_family.png","hash":"9d813f12f82d8702886f7ede72c5a25151390ba9","modified":1712932424673},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_related_model.png","hash":"0f109231fc1b425c7364401c41ce5f3aecfd76c7","modified":1713709711124},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/mistral_8_22b_multiling.png","hash":"4af49ffc09a0de3793ec7137d3dfbddc9c309d38","modified":1713706946046},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/gshard_moe_family.png","hash":"21a6c80f1dac39eb364fb417ed83afde2b212675","modified":1713449413788},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_capacity_factor_speed.png","hash":"60b624b7595e1f90763ea745ea358b6852eeefb0","modified":1713881294917},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_more_dense_layer.png","hash":"67c37482d73cd7ece7e384c0bd73e6891fc752e1","modified":1713856367887},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_distill_sft.png","hash":"7a59660f367634d68aa392698042f3d9cfe190da","modified":1712979135172},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_dropout.png","hash":"63f36ca61aaa71e88ddf23339e63a9ccd898a6ce","modified":1712934811239},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_init.png","hash":"f9fb36a0defac7a5990b5c58a614f3165af0ae3e","modified":1712934593938},{"_id":"source/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1707045207190},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1709198077742},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp3_plot.png","hash":"5bf2e800c8583d545e30a084400ec34eca436025","modified":1718509738469},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/cos_loss.png","hash":"2b5a94aea83ca359aac95432298a8b32b29672b0","modified":1718801980800},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/128k_result.png","hash":"97987dc9ae6ef342ce5bb4a34072a558b11a8770","modified":1718958174001},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_exp1.png","hash":"2c556e50353e6b1528918f310f3150afdfd2f549","modified":1718873769123},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/4.png","hash":"f9c2e248f1416369b19e434fd28e7160f85c9d22","modified":1718713634456},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/tree_attention.png","hash":"c3c0b10e0bd043235b307ebddb730b43a84fded4","modified":1718265969822},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/exp1.png","hash":"1d2454407467f240c2fa549a73589bcfa462a78b","modified":1718356038625},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/threshold.png","hash":"fb52a6f66fddabc8ae9f72d027f4605178f65fb1","modified":1718357328544},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/construct_tree.png","hash":"ba3dc50c0c35e13fb15e18ad7748494d7e20f532","modified":1718336888466},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/result_2.png","hash":"6baa634147220fed9edfff7c70e83c56a2b24913","modified":1716985357034},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/odpo_intro.png","hash":"e32faada4824a2654e32d125cb7dded9895f87dc","modified":1717141374659},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/toxicity_control.png","hash":"42173484be1fd33e29244f43d658ba03ec9bacb2","modified":1717155371840},{"_id":"source/_posts/cs/nlp/2024/05/大模型算法题-5/ntk_by_parts.png","hash":"5b49750dc6a2d1b878f34bc71e3961d96282499a","modified":1714809199814},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_sd_algo.png","hash":"576ffae274518c5a4e6c049d199e0714b06bba86","modified":1715671754784},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_config.png","hash":"8edac537bd1aefea28406c414e0c0a4c888234be","modified":1714840521116},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/pose_passkey.png","hash":"6202690a895f0114c90f6821c8cf1ad7388e1592","modified":1714918508416},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/pose_ppl.png","hash":"fded043b94c97c1a782869963f5dea371e257b80","modified":1714918324529},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/pose_method.png","hash":"db8784c9e4b14c5963f62f072df6a4c3c5405874","modified":1714916547117},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_design.png","hash":"e96faad18cb526d201ce069f9ce09dc3c9c0d16e","modified":1715245088507},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/stremingllm_exp.png","hash":"cdd21419055ca9bed86b46675e118ad4bdf55544","modified":1714394556802},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/stremingllm_init_token_num.png","hash":"ffe78c31d901311249530e786bc5ed321e2e242f","modified":1714392839583},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/stremingllm_perf_4m.png","hash":"c4545d7f0bbd0c5f6baa85dd64b747a3723fdf2e","modified":1714394913980},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/multimodal.png","hash":"b14a4eb4d377101acf7b50904b9ee0f1d473aacc","modified":1711614412507},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/pretrain_data_dist.png","hash":"8c66a625723cb87ee67a9ff60d3614b369f50592","modified":1711463822846},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/eval.png","hash":"e78d1d820de4c455b9301124d6016a19762eae1f","modified":1711446016442},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/warmup_effect.png","hash":"3e936786065e1ab9cbad17f5b86a5b8129720270","modified":1711204451931},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_infer_efficiency.png","hash":"34245e99c2b29dbd54104ccbbb3d8c15706307b9","modified":1713699575045},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe_specilized.png","hash":"138ad5a388c77cc02202de794ec4cb734d633065","modified":1712043657680},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe_perf.png","hash":"ca3c95d4b1c1c8f986d7adcacbf04da444e91610","modified":1712049369889},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_models.png","hash":"bc59770bc8ea44bfe480484a99aa9143acbaa6fa","modified":1713795616340},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_capacity_factor.png","hash":"a960540d3419de77c3823d343247abfddeadde1c","modified":1713881000881},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_multiling_specialization.png","hash":"bac636d8da61768adb5a9b7c5bd75547267ae470","modified":1714048322261},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_structure.png","hash":"d906a9148e035025683e8da1eee5fa3d87164aa5","modified":1713585604066},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1709197025669},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/param_search.png","hash":"82785189286d4e7b69d24998d14ca9b78fb4d890","modified":1718797744783},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/metrics.png","hash":"7bb73cea5404b78b78799e7069c914c50f847b39","modified":1718520589707},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/3.png","hash":"d0bbe995cc75de29aaa21f1abc144d3bedf5556f","modified":1718713442312},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/param_search_2.png","hash":"601bf31d9a7aaac8d1e1e60f1a4c2d40224160e5","modified":1718798191336},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/6.png","hash":"c22d52b720cc97628669ba318f13c39692ff8c52","modified":1718714705043},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/evaluation.png","hash":"ee53ffd386d5ba9b276dc3f55b87e68b5f8dc378","modified":1719391004272},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/mtbench.png","hash":"3411a742f62a58e4ff435ca06641e8830c2c80f6","modified":1719391072662},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/data2.png","hash":"29418adf51a48bca2340e506dd86dbb8882f913e","modified":1719386681496},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/tree_attention_exp.png","hash":"0b516ebc1df1418eb6f5734b05f3975b9bc71d18","modified":1718357514716},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/dpo_loss_code.png","hash":"279b32cc1c4dfefa8790fcbb597659e8b974ac61","modified":1716975072126},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/perf.png","hash":"02a9e32039679d6f1249e0bd6bfbc3cff00228c8","modified":1717564274115},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/result_1.png","hash":"725c6be46c42e8fc8184304bb0cdf5071b09e8b2","modified":1716985304979},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/hyperparameters.png","hash":"b3595e75eff0cb8ea8f86fd9e2f8c6ae5f7892bc","modified":1717242348046},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/scaling_function.png","hash":"b554e0bd697e72bb2e5e24a16c678d80c2efcc52","modified":1717156880112},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/sentiment_control.png","hash":"fc200cc3802fdee9d80e4bc259f4baca7b425ae7","modified":1717154380930},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/acce_alog.png","hash":"61f4653292bbd93debee66cebfb44ac7198e5818","modified":1715937599067},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/acce_k.png","hash":"ca37e1983347a1a835389de4d17047e3b0d02af4","modified":1715691856143},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_perf.png","hash":"5cf407e2e2ee61bb2b6bdae0570c3b8c4a3a9374","modified":1714913782279},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_compare.png","hash":"9b272eba596a790b1d40ef3a8b041bdffe1660d3","modified":1715159194851},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_ppl_figure.png","hash":"34397c88e268b769c97a2b6e2ad63bf6ad5270ef","modified":1715241210130},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/streamingllm_model_ppl.png","hash":"2fc1c11d7cfa2598b414e5e4c145181ec9e10648","modified":1714381636336},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/xl_attention.png","hash":"395424d6c048880e143b9b2f93585597fbebebd7","modified":1715138841538},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1710558943189},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1710252446580},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/ics_measure.png","hash":"e9fe87cfea7dcef7cb66e1d76c17d883cbbc3cbd","modified":1711115709830},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/28.png","hash":"d438e857378575809c880b78ca715dc69e50b364","modified":1710643355076},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1709986493059},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_perf.png","hash":"035a992ef2e7e5a9165c9488e2696e38fa29165c","modified":1713699612482},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_16b_perf_1.png","hash":"d540e8a58e166c0ba894708c9b0c277c31107487","modified":1713690316202},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_16b_perf_2.png","hash":"6886e18a2e7601202e8721b83f998faf028e19eb","modified":1713690412061},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_upper_bound_13b.png","hash":"cec21f0cf3809011ce210dffda35da3671147008","modified":1712803976611},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_structure.png","hash":"b0564913ecc1f78e5052dbc07eb65b3f048846e3","modified":1712752326556},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_model.png","hash":"63c95ea5ae77528f7d94a94b21cf12ed63e0bfdb","modified":1712849528728},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_perf.png","hash":"b6ed751d2f171dac971bcf1d7f12e8a2d7fad388","modified":1712672178340},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_compare_gpt3_2.png","hash":"89172dda5ac569780ea47e85bc43eaef1d6918ae","modified":1712848396056},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_distill.png","hash":"be5cae81fafbda6b638ac185421bd04e00b7a60d","modified":1712978653926},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_scaling_dense.png","hash":"b8de8c427de51d62e69915da7a97bf4a9b505317","modified":1712976996880},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_scaling_time.png","hash":"b56568665d2680cc0723269ae39dc4b60de1b01c","modified":1712976813025},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_pretrain_result.png","hash":"880f70f371b8392e3021bf56d282be5640c232f7","modified":1712135713134},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp1_compute.png","hash":"4b2bb4c651292fa438292ea1adcd5fb6530fec78","modified":1718523259881},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/exp_1.png","hash":"31f27f3659a4c3e0af9894e6a9b85cccec611889","modified":1717509313256},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/intro.png","hash":"c5175eef020ec3499aa67163813eab1c4c13a84a","modified":1716887862324},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/ln.png","hash":"4d4c831fc95c591ea0415e07dd5f46d1b1494e60","modified":1717251987860},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_example.png","hash":"edbdb72af30cac5f036e47b3d0d426919f336e62","modified":1715609386236},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_tokens.png","hash":"b6c43c289004164e90de39c30e170de5ac1088aa","modified":1714890071917},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_starting_tokens.png","hash":"18e3a03213a7275617201b71eb274cd8fd8b0bf9","modified":1715178569704},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_dataset.png","hash":"b85a9077cffcace583a7dbcdbd235ab646086ea1","modified":1714913285654},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/realformer.png","hash":"3bc805db3177c7e6521362b063543941da8d2bd3","modified":1711206089667},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/21.png","hash":"fb2577b5fa73b06b786484b3723f7aa3819638a0","modified":1710643325115},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_sft.png","hash":"8bc26fcfef1aab448d0db0b28666852f62961a3b","modified":1712821943440},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/gshard_algo_1.png","hash":"aa8bcd982c78e4304c63f81e44b20519dc04f18f","modified":1712070648688},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_perf.png","hash":"01fb9d4f232e9f0b86abb91dbe5d8fb9fac456ce","modified":1712933127383},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/gshard_perf.png","hash":"27157f620e2b7c4be60b2a58f7857a888794cde1","modified":1713968695747},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/gshard_result.png","hash":"32ce9bba1b65ceb2e69c079e113d8b4c524bc479","modified":1712067349025},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe_hierarchical_gating.png","hash":"067160c735e9c0b8cff777df60d52dfca21ea783","modified":1712045417559},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_perf.png","hash":"58927fa39b5e56e8da00144b417bbae5256d6bdf","modified":1714048423893},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_distill_diff_model.png","hash":"05c4d885f5563e3dbd56c055a52df5c1531677a7","modified":1712978973713},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_diff_expert_capacity.png","hash":"23f2234b45a2a05ff8b098e68a361a71f46816e9","modified":1712133650991},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1709206562025},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/downstream_dataset.png","hash":"a6cda82185bcb7c1f70931d792e10583825c2d01","modified":1718508214406},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp1_plot.png","hash":"27b2613a83f5873538e0e5832ea957e47625d719","modified":1718508482972},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp2_plot.png","hash":"e65d2e11a5a28b4b1ff8b54ffff65be2298d7c1f","modified":1718509536128},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/scaling_law.png","hash":"19067e52c8d6e073c86fbf4c4fee258574f9b348","modified":1718893969072},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/intro.png","hash":"ab8abb35042127509f264d1f0e8ce838350b08ff","modified":1718249943493},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/ln_effect.png","hash":"714ba6cdf67ea33d507e3358b113a94bcd24e1ce","modified":1717252559659},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_alpha.png","hash":"f7f5a24106f1b9d16fd805b3ed3d3c2efb4a8c03","modified":1715674930516},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_data_dist.png","hash":"157286ce140bd5acc7c45fb12785649cc7214472","modified":1714837490239},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_data.png","hash":"9262b3bbc1b415d9c776723fe85c0a43f9fb562a","modified":1714835715477},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/cover.png","hash":"0493fd58fd2dad33394399d960924dbff6b386b1","modified":1711459395465},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/20.png","hash":"5d42628c8dac91c9671a58535b730e91966c0cbc","modified":1710643321378},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_comparison.png","hash":"20c7e90a390604d2146b4984678524046ad941b8","modified":1712803733203},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/gshard_model.png","hash":"5179df7e42bb7dc49fb6f92f4ec68ed820aeaae2","modified":1712068764148},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/mistral_8_22b_code.png","hash":"f9b78e0669e0c83f716c8e4abfa4d97b6f9b8143","modified":1713707063629},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/mistral_8_22b_reasoning.png","hash":"ce00d9ba7b8a65eb238c99f79a17ca7a3cd2238a","modified":1713706908014},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_encoder_specialization.png","hash":"382cb53fcc2e9172ffd7554a04714feed4d9706b","modified":1713882897561},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_scaling_step.png","hash":"dbfa55ab1c94e266344315fd215f2d646eaefda1","modified":1712976642477},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp2_param.png","hash":"1412eccf1c2cf485bde50b1cbf9223ee5a5ff2cf","modified":1718509235904},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/batch_size.png","hash":"8e754e7b97b0290618e90e499d33f300e149b39c","modified":1718799768529},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/normaization.png","hash":"8c4ce11d34256acc7e3800355186ab00f2236293","modified":1717556835290},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/main_results.png","hash":"a7388a503f0157c2ebe9ef765d63daab358b67d7","modified":1717249875414},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_needle_comp.png","hash":"e4c9f5c51548faf11a7ff64d23aae5bd2927ab0e","modified":1714830276600},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_sample.png","hash":"6b0d7ed89b16a3e9c6297219f33e59f204923828","modified":1714890968069},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_gating.png","hash":"5e875c3de6bde62ea8e15ea4995a56f9fd28d67c","modified":1715159702016},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/streamingllm_compare.png","hash":"40394890082b1666d1221f302ed52a80fc358477","modified":1714317603622},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1709965340148},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_145b.png","hash":"502a377d8625d78d2e3ba7281bfb11732c14a61c","modified":1712822142201},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_upper_bound_2b.png","hash":"4338539fe123371c5512c730fc8a35864236323b","modified":1712803847753},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_capacity_effect.png","hash":"16268929d0ec965d3771fccbb595b75d82f05912","modified":1712134554223},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_sft_result.png","hash":"0dd7cfaac788c5d112dd10fe98f0052b369420bf","modified":1712978414761},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/vanilla_moe_result.png","hash":"e491e44cfff384422f3d9cf87cd5e52fd976aed7","modified":1711963546083},{"_id":"source/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1706779539112},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/gradient.png","hash":"58524a8fd10f4d76e8e419e1ed46bd9a99cf58d5","modified":1717234900278},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_example.png","hash":"946cdbb0a425b9d11b0ff587007885990abc9f99","modified":1714912943693},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_intro.png","hash":"6b810c88945281a9cdf9749941cdbe08346fd42b","modified":1714914076240},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/ict.png","hash":"2445ecbbf5ec6a96695f21c03a5fcbf67640b9f0","modified":1711615224209},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/bn_ics.png","hash":"f92751ea20430f25caa3d6bb892c5894bf7509d6","modified":1711115173958},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/17.png","hash":"8ea1c5d90f3da5c469eb17aea50f377cb9c28ba0","modified":1710643306201},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/18.png","hash":"fe0a8e7005110abca19bc7ae506f3e35042b70ec","modified":1710643312126},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/26.png","hash":"f74d03dae65109740f48924f30b52a742b5e4273","modified":1710643346427},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/eval.png","hash":"982f977c073703051fdecb951c6231f7b74aa58a","modified":1718956887234},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/pretrain_data_pipeline.png","hash":"91218a2272eab9284904c91bacd8d8a40e3c1580","modified":1711462239524},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/10.png","hash":"2c52d4f90dd9356eeb4c9a39f1df1038ccec4693","modified":1710643262247},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/3.png","hash":"4c2c2a30d9ac8db03bab56da5d16ce2042ef73bc","modified":1710643220743},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/6.png","hash":"4b32a49bfead98f5238871b81076176e38168333","modified":1710643241274},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/1.png","hash":"a8898b3f3b7c64fabc5fad9bf8ef5524501d2aeb","modified":1710643881936},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/19.png","hash":"1d7b929e709657c9b7d7ca4da8eadc8c4ca4b3ca","modified":1710643317015},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/24.png","hash":"b89b0a0ca774a4efc1ece628fb20379b5f6a0b69","modified":1710643338042},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1709986303475},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/8.png","hash":"92b0e3b75ce97bbaf4aa69e484216702293589ee","modified":1710643252580},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/27.png","hash":"557c04a2134b6ae147e076cdb80de1730e937d9b","modified":1710643350772},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/9.png","hash":"2a0fb56563b13411035ed41a3ad882f66f948b26","modified":1710643257459},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/11.png","hash":"9b964f3aa6f82a09eb2f2f944508bf0a9d29efb3","modified":1710643267156},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/22.png","hash":"3b27321ef8d76844f6720e1a27d65d1946d48ea7","modified":1710643330423},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/23.png","hash":"9a9af6308620b59f2ee00a3d0da4e942d953e406","modified":1710643334116},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/25.png","hash":"aa259b58be90eed6af0c4ba800a991b9464453d0","modified":1710643342514},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/15.png","hash":"15abcbcf7340941e98dac7a0ab42d922e7fea1b4","modified":1710643288058},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/2.png","hash":"768421239ad7c838dd86714fd9f17b3c73cbb887","modified":1710643214870},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/4.png","hash":"4dacbfb89079d528da1208773961e1366debde9b","modified":1710643230331},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/12.png","hash":"c6c493b14e0a1cc4863a912c4ccc998de194bfc0","modified":1710643275198},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/14.png","hash":"71f75960246f7528b3b83b84f8f91775f9e2fb45","modified":1710643284450},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/16.png","hash":"ba7b2bc65e10389cf9a87ddef69f462e806304f5","modified":1710643293307},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1709986434286},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/7.png","hash":"95640525b0706d3118eeb88c6c4c6217a96c39d0","modified":1710643246812},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Markdown _ 让排版变 Nice.html","hash":"c905c942579a520c7b3c788a00cdb9ae359d4a32","modified":1709897264700},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/13.png","hash":"68b7da3e3074e4d6995eaf96c7d8cf622eadffb7","modified":1710643279584},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/5.png","hash":"7ae786b309a757c5f61a713c7ceef4d2824b024e","modified":1710643235878},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/speculative_decoding.png","hash":"fe277fa76f9f9c71e2030a41ca9eab458c33826a","modified":1716543143782},{"_id":"source/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1707045618160},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1710316114714},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/ellipse_1.png","hash":"ef2470f6bf1511dc9aac9f1c6489b9d2ffdcb45f","modified":1711006814272},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/digimon.png","hash":"247f4059dd9671047f5d6707d8cef75a93d93f40","modified":1715070986892},{"_id":"source/_posts/cs/nlp/2024/04/从实现看normalization-到底干了什么/norm_in_nlp.png","hash":"7be79b0e55d7d00ff6c16c247d0e506771453380","modified":1712575607757},{"_id":"source/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1707045245660},{"_id":"source/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1706875075740},{"_id":"public/baidusitemap.xml","hash":"52f25b109bb3b3601ff4b09de41e1b15fe0e603d","modified":1719406315097},{"_id":"public/search.xml","hash":"2586b12bdb49c00582a147571ba641263cee5441","modified":1719406315097},{"_id":"public/sitemap.xml","hash":"8753a95ab0c0c9fd0f0fc77be7746e36f09d6133","modified":1719406315097},{"_id":"public/sitemap.txt","hash":"9c0e5c98e0e6bdba4b6447619b487ee2d370768d","modified":1719406315097},{"_id":"public/about/index.html","hash":"c3f49ce45bbe3bc7faeb650995f70a9747edad02","modified":1719406315097},{"_id":"public/categories/index.html","hash":"02247316ea3891eeeda1b0028116a15dfdbd34cd","modified":1719406315097},{"_id":"public/tags/index.html","hash":"207365ab38fb2e8470a25062e789850c0db37bf2","modified":1719406315097},{"_id":"public/f3acf042.html","hash":"a61e1dcbac93b084df558e606ff8b0c6be18209f","modified":1719406315097},{"_id":"public/f0902f1a.html","hash":"6dd7acd42a441d55de2bc00633332b95476ac6fc","modified":1719406315097},{"_id":"public/376db710.html","hash":"cd4c6751c6363e01aba5593c79ae5aa7ecfee32a","modified":1719406315097},{"_id":"public/7381cae3.html","hash":"f58dc2e3f8fe67ea6eccb6039cc4e251a0cdc985","modified":1719406315097},{"_id":"public/f5fb75e4.html","hash":"b14020f2530d7e3f446b694c480d3c2ba091e938","modified":1719406315097},{"_id":"public/dd614e12.html","hash":"2274a71e965ab5c8227c3064d4bebae1d5200623","modified":1719406315097},{"_id":"public/7bbe2df6.html","hash":"6cf75357d5350a74cc5a9abca0fbde72e8945c1c","modified":1719406315097},{"_id":"public/1d5bcd45.html","hash":"92a33f7f5319bd1a31fc6a4a74ce8a792d575388","modified":1719406315097},{"_id":"public/4fe7b810.html","hash":"2a04a39fa555948a0c57bc7b49e939a80046f385","modified":1719406315097},{"_id":"public/280fa97a.html","hash":"3676f8141448ed3bebd41b15eeb20639e1a88aa8","modified":1719406315097},{"_id":"public/da871ebe.html","hash":"55fb5ffdc338a720c4d0b27eb7a798226487f627","modified":1719406315097},{"_id":"public/473f2b43.html","hash":"caa6d6128ae2b9a10e35cbd311913033c946e8e3","modified":1719406315097},{"_id":"public/7c04944d.html","hash":"e7926269e561f3a9ffdd5461494d42fc49a1e404","modified":1719406315097},{"_id":"public/f5c015c.html","hash":"f9aec0dda5a6c5c44077160ba3350cec472834d1","modified":1719406315097},{"_id":"public/45ee1a6d.html","hash":"7aa5c07cadcbe7bbf95abb3d0e85a24a09fd5701","modified":1719406315097},{"_id":"public/cc852861.html","hash":"0b38c591f206183ef3f49337f13cfefd52b0e396","modified":1719406315097},{"_id":"public/336f2f3e.html","hash":"87f7982603808bc568b52868c3c8d911de374d35","modified":1719406315097},{"_id":"public/1736008.html","hash":"a5061ede9013ebdc6d6da72a3b1c8006d4908401","modified":1719406315097},{"_id":"public/b70b4a2d.html","hash":"1c1b0a3609c69a246a6c94c2c3f3940a36d016f3","modified":1719406315097},{"_id":"public/44e38c1b.html","hash":"f24a6d16ec237e58e33e2b1e001b428180a0fcde","modified":1719406315097},{"_id":"public/41b6a819.html","hash":"548388a8a57d16b6e514efe759bbc9186a3f14b5","modified":1719406315097},{"_id":"public/ad0bba9d.html","hash":"1546f19e3e1fbc51fc1abbc5a0881310d1726bea","modified":1719406315097},{"_id":"public/6a40bfa5.html","hash":"5aba2aea7373339a3eb94b4125dffc51eb746f6e","modified":1719406315097},{"_id":"public/3345028a.html","hash":"67dbd3a3df5bcbc3c8449ee99283d04ced85aabf","modified":1719406315097},{"_id":"public/c61d17e3.html","hash":"47d7f25f59d0984d41520966a0d000be7ca3a1a9","modified":1719406315097},{"_id":"public/3dc22f96.html","hash":"5b3ee2435094f2b5dfb78a6cc14ecf92eb84ece1","modified":1719406315097},{"_id":"public/c4da56c0.html","hash":"39c3dda4ed0c77e3fc96d38748498b14222cc7ec","modified":1719406315097},{"_id":"public/a051710f.html","hash":"a890ab87774c5593dc23c3b1e2732c4f1db1df91","modified":1719406315097},{"_id":"public/14e576c.html","hash":"c9f5c09a659ee270e69a198abed9de3c7613e00b","modified":1719406315097},{"_id":"public/archives/index.html","hash":"28fd5150dfaff43c3ee440f2e23b4cb15b292524","modified":1719406315097},{"_id":"public/archives/page/2/index.html","hash":"abba49cebd8bc83a8b90fb2965b1d58eed42ce33","modified":1719406315097},{"_id":"public/archives/page/3/index.html","hash":"1b012a0e98861b3c477cce0fe3970f1777e2674a","modified":1719406315097},{"_id":"public/archives/2023/index.html","hash":"25c446a53529795915db9005981f7fc4e7839f51","modified":1719406315097},{"_id":"public/archives/2023/03/index.html","hash":"cfa0823014ee688efa44113e9ccd629ba692e8cd","modified":1719406315097},{"_id":"public/archives/2024/index.html","hash":"2cd3d6eac70541a7aaf551e2cdb0901f3d35a7f3","modified":1719406315097},{"_id":"public/archives/2024/page/2/index.html","hash":"1d152bcfee14bea2697063f5fb24ba78207c3d6c","modified":1719406315097},{"_id":"public/archives/2024/page/3/index.html","hash":"447e91e0652145c1bcf3e7cae9677853def405cb","modified":1719406315097},{"_id":"public/archives/2024/02/index.html","hash":"cb38306bceba4b68f107ca63f5143294c5cac87e","modified":1719406315097},{"_id":"public/archives/2024/03/index.html","hash":"2498dfb5a506401a490da7fc1b0323091c8962ce","modified":1719406315097},{"_id":"public/archives/2024/04/index.html","hash":"e29ec9a3ce3f5d29b5b1eb243001f389b48472dd","modified":1719406315097},{"_id":"public/archives/2024/05/index.html","hash":"0cf32ed25ad47fb832fee64b159aa4caeaa0d583","modified":1719406315097},{"_id":"public/archives/2024/06/index.html","hash":"2456457e5ab460737dacd56edc790826221008a8","modified":1719406315097},{"_id":"public/categories/CS/index.html","hash":"55268b70aafdeca93dd0c210d32a570e3b72271c","modified":1719406315097},{"_id":"public/categories/CS/page/2/index.html","hash":"f6274f0019d041a8a01899cb216778441915cdd6","modified":1719406315097},{"_id":"public/categories/CS/page/3/index.html","hash":"3e81bf5566acdafb27f0ea9e60490d83e6a50e88","modified":1719406315097},{"_id":"public/categories/CS/NLP/index.html","hash":"73295e39b64965f6c50dfb6ed384eb34f84a546d","modified":1719406315097},{"_id":"public/categories/CS/NLP/page/2/index.html","hash":"8e0c95cd5ca010cf1757a4982ad0940b96e5c10d","modified":1719406315097},{"_id":"public/categories/CS/NLP/page/3/index.html","hash":"2f577ac892b01dac819486e8f96a863c54d0aa9a","modified":1719406315097},{"_id":"public/categories/CS/NLP/LLM/index.html","hash":"a91e990956ccd3d45c17d09329a04a4ad66917e2","modified":1719406315097},{"_id":"public/categories/CS/NLP/LLM/page/2/index.html","hash":"080276cd328c30b3477e694e1c9f4bc3791fea8d","modified":1719406315097},{"_id":"public/categories/CS/NLP/LLM/page/3/index.html","hash":"98ff02b98b6843039ba6f1f1c1f14408d4c70152","modified":1719406315097},{"_id":"public/index.html","hash":"99af342c47e976183f454fab4cd2ea2219df1533","modified":1719406315097},{"_id":"public/page/2/index.html","hash":"ed41b1ceb1cf29682db8119e3ad6ae35bf00f6e0","modified":1719406315097},{"_id":"public/tags/NLP/index.html","hash":"d9b9536f4f9654b3dfdab3caa4e833452a792817","modified":1719406315097},{"_id":"public/tags/NLP/page/2/index.html","hash":"1d85a4db13ddb86cab658449d5f7044f41b373fa","modified":1719406315097},{"_id":"public/tags/NLP/page/3/index.html","hash":"4f1ed6b3925171c4ba5b24517517d83fbea1070b","modified":1719406315097},{"_id":"public/tags/LLM/index.html","hash":"f238896d53b5f08fe5fc1a98d66656654a79cfa7","modified":1719406315097},{"_id":"public/tags/LLM/page/2/index.html","hash":"0acceea4a04c58620f6a43a94046335c51ffbc58","modified":1719406315097},{"_id":"public/tags/LLM/page/3/index.html","hash":"39fee81c8640bf42170be60f29640cf7a00c07f8","modified":1719406315097},{"_id":"public/tags/transformer/index.html","hash":"d7ffc791e91b06fe37c4ce71238bf8be7c71fd77","modified":1719406315097},{"_id":"public/tags/transformer/page/2/index.html","hash":"c79a298c10ad8e906a9d8febfd8ff50a04f3c2d5","modified":1719406315097},{"_id":"public/tags/transformer/page/3/index.html","hash":"8e4ef6f9170e41748c319a3278b46c7bb1af8fe3","modified":1719406315097},{"_id":"public/tags/positional-encoding/index.html","hash":"69e650ceed0259a4d0840899f7bc61aa778caf86","modified":1719406315097},{"_id":"public/tags/RoPE/index.html","hash":"f69abdc926ee9bb02a8181855e646ecd82ad1a39","modified":1719406315097},{"_id":"public/tags/长上下文/index.html","hash":"c287ea84b76489648a6907ee54a96cd436b494e8","modified":1719406315097},{"_id":"public/tags/窗口外推/index.html","hash":"c6b928e5e08c85d53e9184483619e6dd80a3bca7","modified":1719406315097},{"_id":"public/tags/layernorm/index.html","hash":"502a48d6653283a483b8ed866c9942fe8230f72c","modified":1719406315097},{"_id":"public/tags/normalization/index.html","hash":"c81183ce365f347b434c734a418ad618c8841cac","modified":1719406315097},{"_id":"public/tags/batchnorm/index.html","hash":"5abc140228584e4e1d9cd01267d1ba81456a2eff","modified":1719406315097},{"_id":"public/tags/算法题/index.html","hash":"b73320a3fcb1eb835ad8fdfb9747bd4f6a6dc81c","modified":1719406315097},{"_id":"public/tags/技术报告/index.html","hash":"96673dd835c51e90a35cc33bfd1b4bf6399d23a3","modified":1719406315097},{"_id":"public/tags/学习率/index.html","hash":"f0395294e90f42f6bf1726c7ff3e30a9ca955a64","modified":1719406315097},{"_id":"public/tags/预训练/index.html","hash":"c1d935551d5b3b0d7134eb2999555ea547932615","modified":1719406315097},{"_id":"public/tags/复读机/index.html","hash":"e5be2aa5e350fa40d208ef62854f35784e36e771","modified":1719406315097},{"_id":"public/tags/重复生成/index.html","hash":"f6768f165055f3ae6f55b94a1d0d4ff1ba6eb6ec","modified":1719406315097},{"_id":"public/tags/涌现能力/index.html","hash":"0c056d0fb26b84cec3c89b61028e5949431fb1b9","modified":1719406315097},{"_id":"public/tags/强化学习/index.html","hash":"54aa2405605e27d8334a4dc0a1f384731590d2e3","modified":1719406315097},{"_id":"public/tags/微调/index.html","hash":"28f98dd8b71461a172360aac9cd27714d6f6fee1","modified":1719406315097},{"_id":"public/tags/SFT/index.html","hash":"bd66c53644004eea67760377fcef7599c8219de4","modified":1719406315097},{"_id":"public/tags/偏好对齐/index.html","hash":"14fe82cd97fb161a4264dd102610408825f77254","modified":1719406315097},{"_id":"public/tags/MoE/index.html","hash":"58af76acd755f90853c8f5d6d0ca0d72b352cd76","modified":1719406315097},{"_id":"public/tags/推理加速/index.html","hash":"0e8d6fd8365f492f763913018347a317b6297c07","modified":1719406315097},{"_id":"public/tags/无限大/index.html","hash":"2aa7af916418cd62f91989961df8814bf188e684","modified":1719406315097},{"_id":"public/tags/attention/index.html","hash":"91f7792de16c9550d13ab21728c1baa93d6ccc63","modified":1719406315097},{"_id":"public/tags/sliding-window-attention/index.html","hash":"901c7a7b0dcbf5e301019b86563de53a7b9519d1","modified":1719406315097},{"_id":"public/tags/sparse-attention/index.html","hash":"ff08b9756748a7bfee378261beb1a9b9ad0ae72d","modified":1719406315097},{"_id":"public/tags/post-norm/index.html","hash":"71dccba69025baf454e29140481ede2cc7a9eba9","modified":1719406315097},{"_id":"public/tags/pre-norm/index.html","hash":"3247379d91a5e71febcf448540ae166f5bdf06c8","modified":1719406315097},{"_id":"public/tags/多模态/index.html","hash":"1b540adc271b9d5f56d3b199f2aa7c570fc55422","modified":1719406315097},{"_id":"public/tags/ChatGPT/index.html","hash":"1ec753af599df7682c1bec2a884b88d766c99419","modified":1719406315097},{"_id":"public/tags/Sparrow/index.html","hash":"5d5cea1028c21bafc90f9fa18e1187571a22d15f","modified":1719406315097},{"_id":"public/tags/LaMDA/index.html","hash":"4f9832941c06339b78e22e10a902ee28b12d514a","modified":1719406315097},{"_id":"public/tags/GopherCite/index.html","hash":"08fadcb54386f5b4e12f1e4c5436317e1f80b511","modified":1719406315097},{"_id":"public/tags/WebGPT/index.html","hash":"b2c40faca2aa0cbf74f9ad3138fdeb65b0db7a48","modified":1719406315097},{"_id":"public/tags/InstructGPT/index.html","hash":"d87317ed3ecdbecb389f0fe221baede849fa31b6","modified":1719406315097},{"_id":"public/tags/KV-Cache/index.html","hash":"3d145dc095bfecb01aa03aecd06aaa97418ff89c","modified":1719406315097},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1719406315097},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1719406315097},{"_id":"public/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1719406315097},{"_id":"public/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1719406315097},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1719406315097},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1719406315097},{"_id":"public/images/cover.png","hash":"2f2aa6173619dd38425673ba110b50b9156d4d10","modified":1719406315097},{"_id":"public/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1719406315097},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1719406315097},{"_id":"public/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1719406315097},{"_id":"public/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1719406315097},{"_id":"public/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1719406315097},{"_id":"public/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1719406315097},{"_id":"public/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1719406315097},{"_id":"public/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1719406315097},{"_id":"public/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1719406315097},{"_id":"public/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1719406315097},{"_id":"public/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1719406315097},{"_id":"public/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1719406315097},{"_id":"public/a051710f/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1719406315097},{"_id":"public/b70b4a2d/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1719406315097},{"_id":"public/f0902f1a/1.png","hash":"cb86a4ce4589b590aa1b220dbdf32f9db559a1b8","modified":1719406315097},{"_id":"public/f0902f1a/2.png","hash":"8c8c02a78f8e874162574caa267c32f469665110","modified":1719406315097},{"_id":"public/f0902f1a/3.png","hash":"8bff531083b17baa6de7d44ecdf464c29d175a31","modified":1719406315097},{"_id":"public/f0902f1a/4.png","hash":"491537b72fcab5676e6322c47d9ae7e1054f3387","modified":1719406315097},{"_id":"public/f0902f1a/5.png","hash":"982701399546708e62d974938e35f17a6aa5abb5","modified":1719406315097},{"_id":"public/c4da56c0/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1719406315097},{"_id":"public/280fa97a/contingency_table.png","hash":"94b25e2d4803d9802d3c5455aed84911fe506089","modified":1719406315097},{"_id":"public/280fa97a/reward_accuracy_compare.png","hash":"0b5526d61a1cbb3188e8e53ea858b1d9d1953660","modified":1719406315097},{"_id":"public/f5c015c/formula.png","hash":"65fe200098b51d1712b6c38d039aa8be22d38e82","modified":1719406315097},{"_id":"public/c61d17e3/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1719406315097},{"_id":"public/c61d17e3/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1719406315097},{"_id":"public/c61d17e3/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1719406315097},{"_id":"public/45ee1a6d/lm_infinite_attention_entropy.png","hash":"be91b6a49cfe30dd51ed4f8eb258eb4715a70e37","modified":1719406315097},{"_id":"public/45ee1a6d/lm_infinite_middle_k.png","hash":"869d4b687714409a5a4b89c85dc5ee2c1f0c2c86","modified":1719406315097},{"_id":"public/45ee1a6d/lm_infinite_attention_logits_explode.png","hash":"3cc54ee973126c1ca3bcd85d75039e490efc9acf","modified":1719406315097},{"_id":"public/45ee1a6d/lm_infinite_starting_tokens_num.png","hash":"6a2c841a3d3fd354f57757aa7e663e83585545d6","modified":1719406315097},{"_id":"public/6a40bfa5/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1719406315097},{"_id":"public/6a40bfa5/bn_ln_gn_in.png","hash":"9783c818f5e0eaea33d169718476bbe8874cf945","modified":1719406315097},{"_id":"public/6a40bfa5/lossfunc_surface.jpeg","hash":"c78a8df335da0d507963fb73a62fe2c3d145c91f","modified":1719406315097},{"_id":"public/6a40bfa5/realformer_attention.png","hash":"e9a92e5c07c8ca6873ea70671ad54eb2f1a13332","modified":1719406315097},{"_id":"public/6a40bfa5/rmsnorm.png","hash":"55bbcb42145011f7b5adf90cc613e22e2b94f060","modified":1719406315097},{"_id":"public/336f2f3e/yarn.png","hash":"ee124a0823b429842082acebe78a7162915cc11c","modified":1719406315097},{"_id":"public/3dc22f96/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1719406315097},{"_id":"public/3dc22f96/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1719406315097},{"_id":"public/3dc22f96/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1719406315097},{"_id":"public/3dc22f96/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1719406315097},{"_id":"public/3dc22f96/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1719406315097},{"_id":"public/3dc22f96/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1719406315097},{"_id":"public/3dc22f96/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1719406315097},{"_id":"public/3dc22f96/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1719406315097},{"_id":"public/44e38c1b/qwen1.5_moe_tps.png","hash":"5478a6583a6c6fb68f1bc9429c103e84fe39efaf","modified":1719406315097},{"_id":"public/44e38c1b/softplus.png","hash":"bdc66c39227441390f2241b4f26c0b1fbab331d9","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_round_error.png","hash":"0172ba008837b3490a3e456306aa72be65636d90","modified":1719406315097},{"_id":"public/44e38c1b/xiaomi_moe.jpg","hash":"d898ba33f1ee70efa136dbff3cb38983b461524f","modified":1719406315097},{"_id":"public/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1719406315097},{"_id":"public/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1719406315097},{"_id":"public/376db710/2_stage.png","hash":"ca7a6d52aa5dbf3afa1e00de6548386ad0ea738b","modified":1719406315097},{"_id":"public/376db710/batch_size_2.png","hash":"d2f8c192c90acd0ccba2ec814bd36fc2e646e6af","modified":1719406315097},{"_id":"public/376db710/cos_lr.png","hash":"951b0fc36a8b0e7b9c58a39345950546f090fb3a","modified":1719406315097},{"_id":"public/a051710f/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1719406315097},{"_id":"public/b70b4a2d/cv_layernorm.jpeg","hash":"f0874ecc4b9d8da8bf3bff0e13a6313ed19a7b15","modified":1719406315097},{"_id":"public/c4da56c0/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1719406315097},{"_id":"public/4fe7b810/curve.png","hash":"f81e68f050653f66ca087c2f13a222aca426384a","modified":1719406315097},{"_id":"public/c4da56c0/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1719406315097},{"_id":"public/dd614e12/lora.png","hash":"c044aa39cff915b64ee576c558756cb1093e5075","modified":1719406315097},{"_id":"public/7381cae3/5.png","hash":"8fa2b214c490a5262951301bbd4d0510d5199b3c","modified":1719406315097},{"_id":"public/7381cae3/ditto_1.png","hash":"bfe3f56dee05d35b5b816a356ec7688d81c33b92","modified":1719406315097},{"_id":"public/f3acf042/model_param.png","hash":"a053d5204998fb025425706c16409d01f6589548","modified":1719406315097},{"_id":"public/7381cae3/ditto_2.png","hash":"abd8e2173d83ff20cc0adc636163a97f7fc46de2","modified":1719406315097},{"_id":"public/f5fb75e4/eng_data.png","hash":"3d16e16eb4a57436ff9d2c0b64e7042944dbfab8","modified":1719406315097},{"_id":"public/f5fb75e4/exp1_param.png","hash":"5f940193d379f18bb36ac38f2bfa5d1aaef65b15","modified":1719406315097},{"_id":"public/1d5bcd45/gate_dist.png","hash":"912b7bf9c26345d2c1513050ac484efde65580b2","modified":1719406315097},{"_id":"public/1d5bcd45/lr_exp.png","hash":"32b3dda778f6dd42388ec8ad6f8782738068f54f","modified":1719406315097},{"_id":"public/7bbe2df6/speed.png","hash":"402da175e2e365db3cc0e6c377b89df5b8615b6c","modified":1719406315097},{"_id":"public/473f2b43/gradient.png","hash":"0b72939cfd4770fc21727bb203efb9c5dd2491d1","modified":1719406315097},{"_id":"public/473f2b43/result_3.png","hash":"df5861c846176c90bd9a90bd5836919ef023b13e","modified":1719406315097},{"_id":"public/280fa97a/dpo_correlation.png","hash":"2c1dafd42b7ffa3318395a4934df87692ba5fd62","modified":1719406315097},{"_id":"public/280fa97a/simpo_hyperparameters.png","hash":"f976162c0882c49b43b503beb1384b447e2d5d00","modified":1719406315097},{"_id":"public/280fa97a/simpo_contingency.png","hash":"773700b1d041aba8b3912deee9c8bc7886fec099","modified":1719406315097},{"_id":"public/280fa97a/reward_accuracy.png","hash":"7fb3d4dd64e3013534bd77eb1f2def23ec57c8cd","modified":1719406315097},{"_id":"public/f5c015c/acce_draft_model_param.png","hash":"2e5b1852eaf4745f3d9bfc9b0fcccbd37621bb93","modified":1719406315097},{"_id":"public/f5c015c/fi_choose_gamma.png","hash":"65be032bf276290ca97b7d983bc4e1e2deaa95fc","modified":1719406315097},{"_id":"public/f5c015c/fi_speed_and_op_table.png","hash":"416238792292bff7178830267d53941da202eadc","modified":1719406315097},{"_id":"public/cc852861/add_money.jpg","hash":"0b00f9f1dd128e5601f0c7502dd2cf9233898f0f","modified":1719406315097},{"_id":"public/c61d17e3/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1719406315097},{"_id":"public/c61d17e3/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1719406315097},{"_id":"public/45ee1a6d/lm_infinite_ppl_200m.png","hash":"7b232b7bf3c7238836f71b4b99e492d9f6c285f0","modified":1719406315097},{"_id":"public/45ee1a6d/stremingllm_kv_cache.png","hash":"11b09e96662feb7cc246e60e1b21d7ffceb47ae6","modified":1719406315097},{"_id":"public/6a40bfa5/ellipse_2.png","hash":"ee20ecce8c3470d17b1a4bd43df811d16269ffd3","modified":1719406315097},{"_id":"public/css/noscript.css","hash":"4cd5301e478e0e0d4b176740ec314087ec5cb707","modified":1719406315097},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1719406315097},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1719406315097},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1719406315097},{"_id":"public/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1719406315097},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1719406315097},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1719406315097},{"_id":"public/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1719406315097},{"_id":"public/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1719406315097},{"_id":"public/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1719406315097},{"_id":"public/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1719406315097},{"_id":"public/css/main.css","hash":"6aea217c0462e6970606601a9fe39183cf15614c","modified":1719406315097},{"_id":"public/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1719406315097},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1719406315097},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1719406315097},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1719406315097},{"_id":"public/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1719406315097},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1719406315097},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1719406315097},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1719406315097},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1719406315097},{"_id":"public/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1719406315097},{"_id":"public/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1719406315097},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1719406315097},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1719406315097},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1719406315097},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1719406315097},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1719406315097},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1719406315097},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1719406315097},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1719406315097},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1719406315097},{"_id":"public/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1719406315097},{"_id":"public/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1719406315097},{"_id":"public/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1719406315097},{"_id":"public/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1719406315097},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1719406315097},{"_id":"public/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1719406315097},{"_id":"public/6a40bfa5/postnorm_prenorm.png","hash":"d8830735e89c73ca416baabf4a195d7891d9f0ed","modified":1719406315097},{"_id":"public/6a40bfa5/prmsnorm.png","hash":"d5826342f665f4cb04fdbb2e3d83e0b2607355c9","modified":1719406315097},{"_id":"public/6a40bfa5/sigmoid.png","hash":"f1ced5f06861a2e0296050aff17eebfe3d023a6f","modified":1719406315097},{"_id":"public/41b6a819/model.png","hash":"631efc6d4e92da128d7a10a4dc6af307ee4ddcbf","modified":1719406315097},{"_id":"public/41b6a819/third_party.png","hash":"673fe2b2cad3b1f40c0fcfd190c0034d8dbe7f31","modified":1719406315097},{"_id":"public/336f2f3e/bfloat16.jpeg","hash":"8678b705b0d6e0b7deb230bf28f2d92ce0d42088","modified":1719406315097},{"_id":"public/3dc22f96/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1719406315097},{"_id":"public/3dc22f96/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1719406315097},{"_id":"public/3dc22f96/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1719406315097},{"_id":"public/44e38c1b/dbrx_long_perf_2.png","hash":"2b6be0099f1eeb0ee2d2056eae7cf541e146b636","modified":1719406315097},{"_id":"public/44e38c1b/dbrx_train_efficiency.png","hash":"c117407ada2adab8d97250268e2eafa533bb9083","modified":1719406315097},{"_id":"public/44e38c1b/mistral_8_7b_perf.png","hash":"f90d9ff5b14326b0eef2a0026b3f5940e0d42f0a","modified":1719406315097},{"_id":"public/44e38c1b/qwen1.5_moe_params.png","hash":"93d14a2645969b08a4fb80a31aa75fd8e5201ff8","modified":1719406315097},{"_id":"public/44e38c1b/modular_connectionist.png","hash":"b5865cf34faba075b4f2316c2cae0559dac2d883","modified":1719406315097},{"_id":"public/44e38c1b/qwen1.5_moe_perf.png","hash":"b79ad1a909081fd0537bd9d44cfac2dc2133de6c","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_more_add_bias.png","hash":"f0de5347918e4928dbcbc59a897c3f3227c3d30f","modified":1719406315097},{"_id":"public/44e38c1b/rnn_moe_load_function.png","hash":"644684f21f85d565328d98334d41bbe019acbbfc","modified":1719406315097},{"_id":"public/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1719406315097},{"_id":"public/a051710f/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1719406315097},{"_id":"public/376db710/data.png","hash":"86cde89044da233481994e8728ccb256146b3ae6","modified":1719406315097},{"_id":"public/376db710/layers.png","hash":"fc52ceb86ba2af1a3898f7afc18ab138d63c529d","modified":1719406315097},{"_id":"public/376db710/learning_rate.png","hash":"e53cbb5a0d395e77bdc09779eb003134b7bbbe18","modified":1719406315097},{"_id":"public/376db710/moe_result.png","hash":"e31fb8b893615de6f8a46c79d4cc7b813c7e9509","modified":1719406315097},{"_id":"public/376db710/exp_model.png","hash":"39ea7fc867afbeeaa1e764513ca21019f4076cc3","modified":1719406315097},{"_id":"public/376db710/tokenizer.png","hash":"c4e4116baccb9bdf88048b5e38827937ad48c045","modified":1719406315097},{"_id":"public/376db710/wsd_exp2.png","hash":"86256736d0e7b22c1f57778d3973202e4ac00f69","modified":1719406315097},{"_id":"public/b70b4a2d/cv_batchnorm.png","hash":"d9e8d897c36125fddcf1cbcfa5c237a37158a939","modified":1719406315097},{"_id":"public/1d5bcd45/100B.png","hash":"c9cc7f93992b0288d219a5926ba764860ed40c76","modified":1719406315097},{"_id":"public/1d5bcd45/diff_dense.png","hash":"541feaefbc8790820c45a5bf0317e69c69087c24","modified":1719406315097},{"_id":"public/1d5bcd45/structure.png","hash":"b71c6bfe51757237c56124d801bf0409c5d34b19","modified":1719406315097},{"_id":"public/da871ebe/alpha.png","hash":"4e3ad45447f5757d2cfcdf9d9555351233456c3b","modified":1719406315097},{"_id":"public/da871ebe/summarization.png","hash":"4934f3e42f20bc3a44ad07267e91a14c4c005543","modified":1719406315097},{"_id":"public/280fa97a/margin_dist.png","hash":"cf82f48158e4ba3e503cbe25cc811910804489cf","modified":1719406315097},{"_id":"public/f5c015c/fi_expected_token_num.png","hash":"52be2409ca9513de2e5ce10a0e77d8aa98dfc328","modified":1719406315097},{"_id":"public/f5c015c/fi_speed_and_op.png","hash":"9b5e3a6c9276309e7aa5a8848d52ef361e62bb36","modified":1719406315097},{"_id":"public/f5c015c/fi_walltime.png","hash":"b645987bec587e743021bc330de277416ef36d5e","modified":1719406315097},{"_id":"public/cc852861/paraphrasing_quality.png","hash":"fab44d68fc27f7bb2c06f758e537b9b249be0699","modified":1719406315097},{"_id":"public/c61d17e3/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1719406315097},{"_id":"public/c61d17e3/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1719406315097},{"_id":"public/45ee1a6d/infini_attention_process.png","hash":"259ea90040b7a5b98a27afcb992a6bee31707ab9","modified":1719406315097},{"_id":"public/45ee1a6d/infini_attention_structure.png","hash":"35f958d9ba50460689727c7038bf3a344000fa52","modified":1719406315097},{"_id":"public/45ee1a6d/lm_infinite_downstream.png","hash":"192160ca6972003a61678e2f7f2467f5bbd94451","modified":1719406315097},{"_id":"public/6a40bfa5/bn_algo.png","hash":"56f1ab55c0e94814e6e37c30421012ed82098d62","modified":1719406315097},{"_id":"public/6a40bfa5/deepnorm.png","hash":"4726d8a40d1d0db397005408295e1ba54809a7e4","modified":1719406315097},{"_id":"public/6a40bfa5/ics_define.png","hash":"6bf3240ef78bad2cf76897a29c05428f4c195fba","modified":1719406315097},{"_id":"public/6a40bfa5/deepnorm_result.png","hash":"138cafc159f1e2e02455c540b4754f7cbb7f521d","modified":1719406315097},{"_id":"public/41b6a819/9B.png","hash":"7de19972e48b1f43c501d60a1c43a28c46b198e8","modified":1719406315097},{"_id":"public/41b6a819/long_context_result.png","hash":"daea6f734d64bdf5d24c6e17a640f23b1bd35b5f","modified":1719406315097},{"_id":"public/3dc22f96/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1719406315097},{"_id":"public/44e38c1b/dbrx_long_perf_1.png","hash":"bc9c40bde860e78882965a25056a848aa4a89c77","modified":1719406315097},{"_id":"public/44e38c1b/ds_moe_expert_specialization.png","hash":"64947872486dd083a7076bfdfe67cb0626208579","modified":1719406315097},{"_id":"public/44e38c1b/ds_moe_less_activated_expert.png","hash":"ac7ad86dabe94564bdb17399231c6ddc7da83962","modified":1719406315097},{"_id":"public/44e38c1b/ds_model_param.png","hash":"7b838274937cf45d73e59ac1fb5c2034e46586ae","modified":1719406315097},{"_id":"public/44e38c1b/glam_compare_gpt3.png","hash":"311b21079599473054378e339885e0b87719e63e","modified":1719406315097},{"_id":"public/44e38c1b/mistral_8_7b_active_perf.png","hash":"0c67d935657e62b9e8eeabc6403c269e09016626","modified":1719406315097},{"_id":"public/44e38c1b/rnn_moe.png","hash":"2a2ee095b8cc0727daa2cdd0c63891e4a470eae8","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_remove_multiplications.png","hash":"52bc94bfe726dfc832534dde409154efe0ce7b0f","modified":1719406315097},{"_id":"public/44e38c1b/rnn_moe_137b.png","hash":"fc11cfc87b2994956a9adbee76621fe5d964dc30","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_z_loss_result.png","hash":"3edd8e9eb069c9557c98fd21600c02b3a1978cc5","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_more_add_noise.png","hash":"12d13a9ee6c28553626e469ed18d022e0176a873","modified":1719406315097},{"_id":"public/44e38c1b/vanilla_moe.png","hash":"7da0a6e9d529256107b5f6b287737ac47513a797","modified":1719406315097},{"_id":"public/1736008/transformer.png","hash":"9dddf171ca51f2ed1218baa9b84f4b98e9b911cf","modified":1719406315097},{"_id":"public/376db710/train_loss.png","hash":"ac1261a27009436fae1bf50412017e8f25ea7d38","modified":1719406315097},{"_id":"public/376db710/wsd_update.png","hash":"bdee7c642152f421d099748d5afd6570b7c99a5a","modified":1719406315097},{"_id":"public/f3acf042/data1.png","hash":"050a2ebcc9784a20ca811c99215b137c8257c0c7","modified":1719406315097},{"_id":"public/f3acf042/structure.png","hash":"61969133bdb9770d5139d27374eb3e3f4cc44d0e","modified":1719406315097},{"_id":"public/f5fb75e4/downstream_dataset_num.png","hash":"ac409ad0cd39c968f3d59a0ab7d4e75f9922d682","modified":1719406315097},{"_id":"public/1d5bcd45/lr_result.png","hash":"96674fe834c8223d29136a4c8d36abf28cb3c195","modified":1719406315097},{"_id":"public/473f2b43/result_4.png","hash":"6f554d1b6911c3db211a7e57e886e62f03b46ffd","modified":1719406315097},{"_id":"public/280fa97a/ablation.png","hash":"4c73c83eb527141e17d1109b6c2cff3488de6259","modified":1719406315097},{"_id":"public/280fa97a/intro.png","hash":"3ff9cce772cd825ec8b88591d576a5f52982d679","modified":1719406315097},{"_id":"public/280fa97a/benchmark.png","hash":"83084a5f64006000898d5252b3f8afccb635b3f0","modified":1719406315097},{"_id":"public/f5c015c/fi_t5_result.png","hash":"83b63aafceeb8f2bc3f89ee6a1e3caec7987a1c9","modified":1719406315097},{"_id":"public/cc852861/eng_ppl.png","hash":"fccca1509ebab2e89a3ceaad0dfeedc700de2691","modified":1719406315097},{"_id":"public/cc852861/paraphrasing_dataset_dist.png","hash":"e5754afcb70a45c0d11ee5db43c724350ec64257","modified":1719406315097},{"_id":"public/cc852861/paraphrasing_lost.png","hash":"b9d73b8022266af17789ab049c7adda621729cc9","modified":1719406315097},{"_id":"public/c61d17e3/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1719406315097},{"_id":"public/45ee1a6d/infini_attention_language_modeling.png","hash":"a49a7cc02694e7017c2a20dc0666046108b3c4c5","modified":1719406315097},{"_id":"public/45ee1a6d/infini_attention_passkey.png","hash":"811c5c677f7616b6625a0b86f2004f6d3ebeefe9","modified":1719406315097},{"_id":"public/45ee1a6d/lm_infinite_ablation.png","hash":"babe57024a1c2212405220124dfd376a8a2bcfb6","modified":1719406315097},{"_id":"public/45ee1a6d/infini_attention_booksum.png","hash":"276dfa014d35f7d3375fbb7cee6eed21127f9955","modified":1719406315097},{"_id":"public/45ee1a6d/stremingllm_attention_sink.png","hash":"933f34f3bc1d04e2b36f3305ac9fe2acd8bc9939","modified":1719406315097},{"_id":"public/45ee1a6d/xl_vanilla_sw.png","hash":"3259a751066a0083ef249a0412f43fb582e6544c","modified":1719406315097},{"_id":"public/6a40bfa5/bs_bn.png","hash":"aa28241d75f914603b9f7f67cc54db4e61bac668","modified":1719406315097},{"_id":"public/3dc22f96/Markdown _ 让排版变 Nice.html","hash":"c905c942579a520c7b3c788a00cdb9ae359d4a32","modified":1719406315097},{"_id":"public/41b6a819/base_model_eval.png","hash":"9b4e65d246865683f8e3348d74bfad03c937b65f","modified":1719406315097},{"_id":"public/6a40bfa5/rmsnorm_eff.png","hash":"350a7a2703eef1ee9357609ae5820bfc30835681","modified":1719406315097},{"_id":"public/41b6a819/perf.png","hash":"3c068a423dcd32cac7f1630bd69fbe5a4c6789af","modified":1719406315097},{"_id":"public/41b6a819/sft.png","hash":"ea9aea143af836012f44d21956ab5455487e9bfb","modified":1719406315097},{"_id":"public/3dc22f96/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1719406315097},{"_id":"public/3dc22f96/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1719406315097},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1719406315097},{"_id":"public/3dc22f96/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1719406315097},{"_id":"public/44e38c1b/cover.jpeg","hash":"6226a5276377816b37a20572a8b725af3ddf5760","modified":1719406315097},{"_id":"public/3dc22f96/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1719406315097},{"_id":"public/44e38c1b/dbrx_vs_open_models.png","hash":"ba0348d3fe68a27f8c6435c4c3a6d08d9c8869c6","modified":1719406315097},{"_id":"public/44e38c1b/dbrx_vs_closed_models.png","hash":"6211c05b7d69194f2d820622525273110467a0d5","modified":1719406315097},{"_id":"public/44e38c1b/ds_2b_less_expert.png","hash":"e03a00a194efd33890517b4ad642bca5566cf9df","modified":1719406315097},{"_id":"public/44e38c1b/ds_moe_ablation.png","hash":"108dc8d66ae0e370e7969403efd85178b9a8523a","modified":1719406315097},{"_id":"public/44e38c1b/gshard_moe_family.png","hash":"21a6c80f1dac39eb364fb417ed83afde2b212675","modified":1719406315097},{"_id":"public/44e38c1b/glam_related_model.png","hash":"0f109231fc1b425c7364401c41ce5f3aecfd76c7","modified":1719406315097},{"_id":"public/44e38c1b/mistral_8_22b_multiling.png","hash":"4af49ffc09a0de3793ec7137d3dfbddc9c309d38","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_capacity_factor_speed.png","hash":"60b624b7595e1f90763ea745ea358b6852eeefb0","modified":1719406315097},{"_id":"public/44e38c1b/glam_family.png","hash":"9d813f12f82d8702886f7ede72c5a25151390ba9","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_more_dense_layer.png","hash":"67c37482d73cd7ece7e384c0bd73e6891fc752e1","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_init.png","hash":"f9fb36a0defac7a5990b5c58a614f3165af0ae3e","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_dropout.png","hash":"63f36ca61aaa71e88ddf23339e63a9ccd898a6ce","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_distill_sft.png","hash":"7a59660f367634d68aa392698042f3d9cfe190da","modified":1719406315097},{"_id":"public/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1719406315097},{"_id":"public/376db710/128k_result.png","hash":"97987dc9ae6ef342ce5bb4a34072a558b11a8770","modified":1719406315097},{"_id":"public/376db710/cos_loss.png","hash":"2b5a94aea83ca359aac95432298a8b32b29672b0","modified":1719406315097},{"_id":"public/376db710/wsd_exp1.png","hash":"2c556e50353e6b1528918f310f3150afdfd2f549","modified":1719406315097},{"_id":"public/c4da56c0/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1719406315097},{"_id":"public/7381cae3/4.png","hash":"f9c2e248f1416369b19e434fd28e7160f85c9d22","modified":1719406315097},{"_id":"public/f5fb75e4/exp3_plot.png","hash":"5bf2e800c8583d545e30a084400ec34eca436025","modified":1719406315097},{"_id":"public/7bbe2df6/construct_tree.png","hash":"ba3dc50c0c35e13fb15e18ad7748494d7e20f532","modified":1719406315097},{"_id":"public/7bbe2df6/exp1.png","hash":"1d2454407467f240c2fa549a73589bcfa462a78b","modified":1719406315097},{"_id":"public/7bbe2df6/tree_attention.png","hash":"c3c0b10e0bd043235b307ebddb730b43a84fded4","modified":1719406315097},{"_id":"public/7bbe2df6/threshold.png","hash":"fb52a6f66fddabc8ae9f72d027f4605178f65fb1","modified":1719406315097},{"_id":"public/473f2b43/result_2.png","hash":"6baa634147220fed9edfff7c70e83c56a2b24913","modified":1719406315097},{"_id":"public/da871ebe/odpo_intro.png","hash":"e32faada4824a2654e32d125cb7dded9895f87dc","modified":1719406315097},{"_id":"public/da871ebe/toxicity_control.png","hash":"42173484be1fd33e29244f43d658ba03ec9bacb2","modified":1719406315097},{"_id":"public/f5c015c/fi_sd_algo.png","hash":"576ffae274518c5a4e6c049d199e0714b06bba86","modified":1719406315097},{"_id":"public/cc852861/eng_config.png","hash":"8edac537bd1aefea28406c414e0c0a4c888234be","modified":1719406315097},{"_id":"public/cc852861/pose_ppl.png","hash":"fded043b94c97c1a782869963f5dea371e257b80","modified":1719406315097},{"_id":"public/cc852861/pose_method.png","hash":"db8784c9e4b14c5963f62f072df6a4c3c5405874","modified":1719406315097},{"_id":"public/cc852861/pose_passkey.png","hash":"6202690a895f0114c90f6821c8cf1ad7388e1592","modified":1719406315097},{"_id":"public/45ee1a6d/lm_infinite_design.png","hash":"e96faad18cb526d201ce069f9ce09dc3c9c0d16e","modified":1719406315097},{"_id":"public/45ee1a6d/stremingllm_exp.png","hash":"cdd21419055ca9bed86b46675e118ad4bdf55544","modified":1719406315097},{"_id":"public/45ee1a6d/stremingllm_init_token_num.png","hash":"ffe78c31d901311249530e786bc5ed321e2e242f","modified":1719406315097},{"_id":"public/45ee1a6d/stremingllm_perf_4m.png","hash":"c4545d7f0bbd0c5f6baa85dd64b747a3723fdf2e","modified":1719406315097},{"_id":"public/6a40bfa5/warmup_effect.png","hash":"3e936786065e1ab9cbad17f5b86a5b8129720270","modified":1719406315097},{"_id":"public/41b6a819/eval.png","hash":"e78d1d820de4c455b9301124d6016a19762eae1f","modified":1719406315097},{"_id":"public/41b6a819/pretrain_data_dist.png","hash":"8c66a625723cb87ee67a9ff60d3614b369f50592","modified":1719406315097},{"_id":"public/336f2f3e/ntk_by_parts.png","hash":"5b49750dc6a2d1b878f34bc71e3961d96282499a","modified":1719406315097},{"_id":"public/44e38c1b/dbrx_infer_efficiency.png","hash":"34245e99c2b29dbd54104ccbbb3d8c15706307b9","modified":1719406315097},{"_id":"public/41b6a819/multimodal.png","hash":"b14a4eb4d377101acf7b50904b9ee0f1d473aacc","modified":1719406315097},{"_id":"public/44e38c1b/rnn_moe_perf.png","hash":"ca3c95d4b1c1c8f986d7adcacbf04da444e91610","modified":1719406315097},{"_id":"public/44e38c1b/rnn_moe_specilized.png","hash":"138ad5a388c77cc02202de794ec4cb734d633065","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_capacity_factor.png","hash":"a960540d3419de77c3823d343247abfddeadde1c","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_models.png","hash":"bc59770bc8ea44bfe480484a99aa9143acbaa6fa","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_multiling_specialization.png","hash":"bac636d8da61768adb5a9b7c5bd75547267ae470","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_structure.png","hash":"d906a9148e035025683e8da1eee5fa3d87164aa5","modified":1719406315097},{"_id":"public/376db710/param_search.png","hash":"82785189286d4e7b69d24998d14ca9b78fb4d890","modified":1719406315097},{"_id":"public/376db710/param_search_2.png","hash":"601bf31d9a7aaac8d1e1e60f1a4c2d40224160e5","modified":1719406315097},{"_id":"public/c4da56c0/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1719406315097},{"_id":"public/7381cae3/3.png","hash":"d0bbe995cc75de29aaa21f1abc144d3bedf5556f","modified":1719406315097},{"_id":"public/f3acf042/data2.png","hash":"29418adf51a48bca2340e506dd86dbb8882f913e","modified":1719406315097},{"_id":"public/7381cae3/6.png","hash":"c22d52b720cc97628669ba318f13c39692ff8c52","modified":1719406315097},{"_id":"public/f3acf042/mtbench.png","hash":"3411a742f62a58e4ff435ca06641e8830c2c80f6","modified":1719406315097},{"_id":"public/f3acf042/evaluation.png","hash":"ee53ffd386d5ba9b276dc3f55b87e68b5f8dc378","modified":1719406315097},{"_id":"public/1d5bcd45/perf.png","hash":"02a9e32039679d6f1249e0bd6bfbc3cff00228c8","modified":1719406315097},{"_id":"public/7bbe2df6/tree_attention_exp.png","hash":"0b516ebc1df1418eb6f5734b05f3975b9bc71d18","modified":1719406315097},{"_id":"public/f5fb75e4/metrics.png","hash":"7bb73cea5404b78b78799e7069c914c50f847b39","modified":1719406315097},{"_id":"public/473f2b43/result_1.png","hash":"725c6be46c42e8fc8184304bb0cdf5071b09e8b2","modified":1719406315097},{"_id":"public/473f2b43/dpo_loss_code.png","hash":"279b32cc1c4dfefa8790fcbb597659e8b974ac61","modified":1719406315097},{"_id":"public/da871ebe/scaling_function.png","hash":"b554e0bd697e72bb2e5e24a16c678d80c2efcc52","modified":1719406315097},{"_id":"public/da871ebe/sentiment_control.png","hash":"fc200cc3802fdee9d80e4bc259f4baca7b425ae7","modified":1719406315097},{"_id":"public/280fa97a/hyperparameters.png","hash":"b3595e75eff0cb8ea8f86fd9e2f8c6ae5f7892bc","modified":1719406315097},{"_id":"public/f5c015c/acce_alog.png","hash":"61f4653292bbd93debee66cebfb44ac7198e5818","modified":1719406315097},{"_id":"public/f5c015c/acce_k.png","hash":"ca37e1983347a1a835389de4d17047e3b0d02af4","modified":1719406315097},{"_id":"public/cc852861/paraphrasing_perf.png","hash":"5cf407e2e2ee61bb2b6bdae0570c3b8c4a3a9374","modified":1719406315097},{"_id":"public/c61d17e3/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1719406315097},{"_id":"public/c61d17e3/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1719406315097},{"_id":"public/45ee1a6d/infini_attention_compare.png","hash":"9b272eba596a790b1d40ef3a8b041bdffe1660d3","modified":1719406315097},{"_id":"public/45ee1a6d/lm_infinite_ppl_figure.png","hash":"34397c88e268b769c97a2b6e2ad63bf6ad5270ef","modified":1719406315097},{"_id":"public/45ee1a6d/streamingllm_model_ppl.png","hash":"2fc1c11d7cfa2598b414e5e4c145181ec9e10648","modified":1719406315097},{"_id":"public/45ee1a6d/xl_attention.png","hash":"395424d6c048880e143b9b2f93585597fbebebd7","modified":1719406315097},{"_id":"public/6a40bfa5/ics_measure.png","hash":"e9fe87cfea7dcef7cb66e1d76c17d883cbbc3cbd","modified":1719406315097},{"_id":"public/3dc22f96/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1719406315097},{"_id":"public/44e38c1b/dbrx_perf.png","hash":"035a992ef2e7e5a9165c9488e2696e38fa29165c","modified":1719406315097},{"_id":"public/44e38c1b/ds_16b_perf_1.png","hash":"d540e8a58e166c0ba894708c9b0c277c31107487","modified":1719406315097},{"_id":"public/44e38c1b/ds_16b_perf_2.png","hash":"6886e18a2e7601202e8721b83f998faf028e19eb","modified":1719406315097},{"_id":"public/44e38c1b/ds_moe_perf.png","hash":"b6ed751d2f171dac971bcf1d7f12e8a2d7fad388","modified":1719406315097},{"_id":"public/44e38c1b/ds_moe_structure.png","hash":"b0564913ecc1f78e5052dbc07eb65b3f048846e3","modified":1719406315097},{"_id":"public/44e38c1b/ds_moe_upper_bound_13b.png","hash":"cec21f0cf3809011ce210dffda35da3671147008","modified":1719406315097},{"_id":"public/44e38c1b/glam_compare_gpt3_2.png","hash":"89172dda5ac569780ea47e85bc43eaef1d6918ae","modified":1719406315097},{"_id":"public/44e38c1b/glam_model.png","hash":"63c95ea5ae77528f7d94a94b21cf12ed63e0bfdb","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_scaling_dense.png","hash":"b8de8c427de51d62e69915da7a97bf4a9b505317","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_pretrain_result.png","hash":"880f70f371b8392e3021bf56d282be5640c232f7","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_distill.png","hash":"be5cae81fafbda6b638ac185421bd04e00b7a60d","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_scaling_time.png","hash":"b56568665d2680cc0723269ae39dc4b60de1b01c","modified":1719406315097},{"_id":"public/14e576c/28.png","hash":"d438e857378575809c880b78ca715dc69e50b364","modified":1719406315097},{"_id":"public/f5fb75e4/exp1_compute.png","hash":"4b2bb4c651292fa438292ea1adcd5fb6530fec78","modified":1719406315097},{"_id":"public/1d5bcd45/exp_1.png","hash":"31f27f3659a4c3e0af9894e6a9b85cccec611889","modified":1719406315097},{"_id":"public/473f2b43/intro.png","hash":"c5175eef020ec3499aa67163813eab1c4c13a84a","modified":1719406315097},{"_id":"public/280fa97a/ln.png","hash":"4d4c831fc95c591ea0415e07dd5f46d1b1494e60","modified":1719406315097},{"_id":"public/f5c015c/fi_example.png","hash":"edbdb72af30cac5f036e47b3d0d426919f336e62","modified":1719406315097},{"_id":"public/cc852861/eng_tokens.png","hash":"b6c43c289004164e90de39c30e170de5ac1088aa","modified":1719406315097},{"_id":"public/cc852861/paraphrasing_dataset.png","hash":"b85a9077cffcace583a7dbcdbd235ab646086ea1","modified":1719406315097},{"_id":"public/45ee1a6d/lm_infinite_starting_tokens.png","hash":"18e3a03213a7275617201b71eb274cd8fd8b0bf9","modified":1719406315097},{"_id":"public/6a40bfa5/realformer.png","hash":"3bc805db3177c7e6521362b063543941da8d2bd3","modified":1719406315097},{"_id":"public/44e38c1b/ds_moe_sft.png","hash":"8bc26fcfef1aab448d0db0b28666852f62961a3b","modified":1719406315097},{"_id":"public/44e38c1b/glam_perf.png","hash":"01fb9d4f232e9f0b86abb91dbe5d8fb9fac456ce","modified":1719406315097},{"_id":"public/44e38c1b/gshard_perf.png","hash":"27157f620e2b7c4be60b2a58f7857a888794cde1","modified":1719406315097},{"_id":"public/44e38c1b/gshard_algo_1.png","hash":"aa8bcd982c78e4304c63f81e44b20519dc04f18f","modified":1719406315097},{"_id":"public/44e38c1b/gshard_result.png","hash":"32ce9bba1b65ceb2e69c079e113d8b4c524bc479","modified":1719406315097},{"_id":"public/44e38c1b/rnn_moe_hierarchical_gating.png","hash":"067160c735e9c0b8cff777df60d52dfca21ea783","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_perf.png","hash":"58927fa39b5e56e8da00144b417bbae5256d6bdf","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_distill_diff_model.png","hash":"05c4d885f5563e3dbd56c055a52df5c1531677a7","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_diff_expert_capacity.png","hash":"23f2234b45a2a05ff8b098e68a361a71f46816e9","modified":1719406315097},{"_id":"public/14e576c/21.png","hash":"fb2577b5fa73b06b786484b3723f7aa3819638a0","modified":1719406315097},{"_id":"public/376db710/scaling_law.png","hash":"19067e52c8d6e073c86fbf4c4fee258574f9b348","modified":1719406315097},{"_id":"public/c4da56c0/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1719406315097},{"_id":"public/f5fb75e4/downstream_dataset.png","hash":"a6cda82185bcb7c1f70931d792e10583825c2d01","modified":1719406315097},{"_id":"public/f5fb75e4/exp1_plot.png","hash":"27b2613a83f5873538e0e5832ea957e47625d719","modified":1719406315097},{"_id":"public/f5fb75e4/exp2_plot.png","hash":"e65d2e11a5a28b4b1ff8b54ffff65be2298d7c1f","modified":1719406315097},{"_id":"public/7bbe2df6/intro.png","hash":"ab8abb35042127509f264d1f0e8ce838350b08ff","modified":1719406315097},{"_id":"public/280fa97a/ln_effect.png","hash":"714ba6cdf67ea33d507e3358b113a94bcd24e1ce","modified":1719406315097},{"_id":"public/f5c015c/fi_alpha.png","hash":"f7f5a24106f1b9d16fd805b3ed3d3c2efb4a8c03","modified":1719406315097},{"_id":"public/cc852861/eng_data.png","hash":"9262b3bbc1b415d9c776723fe85c0a43f9fb562a","modified":1719406315097},{"_id":"public/cc852861/eng_data_dist.png","hash":"157286ce140bd5acc7c45fb12785649cc7214472","modified":1719406315097},{"_id":"public/41b6a819/cover.png","hash":"0493fd58fd2dad33394399d960924dbff6b386b1","modified":1719406315097},{"_id":"public/44e38c1b/ds_moe_comparison.png","hash":"20c7e90a390604d2146b4984678524046ad941b8","modified":1719406315097},{"_id":"public/44e38c1b/gshard_model.png","hash":"5179df7e42bb7dc49fb6f92f4ec68ed820aeaae2","modified":1719406315097},{"_id":"public/44e38c1b/mistral_8_22b_reasoning.png","hash":"ce00d9ba7b8a65eb238c99f79a17ca7a3cd2238a","modified":1719406315097},{"_id":"public/44e38c1b/mistral_8_22b_code.png","hash":"f9b78e0669e0c83f716c8e4abfa4d97b6f9b8143","modified":1719406315097},{"_id":"public/44e38c1b/st_moe_encoder_specialization.png","hash":"382cb53fcc2e9172ffd7554a04714feed4d9706b","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_scaling_step.png","hash":"dbfa55ab1c94e266344315fd215f2d646eaefda1","modified":1719406315097},{"_id":"public/14e576c/20.png","hash":"5d42628c8dac91c9671a58535b730e91966c0cbc","modified":1719406315097},{"_id":"public/376db710/batch_size.png","hash":"8e754e7b97b0290618e90e499d33f300e149b39c","modified":1719406315097},{"_id":"public/f5fb75e4/exp2_param.png","hash":"1412eccf1c2cf485bde50b1cbf9223ee5a5ff2cf","modified":1719406315097},{"_id":"public/1d5bcd45/normaization.png","hash":"8c4ce11d34256acc7e3800355186ab00f2236293","modified":1719406315097},{"_id":"public/280fa97a/main_results.png","hash":"a7388a503f0157c2ebe9ef765d63daab358b67d7","modified":1719406315097},{"_id":"public/cc852861/eng_needle_comp.png","hash":"e4c9f5c51548faf11a7ff64d23aae5bd2927ab0e","modified":1719406315097},{"_id":"public/cc852861/eng_sample.png","hash":"6b0d7ed89b16a3e9c6297219f33e59f204923828","modified":1719406315097},{"_id":"public/45ee1a6d/infini_attention_gating.png","hash":"5e875c3de6bde62ea8e15ea4995a56f9fd28d67c","modified":1719406315097},{"_id":"public/45ee1a6d/streamingllm_compare.png","hash":"40394890082b1666d1221f302ed52a80fc358477","modified":1719406315097},{"_id":"public/3dc22f96/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1719406315097},{"_id":"public/44e38c1b/ds_moe_145b.png","hash":"502a377d8625d78d2e3ba7281bfb11732c14a61c","modified":1719406315097},{"_id":"public/44e38c1b/ds_moe_upper_bound_2b.png","hash":"4338539fe123371c5512c730fc8a35864236323b","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_capacity_effect.png","hash":"16268929d0ec965d3771fccbb595b75d82f05912","modified":1719406315097},{"_id":"public/44e38c1b/switch_transformer_sft_result.png","hash":"0dd7cfaac788c5d112dd10fe98f0052b369420bf","modified":1719406315097},{"_id":"public/44e38c1b/vanilla_moe_result.png","hash":"e491e44cfff384422f3d9cf87cd5e52fd976aed7","modified":1719406315097},{"_id":"public/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1719406315097},{"_id":"public/280fa97a/gradient.png","hash":"58524a8fd10f4d76e8e419e1ed46bd9a99cf58d5","modified":1719406315097},{"_id":"public/cc852861/paraphrasing_example.png","hash":"946cdbb0a425b9d11b0ff587007885990abc9f99","modified":1719406315097},{"_id":"public/cc852861/paraphrasing_intro.png","hash":"6b810c88945281a9cdf9749941cdbe08346fd42b","modified":1719406315097},{"_id":"public/6a40bfa5/bn_ics.png","hash":"f92751ea20430f25caa3d6bb892c5894bf7509d6","modified":1719406315097},{"_id":"public/41b6a819/ict.png","hash":"2445ecbbf5ec6a96695f21c03a5fcbf67640b9f0","modified":1719406315097},{"_id":"public/14e576c/17.png","hash":"8ea1c5d90f3da5c469eb17aea50f377cb9c28ba0","modified":1719406315097},{"_id":"public/14e576c/18.png","hash":"fe0a8e7005110abca19bc7ae506f3e35042b70ec","modified":1719406315097},{"_id":"public/14e576c/26.png","hash":"f74d03dae65109740f48924f30b52a742b5e4273","modified":1719406315097},{"_id":"public/376db710/eval.png","hash":"982f977c073703051fdecb951c6231f7b74aa58a","modified":1719406315097},{"_id":"public/41b6a819/pretrain_data_pipeline.png","hash":"91218a2272eab9284904c91bacd8d8a40e3c1580","modified":1719406315097},{"_id":"public/14e576c/10.png","hash":"2c52d4f90dd9356eeb4c9a39f1df1038ccec4693","modified":1719406315097},{"_id":"public/14e576c/6.png","hash":"4b32a49bfead98f5238871b81076176e38168333","modified":1719406315097},{"_id":"public/14e576c/3.png","hash":"4c2c2a30d9ac8db03bab56da5d16ce2042ef73bc","modified":1719406315097},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1719406315097},{"_id":"public/14e576c/1.png","hash":"a8898b3f3b7c64fabc5fad9bf8ef5524501d2aeb","modified":1719406315097},{"_id":"public/14e576c/19.png","hash":"1d7b929e709657c9b7d7ca4da8eadc8c4ca4b3ca","modified":1719406315097},{"_id":"public/14e576c/24.png","hash":"b89b0a0ca774a4efc1ece628fb20379b5f6a0b69","modified":1719406315097},{"_id":"public/14e576c/8.png","hash":"92b0e3b75ce97bbaf4aa69e484216702293589ee","modified":1719406315097},{"_id":"public/14e576c/27.png","hash":"557c04a2134b6ae147e076cdb80de1730e937d9b","modified":1719406315097},{"_id":"public/14e576c/9.png","hash":"2a0fb56563b13411035ed41a3ad882f66f948b26","modified":1719406315097},{"_id":"public/14e576c/11.png","hash":"9b964f3aa6f82a09eb2f2f944508bf0a9d29efb3","modified":1719406315097},{"_id":"public/14e576c/22.png","hash":"3b27321ef8d76844f6720e1a27d65d1946d48ea7","modified":1719406315097},{"_id":"public/14e576c/23.png","hash":"9a9af6308620b59f2ee00a3d0da4e942d953e406","modified":1719406315097},{"_id":"public/14e576c/25.png","hash":"aa259b58be90eed6af0c4ba800a991b9464453d0","modified":1719406315097},{"_id":"public/14e576c/15.png","hash":"15abcbcf7340941e98dac7a0ab42d922e7fea1b4","modified":1719406315097},{"_id":"public/14e576c/2.png","hash":"768421239ad7c838dd86714fd9f17b3c73cbb887","modified":1719406315097},{"_id":"public/14e576c/4.png","hash":"4dacbfb89079d528da1208773961e1366debde9b","modified":1719406315097},{"_id":"public/14e576c/14.png","hash":"71f75960246f7528b3b83b84f8f91775f9e2fb45","modified":1719406315097},{"_id":"public/14e576c/12.png","hash":"c6c493b14e0a1cc4863a912c4ccc998de194bfc0","modified":1719406315097},{"_id":"public/3dc22f96/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1719406315097},{"_id":"public/14e576c/16.png","hash":"ba7b2bc65e10389cf9a87ddef69f462e806304f5","modified":1719406315097},{"_id":"public/14e576c/7.png","hash":"95640525b0706d3118eeb88c6c4c6217a96c39d0","modified":1719406315097},{"_id":"public/14e576c/13.png","hash":"68b7da3e3074e4d6995eaf96c7d8cf622eadffb7","modified":1719406315097},{"_id":"public/14e576c/5.png","hash":"7ae786b309a757c5f61a713c7ceef4d2824b024e","modified":1719406315097},{"_id":"public/f5c015c/speculative_decoding.png","hash":"fe277fa76f9f9c71e2030a41ca9eab458c33826a","modified":1719406315097},{"_id":"public/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1719406315097},{"_id":"public/c61d17e3/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1719406315097},{"_id":"public/6a40bfa5/ellipse_1.png","hash":"ef2470f6bf1511dc9aac9f1c6489b9d2ffdcb45f","modified":1719406315097},{"_id":"public/45ee1a6d/digimon.png","hash":"247f4059dd9671047f5d6707d8cef75a93d93f40","modified":1719406315097},{"_id":"public/b70b4a2d/norm_in_nlp.png","hash":"7be79b0e55d7d00ff6c16c247d0e506771453380","modified":1719406315097},{"_id":"public/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1719406315097},{"_id":"public/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1719406315097}],"Category":[{"name":"CS","_id":"clxvu4k9s0004314k9z5n2qt7"},{"name":"NLP","parent":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4k9w000i314k1ig5fcwv"},{"name":"LLM","parent":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka0001o314k17huehv3"}],"Data":[{"_id":"styles","data":".post-toc .nav .nav-child {\n  display: block;\n}\n.post-toc ol {\n  font-size: 13px;\n}\nbody {\n  background: url(\"/images/background/wallhaven-p97q73.png\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-size: cover;\n  background-position: 50% 50%;\n}\n:root {\n  --content-bg-color: rgba(32,32,32,0.816);\n}\n"}],"Page":[{"title":"about","date":"2024-01-31T10:57:44.000Z","type":"about","comments":0,"_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2024-01-31 18:57:44\ntype: \"about\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:21.349Z","path":"about/index.html","layout":"page","_id":"clxvu4k9o0000314kee19cdpv","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"categories","date":"2024-01-31T10:57:57.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2024-01-31 18:57:57\ntype: \"categories\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:37.077Z","path":"categories/index.html","layout":"page","_id":"clxvu4k9r0002314k30gm4wbc","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"tags","date":"2024-01-31T10:50:02.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2024-01-31 18:50:02\ntype: \"tags\"\ncomments: false\n---\n","updated":"2024-01-31T10:57:24.396Z","path":"tags/index.html","layout":"page","_id":"clxvu4k9s0006314k0weaasjq","content":"\n","length":0,"excerpt":"","more":"\n"}],"Post":[{"title":"理解LLM位置编码:RoPE","abbrlink":"a051710f","date":"2024-02-21T13:18:13.000Z","mathjax":true,"_content":"\n最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。\n\n# 关于RoPE\n\nRoPE（Rotary Position Embedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u>**以绝对位置编码形式实现的相对位置编码**</u></big>，兼顾了模型性能和效率。\n\n2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。\n\n苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。  \n\n# 以绝对位置编码的方式实现相对位置编码\n\n前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？\n\n先说原因：  \n\n在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。  \n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。  \n而使用相对位置编码则<u>**更容易外推**</u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。  \n但是传统相对位置编码的实现相对<u>**复杂**</u>，有些也会有<u>**计算效率低**</u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u>**线性注意力**</u>计算法模型中。  \n总结来说，就是绝对位置编码<u>**好实现**</u>，<u>**效率高**</u>，<u>**适用线性注意力**</u>，而相对位置编码<u>**易外推**</u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。  \n\n下面简单回顾一下绝对位置编码和相对位置编码。  \n\n（对位置编码比较熟悉的朋友可以直接跳到第3节。）  \n\n## 绝对位置编码\n\n先回顾一下带绝对位置编码的self-attention。  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$ 和 $x_j$ 分别是位置 $i$ 和 $j$ 的输入，$p$ 是对应位置的位置编码向量。  \n\n这里的位置编码$p$可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量 $x$ 和位置向量 $p$ 相加即可，相比attention中的softmax计算，element-wise addition操作的计算量非常小，是可以忽略不计的。\n\n大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把 $x + p$ 变成  $x * p$ 这样，效果上也是大差不差。\n\n## 相对位置编码\n\n在绝对位置编码中，可以在输入阶段就把 $x$ 和 $p$ 直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。  \n\n比如“我”这个词放在位置1时，形成一个 $e_1 = x_我 + p_1$ 这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量 $e_8 = x_我 + p_8$ 。两个向量 $e_1$ 和 $e_8$ 虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u>**耦合**</u>在一起共同构成了一个完整的输入。  \n\n直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。\n\n扯远了，现在回来看一下相对位置编码。把公式（1）中的 $q_{i}k_{j}^{T}$展开来  \n\n$$\\begin{align*}q_ik_j^\\top&=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n和位置相关的有 $p_iW_\\mathbb{Q}$ 和 $W_K^\\top p_j^\\top$ 两项。  \n\n### Google式\n\n在最早引入相对位置编码的Google的论文《Self-Attention with Relative Position Representations》中，把第一项 $p_iW_\\mathbb{Q}$ 去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置 $j$），把第二项 $W_K^\\top p_j^\\top$ 改成和位置 $i$、$j$ 都相关的位置向量 $R_{ij}^K$，于是在这个使用相对位置编码的attention计算中，<u>**不再是直接计算input projection的内积来获取权重**</u>，而变成  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ 是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n其中 $p_\\mathrm{K}$ 就是可训练的向量或者三角函数向量。 \n\n为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系**理应**更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到>256的长度。\n\n本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle的方法把 $p_{j}W_{\\mathrm{V}}$ 也改成了包含相对位置信息的向量\n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$ 和 $R_{ij}^K$ 相似，都是一个相对位置向量 + clip操作。\n\n### XLNET式\n\nXLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。  \n\n在公式（2）的基础上继续展开  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n把绝对位置相关的几个参数改成相对位置相关的参数，变成：\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n把 $p_i$ 变成了两个可训练的向量 $u$ 和 $\\nu$ ，把 $p_j$ 变成相对位置向量 $R_{i-j}^\\top$ 。  \n\n实际实现上可以把 $u$ 和 $\\nu$ 后面跟着的矩阵省掉了，去掉这个线性变化不影响 $u$ 和 $\\nu$ 的训练，变成\n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\n此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\n可以看到，Google式和XLNET式的相对位置编码在权重 $\\mathrm{a_{i,j}}$ 的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置 $i$ 、 $j$ 都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。\n\n当然，也有简单一点的实现，比如T5的方法。  \n\n### T5式\n\n公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置 $i$ 和位置 $j$ 的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\n（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）\n\n## 对比\n\n看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。  \n\n公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法\n\n从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。  \n\n总之在实现方式上和计算效率上，绝对位置编码具有一些优势。\n\n而在输入输出窗口外推方面，相对位置编码有着天然的优势。\n\n另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear attention方案中去，这个以后再展开讲（又挖了个坑）。  \n\n# RoPE的设计思路\n\n## 保持attention计算形式\n\n回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。\n\n先说设计思路：  \n\n首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端 = 内积 + softmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。  \n\n也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n其中 $q_m$ 是在位置 $m$ 的query向量，$k_n$ 是在位置 $n$ 的key向量，$f_q$ 和 $f_k$ 是分别针对这query和key向量的操作函数。  \n\n我们的任务就是要找到一组 $f_q$ 、 $f_k$ 和 $g$ ，使得公式（11）恒成立。  \n\n当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？  \n\n## 借用复数寻找组合\n\n式（11）中， $g$ 的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。\n\n这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量  \n\n{% asset_img complex_number.png 282 401 复数平面 %}\n\n现在考虑query和key向量都是2维的情况，那么可以代入复数的操作  \n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）  \n\n那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n其中 $\\boldsymbol{k}_n^*$ 是 $\\boldsymbol{k}_n$ 的共轭复数。  \n\n（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）\n\n共轭复数是这样的关系  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n先证明一下这个组合的正确性，是不是真的满足公式（11）。  \n\n（也可以先跳过证明，选择先相信这个组合）  \n\n回顾一下欧拉公式  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n因为现在我们讨论的是2维的情况，那2维向量 $q_m$ 可以用一个复数来表示  \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n那从复数角度来看，就有\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）  \n\n类似地，有\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n和\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n则有  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n用了三角函数和差公式\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n再看 $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n证毕。\n\n## “旋转”位置编码\n\n发现式（17）可以写成这样\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n同样地  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n如果从向量视角来看，则有  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n看式（22）和（23），可以看到等号右边都有  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n这正是一个二维平面的旋转矩阵。 $f_q$ 、 $f_k$ 的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。  \n\n这也是为什么叫做“旋转”位置编码。\n\n## 从2维推广到高维\n\n我们现在已经确认，对于2维的情况，经过 $f_q$ 、 $f_k$ 和 $g$ 这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？  \n\n答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$ 是的输入向量的维度，由于是两个两个一组，所以一共有 $d/2$ 组小旋转矩阵，这 $d/2$ 组矩阵为了区分，设计使用了不同的 $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n那么在实际操作的时候，给位置 $m$ 和位置 $n$ 的输入向量分别乘以 $R_m$ 和 $R_n$，再进行self-attention，就能获得仅使用相对位置信息编码的效果。  \n\n另外 $\\theta$ 是怎么来的呢？这里是参考了Google最初在《Attention is All You Need》中提出的，这里就先不展开了，可以看看论文原文。\n\n## 高效率实现\n\n式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\n只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。  \n\n另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。\n\n## 远程衰减的特性\n\n至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。  \n\n直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。\n\n回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个 $\\theta$ 的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。  \n\n证明过程这里就偷偷懒略过了，具体可以看[Roformer的论文](https://arxiv.org/abs/2104.09864)或者[苏神的博客](https://spaces.ac.cn/archives/8265)。  \n\n当 $d = 128$ 时，画出来的图像如下\n\n{% asset_img remote_attenuation.png 775 457 远程衰减 %}  \n\n# 小结  \n\n总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。\n\n# Reference\n【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130  \n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265  \n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n【4】十分钟读懂旋转编码（RoPE） https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLM位置编码RoPE.md","raw":"---\ntitle: 理解LLM位置编码:RoPE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - positional encoding\n  - RoPE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: a051710f\ndate: 2024-02-21 21:18:13\nmathjax: true\n---\n\n最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。\n\n# 关于RoPE\n\nRoPE（Rotary Position Embedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u>**以绝对位置编码形式实现的相对位置编码**</u></big>，兼顾了模型性能和效率。\n\n2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。\n\n苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。  \n\n# 以绝对位置编码的方式实现相对位置编码\n\n前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？\n\n先说原因：  \n\n在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。  \n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。  \n而使用相对位置编码则<u>**更容易外推**</u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。  \n但是传统相对位置编码的实现相对<u>**复杂**</u>，有些也会有<u>**计算效率低**</u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u>**线性注意力**</u>计算法模型中。  \n总结来说，就是绝对位置编码<u>**好实现**</u>，<u>**效率高**</u>，<u>**适用线性注意力**</u>，而相对位置编码<u>**易外推**</u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。  \n\n下面简单回顾一下绝对位置编码和相对位置编码。  \n\n（对位置编码比较熟悉的朋友可以直接跳到第3节。）  \n\n## 绝对位置编码\n\n先回顾一下带绝对位置编码的self-attention。  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$ 和 $x_j$ 分别是位置 $i$ 和 $j$ 的输入，$p$ 是对应位置的位置编码向量。  \n\n这里的位置编码$p$可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量 $x$ 和位置向量 $p$ 相加即可，相比attention中的softmax计算，element-wise addition操作的计算量非常小，是可以忽略不计的。\n\n大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把 $x + p$ 变成  $x * p$ 这样，效果上也是大差不差。\n\n## 相对位置编码\n\n在绝对位置编码中，可以在输入阶段就把 $x$ 和 $p$ 直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。  \n\n比如“我”这个词放在位置1时，形成一个 $e_1 = x_我 + p_1$ 这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量 $e_8 = x_我 + p_8$ 。两个向量 $e_1$ 和 $e_8$ 虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u>**耦合**</u>在一起共同构成了一个完整的输入。  \n\n直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。\n\n扯远了，现在回来看一下相对位置编码。把公式（1）中的 $q_{i}k_{j}^{T}$展开来  \n\n$$\\begin{align*}q_ik_j^\\top&=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n和位置相关的有 $p_iW_\\mathbb{Q}$ 和 $W_K^\\top p_j^\\top$ 两项。  \n\n### Google式\n\n在最早引入相对位置编码的Google的论文《Self-Attention with Relative Position Representations》中，把第一项 $p_iW_\\mathbb{Q}$ 去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置 $j$），把第二项 $W_K^\\top p_j^\\top$ 改成和位置 $i$、$j$ 都相关的位置向量 $R_{ij}^K$，于是在这个使用相对位置编码的attention计算中，<u>**不再是直接计算input projection的内积来获取权重**</u>，而变成  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ 是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n其中 $p_\\mathrm{K}$ 就是可训练的向量或者三角函数向量。 \n\n为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系**理应**更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到>256的长度。\n\n本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle的方法把 $p_{j}W_{\\mathrm{V}}$ 也改成了包含相对位置信息的向量\n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$ 和 $R_{ij}^K$ 相似，都是一个相对位置向量 + clip操作。\n\n### XLNET式\n\nXLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。  \n\n在公式（2）的基础上继续展开  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n把绝对位置相关的几个参数改成相对位置相关的参数，变成：\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n把 $p_i$ 变成了两个可训练的向量 $u$ 和 $\\nu$ ，把 $p_j$ 变成相对位置向量 $R_{i-j}^\\top$ 。  \n\n实际实现上可以把 $u$ 和 $\\nu$ 后面跟着的矩阵省掉了，去掉这个线性变化不影响 $u$ 和 $\\nu$ 的训练，变成\n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\n此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\n可以看到，Google式和XLNET式的相对位置编码在权重 $\\mathrm{a_{i,j}}$ 的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置 $i$ 、 $j$ 都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。\n\n当然，也有简单一点的实现，比如T5的方法。  \n\n### T5式\n\n公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置 $i$ 和位置 $j$ 的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\n（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）\n\n## 对比\n\n看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。  \n\n公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法\n\n从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。  \n\n总之在实现方式上和计算效率上，绝对位置编码具有一些优势。\n\n而在输入输出窗口外推方面，相对位置编码有着天然的优势。\n\n另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear attention方案中去，这个以后再展开讲（又挖了个坑）。  \n\n# RoPE的设计思路\n\n## 保持attention计算形式\n\n回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。\n\n先说设计思路：  \n\n首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端 = 内积 + softmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。  \n\n也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n其中 $q_m$ 是在位置 $m$ 的query向量，$k_n$ 是在位置 $n$ 的key向量，$f_q$ 和 $f_k$ 是分别针对这query和key向量的操作函数。  \n\n我们的任务就是要找到一组 $f_q$ 、 $f_k$ 和 $g$ ，使得公式（11）恒成立。  \n\n当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？  \n\n## 借用复数寻找组合\n\n式（11）中， $g$ 的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。\n\n这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量  \n\n{% asset_img complex_number.png 282 401 复数平面 %}\n\n现在考虑query和key向量都是2维的情况，那么可以代入复数的操作  \n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）  \n\n那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n其中 $\\boldsymbol{k}_n^*$ 是 $\\boldsymbol{k}_n$ 的共轭复数。  \n\n（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）\n\n共轭复数是这样的关系  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n先证明一下这个组合的正确性，是不是真的满足公式（11）。  \n\n（也可以先跳过证明，选择先相信这个组合）  \n\n回顾一下欧拉公式  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n因为现在我们讨论的是2维的情况，那2维向量 $q_m$ 可以用一个复数来表示  \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n那从复数角度来看，就有\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）  \n\n类似地，有\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n和\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n则有  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n用了三角函数和差公式\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n再看 $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n证毕。\n\n## “旋转”位置编码\n\n发现式（17）可以写成这样\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n同样地  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n如果从向量视角来看，则有  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n看式（22）和（23），可以看到等号右边都有  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n这正是一个二维平面的旋转矩阵。 $f_q$ 、 $f_k$ 的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。  \n\n这也是为什么叫做“旋转”位置编码。\n\n## 从2维推广到高维\n\n我们现在已经确认，对于2维的情况，经过 $f_q$ 、 $f_k$ 和 $g$ 这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？  \n\n答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$ 是的输入向量的维度，由于是两个两个一组，所以一共有 $d/2$ 组小旋转矩阵，这 $d/2$ 组矩阵为了区分，设计使用了不同的 $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n那么在实际操作的时候，给位置 $m$ 和位置 $n$ 的输入向量分别乘以 $R_m$ 和 $R_n$，再进行self-attention，就能获得仅使用相对位置信息编码的效果。  \n\n另外 $\\theta$ 是怎么来的呢？这里是参考了Google最初在《Attention is All You Need》中提出的，这里就先不展开了，可以看看论文原文。\n\n## 高效率实现\n\n式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\n只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。  \n\n另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。\n\n## 远程衰减的特性\n\n至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。  \n\n直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。\n\n回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个 $\\theta$ 的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。  \n\n证明过程这里就偷偷懒略过了，具体可以看[Roformer的论文](https://arxiv.org/abs/2104.09864)或者[苏神的博客](https://spaces.ac.cn/archives/8265)。  \n\n当 $d = 128$ 时，画出来的图像如下\n\n{% asset_img remote_attenuation.png 775 457 远程衰减 %}  \n\n# 小结  \n\n总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。\n\n# Reference\n【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130  \n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265  \n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n【4】十分钟读懂旋转编码（RoPE） https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLM位置编码RoPE","published":1,"updated":"2024-04-05T06:44:27.271Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9q0001314k8wto8dcn","content":"<p>最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。</p>\n<h1 id=\"关于rope\">关于RoPE</h1>\n<p>RoPE（Rotary Position\nEmbedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u><strong>以绝对位置编码形式实现的相对位置编码</strong></u></big>，兼顾了模型性能和效率。</p>\n<p>2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。</p>\n<p>苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。</p>\n<h1 id=\"以绝对位置编码的方式实现相对位置编码\">以绝对位置编码的方式实现相对位置编码</h1>\n<p>前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？</p>\n<p>先说原因：</p>\n<p>在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。<br>\n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。<br>\n而使用相对位置编码则<u><strong>更容易外推</strong></u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。<br>\n但是传统相对位置编码的实现相对<u><strong>复杂</strong></u>，有些也会有<u><strong>计算效率低</strong></u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u><strong>线性注意力</strong></u>计算法模型中。<br>\n总结来说，就是绝对位置编码<u><strong>好实现</strong></u>，<u><strong>效率高</strong></u>，<u><strong>适用线性注意力</strong></u>，而相对位置编码<u><strong>易外推</strong></u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。</p>\n<p>下面简单回顾一下绝对位置编码和相对位置编码。</p>\n<p>（对位置编码比较熟悉的朋友可以直接跳到第3节。）</p>\n<h2 id=\"绝对位置编码\">绝对位置编码</h2>\n<p>先回顾一下带绝对位置编码的self-attention。</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span> 和 <span class=\"math inline\">\\(x_j\\)</span> 分别是位置 <span class=\"math inline\">\\(i\\)</span> 和 <span class=\"math inline\">\\(j\\)</span> 的输入，<span class=\"math inline\">\\(p\\)</span> 是对应位置的位置编码向量。</p>\n<p>这里的位置编码<span class=\"math inline\">\\(p\\)</span>可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量\n<span class=\"math inline\">\\(x\\)</span> 和位置向量 <span class=\"math inline\">\\(p\\)</span>\n相加即可，相比attention中的softmax计算，element-wise\naddition操作的计算量非常小，是可以忽略不计的。</p>\n<p>大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把\n<span class=\"math inline\">\\(x + p\\)</span> 变成 <span class=\"math inline\">\\(x * p\\)</span> 这样，效果上也是大差不差。</p>\n<h2 id=\"相对位置编码\">相对位置编码</h2>\n<p>在绝对位置编码中，可以在输入阶段就把 <span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(p\\)</span>\n直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。</p>\n<p>比如“我”这个词放在位置1时，形成一个 <span class=\"math inline\">\\(e_1 =\nx_我 + p_1\\)</span>\n这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量\n<span class=\"math inline\">\\(e_8 = x_我 + p_8\\)</span> 。两个向量 <span class=\"math inline\">\\(e_1\\)</span> 和 <span class=\"math inline\">\\(e_8\\)</span>\n虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u><strong>耦合</strong></u>在一起共同构成了一个完整的输入。</p>\n<p>直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。</p>\n<p>扯远了，现在回来看一下相对位置编码。把公式（1）中的 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span>展开来</p>\n<p><span class=\"math display\">\\[\\begin{align*}q_ik_j^\\top&amp;=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p>和位置相关的有 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n和 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 两项。</p>\n<h3 id=\"google式\">Google式</h3>\n<p>在最早引入相对位置编码的Google的论文《Self-Attention with Relative\nPosition Representations》中，把第一项 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置\n<span class=\"math inline\">\\(j\\)</span>），把第二项 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 改成和位置 <span class=\"math inline\">\\(i\\)</span>、<span class=\"math inline\">\\(j\\)</span>\n都相关的位置向量 <span class=\"math inline\">\\(R_{ij}^K\\)</span>，于是在这个使用相对位置编码的attention计算中，<u><strong>不再是直接计算input\nprojection的内积来获取权重</strong></u>，而变成</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\n是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n就是可训练的向量或者三角函数向量。</p>\n<p>为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系<strong>理应</strong>更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到&gt;256的长度。</p>\n<p>本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google的方法把 <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n也改成了包含相对位置信息的向量</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> 和\n<span class=\"math inline\">\\(R_{ij}^K\\)</span> 相似，都是一个相对位置向量\n+ clip操作。</p>\n<h3 id=\"xlnet式\">XLNET式</h3>\n<p>XLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。</p>\n<p>在公式（2）的基础上继续展开</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p>把绝对位置相关的几个参数改成相对位置相关的参数，变成：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p>把 <span class=\"math inline\">\\(p_i\\)</span> 变成了两个可训练的向量\n<span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> ，把 <span class=\"math inline\">\\(p_j\\)</span> 变成相对位置向量 <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> 。</p>\n<p>实际实现上可以把 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span>\n后面跟着的矩阵省掉了，去掉这个线性变化不影响 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> 的训练，变成</p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>可以看到，Google式和XLNET式的相对位置编码在权重 <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置\n<span class=\"math inline\">\\(i\\)</span> 、 <span class=\"math inline\">\\(j\\)</span>\n都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。</p>\n<p>当然，也有简单一点的实现，比如T5的方法。</p>\n<h3 id=\"t5式\">T5式</h3>\n<p>公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置\n<span class=\"math inline\">\\(i\\)</span> 和位置 <span class=\"math inline\">\\(j\\)</span>\n的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）</p>\n<h2 id=\"对比\">对比</h2>\n<p>看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。</p>\n<p>公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法</p>\n<p>从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。</p>\n<p>总之在实现方式上和计算效率上，绝对位置编码具有一些优势。</p>\n<p>而在输入输出窗口外推方面，相对位置编码有着天然的优势。</p>\n<p>另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear\nattention方案中去，这个以后再展开讲（又挖了个坑）。</p>\n<h1 id=\"rope的设计思路\">RoPE的设计思路</h1>\n<h2 id=\"保持attention计算形式\">保持attention计算形式</h2>\n<p>回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。</p>\n<p>先说设计思路：</p>\n<p>首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端\n= 内积 +\nsoftmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。</p>\n<p>也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是</p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(q_m\\)</span> 是在位置 <span class=\"math inline\">\\(m\\)</span> 的query向量，<span class=\"math inline\">\\(k_n\\)</span> 是在位置 <span class=\"math inline\">\\(n\\)</span> 的key向量，<span class=\"math inline\">\\(f_q\\)</span> 和 <span class=\"math inline\">\\(f_k\\)</span>\n是分别针对这query和key向量的操作函数。</p>\n<p>我们的任务就是要找到一组 <span class=\"math inline\">\\(f_q\\)</span> 、\n<span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span> ，使得公式（11）恒成立。</p>\n<p>当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？</p>\n<h2 id=\"借用复数寻找组合\">借用复数寻找组合</h2>\n<p>式（11）中， <span class=\"math inline\">\\(g\\)</span>\n的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。</p>\n<p>这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"复数平面\">\n<p>现在考虑query和key向量都是2维的情况，那么可以代入复数的操作<br>\n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）</p>\n<p>那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span> 是 <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> 的共轭复数。</p>\n<p>（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）</p>\n<p>共轭复数是这样的关系</p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>先证明一下这个组合的正确性，是不是真的满足公式（11）。</p>\n<p>（也可以先跳过证明，选择先相信这个组合）</p>\n<p>回顾一下欧拉公式</p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>因为现在我们讨论的是2维的情况，那2维向量 <span class=\"math inline\">\\(q_m\\)</span> 可以用一个复数来表示</p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p>那从复数角度来看，就有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）</p>\n<p>类似地，有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p>和</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p>则有<br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p>用了三角函数和差公式 <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p>再看 <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p>证毕。</p>\n<h2 id=\"旋转位置编码\">“旋转”位置编码</h2>\n<p>发现式（17）可以写成这样</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p>同样地</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p>如果从向量视角来看，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>看式（22）和（23），可以看到等号右边都有</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p>这正是一个二维平面的旋转矩阵。 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span>\n的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。</p>\n<p>这也是为什么叫做“旋转”位置编码。</p>\n<h2 id=\"从2维推广到高维\">从2维推广到高维</h2>\n<p>我们现在已经确认，对于2维的情况，经过 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span>\n这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？</p>\n<p>答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n是的输入向量的维度，由于是两个两个一组，所以一共有 <span class=\"math inline\">\\(d/2\\)</span> 组小旋转矩阵，这 <span class=\"math inline\">\\(d/2\\)</span> 组矩阵为了区分，设计使用了不同的\n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p>那么在实际操作的时候，给位置 <span class=\"math inline\">\\(m\\)</span>\n和位置 <span class=\"math inline\">\\(n\\)</span> 的输入向量分别乘以 <span class=\"math inline\">\\(R_m\\)</span> 和 <span class=\"math inline\">\\(R_n\\)</span>，再进行self-attention，就能获得仅使用相对位置信息编码的效果。</p>\n<p>另外 <span class=\"math inline\">\\(\\theta\\)</span>\n是怎么来的呢？这里是参考了Google最初在《Attention is All You\nNeed》中提出的，这里就先不展开了，可以看看论文原文。</p>\n<h2 id=\"高效率实现\">高效率实现</h2>\n<p>式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。</p>\n<p>另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。</p>\n<h2 id=\"远程衰减的特性\">远程衰减的特性</h2>\n<p>至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。</p>\n<p>直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。</p>\n<p>回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个\n<span class=\"math inline\">\\(\\theta\\)</span>\n的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。</p>\n<p>证明过程这里就偷偷懒略过了，具体可以看<a href=\"https://arxiv.org/abs/2104.09864\">Roformer的论文</a>或者<a href=\"https://spaces.ac.cn/archives/8265\">苏神的博客</a>。</p>\n<p>当 <span class=\"math inline\">\\(d = 128\\)</span>\n时，画出来的图像如下</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"远程衰减\">\n<h1 id=\"小结\">小结</h1>\n<p>总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130<br>\n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265<br>\n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n【4】十分钟读懂旋转编码（RoPE）\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":14264,"excerpt":"","more":"<p>最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。</p>\n<h1 id=\"关于rope\">关于RoPE</h1>\n<p>RoPE（Rotary Position\nEmbedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u><strong>以绝对位置编码形式实现的相对位置编码</strong></u></big>，兼顾了模型性能和效率。</p>\n<p>2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。</p>\n<p>苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。</p>\n<h1 id=\"以绝对位置编码的方式实现相对位置编码\">以绝对位置编码的方式实现相对位置编码</h1>\n<p>前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？</p>\n<p>先说原因：</p>\n<p>在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。<br>\n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。<br>\n而使用相对位置编码则<u><strong>更容易外推</strong></u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。<br>\n但是传统相对位置编码的实现相对<u><strong>复杂</strong></u>，有些也会有<u><strong>计算效率低</strong></u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u><strong>线性注意力</strong></u>计算法模型中。<br>\n总结来说，就是绝对位置编码<u><strong>好实现</strong></u>，<u><strong>效率高</strong></u>，<u><strong>适用线性注意力</strong></u>，而相对位置编码<u><strong>易外推</strong></u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。</p>\n<p>下面简单回顾一下绝对位置编码和相对位置编码。</p>\n<p>（对位置编码比较熟悉的朋友可以直接跳到第3节。）</p>\n<h2 id=\"绝对位置编码\">绝对位置编码</h2>\n<p>先回顾一下带绝对位置编码的self-attention。</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span> 和 <span class=\"math inline\">\\(x_j\\)</span> 分别是位置 <span class=\"math inline\">\\(i\\)</span> 和 <span class=\"math inline\">\\(j\\)</span> 的输入，<span class=\"math inline\">\\(p\\)</span> 是对应位置的位置编码向量。</p>\n<p>这里的位置编码<span class=\"math inline\">\\(p\\)</span>可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量\n<span class=\"math inline\">\\(x\\)</span> 和位置向量 <span class=\"math inline\">\\(p\\)</span>\n相加即可，相比attention中的softmax计算，element-wise\naddition操作的计算量非常小，是可以忽略不计的。</p>\n<p>大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把\n<span class=\"math inline\">\\(x + p\\)</span> 变成 <span class=\"math inline\">\\(x * p\\)</span> 这样，效果上也是大差不差。</p>\n<h2 id=\"相对位置编码\">相对位置编码</h2>\n<p>在绝对位置编码中，可以在输入阶段就把 <span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(p\\)</span>\n直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。</p>\n<p>比如“我”这个词放在位置1时，形成一个 <span class=\"math inline\">\\(e_1 =\nx_我 + p_1\\)</span>\n这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量\n<span class=\"math inline\">\\(e_8 = x_我 + p_8\\)</span> 。两个向量 <span class=\"math inline\">\\(e_1\\)</span> 和 <span class=\"math inline\">\\(e_8\\)</span>\n虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u><strong>耦合</strong></u>在一起共同构成了一个完整的输入。</p>\n<p>直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。</p>\n<p>扯远了，现在回来看一下相对位置编码。把公式（1）中的 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span>展开来</p>\n<p><span class=\"math display\">\\[\\begin{align*}q_ik_j^\\top&amp;=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p>和位置相关的有 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n和 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 两项。</p>\n<h3 id=\"google式\">Google式</h3>\n<p>在最早引入相对位置编码的Google的论文《Self-Attention with Relative\nPosition Representations》中，把第一项 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置\n<span class=\"math inline\">\\(j\\)</span>），把第二项 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 改成和位置 <span class=\"math inline\">\\(i\\)</span>、<span class=\"math inline\">\\(j\\)</span>\n都相关的位置向量 <span class=\"math inline\">\\(R_{ij}^K\\)</span>，于是在这个使用相对位置编码的attention计算中，<u><strong>不再是直接计算input\nprojection的内积来获取权重</strong></u>，而变成</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\n是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n就是可训练的向量或者三角函数向量。</p>\n<p>为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系<strong>理应</strong>更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到&gt;256的长度。</p>\n<p>本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google的方法把 <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n也改成了包含相对位置信息的向量</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> 和\n<span class=\"math inline\">\\(R_{ij}^K\\)</span> 相似，都是一个相对位置向量\n+ clip操作。</p>\n<h3 id=\"xlnet式\">XLNET式</h3>\n<p>XLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。</p>\n<p>在公式（2）的基础上继续展开</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p>把绝对位置相关的几个参数改成相对位置相关的参数，变成：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p>把 <span class=\"math inline\">\\(p_i\\)</span> 变成了两个可训练的向量\n<span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> ，把 <span class=\"math inline\">\\(p_j\\)</span> 变成相对位置向量 <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> 。</p>\n<p>实际实现上可以把 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span>\n后面跟着的矩阵省掉了，去掉这个线性变化不影响 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> 的训练，变成</p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>可以看到，Google式和XLNET式的相对位置编码在权重 <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置\n<span class=\"math inline\">\\(i\\)</span> 、 <span class=\"math inline\">\\(j\\)</span>\n都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。</p>\n<p>当然，也有简单一点的实现，比如T5的方法。</p>\n<h3 id=\"t5式\">T5式</h3>\n<p>公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置\n<span class=\"math inline\">\\(i\\)</span> 和位置 <span class=\"math inline\">\\(j\\)</span>\n的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）</p>\n<h2 id=\"对比\">对比</h2>\n<p>看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。</p>\n<p>公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法</p>\n<p>从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。</p>\n<p>总之在实现方式上和计算效率上，绝对位置编码具有一些优势。</p>\n<p>而在输入输出窗口外推方面，相对位置编码有着天然的优势。</p>\n<p>另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear\nattention方案中去，这个以后再展开讲（又挖了个坑）。</p>\n<h1 id=\"rope的设计思路\">RoPE的设计思路</h1>\n<h2 id=\"保持attention计算形式\">保持attention计算形式</h2>\n<p>回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。</p>\n<p>先说设计思路：</p>\n<p>首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端\n= 内积 +\nsoftmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。</p>\n<p>也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是</p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(q_m\\)</span> 是在位置 <span class=\"math inline\">\\(m\\)</span> 的query向量，<span class=\"math inline\">\\(k_n\\)</span> 是在位置 <span class=\"math inline\">\\(n\\)</span> 的key向量，<span class=\"math inline\">\\(f_q\\)</span> 和 <span class=\"math inline\">\\(f_k\\)</span>\n是分别针对这query和key向量的操作函数。</p>\n<p>我们的任务就是要找到一组 <span class=\"math inline\">\\(f_q\\)</span> 、\n<span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span> ，使得公式（11）恒成立。</p>\n<p>当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？</p>\n<h2 id=\"借用复数寻找组合\">借用复数寻找组合</h2>\n<p>式（11）中， <span class=\"math inline\">\\(g\\)</span>\n的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。</p>\n<p>这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"复数平面\">\n<p>现在考虑query和key向量都是2维的情况，那么可以代入复数的操作<br>\n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）</p>\n<p>那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span> 是 <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> 的共轭复数。</p>\n<p>（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）</p>\n<p>共轭复数是这样的关系</p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>先证明一下这个组合的正确性，是不是真的满足公式（11）。</p>\n<p>（也可以先跳过证明，选择先相信这个组合）</p>\n<p>回顾一下欧拉公式</p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>因为现在我们讨论的是2维的情况，那2维向量 <span class=\"math inline\">\\(q_m\\)</span> 可以用一个复数来表示</p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p>那从复数角度来看，就有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）</p>\n<p>类似地，有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p>和</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p>则有<br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p>用了三角函数和差公式 <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p>再看 <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p>证毕。</p>\n<h2 id=\"旋转位置编码\">“旋转”位置编码</h2>\n<p>发现式（17）可以写成这样</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p>同样地</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p>如果从向量视角来看，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>看式（22）和（23），可以看到等号右边都有</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p>这正是一个二维平面的旋转矩阵。 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span>\n的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。</p>\n<p>这也是为什么叫做“旋转”位置编码。</p>\n<h2 id=\"从2维推广到高维\">从2维推广到高维</h2>\n<p>我们现在已经确认，对于2维的情况，经过 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span>\n这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？</p>\n<p>答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n是的输入向量的维度，由于是两个两个一组，所以一共有 <span class=\"math inline\">\\(d/2\\)</span> 组小旋转矩阵，这 <span class=\"math inline\">\\(d/2\\)</span> 组矩阵为了区分，设计使用了不同的\n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p>那么在实际操作的时候，给位置 <span class=\"math inline\">\\(m\\)</span>\n和位置 <span class=\"math inline\">\\(n\\)</span> 的输入向量分别乘以 <span class=\"math inline\">\\(R_m\\)</span> 和 <span class=\"math inline\">\\(R_n\\)</span>，再进行self-attention，就能获得仅使用相对位置信息编码的效果。</p>\n<p>另外 <span class=\"math inline\">\\(\\theta\\)</span>\n是怎么来的呢？这里是参考了Google最初在《Attention is All You\nNeed》中提出的，这里就先不展开了，可以看看论文原文。</p>\n<h2 id=\"高效率实现\">高效率实现</h2>\n<p>式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。</p>\n<p>另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。</p>\n<h2 id=\"远程衰减的特性\">远程衰减的特性</h2>\n<p>至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。</p>\n<p>直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。</p>\n<p>回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个\n<span class=\"math inline\">\\(\\theta\\)</span>\n的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。</p>\n<p>证明过程这里就偷偷懒略过了，具体可以看<a href=\"https://arxiv.org/abs/2104.09864\">Roformer的论文</a>或者<a href=\"https://spaces.ac.cn/archives/8265\">苏神的博客</a>。</p>\n<p>当 <span class=\"math inline\">\\(d = 128\\)</span>\n时，画出来的图像如下</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"远程衰减\">\n<h1 id=\"小结\">小结</h1>\n<p>总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130<br>\n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265<br>\n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n【4】十分钟读懂旋转编码（RoPE）\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"LLM长上下文的问题","abbrlink":"c4da56c0","date":"2024-02-28T07:19:28.000Z","_content":"\n最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。  \n\n跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：[博客](http://www.linsight.cn/a051710f.html) [知乎](https://zhuanlan.zhihu.com/p/684072868) [微信公众号](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n# 关于长上下文  \n\n2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k tokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。  \n\n（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）  \n\n差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。\n\n今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型  \n\n<center>\n\n| 模型 | 支持长度 |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20万汉字) |\n| Claude2 | 200k |  \n\n</center>\n\n大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。  \n\n为什么要那么长？  \n\n# 长上下文的需求  \n\n取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都>1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。  \n\n最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。  \n\n上面这个场景对应的是大模型的<big><u>**工具化**</u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。  \n\n另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented generation），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。  \n\n除了工具化的应用场景，还有一些<big><u>**个性化**</u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。  \n\n实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。  \n\n上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。\n\n# 模型怎么支持长上下文\n\n看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？  \n\n如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u>**位置编码**</u>，模型不能很好地处理。  \n\n## 直接训练\n\n既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？  \n\n这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。  \n\n1.训练数据  \n\n直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。  \n\n当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention mask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention mask，效果也挺好。  \n\n总的来说，就是【连续长文本】>【多个中等文本拼接】（也可用）  \n\n2.资源消耗  \n\n来简单看一下transformer在训练中所消耗的资源。  \n\n假设模型有 $l$ 层，词表大小为 $V$ ，hidden size为 $h$ ，batch size为 $b$ ，训练窗口长度为 $s$ ，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。  \n\n(1) 参数量\n\n模型总参数量 $\\Phi$  = 词向量参数量 + $l$ * decoder层参数量 = $Vh + l(12h^2 + 13h)$  \n\n可以看到参数量和窗口长度 $s$ 无关，模型确定了就是一个固定值。  \n\n(2) 计算量  \n\n一次前向计算量 = 输出分类头logits计算 + $l$ * 每层计算量 $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\n（这里的计算忽略了softmax，实际上softmax计算量也是和长度 $s$ 成平方关系）\n\n看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n可以看到，总计算量随着输入长度的增长是平方的。在 $s << h$ 的时候，基本还可以认为是线性的。目前大部分模型的 $h$ 是在1k到1w这个范围，基本上可以认为 $s$ 和 $sh$ 在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系  \n\n(3) 显存  \n\n训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。\n\n训练中，每个参数（$\\Phi$）有一个对应梯度（$\\Phi$），每个参数又对应优化器一个一阶动量和二阶动量（$2\\Phi$）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有 $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ 的参数占用。\n\n{% asset_img mix_precision_fp16.png 混合精度训练 %}  \n\n这部分跟输入长度没有直接关系。\n\n另外一个需要占用显存的部分是中间激活值。  \n\n保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。  \n\n对于attention层，输入时先要对 $x$ 做 $Q、K、V$ 投影，需要保存 $x$ 的中间值；计算权重的时候有 $Q、K$ 矩阵的相乘，需要保存 $Q、K$ 矩阵的值；做softmax的时候输入有 $QK^T$ 要保存；以此类推，则需要保存的所有中间激活值为 $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$ 。对于 $l$ 层的模型，就再乘以 $l$ 。  \n\n可以看到中间激活值随着 $s$ 增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch size，或者提升gradient accumulation的值，无论如何，都会增加<big><u>**训练成本**</u></big>。  \n\n小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。  \n\n现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。  \n\n而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。  \n\n## 线性插值 Position Interpolation\n\n23年6月，Meta在[《EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION》](https://arxiv.org/pdf/2306.15595.pdf)中就提出了针对RoPE的线性插值方法PI（Position Interpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。\n\n{% asset_img meta_pi.png PI效果 %}  \n{% asset_img LLM长上下文的问题/meta_pi.png PI效果 %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。\n\n看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。  \n\n论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差 $\\left|m-n \\right|$ 不太大的时候（<2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦 $\\left|m-n \\right|$ 超过这个区间，还是有可能出现很大的值。\n\n{% asset_img meta_rope_ext.png RoPE外推 %}  \n\n看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention score。而右边的图使用了插值的方式，就相对稳定。\n\n（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）\n\n而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。  \n\n{% asset_img meta_pi_nosft.png PI效果 %}  \n\n插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。\n\n{% asset_img meta_pi_explanation.png PI效果 %}  \n\n这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。  \n\n由于三角函数光滑的特性，我们可以重新定义attention score的计算，使得结果不要出现异常大的值，也就是 $\\tilde{a}(s)=a(Ls/L^{\\prime})$ ，$L$ 是原长度（也就是2048），$L^{\\prime}$ 是我们想要增大的长度（8k/16k/32k等）。\n\n更具体来说，就是对RoPE做一点修改  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n相当于位置 $m$ 的分辨率从1下降成了 ${L}/{L'}$。  \n\n（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）  \n\n然后使用几万到几十万条样本进行预训练，就可以了。\n\n（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）\n\n## NTK-Aware Interpolation \n\n线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware Interpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u>**非线性插值**</u>的方法，NTK-Aware Scaled RoPE。CodeLlama就是用这种方法把长度推广到1M。  \n\nNTK，就是Neural Tangent Kernel，神经正切核。具体是什么，让GLM4帮忙解答一下  \n\n>Neural Tangent Kernel (NTK) 是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK 的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。  \n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是 Neural Tangent Kernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。  \n具体来说，NTK 使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。  \nNTK 的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。\n\n这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？  \n\n它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。  \n\n回顾一下在RoPE中，对位置 $m$ 的输入向量进行“旋转”的矩阵长这样  \n\n{% asset_img rope_matrix.png RoPE旋转矩阵 %}  \n\n它把输入向量的元素划分成2个2个一组，共有 $d/2$ 组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于 $\\theta_j=10000^{-2j/d}$ ，可以看到， $j$ 越小越靠前的组旋转越快，$j$ 越大的旋转越慢。这里 $base=10000$ ， $base$ 越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。  \n\n不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。  \n\n怎么实现“高频外推，低频内插”？  \n\n先看回讲[RoPE](https://www.zhihu.com/people/us4ever)的时候，对于2维情况，有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n推广到高维的情况，则有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ ，$s=m-n$ 。  \n\n在这个公式下，线性插值相当于把  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n变成了  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $\\alpha=L'/L>1$ ，相当于把 $s$ 压缩了。  \n\n而NTK-Aware Scaled RoPE则是对 $\\theta_j$ 进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n相当于 $\\theta$ 乘了一个系数 $\\alpha^{\\frac{-2j}{d-2}}$ ，当 $j$ 比较小的时候， $\\alpha^{\\frac{-2j}{d-2}}$ 接近1，相当于直接进行了外推，而当 $j$ 比较大的时候（注意 $j$ 的取值是从0到 $d/2 - 1$），$\\alpha^{\\frac{-2j}{d-2}}$ 就接近 $\\alpha^{-1}$ ，这就和线性插值趋近了。\n\n引用来自[知乎一篇文章](https://zhuanlan.zhihu.com/p/645770522)的一个视角来理解NTK-Aware Interpolation  \n\n>有意思的解释一下，RoPE 的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的 RoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动 1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE 缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware RoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5 倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24 小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k 秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。  \n\n另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下[原文](https://kexue.fm/archives/9675)，也很巧妙。  \n\n在YaRN的[论文](https://arxiv.org/pdf/2309.00071.pdf)中，对NTK的优缺点作了点评  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那 $\\alpha=L'/L$ 就要选得比8更大一些，比如16。\n\n## NTK-by-parts\n\nNTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。  \n\n对于分量 $j$ ，RoPE嵌入的波长  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$ 代表旋转一周所需的长度。当 $j$ 比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。  \n\n这里观察到，当 $j$ 比较大时，波长就可能比 $L$ 要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如 $sin$ 只转了1/4圈，那值全都集中在0~1之间，-1~0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当 $j$ 比较小时，模型只能访问到相对位置信息。  \n\n此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是  \n\n- 如果维度 $j$ 的波长 $\\lambda_j$ 远小于上下文长度 ，就不插值只外推  \n- 如果波长 $\\lambda_j\\geq$ 上下文长度，就只插值不外推  \n- 中间的部分就同时存在两种，类似NTK-aware interpolation  \n\n引入一个比例 $r(j)=\\frac{L}{\\lambda_j}$ 来表示波长和上下文长度的关系。另外还需要两个阈值 $\\beta_1、\\beta_2$ 来区分以上三种情况。如果 $r(j)<\\beta_1$ ，就认为波长大，如果 $r(j)\\geq \\beta_2$ ，就认为波长小。方便起见，定义一个斜坡函数  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts插值可以定义为对 $\\theta_j$ 的一个操作  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n这里有两个超参 $\\beta_1、\\beta_2$ 要定，文中根据实验给出的推荐值是 $\\beta_1=1，\\beta_2=32$ ，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。   \n\n## Dynamically NTK Scaled RoPE  \n\n无论是线性插值还是NTK-Aware Interpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention score暴增的风险。另一方面，在解码过程中，当已解码的长度 $l$ 还没有达到训练长度 $L$ 时，就使用 $\\alpha$ 来修改base，也可能带来一些损失。Dynamically NTK Scaled RoPE是在NTK插值的基础上，把固定的系数改成动态的系数。  \n\n具体来说，就是  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n这样随着解码长度 $l$ 的增长，当 $l>L$ 之后 $\\alpha$ 从1逐渐增大， $l\\leq L$ 时则不需要改动。  \n\n有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。  \n\n## YaRN  \n\n上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。  \n\n当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度 $t>1$ 来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 $\\sqrt{t}$ 来扩展RoPE的长度。这样可以不必修改注意力的代码。  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\n通过对Llama 1和Llama 2的实验，文章提出了建议值$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。  \n\nYaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention score进行调整。  \n\nYaRN在微调以及无微调的情况下，效果都比上面的几种都要好。\n\n## logn  \n\nlogn指的是对attention计算中的缩放因子 $\\sqrt{d}$ 进行通过logn进行改进的一个方法，苏剑林在[博客](https://zhuanlan.zhihu.com/p/678755776)中进行了分析。大致的思路和YaRN中的缩放颇有些相似。  \n\n简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention score公式  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n可以看到，当 $L'>L$ 时，其效果和YaRN中的放缩是类似的。\n\n## 其他\n\n在扩展推理长度上，还有很多其他有效的工作，比如各种window attention，streaming LLM，LongLoRA，Focus Transformer等，还有数据、评测等更方面的分析，待逐个梳理。\n\n# 小结  \n\n较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降  \n\n- 推理时用到了没训练过的位置编码  \n- 推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏  \n\n这两个问题分别可以从位置编码和attention score的放缩来缓解。  \n\n线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。  \n\n# Reference  \n【1】分析transformer模型的参数量、计算量、中间激活、KV cache https://zhuanlan.zhihu.com/p/624740065  \n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n【3】Transformer升级之路：10、RoPE是一种β进制编码 https://kexue.fm/archives/9675  \n【4】YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n【5】详解基于调整RoPE旋转角度的大模型长度外推方法 https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符 https://cloud.tencent.com/developer/article/2330611  \n【8】Transformer升级之路：8、长度外推性与位置鲁棒性 https://spaces.ac.cn/archives/9444  \n【9】RoPE外推优化——支持192K上下文长度 https://zhuanlan.zhihu.com/p/678755776\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLM长上下文的问题.md","raw":"---\ntitle: LLM长上下文的问题\nabbrlink: c4da56c0\ndate: 2024-02-28 15:19:28\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 长上下文\n  - 窗口外推\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。  \n\n跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：[博客](http://www.linsight.cn/a051710f.html) [知乎](https://zhuanlan.zhihu.com/p/684072868) [微信公众号](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n# 关于长上下文  \n\n2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k tokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。  \n\n（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）  \n\n差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。\n\n今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型  \n\n<center>\n\n| 模型 | 支持长度 |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20万汉字) |\n| Claude2 | 200k |  \n\n</center>\n\n大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。  \n\n为什么要那么长？  \n\n# 长上下文的需求  \n\n取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都>1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。  \n\n最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。  \n\n上面这个场景对应的是大模型的<big><u>**工具化**</u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。  \n\n另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented generation），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。  \n\n除了工具化的应用场景，还有一些<big><u>**个性化**</u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。  \n\n实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。  \n\n上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。\n\n# 模型怎么支持长上下文\n\n看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？  \n\n如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u>**位置编码**</u>，模型不能很好地处理。  \n\n## 直接训练\n\n既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？  \n\n这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。  \n\n1.训练数据  \n\n直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。  \n\n当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention mask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention mask，效果也挺好。  \n\n总的来说，就是【连续长文本】>【多个中等文本拼接】（也可用）  \n\n2.资源消耗  \n\n来简单看一下transformer在训练中所消耗的资源。  \n\n假设模型有 $l$ 层，词表大小为 $V$ ，hidden size为 $h$ ，batch size为 $b$ ，训练窗口长度为 $s$ ，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。  \n\n(1) 参数量\n\n模型总参数量 $\\Phi$  = 词向量参数量 + $l$ * decoder层参数量 = $Vh + l(12h^2 + 13h)$  \n\n可以看到参数量和窗口长度 $s$ 无关，模型确定了就是一个固定值。  \n\n(2) 计算量  \n\n一次前向计算量 = 输出分类头logits计算 + $l$ * 每层计算量 $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\n（这里的计算忽略了softmax，实际上softmax计算量也是和长度 $s$ 成平方关系）\n\n看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n可以看到，总计算量随着输入长度的增长是平方的。在 $s << h$ 的时候，基本还可以认为是线性的。目前大部分模型的 $h$ 是在1k到1w这个范围，基本上可以认为 $s$ 和 $sh$ 在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系  \n\n(3) 显存  \n\n训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。\n\n训练中，每个参数（$\\Phi$）有一个对应梯度（$\\Phi$），每个参数又对应优化器一个一阶动量和二阶动量（$2\\Phi$）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有 $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ 的参数占用。\n\n{% asset_img mix_precision_fp16.png 混合精度训练 %}  \n\n这部分跟输入长度没有直接关系。\n\n另外一个需要占用显存的部分是中间激活值。  \n\n保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。  \n\n对于attention层，输入时先要对 $x$ 做 $Q、K、V$ 投影，需要保存 $x$ 的中间值；计算权重的时候有 $Q、K$ 矩阵的相乘，需要保存 $Q、K$ 矩阵的值；做softmax的时候输入有 $QK^T$ 要保存；以此类推，则需要保存的所有中间激活值为 $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$ 。对于 $l$ 层的模型，就再乘以 $l$ 。  \n\n可以看到中间激活值随着 $s$ 增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch size，或者提升gradient accumulation的值，无论如何，都会增加<big><u>**训练成本**</u></big>。  \n\n小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。  \n\n现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。  \n\n而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。  \n\n## 线性插值 Position Interpolation\n\n23年6月，Meta在[《EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION》](https://arxiv.org/pdf/2306.15595.pdf)中就提出了针对RoPE的线性插值方法PI（Position Interpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。\n\n{% asset_img meta_pi.png PI效果 %}  \n{% asset_img LLM长上下文的问题/meta_pi.png PI效果 %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。\n\n看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。  \n\n论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差 $\\left|m-n \\right|$ 不太大的时候（<2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦 $\\left|m-n \\right|$ 超过这个区间，还是有可能出现很大的值。\n\n{% asset_img meta_rope_ext.png RoPE外推 %}  \n\n看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention score。而右边的图使用了插值的方式，就相对稳定。\n\n（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）\n\n而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。  \n\n{% asset_img meta_pi_nosft.png PI效果 %}  \n\n插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。\n\n{% asset_img meta_pi_explanation.png PI效果 %}  \n\n这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。  \n\n由于三角函数光滑的特性，我们可以重新定义attention score的计算，使得结果不要出现异常大的值，也就是 $\\tilde{a}(s)=a(Ls/L^{\\prime})$ ，$L$ 是原长度（也就是2048），$L^{\\prime}$ 是我们想要增大的长度（8k/16k/32k等）。\n\n更具体来说，就是对RoPE做一点修改  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n相当于位置 $m$ 的分辨率从1下降成了 ${L}/{L'}$。  \n\n（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）  \n\n然后使用几万到几十万条样本进行预训练，就可以了。\n\n（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）\n\n## NTK-Aware Interpolation \n\n线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware Interpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u>**非线性插值**</u>的方法，NTK-Aware Scaled RoPE。CodeLlama就是用这种方法把长度推广到1M。  \n\nNTK，就是Neural Tangent Kernel，神经正切核。具体是什么，让GLM4帮忙解答一下  \n\n>Neural Tangent Kernel (NTK) 是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK 的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。  \n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是 Neural Tangent Kernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。  \n具体来说，NTK 使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。  \nNTK 的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。\n\n这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？  \n\n它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。  \n\n回顾一下在RoPE中，对位置 $m$ 的输入向量进行“旋转”的矩阵长这样  \n\n{% asset_img rope_matrix.png RoPE旋转矩阵 %}  \n\n它把输入向量的元素划分成2个2个一组，共有 $d/2$ 组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于 $\\theta_j=10000^{-2j/d}$ ，可以看到， $j$ 越小越靠前的组旋转越快，$j$ 越大的旋转越慢。这里 $base=10000$ ， $base$ 越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。  \n\n不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。  \n\n怎么实现“高频外推，低频内插”？  \n\n先看回讲[RoPE](https://www.zhihu.com/people/us4ever)的时候，对于2维情况，有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n推广到高维的情况，则有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ ，$s=m-n$ 。  \n\n在这个公式下，线性插值相当于把  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n变成了  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $\\alpha=L'/L>1$ ，相当于把 $s$ 压缩了。  \n\n而NTK-Aware Scaled RoPE则是对 $\\theta_j$ 进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n相当于 $\\theta$ 乘了一个系数 $\\alpha^{\\frac{-2j}{d-2}}$ ，当 $j$ 比较小的时候， $\\alpha^{\\frac{-2j}{d-2}}$ 接近1，相当于直接进行了外推，而当 $j$ 比较大的时候（注意 $j$ 的取值是从0到 $d/2 - 1$），$\\alpha^{\\frac{-2j}{d-2}}$ 就接近 $\\alpha^{-1}$ ，这就和线性插值趋近了。\n\n引用来自[知乎一篇文章](https://zhuanlan.zhihu.com/p/645770522)的一个视角来理解NTK-Aware Interpolation  \n\n>有意思的解释一下，RoPE 的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的 RoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动 1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE 缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware RoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5 倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24 小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k 秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。  \n\n另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下[原文](https://kexue.fm/archives/9675)，也很巧妙。  \n\n在YaRN的[论文](https://arxiv.org/pdf/2309.00071.pdf)中，对NTK的优缺点作了点评  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那 $\\alpha=L'/L$ 就要选得比8更大一些，比如16。\n\n## NTK-by-parts\n\nNTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。  \n\n对于分量 $j$ ，RoPE嵌入的波长  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$ 代表旋转一周所需的长度。当 $j$ 比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。  \n\n这里观察到，当 $j$ 比较大时，波长就可能比 $L$ 要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如 $sin$ 只转了1/4圈，那值全都集中在0~1之间，-1~0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当 $j$ 比较小时，模型只能访问到相对位置信息。  \n\n此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是  \n\n- 如果维度 $j$ 的波长 $\\lambda_j$ 远小于上下文长度 ，就不插值只外推  \n- 如果波长 $\\lambda_j\\geq$ 上下文长度，就只插值不外推  \n- 中间的部分就同时存在两种，类似NTK-aware interpolation  \n\n引入一个比例 $r(j)=\\frac{L}{\\lambda_j}$ 来表示波长和上下文长度的关系。另外还需要两个阈值 $\\beta_1、\\beta_2$ 来区分以上三种情况。如果 $r(j)<\\beta_1$ ，就认为波长大，如果 $r(j)\\geq \\beta_2$ ，就认为波长小。方便起见，定义一个斜坡函数  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts插值可以定义为对 $\\theta_j$ 的一个操作  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n这里有两个超参 $\\beta_1、\\beta_2$ 要定，文中根据实验给出的推荐值是 $\\beta_1=1，\\beta_2=32$ ，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。   \n\n## Dynamically NTK Scaled RoPE  \n\n无论是线性插值还是NTK-Aware Interpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention score暴增的风险。另一方面，在解码过程中，当已解码的长度 $l$ 还没有达到训练长度 $L$ 时，就使用 $\\alpha$ 来修改base，也可能带来一些损失。Dynamically NTK Scaled RoPE是在NTK插值的基础上，把固定的系数改成动态的系数。  \n\n具体来说，就是  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n这样随着解码长度 $l$ 的增长，当 $l>L$ 之后 $\\alpha$ 从1逐渐增大， $l\\leq L$ 时则不需要改动。  \n\n有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。  \n\n## YaRN  \n\n上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。  \n\n当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度 $t>1$ 来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 $\\sqrt{t}$ 来扩展RoPE的长度。这样可以不必修改注意力的代码。  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\n通过对Llama 1和Llama 2的实验，文章提出了建议值$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。  \n\nYaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention score进行调整。  \n\nYaRN在微调以及无微调的情况下，效果都比上面的几种都要好。\n\n## logn  \n\nlogn指的是对attention计算中的缩放因子 $\\sqrt{d}$ 进行通过logn进行改进的一个方法，苏剑林在[博客](https://zhuanlan.zhihu.com/p/678755776)中进行了分析。大致的思路和YaRN中的缩放颇有些相似。  \n\n简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention score公式  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n可以看到，当 $L'>L$ 时，其效果和YaRN中的放缩是类似的。\n\n## 其他\n\n在扩展推理长度上，还有很多其他有效的工作，比如各种window attention，streaming LLM，LongLoRA，Focus Transformer等，还有数据、评测等更方面的分析，待逐个梳理。\n\n# 小结  \n\n较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降  \n\n- 推理时用到了没训练过的位置编码  \n- 推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏  \n\n这两个问题分别可以从位置编码和attention score的放缩来缓解。  \n\n线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。  \n\n# Reference  \n【1】分析transformer模型的参数量、计算量、中间激活、KV cache https://zhuanlan.zhihu.com/p/624740065  \n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n【3】Transformer升级之路：10、RoPE是一种β进制编码 https://kexue.fm/archives/9675  \n【4】YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n【5】详解基于调整RoPE旋转角度的大模型长度外推方法 https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符 https://cloud.tencent.com/developer/article/2330611  \n【8】Transformer升级之路：8、长度外推性与位置鲁棒性 https://spaces.ac.cn/archives/9444  \n【9】RoPE外推优化——支持192K上下文长度 https://zhuanlan.zhihu.com/p/678755776\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLM长上下文的问题","published":1,"updated":"2024-03-13T07:23:38.942Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9r0003314kgrlbb9js","content":"<p>最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。</p>\n<p>跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：<a href=\"http://www.linsight.cn/a051710f.html\">博客</a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\">知乎</a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\">微信公众号</a></p>\n<h1 id=\"关于长上下文\">关于长上下文</h1>\n<p>2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k\ntokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。</p>\n<p>（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）</p>\n<p>差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。</p>\n<p>今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\">模型</th>\n<th style=\"text-align: center;\">支持长度</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20万汉字)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。</p>\n<p>为什么要那么长？</p>\n<h1 id=\"长上下文的需求\">长上下文的需求</h1>\n<p>取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都&gt;1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。</p>\n<p>最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。</p>\n<p>上面这个场景对应的是大模型的<big><u><strong>工具化</strong></u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。</p>\n<p>另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented\ngeneration），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。</p>\n<p>除了工具化的应用场景，还有一些<big><u><strong>个性化</strong></u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。</p>\n<p>实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。</p>\n<p>上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。</p>\n<h1 id=\"模型怎么支持长上下文\">模型怎么支持长上下文</h1>\n<p>看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？</p>\n<p>如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u><strong>位置编码</strong></u>，模型不能很好地处理。</p>\n<h2 id=\"直接训练\">直接训练</h2>\n<p>既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？</p>\n<p>这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。</p>\n<p>1.训练数据</p>\n<p>直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。</p>\n<p>当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention\nmask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention\nmask，效果也挺好。</p>\n<p>总的来说，就是【连续长文本】&gt;【多个中等文本拼接】（也可用）</p>\n<p>2.资源消耗</p>\n<p>来简单看一下transformer在训练中所消耗的资源。</p>\n<p>假设模型有 <span class=\"math inline\">\\(l\\)</span> 层，词表大小为\n<span class=\"math inline\">\\(V\\)</span> ，hidden size为 <span class=\"math inline\">\\(h\\)</span> ，batch size为 <span class=\"math inline\">\\(b\\)</span> ，训练窗口长度为 <span class=\"math inline\">\\(s\\)</span>\n，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。</p>\n<ol type=\"1\">\n<li>参数量</li>\n</ol>\n<p>模型总参数量 <span class=\"math inline\">\\(\\Phi\\)</span> = 词向量参数量\n+ <span class=\"math inline\">\\(l\\)</span> * decoder层参数量 = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p>可以看到参数量和窗口长度 <span class=\"math inline\">\\(s\\)</span>\n无关，模型确定了就是一个固定值。</p>\n<ol start=\"2\" type=\"1\">\n<li>计算量</li>\n</ol>\n<p>一次前向计算量 = 输出分类头logits计算 + <span class=\"math inline\">\\(l\\)</span> * 每层计算量 <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>（这里的计算忽略了softmax，实际上softmax计算量也是和长度 <span class=\"math inline\">\\(s\\)</span> 成平方关系）</p>\n<p>看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，总计算量随着输入长度的增长是平方的。在 <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n的时候，基本还可以认为是线性的。目前大部分模型的 <span class=\"math inline\">\\(h\\)</span> 是在1k到1w这个范围，基本上可以认为\n<span class=\"math inline\">\\(s\\)</span> 和 <span class=\"math inline\">\\(sh\\)</span>\n在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系</p>\n<ol start=\"3\" type=\"1\">\n<li>显存</li>\n</ol>\n<p>训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。</p>\n<p>训练中，每个参数（<span class=\"math inline\">\\(\\Phi\\)</span>）有一个对应梯度（<span class=\"math inline\">\\(\\Phi\\)</span>），每个参数又对应优化器一个一阶动量和二阶动量（<span class=\"math inline\">\\(2\\Phi\\)</span>）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n的参数占用。</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"混合精度训练\">\n<p>这部分跟输入长度没有直接关系。</p>\n<p>另外一个需要占用显存的部分是中间激活值。</p>\n<p>保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。</p>\n<p>对于attention层，输入时先要对 <span class=\"math inline\">\\(x\\)</span>\n做 <span class=\"math inline\">\\(Q、K、V\\)</span> 投影，需要保存 <span class=\"math inline\">\\(x\\)</span> 的中间值；计算权重的时候有 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的相乘，需要保存 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的值；做softmax的时候输入有\n<span class=\"math inline\">\\(QK^T\\)</span>\n要保存；以此类推，则需要保存的所有中间激活值为 <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> 。对于\n<span class=\"math inline\">\\(l\\)</span> 层的模型，就再乘以 <span class=\"math inline\">\\(l\\)</span> 。</p>\n<p>可以看到中间激活值随着 <span class=\"math inline\">\\(s\\)</span>\n增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch\nsize，或者提升gradient\naccumulation的值，无论如何，都会增加<big><u><strong>训练成本</strong></u></big>。</p>\n<p>小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。</p>\n<p>现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。</p>\n<p>而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。</p>\n<h2 id=\"线性插值-position-interpolation\">线性插值 Position\nInterpolation</h2>\n<p>23年6月，Meta在<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">《EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION》</a>中就提出了针对RoPE的线性插值方法PI（Position\nInterpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI效果\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。</p>\n<p>看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。</p>\n<p>论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n不太大的时候（&lt;2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n超过这个区间，还是有可能出现很大的值。</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE外推\">\n<p>看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention\nscore。而右边的图使用了插值的方式，就相对稳定。</p>\n<p>（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）</p>\n<p>而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI效果\">\n<p>插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI效果\">\n<p>这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。</p>\n<p>由于三角函数光滑的特性，我们可以重新定义attention\nscore的计算，使得结果不要出现异常大的值，也就是 <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> ，<span class=\"math inline\">\\(L\\)</span> 是原长度（也就是2048），<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n是我们想要增大的长度（8k/16k/32k等）。</p>\n<p>更具体来说，就是对RoPE做一点修改</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于位置 <span class=\"math inline\">\\(m\\)</span> 的分辨率从1下降成了\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span>。</p>\n<p>（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）</p>\n<p>然后使用几万到几十万条样本进行预训练，就可以了。</p>\n<p>（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）</p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware\nInterpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u><strong>非线性插值</strong></u>的方法，NTK-Aware\nScaled RoPE。CodeLlama就是用这种方法把长度推广到1M。</p>\n<p>NTK，就是Neural Tangent\nKernel，神经正切核。具体是什么，让GLM4帮忙解答一下</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\n是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK\n的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。<br>\n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是\nNeural Tangent\nKernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。<br>\n具体来说，NTK\n使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。<br>\nNTK\n的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。</p>\n</blockquote>\n<p>这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？</p>\n<p>它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。</p>\n<p>回顾一下在RoPE中，对位置 <span class=\"math inline\">\\(m\\)</span>\n的输入向量进行“旋转”的矩阵长这样</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE旋转矩阵\">\n<p>它把输入向量的元素划分成2个2个一组，共有 <span class=\"math inline\">\\(d/2\\)</span>\n组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> ，可以看到，\n<span class=\"math inline\">\\(j\\)</span> 越小越靠前的组旋转越快，<span class=\"math inline\">\\(j\\)</span> 越大的旋转越慢。这里 <span class=\"math inline\">\\(base=10000\\)</span> ， <span class=\"math inline\">\\(base\\)</span>\n越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。</p>\n<p>不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。</p>\n<p>怎么实现“高频外推，低频内插”？</p>\n<p>先看回讲<a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>的时候，对于2维情况，有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>推广到高维的情况，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n，<span class=\"math inline\">\\(s=m-n\\)</span> 。</p>\n<p>在这个公式下，线性插值相当于把</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>变成了</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n，相当于把 <span class=\"math inline\">\\(s\\)</span> 压缩了。</p>\n<p>而NTK-Aware Scaled RoPE则是对 <span class=\"math inline\">\\(\\theta_j\\)</span>\n进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于 <span class=\"math inline\">\\(\\theta\\)</span> 乘了一个系数 <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> ，当 <span class=\"math inline\">\\(j\\)</span> 比较小的时候， <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n接近1，相当于直接进行了外推，而当 <span class=\"math inline\">\\(j\\)</span>\n比较大的时候（注意 <span class=\"math inline\">\\(j\\)</span> 的取值是从0到\n<span class=\"math inline\">\\(d/2 - 1\\)</span>），<span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> 就接近 <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> ，这就和线性插值趋近了。</p>\n<p>引用来自<a href=\"https://zhuanlan.zhihu.com/p/645770522\">知乎一篇文章</a>的一个视角来理解NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>有意思的解释一下，RoPE\n的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的\nRoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动\n1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE\n缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware\nRoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5\n倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24\n小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k\n秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。</p>\n</blockquote>\n<p>另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下<a href=\"https://kexue.fm/archives/9675\">原文</a>，也很巧妙。</p>\n<p>在YaRN的<a href=\"https://arxiv.org/pdf/2309.00071.pdf\">论文</a>中，对NTK的优缺点作了点评</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n就要选得比8更大一些，比如16。</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。</p>\n<p>对于分量 <span class=\"math inline\">\\(j\\)</span> ，RoPE嵌入的波长</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n代表旋转一周所需的长度。当 <span class=\"math inline\">\\(j\\)</span>\n比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。</p>\n<p>这里观察到，当 <span class=\"math inline\">\\(j\\)</span>\n比较大时，波长就可能比 <span class=\"math inline\">\\(L\\)</span>\n要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如\n<span class=\"math inline\">\\(sin\\)</span>\n只转了1/4圈，那值全都集中在0<sub>1之间，-1</sub>0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当\n<span class=\"math inline\">\\(j\\)</span>\n比较小时，模型只能访问到相对位置信息。</p>\n<p>此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是</p>\n<ul>\n<li>如果维度 <span class=\"math inline\">\\(j\\)</span> 的波长 <span class=\"math inline\">\\(\\lambda_j\\)</span> 远小于上下文长度\n，就不插值只外推<br>\n</li>\n<li>如果波长 <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n上下文长度，就只插值不外推<br>\n</li>\n<li>中间的部分就同时存在两种，类似NTK-aware interpolation</li>\n</ul>\n<p>引入一个比例 <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n来表示波长和上下文长度的关系。另外还需要两个阈值 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span> 来区分以上三种情况。如果\n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n，就认为波长大，如果 <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> ，就认为波长小。方便起见，定义一个斜坡函数</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts插值可以定义为对 <span class=\"math inline\">\\(\\theta_j\\)</span> 的一个操作</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这里有两个超参 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span>\n要定，文中根据实验给出的推荐值是 <span class=\"math inline\">\\(\\beta_1=1，\\beta_2=32\\)</span>\n，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>无论是线性插值还是NTK-Aware\nInterpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention\nscore暴增的风险。另一方面，在解码过程中，当已解码的长度 <span class=\"math inline\">\\(l\\)</span> 还没有达到训练长度 <span class=\"math inline\">\\(L\\)</span> 时，就使用 <span class=\"math inline\">\\(\\alpha\\)</span>\n来修改base，也可能带来一些损失。Dynamically NTK Scaled\nRoPE是在NTK插值的基础上，把固定的系数改成动态的系数。</p>\n<p>具体来说，就是</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这样随着解码长度 <span class=\"math inline\">\\(l\\)</span> 的增长，当\n<span class=\"math inline\">\\(l&gt;L\\)</span> 之后 <span class=\"math inline\">\\(\\alpha\\)</span> 从1逐渐增大， <span class=\"math inline\">\\(l\\leq L\\)</span> 时则不需要改动。</p>\n<p>有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。</p>\n<p>当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度\n<span class=\"math inline\">\\(t&gt;1\\)</span>\n来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\n来扩展RoPE的长度。这样可以不必修改注意力的代码。</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>通过对Llama 1和Llama 2的实验，文章提出了建议值<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。</p>\n<p>YaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention\nscore进行调整。</p>\n<p>YaRN在微调以及无微调的情况下，效果都比上面的几种都要好。</p>\n<h2 id=\"logn\">logn</h2>\n<p>logn指的是对attention计算中的缩放因子 <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\n进行通过logn进行改进的一个方法，苏剑林在<a href=\"https://zhuanlan.zhihu.com/p/678755776\">博客</a>中进行了分析。大致的思路和YaRN中的缩放颇有些相似。</p>\n<p>简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention\nscore公式</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，当 <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\n时，其效果和YaRN中的放缩是类似的。</p>\n<h2 id=\"其他\">其他</h2>\n<p>在扩展推理长度上，还有很多其他有效的工作，比如各种window\nattention，streaming LLM，LongLoRA，Focus\nTransformer等，还有数据、评测等更方面的分析，待逐个梳理。</p>\n<h1 id=\"小结\">小结</h1>\n<p>较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降</p>\n<ul>\n<li>推理时用到了没训练过的位置编码<br>\n</li>\n<li>推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏</li>\n</ul>\n<p>这两个问题分别可以从位置编码和attention score的放缩来缓解。</p>\n<p>线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】分析transformer模型的参数量、计算量、中间激活、KV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n【3】Transformer升级之路：10、RoPE是一种β进制编码\nhttps://kexue.fm/archives/9675<br>\n【4】YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n【5】详解基于调整RoPE旋转角度的大模型长度外推方法\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符\nhttps://cloud.tencent.com/developer/article/2330611<br>\n【8】Transformer升级之路：8、长度外推性与位置鲁棒性\nhttps://spaces.ac.cn/archives/9444<br>\n【9】RoPE外推优化——支持192K上下文长度\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":12689,"excerpt":"","more":"<p>最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。</p>\n<p>跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：<a href=\"http://www.linsight.cn/a051710f.html\">博客</a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\">知乎</a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\">微信公众号</a></p>\n<h1 id=\"关于长上下文\">关于长上下文</h1>\n<p>2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k\ntokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。</p>\n<p>（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）</p>\n<p>差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。</p>\n<p>今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\">模型</th>\n<th style=\"text-align: center;\">支持长度</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20万汉字)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。</p>\n<p>为什么要那么长？</p>\n<h1 id=\"长上下文的需求\">长上下文的需求</h1>\n<p>取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都&gt;1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。</p>\n<p>最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。</p>\n<p>上面这个场景对应的是大模型的<big><u><strong>工具化</strong></u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。</p>\n<p>另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented\ngeneration），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。</p>\n<p>除了工具化的应用场景，还有一些<big><u><strong>个性化</strong></u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。</p>\n<p>实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。</p>\n<p>上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。</p>\n<h1 id=\"模型怎么支持长上下文\">模型怎么支持长上下文</h1>\n<p>看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？</p>\n<p>如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u><strong>位置编码</strong></u>，模型不能很好地处理。</p>\n<h2 id=\"直接训练\">直接训练</h2>\n<p>既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？</p>\n<p>这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。</p>\n<p>1.训练数据</p>\n<p>直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。</p>\n<p>当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention\nmask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention\nmask，效果也挺好。</p>\n<p>总的来说，就是【连续长文本】&gt;【多个中等文本拼接】（也可用）</p>\n<p>2.资源消耗</p>\n<p>来简单看一下transformer在训练中所消耗的资源。</p>\n<p>假设模型有 <span class=\"math inline\">\\(l\\)</span> 层，词表大小为\n<span class=\"math inline\">\\(V\\)</span> ，hidden size为 <span class=\"math inline\">\\(h\\)</span> ，batch size为 <span class=\"math inline\">\\(b\\)</span> ，训练窗口长度为 <span class=\"math inline\">\\(s\\)</span>\n，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。</p>\n<ol type=\"1\">\n<li>参数量</li>\n</ol>\n<p>模型总参数量 <span class=\"math inline\">\\(\\Phi\\)</span> = 词向量参数量\n+ <span class=\"math inline\">\\(l\\)</span> * decoder层参数量 = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p>可以看到参数量和窗口长度 <span class=\"math inline\">\\(s\\)</span>\n无关，模型确定了就是一个固定值。</p>\n<ol start=\"2\" type=\"1\">\n<li>计算量</li>\n</ol>\n<p>一次前向计算量 = 输出分类头logits计算 + <span class=\"math inline\">\\(l\\)</span> * 每层计算量 <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>（这里的计算忽略了softmax，实际上softmax计算量也是和长度 <span class=\"math inline\">\\(s\\)</span> 成平方关系）</p>\n<p>看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，总计算量随着输入长度的增长是平方的。在 <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n的时候，基本还可以认为是线性的。目前大部分模型的 <span class=\"math inline\">\\(h\\)</span> 是在1k到1w这个范围，基本上可以认为\n<span class=\"math inline\">\\(s\\)</span> 和 <span class=\"math inline\">\\(sh\\)</span>\n在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系</p>\n<ol start=\"3\" type=\"1\">\n<li>显存</li>\n</ol>\n<p>训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。</p>\n<p>训练中，每个参数（<span class=\"math inline\">\\(\\Phi\\)</span>）有一个对应梯度（<span class=\"math inline\">\\(\\Phi\\)</span>），每个参数又对应优化器一个一阶动量和二阶动量（<span class=\"math inline\">\\(2\\Phi\\)</span>）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n的参数占用。</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"混合精度训练\">\n<p>这部分跟输入长度没有直接关系。</p>\n<p>另外一个需要占用显存的部分是中间激活值。</p>\n<p>保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。</p>\n<p>对于attention层，输入时先要对 <span class=\"math inline\">\\(x\\)</span>\n做 <span class=\"math inline\">\\(Q、K、V\\)</span> 投影，需要保存 <span class=\"math inline\">\\(x\\)</span> 的中间值；计算权重的时候有 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的相乘，需要保存 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的值；做softmax的时候输入有\n<span class=\"math inline\">\\(QK^T\\)</span>\n要保存；以此类推，则需要保存的所有中间激活值为 <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> 。对于\n<span class=\"math inline\">\\(l\\)</span> 层的模型，就再乘以 <span class=\"math inline\">\\(l\\)</span> 。</p>\n<p>可以看到中间激活值随着 <span class=\"math inline\">\\(s\\)</span>\n增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch\nsize，或者提升gradient\naccumulation的值，无论如何，都会增加<big><u><strong>训练成本</strong></u></big>。</p>\n<p>小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。</p>\n<p>现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。</p>\n<p>而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。</p>\n<h2 id=\"线性插值-position-interpolation\">线性插值 Position\nInterpolation</h2>\n<p>23年6月，Meta在<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">《EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION》</a>中就提出了针对RoPE的线性插值方法PI（Position\nInterpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI效果\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。</p>\n<p>看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。</p>\n<p>论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n不太大的时候（&lt;2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n超过这个区间，还是有可能出现很大的值。</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE外推\">\n<p>看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention\nscore。而右边的图使用了插值的方式，就相对稳定。</p>\n<p>（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）</p>\n<p>而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI效果\">\n<p>插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI效果\">\n<p>这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。</p>\n<p>由于三角函数光滑的特性，我们可以重新定义attention\nscore的计算，使得结果不要出现异常大的值，也就是 <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> ，<span class=\"math inline\">\\(L\\)</span> 是原长度（也就是2048），<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n是我们想要增大的长度（8k/16k/32k等）。</p>\n<p>更具体来说，就是对RoPE做一点修改</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于位置 <span class=\"math inline\">\\(m\\)</span> 的分辨率从1下降成了\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span>。</p>\n<p>（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）</p>\n<p>然后使用几万到几十万条样本进行预训练，就可以了。</p>\n<p>（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）</p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware\nInterpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u><strong>非线性插值</strong></u>的方法，NTK-Aware\nScaled RoPE。CodeLlama就是用这种方法把长度推广到1M。</p>\n<p>NTK，就是Neural Tangent\nKernel，神经正切核。具体是什么，让GLM4帮忙解答一下</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\n是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK\n的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。<br>\n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是\nNeural Tangent\nKernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。<br>\n具体来说，NTK\n使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。<br>\nNTK\n的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。</p>\n</blockquote>\n<p>这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？</p>\n<p>它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。</p>\n<p>回顾一下在RoPE中，对位置 <span class=\"math inline\">\\(m\\)</span>\n的输入向量进行“旋转”的矩阵长这样</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE旋转矩阵\">\n<p>它把输入向量的元素划分成2个2个一组，共有 <span class=\"math inline\">\\(d/2\\)</span>\n组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> ，可以看到，\n<span class=\"math inline\">\\(j\\)</span> 越小越靠前的组旋转越快，<span class=\"math inline\">\\(j\\)</span> 越大的旋转越慢。这里 <span class=\"math inline\">\\(base=10000\\)</span> ， <span class=\"math inline\">\\(base\\)</span>\n越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。</p>\n<p>不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。</p>\n<p>怎么实现“高频外推，低频内插”？</p>\n<p>先看回讲<a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>的时候，对于2维情况，有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>推广到高维的情况，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n，<span class=\"math inline\">\\(s=m-n\\)</span> 。</p>\n<p>在这个公式下，线性插值相当于把</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>变成了</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n，相当于把 <span class=\"math inline\">\\(s\\)</span> 压缩了。</p>\n<p>而NTK-Aware Scaled RoPE则是对 <span class=\"math inline\">\\(\\theta_j\\)</span>\n进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于 <span class=\"math inline\">\\(\\theta\\)</span> 乘了一个系数 <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> ，当 <span class=\"math inline\">\\(j\\)</span> 比较小的时候， <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n接近1，相当于直接进行了外推，而当 <span class=\"math inline\">\\(j\\)</span>\n比较大的时候（注意 <span class=\"math inline\">\\(j\\)</span> 的取值是从0到\n<span class=\"math inline\">\\(d/2 - 1\\)</span>），<span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> 就接近 <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> ，这就和线性插值趋近了。</p>\n<p>引用来自<a href=\"https://zhuanlan.zhihu.com/p/645770522\">知乎一篇文章</a>的一个视角来理解NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>有意思的解释一下，RoPE\n的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的\nRoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动\n1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE\n缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware\nRoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5\n倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24\n小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k\n秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。</p>\n</blockquote>\n<p>另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下<a href=\"https://kexue.fm/archives/9675\">原文</a>，也很巧妙。</p>\n<p>在YaRN的<a href=\"https://arxiv.org/pdf/2309.00071.pdf\">论文</a>中，对NTK的优缺点作了点评</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n就要选得比8更大一些，比如16。</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。</p>\n<p>对于分量 <span class=\"math inline\">\\(j\\)</span> ，RoPE嵌入的波长</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n代表旋转一周所需的长度。当 <span class=\"math inline\">\\(j\\)</span>\n比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。</p>\n<p>这里观察到，当 <span class=\"math inline\">\\(j\\)</span>\n比较大时，波长就可能比 <span class=\"math inline\">\\(L\\)</span>\n要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如\n<span class=\"math inline\">\\(sin\\)</span>\n只转了1/4圈，那值全都集中在0<sub>1之间，-1</sub>0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当\n<span class=\"math inline\">\\(j\\)</span>\n比较小时，模型只能访问到相对位置信息。</p>\n<p>此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是</p>\n<ul>\n<li>如果维度 <span class=\"math inline\">\\(j\\)</span> 的波长 <span class=\"math inline\">\\(\\lambda_j\\)</span> 远小于上下文长度\n，就不插值只外推<br>\n</li>\n<li>如果波长 <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n上下文长度，就只插值不外推<br>\n</li>\n<li>中间的部分就同时存在两种，类似NTK-aware interpolation</li>\n</ul>\n<p>引入一个比例 <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n来表示波长和上下文长度的关系。另外还需要两个阈值 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span> 来区分以上三种情况。如果\n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n，就认为波长大，如果 <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> ，就认为波长小。方便起见，定义一个斜坡函数</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts插值可以定义为对 <span class=\"math inline\">\\(\\theta_j\\)</span> 的一个操作</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这里有两个超参 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span>\n要定，文中根据实验给出的推荐值是 <span class=\"math inline\">\\(\\beta_1=1，\\beta_2=32\\)</span>\n，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>无论是线性插值还是NTK-Aware\nInterpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention\nscore暴增的风险。另一方面，在解码过程中，当已解码的长度 <span class=\"math inline\">\\(l\\)</span> 还没有达到训练长度 <span class=\"math inline\">\\(L\\)</span> 时，就使用 <span class=\"math inline\">\\(\\alpha\\)</span>\n来修改base，也可能带来一些损失。Dynamically NTK Scaled\nRoPE是在NTK插值的基础上，把固定的系数改成动态的系数。</p>\n<p>具体来说，就是</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这样随着解码长度 <span class=\"math inline\">\\(l\\)</span> 的增长，当\n<span class=\"math inline\">\\(l&gt;L\\)</span> 之后 <span class=\"math inline\">\\(\\alpha\\)</span> 从1逐渐增大， <span class=\"math inline\">\\(l\\leq L\\)</span> 时则不需要改动。</p>\n<p>有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。</p>\n<p>当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度\n<span class=\"math inline\">\\(t&gt;1\\)</span>\n来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\n来扩展RoPE的长度。这样可以不必修改注意力的代码。</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>通过对Llama 1和Llama 2的实验，文章提出了建议值<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。</p>\n<p>YaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention\nscore进行调整。</p>\n<p>YaRN在微调以及无微调的情况下，效果都比上面的几种都要好。</p>\n<h2 id=\"logn\">logn</h2>\n<p>logn指的是对attention计算中的缩放因子 <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\n进行通过logn进行改进的一个方法，苏剑林在<a href=\"https://zhuanlan.zhihu.com/p/678755776\">博客</a>中进行了分析。大致的思路和YaRN中的缩放颇有些相似。</p>\n<p>简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention\nscore公式</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，当 <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\n时，其效果和YaRN中的放缩是类似的。</p>\n<h2 id=\"其他\">其他</h2>\n<p>在扩展推理长度上，还有很多其他有效的工作，比如各种window\nattention，streaming LLM，LongLoRA，Focus\nTransformer等，还有数据、评测等更方面的分析，待逐个梳理。</p>\n<h1 id=\"小结\">小结</h1>\n<p>较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降</p>\n<ul>\n<li>推理时用到了没训练过的位置编码<br>\n</li>\n<li>推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏</li>\n</ul>\n<p>这两个问题分别可以从位置编码和attention score的放缩来缓解。</p>\n<p>线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】分析transformer模型的参数量、计算量、中间激活、KV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n【3】Transformer升级之路：10、RoPE是一种β进制编码\nhttps://kexue.fm/archives/9675<br>\n【4】YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n【5】详解基于调整RoPE旋转角度的大模型长度外推方法\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符\nhttps://cloud.tencent.com/developer/article/2330611<br>\n【8】Transformer升级之路：8、长度外推性与位置鲁棒性\nhttps://spaces.ac.cn/archives/9444<br>\n【9】RoPE外推优化——支持192K上下文长度\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"从代码实现看normalization-到底做了什么","abbrlink":"b70b4a2d","date":"2024-04-06T04:24:25.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n之前在[《transformer中normalization的二三事》](http://www.linsight.cn/6a40bfa5.html)从思路上梳理了关于常用的normalization的内容。发出之后收到了一些反馈，关于这些norm在实际使用中是怎么实现的，有一些疑问。  \n\n因此本篇从实现的角度，来看下这些norm在不同的场景下，到底做了什么。  \n\n代码已上传至[https://github.com/Saicat/normalization_exp](https://github.com/Saicat/normalization_exp)\n\n# 二维数据\n\n先看下二维数据的情况下normalization是怎么做的。二维数据一般可以对应到神经网络中的全连接层，比如CNN中分类网络最后几个特征层。  \n\n```python\nimport torch\nfrom torch import nn\n\n# epsilon\neps = 1e-8\n\n# 定义一个随机二维输入\nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # 设置随机种子，方便复现\ninputs = torch.randn(batch_size, feature_num)\nprint('二维输入:\\n', inputs)\n```\n\n这里定义了一个3×4的矩阵，相当于batch size=3，特征向量维度为4。得到的随机二维输入是  \n\n```python\n二维输入:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\n```\n\n## batchnorm  \n\n用pytorch自带的BatchNorm1d对二维输入进行操作  \n\n```python\n# torch自带的batchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)  # 注意完整的batchnorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(1)  # 设置随机种子，方便复现\ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# 结果\ntorch_normed = torch_bn(inputs)\nprint('torch bn结果:\\n', torch_normed)\n```\n\n注意完整的batchnorm/layernorm等，是包括①归一化和②仿射变换（缩放+平移，也就是有可训练参数这部分）两步的。在BatchNorm接口中通过参数\"affine\"来决定是否进行放射变换。如果\"affine\"为False，相当于只是在某个维度上对数据进行了归一化处理。  \n\n而且pytorch中各种norm的接口初始化都把缩放系数初始化为1.0，平移系数初始化为0，相当于没有进行变换。为了把仿射变换的影响也一起对比，这里手动给缩放和平移系数都添加了一个随机数，变成如下数值  \n\n```python\nweight:\n Parameter containing:\ntensor([0.6614, 0.2669, 0.0617, 0.6213], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.4519, -0.1661, -1.5228,  0.3817], requires_grad=True) \n```\n\n这里缩放系数weight和平移系数bias的维度都是4，对应特征向量的维度。  \n\n输入矩阵用官方接口batchnorm之后得到的结果如下  \n\n```python\ntorch bn结果:\n tensor([[ 0.4756,  0.0513, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]],\n       grad_fn=<NativeBatchNormBackward0>)\n```\n\n接下来手动实现batchnorm  \n\n```python\n# 手动bn\n\n# 计算均值和标准差\nmean = torch.mean(inputs, dim=0, keepdim=True)\nprint('均值:\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\nprint('标准差:\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('手动bn结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\n在dim=0这个维度上计算均值和标准差，即对整个batch内所有sample的同一个feature，进行操作，获得结果如下    \n\n```python\n均值:\n tensor([[-0.0876, -0.6985, -0.7907,  0.5295]])\n标准差:\n tensor([[1.1612, 0.4971, 1.0630, 0.2692]]) \n```\n\n均值和标准差的维度也是和特征向量的维度一致。这里计算mean和std的时候keepdim设置为True和False都可以，最后都会自动broadcast。  \n\n一个要注意的点是，计算std的时候unbiased要设置为False，表明这里是对标准差的有偏估计，否则算出来的结果和torch的batchnorm接口不一致。  \n\n用手动计算出来的均值和标准差对输入进行归一化，再进行放射变换，得到手动计算的batchnorm结果如下  \n\n```python\n手动bn结果:\n tensor([[ 0.4756,  0.0514, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]], grad_fn=<AddBackward0>)\n```\n\n这里用torch.isclose接口验证官方batchnorm和手动计算的batchnorm是否相同  \n\n```python\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n为什么没有用equal，因为发现两个结果会有一点点误差，相对误差大概在1e-5~1e-4之间，应该是因为使用的eps不同导致。  \n\n## layernorm  \n\n看下layernorm对于二维数据的操作，还是用同样的3×4的输入\n\n使用torch官方接口\n\n```python\n# torch自带的layernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # 注意完整的layernorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(2)  # 设置随机种子，方便复现\ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# 结果\ntorch_normed = torch_ln(inputs)\nprint('torch ln结果:\\n', torch_normed)\n```\n\n得到layernorm仿射变换的系数如下  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.3923, -0.2236, -0.3195, -1.2050], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.0445, -0.6332,  0.5731,  0.5409], requires_grad=True) \n```\n维度依然是和特征向量的维度一致。  \n\n官方layernorm的结果是这样的  \n\n```python\ntorch ln结果:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4324]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n接下来手动实现一下，和官方结果作对比。  \n\n在dim=1计算均值和向量  \n\n```python\n# 手动ln\n\n# 计算均值\nmean = torch.mean(inputs, dim=1, keepdim=True)\nprint('均值:\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\nprint('标准差:\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('手动ln结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\n得到的均值和标准差是这样的  \n\n```python\n均值:\n tensor([[-0.0907],\n        [-0.3104],\n        [-0.3843]])\n标准差:\n tensor([[1.3691],\n        [0.9502],\n        [0.3458]]) \n```\n\n对输入进行归一化和仿射变换，结果如下，和官方接口结果一致  \n\n```python\n手动ln结果:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4325]], grad_fn=<AddBackward0>)\n验证结果:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n## 对比  \n\n对于二维输入，batchnorm和layernorm在做第①步归一化的时候，方向如下图  \n\n{% asset_img bn_and_ln.png bn和ln %}  \n\nbatchnorm在dim=0，即batch方向操作；而layernorm在dim=1，即特征向量内部进行操作。  \n\n但是无论是batchnorm还是layernorm，在做仿射变换的时候，使用的系数形状都和输入的特征向量相同，可以认为在放射变化这一步上，二者的操作是一样。  \n\n# CV数据\n\n再看下CV场景下的情况。  \n\nCV数据形状一般为[N,C,H,W]，N为batch size，C为channel即特征数，H和W分别是feature map的高和宽。先定义一个CV输入数据  \n\n```python\n# 定义一个随机四维输入，[N,C,H,W]\nbatch_size = 2\nchannel = 2\nheight = 2\nwidth = 3\ntorch.manual_seed(3)  # 设置随机种子，方便复现\ninputs = torch.randn(batch_size, channel, height, width)\nprint('四维输入:\\n', inputs)\n```\n\n输入如下  \n\n```python\n四维输入:\n tensor([[[[-0.0766,  0.3599, -0.7820],\n          [ 0.0715,  0.6648, -0.2868]],\n\n         [[ 1.6206, -1.5967,  0.4046],\n          [ 0.6113,  0.7604, -0.0336]]],\n\n\n        [[[-0.3448,  0.4937, -0.0776],\n          [-1.8054,  0.4851,  0.2052]],\n\n         [[ 0.3384,  1.3528,  0.3736],\n          [ 0.0134,  0.7737, -0.1092]]]])\n```\n\n## batchnorm  \n\n图像数据需要用BatchNorm2d，设置的特征数为channel  \n\n```python\n# torch自带的batchnorm\ntorch_bn = nn.BatchNorm2d(num_features=channel, affine=True)  # 注意完整的batchnorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(4)  # 设置随机种子，方便复现\ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# 结果\ntorch_normed = torch_bn(inputs)\nprint('torch bn结果:\\n', torch_normed)\n```\n\n仿射变换的参数如下，形状和channel数是一致的，和二维数据的情况一样。这里同样手动给缩放和平移系数加了个随机数  \n\n```python\nweight:\n Parameter containing:\ntensor([-1.6053,  0.2325], requires_grad=True)\nbias:\n Parameter containing:\ntensor([2.2399, 0.8473], requires_grad=True) \n```\n\n用torch官方batchnorm2d得到的结果是  \n\n```python\ntorch bn结果:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3753, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4684, 0.8186, 1.5090]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<NativeBatchNormBackward0>)\n```\n\n再来手动实现一下batchnorm2d  \n\n```python\n# 手动bn\n\nmanual_normed = []\n# 每个channel分别处理\nfor c in range(channel):\n    # 计算均值和标准差\n    mean = torch.mean(inputs[:, c, :, :])\n    std = torch.std(inputs[:, c, :, :], unbiased=False)\n    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]\n    normed = normed.unsqueeze(1)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 1)\nprint('手动bn结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n如同之前文章所解释，由于CV的卷积计算是通过二维滑动窗口在同一个输入平面上遍历所有位置，因此同一个channel下的多个值对于这个卷积和也是一种\"batch\"。  \n\n相当于对于每一个特征值，计算平均和标准差的范围是N×H×W。  \n\n{% asset_img cv_batchnorm.png CV数据batchnorm %}  \n\n手动计算得到的结果如下，和官方接口一致  \n\n```python\n手动bn结果:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3752, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4685, 0.8186, 1.5089]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<CatBackward0>)\n验证结果:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n## layernorm  \n\n按照[torch的layernorm官方接口文档](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)，对于图像数据，layernorm是这样做的  \n\n```python\n# torch自带的layernorm\ntorch_ln = nn.LayerNorm(\n    normalized_shape=[channel, height, width], \n    elementwise_affine=True\n)  # 注意完整的layernorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(5)  # 设置随机种子，方便复现\ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# 结果\ntorch_normed = torch_ln(inputs)\nprint('torch ln结果:\\n', torch_normed)\n```\n\n如同下面这个图所表示\n\n{% asset_img cv_layernorm.jpeg CV数据layernorm %}  \n\n此时仿射变化系数的形状是这样的，为[channel, height, width]  \n\n```python\nweight:\n Parameter containing:\ntensor([[[-0.4868, -0.6038, -0.5581],\n         [ 0.6675, -0.1974,  1.9428]],\n\n        [[-1.4017, -0.7626,  0.6312],\n         [-0.8991, -0.5578,  0.6907]]], requires_grad=True)\nbias:\n Parameter containing:\ntensor([[[ 0.2225, -0.6662,  0.6846],\n         [ 0.5740, -0.5829,  0.7679]],\n\n        [[ 0.0571, -1.1894, -0.5659],\n         [-0.8327,  0.9014,  0.2116]]], requires_grad=True) \n```\n\n即每个channel内的每一个特征值，都有单独的可训练的仿射变换系数。  \n\nlayernorm的结果如下  \n\n```python\ntorch ln结果:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5089, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8526],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4580, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<NativeLayerNormBackward0>)\n```\n\n手动进行layernorm的归一化和仿射变换，和官方接口对比一下  \n\n```python\n# 手动ln\n\nmanual_normed = []\n# 每个channel分别处理\nfor b in range(batch_size):\n    # 计算均值和标准差\n    mean = torch.mean(inputs[b, :, :, :])\n    std = torch.std(inputs[b, :, :, :], unbiased=False)\n    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\n    normed = normed.unsqueeze(0)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 0)\nprint('手动ln结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n这里计算均值和标准差，是把所有channel内的所有特征值放在一起算的，即每个样本只有一个标量的均值和一个标量的标准差。但是仿射变换的时候就每个特征值都有自己的参数。  \n\n手动计算的结果如下，和官方接口一致  \n\n```python\n手动ln结果:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5090, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8527],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4581, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<CatBackward0>)\n验证结果:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n# NLP数据  \n\n再看下在NLP场景下的情况。  \n\n先定义输入，N是batch size，S是sequence length，H是hidden size。  \n\n```python\n# 定义一个随机三维输入，[N,S,H]\nbatch_size = 2\nseq_len = 3\nhidden_size = 4\ntorch.manual_seed(6)  # 设置随机种子，方便复现\ninputs = torch.randn(batch_size, seq_len, hidden_size)\nprint('三维输入:\\n', inputs)\n```\n\n## batchnorm  \n\n用官方接口计算  \n\n```python\n# torch自带的batchnorm\ntorch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=True)  # 注意完整的batchnorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(7)  # 设置随机种子，方便复现\ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# # 结果\ntorch_normed = torch_bn(inputs.transpose(1, 2)).transpose(1, 2)\nprint('torch bn结果:\\n', torch_normed)\n```\n\n根据官方接口的描述，输入的第二维应该为特征数，第三维为序列长度，因此这里对输入做了transpose，再把结果transpose回来。  \n\n结果如下  \n\n```python\nweight:\n Parameter containing:\ntensor([-0.1468,  0.7861,  0.9468, -1.1143], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.6908, -0.8948, -0.3556,  1.2324], requires_grad=True) \n\ntorch bn结果:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9949,  0.1231]]], grad_fn=<TransposeBackward0>)\n```\n\n可以看到batchnorm的仿射变化系数形状在各种情况下都保持和特征向量维度相同。  \n\n再来手动计算验证一下  \n\n```python\n# 手动bn\n\n# 计算均值\nmean = torch.mean(inputs, dim=(0, 1) , keepdim=True)\nprint('均值:\\n', mean)\nstd = torch.std(inputs, dim=(0, 1), keepdim=True, unbiased=False)\nprint('标准差:\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('手动bn结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n这里计算用于归一化均值和方差，是在dim=(0,1)范围内计算的，相当于把[N, S, H]的输入拉平为[N×S, H]的二维输入，再按二维输入的方式进行batchnorm。  \n\n结果如下  \n\n```python\n均值:\n tensor([[[-0.2151,  0.5444, -0.2633, -0.5424]]])\n标准差:\n tensor([[[0.7984, 0.3537, 0.7799, 0.7986]]]) \n\n手动bn结果:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9950,  0.1231]]], grad_fn=<AddBackward0>)\n验证结果:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n## layernorm  \n\n终于来到NLP数据的layernorm，先确认一下，huggingface中bert是这么使用layernorm的  \n\n```python\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.normTensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n```\n\n用我们的数据跑一下  \n\n```python\n# torch自带的layernorm\ntorch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=True)  # 注意完整的layernorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(8)  # 设置随机种子，方便复现\ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# 结果\ntorch_normed = torch_ln(inputs)\nprint('torch ln结果:\\n', torch_normed)\n```\n\n仿射变化参数的形状和hidden size一致  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.2713, -1.2729,  0.5027,  0.4181], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.6394, -0.6608, -0.1433, -0.1043], requires_grad=True) \n\ntorch ln结果:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5589, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9346, -0.1230]]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n再来手动验证一下  \n\n```python\n# 手动ln\n\n# 计算均值\nmean = torch.mean(inputs, dim=2, keepdim=True)\nprint('均值:\\n', mean)\nstd = torch.std(inputs, dim=2, keepdim=True, unbiased=False)\nprint('标准差:\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('手动ln结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n得到的均值和标准差如下  \n\n```python\n均值:\n tensor([[[-0.8469],\n         [ 0.0745],\n         [ 0.3386]],\n\n        [[ 0.1364],\n         [-0.7003],\n         [ 0.2831]]])\n标准差:\n tensor([[[0.8578],\n         [0.3354],\n         [0.6505]],\n\n        [[0.4426],\n         [0.8448],\n         [0.6816]]]) \n```\n\n每个sample中的每个token，都有各自的均值和标准差，用于归一化。  \n\n最终结果如下  \n\n```python\n手动ln结果:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5590, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9347, -0.1230]]], grad_fn=<AddBackward0>)\n验证结果:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n# 归一化的输入能变回原输入吗  \n\n既然这些操作是先计算均值和标准差进行归一化，再进行仿射变换，那把仿射变换的参数设置为输入的均值和标准差，是不是就可以把归一化过的数据变回跟原数据一模一样了呢？\n\n以二维情况为例，看下batchnorm是否能变回去。  \n\n```python\n# 定义一个随机二维输入\nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # 设置随机种子，方便复现\ninputs = torch.randn(batch_size, feature_num)\nprint('二维输入:\\n', inputs)\n\n# 计算均值和标准差\nmean = torch.mean(inputs, dim=0, keepdim=True)\n# print('均值:\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\n# print('标准差:\\n', std, '\\n')\n\n# torch自带的batchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)\n\n# 把仿射变换的缩放和平移替换为标准差和均值\ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# 结果\ntorch_normed = torch_bn(inputs)\nprint('torch bn结果:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n结果如下  \n\n```python\n二维输入:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch bn结果:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1821]],\n       grad_fn=<NativeBatchNormBackward0>)\n验证结果:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n确认了batchnorm是可以变回去的。  \n\n再来看下layernorm  \n\n```python\nprint('二维输入:\\n', inputs)\n\n# 计算均值和标准差\nmean = torch.mean(inputs, dim=1, keepdim=True)\n# print('均值:\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\n# print('标准差:\\n', std, '\\n')\n\n# torch自带的layernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # 注意完整的layernorm要包括仿射变换\n\n# 把仿射变换的缩放和平移替换为标准差和均值\ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# 结果\ntorch_normed = torch_ln(inputs)\nprint('torch ln结果:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n结果如下\n\n```python\n二维输入:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch ln结果:\n tensor([[ 1.1918, -0.1481, -1.5251,  0.4814],\n        [-0.8146, -1.1451,  0.7512,  1.2086],\n        [-0.9685, -0.0551, -0.6140,  1.6376]],\n       grad_fn=<NativeLayerNormBackward0>)\n验证结果:\n tensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n```\n\n发现layernorm并不能通过这种方式把归一化的输入变回原始值，因为layernorm归一化是在特征向量内进行的，所有特征值共享一个均值和方差，但是仿射变换的时候每个特征却有单独的系数。  \n\n对于CV数据和NLP数据也有一样的结论。\n\n可以认为batchnorm的归一化和仿射变换是互为可逆的一对操作，而layernorm的归一化和仿射变换是在不同范围内的操作，是不可逆的。  \n\n# 小结\n\n本篇从各种输入数据对batchnorm和layernorm做了手动复现。  \n\n需要注意到，batchnorm、layernorm等实际都包含两步操作：①归一化②仿射变换。  \n\n基本上，batchnorm可以总结为，对于特征向量中的每一个特征值，在一个\"大范围\"内进行归一化，这个\"大范围\"根据输入数据形状，可能是batch，可能是batch×序列长度，或者batch×feature map大小。并且归一化和仿射变换在同一个方向上进行，因此这两个操作是互为可逆的。  \n\n而layernorm是在每个特征向量内部进行归一化处理，然后在另一个方向上使用仿射变换。由于归一化和仿射变换的方向不同，因此无法通过把仿射变换，把已经归一化的数据变换为原输入数据。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***\n\n# Reference  \n【1】LAYERNORM https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html  \n【2】BATCHNORM1D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html  \n【3】BATCHNORM2D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html  \n","source":"_posts/cs/nlp/2024/04/从实现看normalization-到底干了什么.md","raw":"---\ntitle: 从代码实现看normalization-到底做了什么\nabbrlink: b70b4a2d\ndate: 2024-04-06 12:24:25\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - layernorm\n  - normalization\n  - batchnorm\ncategories:\n  - CS\n  - NLP\n  - LLM\n\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n之前在[《transformer中normalization的二三事》](http://www.linsight.cn/6a40bfa5.html)从思路上梳理了关于常用的normalization的内容。发出之后收到了一些反馈，关于这些norm在实际使用中是怎么实现的，有一些疑问。  \n\n因此本篇从实现的角度，来看下这些norm在不同的场景下，到底做了什么。  \n\n代码已上传至[https://github.com/Saicat/normalization_exp](https://github.com/Saicat/normalization_exp)\n\n# 二维数据\n\n先看下二维数据的情况下normalization是怎么做的。二维数据一般可以对应到神经网络中的全连接层，比如CNN中分类网络最后几个特征层。  \n\n```python\nimport torch\nfrom torch import nn\n\n# epsilon\neps = 1e-8\n\n# 定义一个随机二维输入\nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # 设置随机种子，方便复现\ninputs = torch.randn(batch_size, feature_num)\nprint('二维输入:\\n', inputs)\n```\n\n这里定义了一个3×4的矩阵，相当于batch size=3，特征向量维度为4。得到的随机二维输入是  \n\n```python\n二维输入:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\n```\n\n## batchnorm  \n\n用pytorch自带的BatchNorm1d对二维输入进行操作  \n\n```python\n# torch自带的batchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)  # 注意完整的batchnorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(1)  # 设置随机种子，方便复现\ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# 结果\ntorch_normed = torch_bn(inputs)\nprint('torch bn结果:\\n', torch_normed)\n```\n\n注意完整的batchnorm/layernorm等，是包括①归一化和②仿射变换（缩放+平移，也就是有可训练参数这部分）两步的。在BatchNorm接口中通过参数\"affine\"来决定是否进行放射变换。如果\"affine\"为False，相当于只是在某个维度上对数据进行了归一化处理。  \n\n而且pytorch中各种norm的接口初始化都把缩放系数初始化为1.0，平移系数初始化为0，相当于没有进行变换。为了把仿射变换的影响也一起对比，这里手动给缩放和平移系数都添加了一个随机数，变成如下数值  \n\n```python\nweight:\n Parameter containing:\ntensor([0.6614, 0.2669, 0.0617, 0.6213], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.4519, -0.1661, -1.5228,  0.3817], requires_grad=True) \n```\n\n这里缩放系数weight和平移系数bias的维度都是4，对应特征向量的维度。  \n\n输入矩阵用官方接口batchnorm之后得到的结果如下  \n\n```python\ntorch bn结果:\n tensor([[ 0.4756,  0.0513, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]],\n       grad_fn=<NativeBatchNormBackward0>)\n```\n\n接下来手动实现batchnorm  \n\n```python\n# 手动bn\n\n# 计算均值和标准差\nmean = torch.mean(inputs, dim=0, keepdim=True)\nprint('均值:\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\nprint('标准差:\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('手动bn结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\n在dim=0这个维度上计算均值和标准差，即对整个batch内所有sample的同一个feature，进行操作，获得结果如下    \n\n```python\n均值:\n tensor([[-0.0876, -0.6985, -0.7907,  0.5295]])\n标准差:\n tensor([[1.1612, 0.4971, 1.0630, 0.2692]]) \n```\n\n均值和标准差的维度也是和特征向量的维度一致。这里计算mean和std的时候keepdim设置为True和False都可以，最后都会自动broadcast。  \n\n一个要注意的点是，计算std的时候unbiased要设置为False，表明这里是对标准差的有偏估计，否则算出来的结果和torch的batchnorm接口不一致。  \n\n用手动计算出来的均值和标准差对输入进行归一化，再进行放射变换，得到手动计算的batchnorm结果如下  \n\n```python\n手动bn结果:\n tensor([[ 0.4756,  0.0514, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]], grad_fn=<AddBackward0>)\n```\n\n这里用torch.isclose接口验证官方batchnorm和手动计算的batchnorm是否相同  \n\n```python\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n为什么没有用equal，因为发现两个结果会有一点点误差，相对误差大概在1e-5~1e-4之间，应该是因为使用的eps不同导致。  \n\n## layernorm  \n\n看下layernorm对于二维数据的操作，还是用同样的3×4的输入\n\n使用torch官方接口\n\n```python\n# torch自带的layernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # 注意完整的layernorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(2)  # 设置随机种子，方便复现\ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# 结果\ntorch_normed = torch_ln(inputs)\nprint('torch ln结果:\\n', torch_normed)\n```\n\n得到layernorm仿射变换的系数如下  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.3923, -0.2236, -0.3195, -1.2050], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.0445, -0.6332,  0.5731,  0.5409], requires_grad=True) \n```\n维度依然是和特征向量的维度一致。  \n\n官方layernorm的结果是这样的  \n\n```python\ntorch ln结果:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4324]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n接下来手动实现一下，和官方结果作对比。  \n\n在dim=1计算均值和向量  \n\n```python\n# 手动ln\n\n# 计算均值\nmean = torch.mean(inputs, dim=1, keepdim=True)\nprint('均值:\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\nprint('标准差:\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('手动ln结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\n得到的均值和标准差是这样的  \n\n```python\n均值:\n tensor([[-0.0907],\n        [-0.3104],\n        [-0.3843]])\n标准差:\n tensor([[1.3691],\n        [0.9502],\n        [0.3458]]) \n```\n\n对输入进行归一化和仿射变换，结果如下，和官方接口结果一致  \n\n```python\n手动ln结果:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4325]], grad_fn=<AddBackward0>)\n验证结果:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n## 对比  \n\n对于二维输入，batchnorm和layernorm在做第①步归一化的时候，方向如下图  \n\n{% asset_img bn_and_ln.png bn和ln %}  \n\nbatchnorm在dim=0，即batch方向操作；而layernorm在dim=1，即特征向量内部进行操作。  \n\n但是无论是batchnorm还是layernorm，在做仿射变换的时候，使用的系数形状都和输入的特征向量相同，可以认为在放射变化这一步上，二者的操作是一样。  \n\n# CV数据\n\n再看下CV场景下的情况。  \n\nCV数据形状一般为[N,C,H,W]，N为batch size，C为channel即特征数，H和W分别是feature map的高和宽。先定义一个CV输入数据  \n\n```python\n# 定义一个随机四维输入，[N,C,H,W]\nbatch_size = 2\nchannel = 2\nheight = 2\nwidth = 3\ntorch.manual_seed(3)  # 设置随机种子，方便复现\ninputs = torch.randn(batch_size, channel, height, width)\nprint('四维输入:\\n', inputs)\n```\n\n输入如下  \n\n```python\n四维输入:\n tensor([[[[-0.0766,  0.3599, -0.7820],\n          [ 0.0715,  0.6648, -0.2868]],\n\n         [[ 1.6206, -1.5967,  0.4046],\n          [ 0.6113,  0.7604, -0.0336]]],\n\n\n        [[[-0.3448,  0.4937, -0.0776],\n          [-1.8054,  0.4851,  0.2052]],\n\n         [[ 0.3384,  1.3528,  0.3736],\n          [ 0.0134,  0.7737, -0.1092]]]])\n```\n\n## batchnorm  \n\n图像数据需要用BatchNorm2d，设置的特征数为channel  \n\n```python\n# torch自带的batchnorm\ntorch_bn = nn.BatchNorm2d(num_features=channel, affine=True)  # 注意完整的batchnorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(4)  # 设置随机种子，方便复现\ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# 结果\ntorch_normed = torch_bn(inputs)\nprint('torch bn结果:\\n', torch_normed)\n```\n\n仿射变换的参数如下，形状和channel数是一致的，和二维数据的情况一样。这里同样手动给缩放和平移系数加了个随机数  \n\n```python\nweight:\n Parameter containing:\ntensor([-1.6053,  0.2325], requires_grad=True)\nbias:\n Parameter containing:\ntensor([2.2399, 0.8473], requires_grad=True) \n```\n\n用torch官方batchnorm2d得到的结果是  \n\n```python\ntorch bn结果:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3753, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4684, 0.8186, 1.5090]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<NativeBatchNormBackward0>)\n```\n\n再来手动实现一下batchnorm2d  \n\n```python\n# 手动bn\n\nmanual_normed = []\n# 每个channel分别处理\nfor c in range(channel):\n    # 计算均值和标准差\n    mean = torch.mean(inputs[:, c, :, :])\n    std = torch.std(inputs[:, c, :, :], unbiased=False)\n    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]\n    normed = normed.unsqueeze(1)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 1)\nprint('手动bn结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n如同之前文章所解释，由于CV的卷积计算是通过二维滑动窗口在同一个输入平面上遍历所有位置，因此同一个channel下的多个值对于这个卷积和也是一种\"batch\"。  \n\n相当于对于每一个特征值，计算平均和标准差的范围是N×H×W。  \n\n{% asset_img cv_batchnorm.png CV数据batchnorm %}  \n\n手动计算得到的结果如下，和官方接口一致  \n\n```python\n手动bn结果:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3752, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4685, 0.8186, 1.5089]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<CatBackward0>)\n验证结果:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n## layernorm  \n\n按照[torch的layernorm官方接口文档](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)，对于图像数据，layernorm是这样做的  \n\n```python\n# torch自带的layernorm\ntorch_ln = nn.LayerNorm(\n    normalized_shape=[channel, height, width], \n    elementwise_affine=True\n)  # 注意完整的layernorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(5)  # 设置随机种子，方便复现\ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# 结果\ntorch_normed = torch_ln(inputs)\nprint('torch ln结果:\\n', torch_normed)\n```\n\n如同下面这个图所表示\n\n{% asset_img cv_layernorm.jpeg CV数据layernorm %}  \n\n此时仿射变化系数的形状是这样的，为[channel, height, width]  \n\n```python\nweight:\n Parameter containing:\ntensor([[[-0.4868, -0.6038, -0.5581],\n         [ 0.6675, -0.1974,  1.9428]],\n\n        [[-1.4017, -0.7626,  0.6312],\n         [-0.8991, -0.5578,  0.6907]]], requires_grad=True)\nbias:\n Parameter containing:\ntensor([[[ 0.2225, -0.6662,  0.6846],\n         [ 0.5740, -0.5829,  0.7679]],\n\n        [[ 0.0571, -1.1894, -0.5659],\n         [-0.8327,  0.9014,  0.2116]]], requires_grad=True) \n```\n\n即每个channel内的每一个特征值，都有单独的可训练的仿射变换系数。  \n\nlayernorm的结果如下  \n\n```python\ntorch ln结果:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5089, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8526],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4580, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<NativeLayerNormBackward0>)\n```\n\n手动进行layernorm的归一化和仿射变换，和官方接口对比一下  \n\n```python\n# 手动ln\n\nmanual_normed = []\n# 每个channel分别处理\nfor b in range(batch_size):\n    # 计算均值和标准差\n    mean = torch.mean(inputs[b, :, :, :])\n    std = torch.std(inputs[b, :, :, :], unbiased=False)\n    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\n    normed = normed.unsqueeze(0)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 0)\nprint('手动ln结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n这里计算均值和标准差，是把所有channel内的所有特征值放在一起算的，即每个样本只有一个标量的均值和一个标量的标准差。但是仿射变换的时候就每个特征值都有自己的参数。  \n\n手动计算的结果如下，和官方接口一致  \n\n```python\n手动ln结果:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5090, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8527],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4581, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<CatBackward0>)\n验证结果:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n# NLP数据  \n\n再看下在NLP场景下的情况。  \n\n先定义输入，N是batch size，S是sequence length，H是hidden size。  \n\n```python\n# 定义一个随机三维输入，[N,S,H]\nbatch_size = 2\nseq_len = 3\nhidden_size = 4\ntorch.manual_seed(6)  # 设置随机种子，方便复现\ninputs = torch.randn(batch_size, seq_len, hidden_size)\nprint('三维输入:\\n', inputs)\n```\n\n## batchnorm  \n\n用官方接口计算  \n\n```python\n# torch自带的batchnorm\ntorch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=True)  # 注意完整的batchnorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(7)  # 设置随机种子，方便复现\ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# # 结果\ntorch_normed = torch_bn(inputs.transpose(1, 2)).transpose(1, 2)\nprint('torch bn结果:\\n', torch_normed)\n```\n\n根据官方接口的描述，输入的第二维应该为特征数，第三维为序列长度，因此这里对输入做了transpose，再把结果transpose回来。  \n\n结果如下  \n\n```python\nweight:\n Parameter containing:\ntensor([-0.1468,  0.7861,  0.9468, -1.1143], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.6908, -0.8948, -0.3556,  1.2324], requires_grad=True) \n\ntorch bn结果:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9949,  0.1231]]], grad_fn=<TransposeBackward0>)\n```\n\n可以看到batchnorm的仿射变化系数形状在各种情况下都保持和特征向量维度相同。  \n\n再来手动计算验证一下  \n\n```python\n# 手动bn\n\n# 计算均值\nmean = torch.mean(inputs, dim=(0, 1) , keepdim=True)\nprint('均值:\\n', mean)\nstd = torch.std(inputs, dim=(0, 1), keepdim=True, unbiased=False)\nprint('标准差:\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('手动bn结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n这里计算用于归一化均值和方差，是在dim=(0,1)范围内计算的，相当于把[N, S, H]的输入拉平为[N×S, H]的二维输入，再按二维输入的方式进行batchnorm。  \n\n结果如下  \n\n```python\n均值:\n tensor([[[-0.2151,  0.5444, -0.2633, -0.5424]]])\n标准差:\n tensor([[[0.7984, 0.3537, 0.7799, 0.7986]]]) \n\n手动bn结果:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9950,  0.1231]]], grad_fn=<AddBackward0>)\n验证结果:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n## layernorm  \n\n终于来到NLP数据的layernorm，先确认一下，huggingface中bert是这么使用layernorm的  \n\n```python\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.normTensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n```\n\n用我们的数据跑一下  \n\n```python\n# torch自带的layernorm\ntorch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=True)  # 注意完整的layernorm要包括仿射变换\n\n# 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果\n# 手动改成别的值，用于对比包含仿射变换的效果\ntorch.manual_seed(8)  # 设置随机种子，方便复现\ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# 结果\ntorch_normed = torch_ln(inputs)\nprint('torch ln结果:\\n', torch_normed)\n```\n\n仿射变化参数的形状和hidden size一致  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.2713, -1.2729,  0.5027,  0.4181], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.6394, -0.6608, -0.1433, -0.1043], requires_grad=True) \n\ntorch ln结果:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5589, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9346, -0.1230]]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n再来手动验证一下  \n\n```python\n# 手动ln\n\n# 计算均值\nmean = torch.mean(inputs, dim=2, keepdim=True)\nprint('均值:\\n', mean)\nstd = torch.std(inputs, dim=2, keepdim=True, unbiased=False)\nprint('标准差:\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('手动ln结果:\\n', manual_normed)\n\n# 手动操作和torch自带操作有点误差，<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n得到的均值和标准差如下  \n\n```python\n均值:\n tensor([[[-0.8469],\n         [ 0.0745],\n         [ 0.3386]],\n\n        [[ 0.1364],\n         [-0.7003],\n         [ 0.2831]]])\n标准差:\n tensor([[[0.8578],\n         [0.3354],\n         [0.6505]],\n\n        [[0.4426],\n         [0.8448],\n         [0.6816]]]) \n```\n\n每个sample中的每个token，都有各自的均值和标准差，用于归一化。  \n\n最终结果如下  \n\n```python\n手动ln结果:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5590, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9347, -0.1230]]], grad_fn=<AddBackward0>)\n验证结果:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n# 归一化的输入能变回原输入吗  \n\n既然这些操作是先计算均值和标准差进行归一化，再进行仿射变换，那把仿射变换的参数设置为输入的均值和标准差，是不是就可以把归一化过的数据变回跟原数据一模一样了呢？\n\n以二维情况为例，看下batchnorm是否能变回去。  \n\n```python\n# 定义一个随机二维输入\nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # 设置随机种子，方便复现\ninputs = torch.randn(batch_size, feature_num)\nprint('二维输入:\\n', inputs)\n\n# 计算均值和标准差\nmean = torch.mean(inputs, dim=0, keepdim=True)\n# print('均值:\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\n# print('标准差:\\n', std, '\\n')\n\n# torch自带的batchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)\n\n# 把仿射变换的缩放和平移替换为标准差和均值\ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# 结果\ntorch_normed = torch_bn(inputs)\nprint('torch bn结果:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n结果如下  \n\n```python\n二维输入:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch bn结果:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1821]],\n       grad_fn=<NativeBatchNormBackward0>)\n验证结果:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n确认了batchnorm是可以变回去的。  \n\n再来看下layernorm  \n\n```python\nprint('二维输入:\\n', inputs)\n\n# 计算均值和标准差\nmean = torch.mean(inputs, dim=1, keepdim=True)\n# print('均值:\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\n# print('标准差:\\n', std, '\\n')\n\n# torch自带的layernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # 注意完整的layernorm要包括仿射变换\n\n# 把仿射变换的缩放和平移替换为标准差和均值\ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# 结果\ntorch_normed = torch_ln(inputs)\nprint('torch ln结果:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint('验证结果:\\n', isclose)\n```\n\n结果如下\n\n```python\n二维输入:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch ln结果:\n tensor([[ 1.1918, -0.1481, -1.5251,  0.4814],\n        [-0.8146, -1.1451,  0.7512,  1.2086],\n        [-0.9685, -0.0551, -0.6140,  1.6376]],\n       grad_fn=<NativeLayerNormBackward0>)\n验证结果:\n tensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n```\n\n发现layernorm并不能通过这种方式把归一化的输入变回原始值，因为layernorm归一化是在特征向量内进行的，所有特征值共享一个均值和方差，但是仿射变换的时候每个特征却有单独的系数。  \n\n对于CV数据和NLP数据也有一样的结论。\n\n可以认为batchnorm的归一化和仿射变换是互为可逆的一对操作，而layernorm的归一化和仿射变换是在不同范围内的操作，是不可逆的。  \n\n# 小结\n\n本篇从各种输入数据对batchnorm和layernorm做了手动复现。  \n\n需要注意到，batchnorm、layernorm等实际都包含两步操作：①归一化②仿射变换。  \n\n基本上，batchnorm可以总结为，对于特征向量中的每一个特征值，在一个\"大范围\"内进行归一化，这个\"大范围\"根据输入数据形状，可能是batch，可能是batch×序列长度，或者batch×feature map大小。并且归一化和仿射变换在同一个方向上进行，因此这两个操作是互为可逆的。  \n\n而layernorm是在每个特征向量内部进行归一化处理，然后在另一个方向上使用仿射变换。由于归一化和仿射变换的方向不同，因此无法通过把仿射变换，把已经归一化的数据变换为原输入数据。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***\n\n# Reference  \n【1】LAYERNORM https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html  \n【2】BATCHNORM1D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html  \n【3】BATCHNORM2D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html  \n","slug":"cs/nlp/2024/04/从实现看normalization-到底干了什么","published":1,"updated":"2024-05-10T06:51:09.591Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9t0007314k6r0fefgg","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>之前在<a href=\"http://www.linsight.cn/6a40bfa5.html\">《transformer中normalization的二三事》</a>从思路上梳理了关于常用的normalization的内容。发出之后收到了一些反馈，关于这些norm在实际使用中是怎么实现的，有一些疑问。</p>\n<p>因此本篇从实现的角度，来看下这些norm在不同的场景下，到底做了什么。</p>\n<p>代码已上传至<a href=\"https://github.com/Saicat/normalization_exp\">https://github.com/Saicat/normalization_exp</a></p>\n<h1 id=\"二维数据\">二维数据</h1>\n<p>先看下二维数据的情况下normalization是怎么做的。二维数据一般可以对应到神经网络中的全连接层，比如CNN中分类网络最后几个特征层。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># epsilon</span></span><br><span class=\"line\">eps = <span class=\"number\">1e-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义一个随机二维输入</span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;二维输入:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p>这里定义了一个3×4的矩阵，相当于batch\nsize=3，特征向量维度为4。得到的随机二维输入是</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">二维输入:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>用pytorch自带的BatchNorm1d对二维输入进行操作</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的batchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的batchnorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">1</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>注意完整的batchnorm/layernorm等，是包括①归一化和②仿射变换（缩放+平移，也就是有可训练参数这部分）两步的。在BatchNorm接口中通过参数\"affine\"来决定是否进行放射变换。如果\"affine\"为False，相当于只是在某个维度上对数据进行了归一化处理。</p>\n<p>而且pytorch中各种norm的接口初始化都把缩放系数初始化为1.0，平移系数初始化为0，相当于没有进行变换。为了把仿射变换的影响也一起对比，这里手动给缩放和平移系数都添加了一个随机数，变成如下数值</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">0.6614</span>, <span class=\"number\">0.2669</span>, <span class=\"number\">0.0617</span>, <span class=\"number\">0.6213</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.4519</span>, -<span class=\"number\">0.1661</span>, -<span class=\"number\">1.5228</span>,  <span class=\"number\">0.3817</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>这里缩放系数weight和平移系数bias的维度都是4，对应特征向量的维度。</p>\n<p>输入矩阵用官方接口batchnorm之后得到的结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0513</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>接下来手动实现batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值和标准差</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;均值:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;标准差:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动bn结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p>在dim=0这个维度上计算均值和标准差，即对整个batch内所有sample的同一个feature，进行操作，获得结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">均值:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0876</span>, -<span class=\"number\">0.6985</span>, -<span class=\"number\">0.7907</span>,  <span class=\"number\">0.5295</span>]])</span><br><span class=\"line\">标准差:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.1612</span>, <span class=\"number\">0.4971</span>, <span class=\"number\">1.0630</span>, <span class=\"number\">0.2692</span>]]) </span><br></pre></td></tr></table></figure>\n<p>均值和标准差的维度也是和特征向量的维度一致。这里计算mean和std的时候keepdim设置为True和False都可以，最后都会自动broadcast。</p>\n<p>一个要注意的点是，计算std的时候unbiased要设置为False，表明这里是对标准差的有偏估计，否则算出来的结果和torch的batchnorm接口不一致。</p>\n<p>用手动计算出来的均值和标准差对输入进行归一化，再进行放射变换，得到手动计算的batchnorm结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">手动bn结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0514</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>这里用torch.isclose接口验证官方batchnorm和手动计算的batchnorm是否相同</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>为什么没有用equal，因为发现两个结果会有一点点误差，相对误差大概在1e-5~1e-4之间，应该是因为使用的eps不同导致。</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>看下layernorm对于二维数据的操作，还是用同样的3×4的输入</p>\n<p>使用torch官方接口</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的layernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的layernorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">2</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>得到layernorm仿射变换的系数如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.3923</span>, -<span class=\"number\">0.2236</span>, -<span class=\"number\">0.3195</span>, -<span class=\"number\">1.2050</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.0445</span>, -<span class=\"number\">0.6332</span>,  <span class=\"number\">0.5731</span>,  <span class=\"number\">0.5409</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>维度依然是和特征向量的维度一致。</p>\n<p>官方layernorm的结果是这样的</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4324</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>接下来手动实现一下，和官方结果作对比。</p>\n<p>在dim=1计算均值和向量</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;均值:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;标准差:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动ln结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p>得到的均值和标准差是这样的</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">均值:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0907</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3104</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3843</span>]])</span><br><span class=\"line\">标准差:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.3691</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.9502</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.3458</span>]]) </span><br></pre></td></tr></table></figure>\n<p>对输入进行归一化和仿射变换，结果如下，和官方接口结果一致</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">手动ln结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4325</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"对比\">对比</h2>\n<p>对于二维输入，batchnorm和layernorm在做第①步归一化的时候，方向如下图</p>\n<img src=\"/b70b4a2d/bn_and_ln.png\" class title=\"bn和ln\">\n<p>batchnorm在dim=0，即batch方向操作；而layernorm在dim=1，即特征向量内部进行操作。</p>\n<p>但是无论是batchnorm还是layernorm，在做仿射变换的时候，使用的系数形状都和输入的特征向量相同，可以认为在放射变化这一步上，二者的操作是一样。</p>\n<h1 id=\"cv数据\">CV数据</h1>\n<p>再看下CV场景下的情况。</p>\n<p>CV数据形状一般为[N,C,H,W]，N为batch\nsize，C为channel即特征数，H和W分别是feature\nmap的高和宽。先定义一个CV输入数据</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义一个随机四维输入，[N,C,H,W]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">channel = <span class=\"number\">2</span></span><br><span class=\"line\">height = <span class=\"number\">2</span></span><br><span class=\"line\">width = <span class=\"number\">3</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">3</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">inputs = torch.randn(batch_size, channel, height, width)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;四维输入:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p>输入如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">四维输入:</span><br><span class=\"line\"> tensor([[[[-<span class=\"number\">0.0766</span>,  <span class=\"number\">0.3599</span>, -<span class=\"number\">0.7820</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0715</span>,  <span class=\"number\">0.6648</span>, -<span class=\"number\">0.2868</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">1.6206</span>, -<span class=\"number\">1.5967</span>,  <span class=\"number\">0.4046</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.6113</span>,  <span class=\"number\">0.7604</span>, -<span class=\"number\">0.0336</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[-<span class=\"number\">0.3448</span>,  <span class=\"number\">0.4937</span>, -<span class=\"number\">0.0776</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.8054</span>,  <span class=\"number\">0.4851</span>,  <span class=\"number\">0.2052</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">0.3384</span>,  <span class=\"number\">1.3528</span>,  <span class=\"number\">0.3736</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0134</span>,  <span class=\"number\">0.7737</span>, -<span class=\"number\">0.1092</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-1\">batchnorm</h2>\n<p>图像数据需要用BatchNorm2d，设置的特征数为channel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的batchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm2d(num_features=channel, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的batchnorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">4</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>仿射变换的参数如下，形状和channel数是一致的，和二维数据的情况一样。这里同样手动给缩放和平移系数加了个随机数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">1.6053</span>,  <span class=\"number\">0.2325</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">2.2399</span>, <span class=\"number\">0.8473</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>用torch官方batchnorm2d得到的结果是</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn结果:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3753</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4684</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5090</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>再来手动实现一下batchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动bn</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># 每个channel分别处理</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(channel):</span><br><span class=\"line\">    <span class=\"comment\"># 计算均值和标准差</span></span><br><span class=\"line\">    mean = torch.mean(inputs[:, c, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[:, c, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动bn结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>如同之前文章所解释，由于CV的卷积计算是通过二维滑动窗口在同一个输入平面上遍历所有位置，因此同一个channel下的多个值对于这个卷积和也是一种\"batch\"。</p>\n<p>相当于对于每一个特征值，计算平均和标准差的范围是N×H×W。</p>\n<img src=\"/b70b4a2d/cv_batchnorm.png\" class title=\"CV数据batchnorm\">\n<p>手动计算得到的结果如下，和官方接口一致</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">手动bn结果:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3752</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4685</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5089</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-1\">layernorm</h2>\n<p>按照<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\">torch的layernorm官方接口文档</a>，对于图像数据，layernorm是这样做的</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的layernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(</span><br><span class=\"line\">    normalized_shape=[channel, height, width], </span><br><span class=\"line\">    elementwise_affine=<span class=\"literal\">True</span></span><br><span class=\"line\">)  <span class=\"comment\"># 注意完整的layernorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">5</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>如同下面这个图所表示</p>\n<img src=\"/b70b4a2d/cv_layernorm.jpeg\" class title=\"CV数据layernorm\">\n<p>此时仿射变化系数的形状是这样的，为[channel, height, width]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[-<span class=\"number\">0.4868</span>, -<span class=\"number\">0.6038</span>, -<span class=\"number\">0.5581</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.6675</span>, -<span class=\"number\">0.1974</span>,  <span class=\"number\">1.9428</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">1.4017</span>, -<span class=\"number\">0.7626</span>,  <span class=\"number\">0.6312</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8991</span>, -<span class=\"number\">0.5578</span>,  <span class=\"number\">0.6907</span>]]], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[ <span class=\"number\">0.2225</span>, -<span class=\"number\">0.6662</span>,  <span class=\"number\">0.6846</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.5740</span>, -<span class=\"number\">0.5829</span>,  <span class=\"number\">0.7679</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.0571</span>, -<span class=\"number\">1.1894</span>, -<span class=\"number\">0.5659</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8327</span>,  <span class=\"number\">0.9014</span>,  <span class=\"number\">0.2116</span>]]], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>即每个channel内的每一个特征值，都有单独的可训练的仿射变换系数。</p>\n<p>layernorm的结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln结果:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5089</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8526</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4580</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>手动进行layernorm的归一化和仿射变换，和官方接口对比一下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动ln</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># 每个channel分别处理</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> b <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(batch_size):</span><br><span class=\"line\">    <span class=\"comment\"># 计算均值和标准差</span></span><br><span class=\"line\">    mean = torch.mean(inputs[b, :, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[b, :, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动ln结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>这里计算均值和标准差，是把所有channel内的所有特征值放在一起算的，即每个样本只有一个标量的均值和一个标量的标准差。但是仿射变换的时候就每个特征值都有自己的参数。</p>\n<p>手动计算的结果如下，和官方接口一致</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">手动ln结果:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5090</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8527</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4581</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"nlp数据\">NLP数据</h1>\n<p>再看下在NLP场景下的情况。</p>\n<p>先定义输入，N是batch size，S是sequence length，H是hidden size。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义一个随机三维输入，[N,S,H]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">seq_len = <span class=\"number\">3</span></span><br><span class=\"line\">hidden_size = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">6</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">inputs = torch.randn(batch_size, seq_len, hidden_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;三维输入:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-2\">batchnorm</h2>\n<p>用官方接口计算</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的batchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的batchnorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">7</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># # 结果</span></span><br><span class=\"line\">torch_normed = torch_bn(inputs.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>根据官方接口的描述，输入的第二维应该为特征数，第三维为序列长度，因此这里对输入做了transpose，再把结果transpose回来。</p>\n<p>结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.1468</span>,  <span class=\"number\">0.7861</span>,  <span class=\"number\">0.9468</span>, -<span class=\"number\">1.1143</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.6908</span>, -<span class=\"number\">0.8948</span>, -<span class=\"number\">0.3556</span>,  <span class=\"number\">1.2324</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch bn结果:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9949</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>可以看到batchnorm的仿射变化系数形状在各种情况下都保持和特征向量维度相同。</p>\n<p>再来手动计算验证一下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>) , keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;均值:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>), keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;标准差:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动bn结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>这里计算用于归一化均值和方差，是在dim=(0,1)范围内计算的，相当于把[N,\nS, H]的输入拉平为[N×S,\nH]的二维输入，再按二维输入的方式进行batchnorm。</p>\n<p>结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">均值:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.2151</span>,  <span class=\"number\">0.5444</span>, -<span class=\"number\">0.2633</span>, -<span class=\"number\">0.5424</span>]]])</span><br><span class=\"line\">标准差:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.7984</span>, <span class=\"number\">0.3537</span>, <span class=\"number\">0.7799</span>, <span class=\"number\">0.7986</span>]]]) </span><br><span class=\"line\"></span><br><span class=\"line\">手动bn结果:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9950</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-2\">layernorm</h2>\n<p>终于来到NLP数据的layernorm，先确认一下，huggingface中bert是这么使用layernorm的</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BertSelfOutput</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, config</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class=\"line\">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class=\"line\">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, hidden_states: torch.normTensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class=\"line\">        hidden_states = self.dense(hidden_states)</span><br><span class=\"line\">        hidden_states = self.dropout(hidden_states)</span><br><span class=\"line\">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hidden_states</span><br></pre></td></tr></table></figure>\n<p>用我们的数据跑一下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的layernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的layernorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">8</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>仿射变化参数的形状和hidden size一致</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.2713</span>, -<span class=\"number\">1.2729</span>,  <span class=\"number\">0.5027</span>,  <span class=\"number\">0.4181</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.6394</span>, -<span class=\"number\">0.6608</span>, -<span class=\"number\">0.1433</span>, -<span class=\"number\">0.1043</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch ln结果:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5589</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9346</span>, -<span class=\"number\">0.1230</span>]]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>再来手动验证一下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;均值:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;标准差:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动ln结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>得到的均值和标准差如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">均值:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.8469</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.0745</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.3386</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.1364</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7003</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.2831</span>]]])</span><br><span class=\"line\">标准差:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.8578</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.3354</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6505</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"number\">0.4426</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.8448</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6816</span>]]]) </span><br></pre></td></tr></table></figure>\n<p>每个sample中的每个token，都有各自的均值和标准差，用于归一化。</p>\n<p>最终结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">手动ln结果:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5590</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9347</span>, -<span class=\"number\">0.1230</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"归一化的输入能变回原输入吗\">归一化的输入能变回原输入吗</h1>\n<p>既然这些操作是先计算均值和标准差进行归一化，再进行仿射变换，那把仿射变换的参数设置为输入的均值和标准差，是不是就可以把归一化过的数据变回跟原数据一模一样了呢？</p>\n<p>以二维情况为例，看下batchnorm是否能变回去。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义一个随机二维输入</span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;二维输入:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值和标准差</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;均值:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;标准差:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch自带的batchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 把仿射变换的缩放和平移替换为标准差和均值</span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn结果:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">二维输入:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch bn结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1821</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>确认了batchnorm是可以变回去的。</p>\n<p>再来看下layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;二维输入:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值和标准差</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;均值:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;标准差:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch自带的layernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的layernorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 把仿射变换的缩放和平移替换为标准差和均值</span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln结果:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">二维输入:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch ln结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.1918</span>, -<span class=\"number\">0.1481</span>, -<span class=\"number\">1.5251</span>,  <span class=\"number\">0.4814</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8146</span>, -<span class=\"number\">1.1451</span>,  <span class=\"number\">0.7512</span>,  <span class=\"number\">1.2086</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.9685</span>, -<span class=\"number\">0.0551</span>, -<span class=\"number\">0.6140</span>,  <span class=\"number\">1.6376</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>]])</span><br></pre></td></tr></table></figure>\n<p>发现layernorm并不能通过这种方式把归一化的输入变回原始值，因为layernorm归一化是在特征向量内进行的，所有特征值共享一个均值和方差，但是仿射变换的时候每个特征却有单独的系数。</p>\n<p>对于CV数据和NLP数据也有一样的结论。</p>\n<p>可以认为batchnorm的归一化和仿射变换是互为可逆的一对操作，而layernorm的归一化和仿射变换是在不同范围内的操作，是不可逆的。</p>\n<h1 id=\"小结\">小结</h1>\n<p>本篇从各种输入数据对batchnorm和layernorm做了手动复现。</p>\n<p>需要注意到，batchnorm、layernorm等实际都包含两步操作：①归一化②仿射变换。</p>\n<p>基本上，batchnorm可以总结为，对于特征向量中的每一个特征值，在一个\"大范围\"内进行归一化，这个\"大范围\"根据输入数据形状，可能是batch，可能是batch×序列长度，或者batch×feature\nmap大小。并且归一化和仿射变换在同一个方向上进行，因此这两个操作是互为可逆的。</p>\n<p>而layernorm是在每个特征向量内部进行归一化处理，然后在另一个方向上使用仿射变换。由于归一化和仿射变换的方向不同，因此无法通过把仿射变换，把已经归一化的数据变换为原输入数据。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】LAYERNORM\nhttps://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html<br>\n【2】BATCHNORM1D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html<br>\n【3】BATCHNORM2D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html</p>\n","length":18670,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>之前在<a href=\"http://www.linsight.cn/6a40bfa5.html\">《transformer中normalization的二三事》</a>从思路上梳理了关于常用的normalization的内容。发出之后收到了一些反馈，关于这些norm在实际使用中是怎么实现的，有一些疑问。</p>\n<p>因此本篇从实现的角度，来看下这些norm在不同的场景下，到底做了什么。</p>\n<p>代码已上传至<a href=\"https://github.com/Saicat/normalization_exp\">https://github.com/Saicat/normalization_exp</a></p>\n<h1 id=\"二维数据\">二维数据</h1>\n<p>先看下二维数据的情况下normalization是怎么做的。二维数据一般可以对应到神经网络中的全连接层，比如CNN中分类网络最后几个特征层。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># epsilon</span></span><br><span class=\"line\">eps = <span class=\"number\">1e-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义一个随机二维输入</span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;二维输入:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p>这里定义了一个3×4的矩阵，相当于batch\nsize=3，特征向量维度为4。得到的随机二维输入是</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">二维输入:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>用pytorch自带的BatchNorm1d对二维输入进行操作</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的batchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的batchnorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">1</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>注意完整的batchnorm/layernorm等，是包括①归一化和②仿射变换（缩放+平移，也就是有可训练参数这部分）两步的。在BatchNorm接口中通过参数\"affine\"来决定是否进行放射变换。如果\"affine\"为False，相当于只是在某个维度上对数据进行了归一化处理。</p>\n<p>而且pytorch中各种norm的接口初始化都把缩放系数初始化为1.0，平移系数初始化为0，相当于没有进行变换。为了把仿射变换的影响也一起对比，这里手动给缩放和平移系数都添加了一个随机数，变成如下数值</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">0.6614</span>, <span class=\"number\">0.2669</span>, <span class=\"number\">0.0617</span>, <span class=\"number\">0.6213</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.4519</span>, -<span class=\"number\">0.1661</span>, -<span class=\"number\">1.5228</span>,  <span class=\"number\">0.3817</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>这里缩放系数weight和平移系数bias的维度都是4，对应特征向量的维度。</p>\n<p>输入矩阵用官方接口batchnorm之后得到的结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0513</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>接下来手动实现batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值和标准差</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;均值:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;标准差:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动bn结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p>在dim=0这个维度上计算均值和标准差，即对整个batch内所有sample的同一个feature，进行操作，获得结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">均值:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0876</span>, -<span class=\"number\">0.6985</span>, -<span class=\"number\">0.7907</span>,  <span class=\"number\">0.5295</span>]])</span><br><span class=\"line\">标准差:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.1612</span>, <span class=\"number\">0.4971</span>, <span class=\"number\">1.0630</span>, <span class=\"number\">0.2692</span>]]) </span><br></pre></td></tr></table></figure>\n<p>均值和标准差的维度也是和特征向量的维度一致。这里计算mean和std的时候keepdim设置为True和False都可以，最后都会自动broadcast。</p>\n<p>一个要注意的点是，计算std的时候unbiased要设置为False，表明这里是对标准差的有偏估计，否则算出来的结果和torch的batchnorm接口不一致。</p>\n<p>用手动计算出来的均值和标准差对输入进行归一化，再进行放射变换，得到手动计算的batchnorm结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">手动bn结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0514</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>这里用torch.isclose接口验证官方batchnorm和手动计算的batchnorm是否相同</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>为什么没有用equal，因为发现两个结果会有一点点误差，相对误差大概在1e-5~1e-4之间，应该是因为使用的eps不同导致。</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>看下layernorm对于二维数据的操作，还是用同样的3×4的输入</p>\n<p>使用torch官方接口</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的layernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的layernorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">2</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>得到layernorm仿射变换的系数如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.3923</span>, -<span class=\"number\">0.2236</span>, -<span class=\"number\">0.3195</span>, -<span class=\"number\">1.2050</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.0445</span>, -<span class=\"number\">0.6332</span>,  <span class=\"number\">0.5731</span>,  <span class=\"number\">0.5409</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>维度依然是和特征向量的维度一致。</p>\n<p>官方layernorm的结果是这样的</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4324</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>接下来手动实现一下，和官方结果作对比。</p>\n<p>在dim=1计算均值和向量</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;均值:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;标准差:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动ln结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p>得到的均值和标准差是这样的</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">均值:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0907</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3104</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3843</span>]])</span><br><span class=\"line\">标准差:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.3691</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.9502</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.3458</span>]]) </span><br></pre></td></tr></table></figure>\n<p>对输入进行归一化和仿射变换，结果如下，和官方接口结果一致</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">手动ln结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4325</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"对比\">对比</h2>\n<p>对于二维输入，batchnorm和layernorm在做第①步归一化的时候，方向如下图</p>\n<img src=\"/b70b4a2d/bn_and_ln.png\" class title=\"bn和ln\">\n<p>batchnorm在dim=0，即batch方向操作；而layernorm在dim=1，即特征向量内部进行操作。</p>\n<p>但是无论是batchnorm还是layernorm，在做仿射变换的时候，使用的系数形状都和输入的特征向量相同，可以认为在放射变化这一步上，二者的操作是一样。</p>\n<h1 id=\"cv数据\">CV数据</h1>\n<p>再看下CV场景下的情况。</p>\n<p>CV数据形状一般为[N,C,H,W]，N为batch\nsize，C为channel即特征数，H和W分别是feature\nmap的高和宽。先定义一个CV输入数据</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义一个随机四维输入，[N,C,H,W]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">channel = <span class=\"number\">2</span></span><br><span class=\"line\">height = <span class=\"number\">2</span></span><br><span class=\"line\">width = <span class=\"number\">3</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">3</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">inputs = torch.randn(batch_size, channel, height, width)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;四维输入:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p>输入如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">四维输入:</span><br><span class=\"line\"> tensor([[[[-<span class=\"number\">0.0766</span>,  <span class=\"number\">0.3599</span>, -<span class=\"number\">0.7820</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0715</span>,  <span class=\"number\">0.6648</span>, -<span class=\"number\">0.2868</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">1.6206</span>, -<span class=\"number\">1.5967</span>,  <span class=\"number\">0.4046</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.6113</span>,  <span class=\"number\">0.7604</span>, -<span class=\"number\">0.0336</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[-<span class=\"number\">0.3448</span>,  <span class=\"number\">0.4937</span>, -<span class=\"number\">0.0776</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.8054</span>,  <span class=\"number\">0.4851</span>,  <span class=\"number\">0.2052</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">0.3384</span>,  <span class=\"number\">1.3528</span>,  <span class=\"number\">0.3736</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0134</span>,  <span class=\"number\">0.7737</span>, -<span class=\"number\">0.1092</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-1\">batchnorm</h2>\n<p>图像数据需要用BatchNorm2d，设置的特征数为channel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的batchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm2d(num_features=channel, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的batchnorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">4</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>仿射变换的参数如下，形状和channel数是一致的，和二维数据的情况一样。这里同样手动给缩放和平移系数加了个随机数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">1.6053</span>,  <span class=\"number\">0.2325</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">2.2399</span>, <span class=\"number\">0.8473</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>用torch官方batchnorm2d得到的结果是</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn结果:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3753</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4684</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5090</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>再来手动实现一下batchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动bn</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># 每个channel分别处理</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(channel):</span><br><span class=\"line\">    <span class=\"comment\"># 计算均值和标准差</span></span><br><span class=\"line\">    mean = torch.mean(inputs[:, c, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[:, c, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动bn结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>如同之前文章所解释，由于CV的卷积计算是通过二维滑动窗口在同一个输入平面上遍历所有位置，因此同一个channel下的多个值对于这个卷积和也是一种\"batch\"。</p>\n<p>相当于对于每一个特征值，计算平均和标准差的范围是N×H×W。</p>\n<img src=\"/b70b4a2d/cv_batchnorm.png\" class title=\"CV数据batchnorm\">\n<p>手动计算得到的结果如下，和官方接口一致</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">手动bn结果:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3752</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4685</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5089</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-1\">layernorm</h2>\n<p>按照<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\">torch的layernorm官方接口文档</a>，对于图像数据，layernorm是这样做的</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的layernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(</span><br><span class=\"line\">    normalized_shape=[channel, height, width], </span><br><span class=\"line\">    elementwise_affine=<span class=\"literal\">True</span></span><br><span class=\"line\">)  <span class=\"comment\"># 注意完整的layernorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">5</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>如同下面这个图所表示</p>\n<img src=\"/b70b4a2d/cv_layernorm.jpeg\" class title=\"CV数据layernorm\">\n<p>此时仿射变化系数的形状是这样的，为[channel, height, width]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[-<span class=\"number\">0.4868</span>, -<span class=\"number\">0.6038</span>, -<span class=\"number\">0.5581</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.6675</span>, -<span class=\"number\">0.1974</span>,  <span class=\"number\">1.9428</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">1.4017</span>, -<span class=\"number\">0.7626</span>,  <span class=\"number\">0.6312</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8991</span>, -<span class=\"number\">0.5578</span>,  <span class=\"number\">0.6907</span>]]], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[ <span class=\"number\">0.2225</span>, -<span class=\"number\">0.6662</span>,  <span class=\"number\">0.6846</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.5740</span>, -<span class=\"number\">0.5829</span>,  <span class=\"number\">0.7679</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.0571</span>, -<span class=\"number\">1.1894</span>, -<span class=\"number\">0.5659</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8327</span>,  <span class=\"number\">0.9014</span>,  <span class=\"number\">0.2116</span>]]], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>即每个channel内的每一个特征值，都有单独的可训练的仿射变换系数。</p>\n<p>layernorm的结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln结果:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5089</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8526</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4580</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>手动进行layernorm的归一化和仿射变换，和官方接口对比一下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动ln</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># 每个channel分别处理</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> b <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(batch_size):</span><br><span class=\"line\">    <span class=\"comment\"># 计算均值和标准差</span></span><br><span class=\"line\">    mean = torch.mean(inputs[b, :, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[b, :, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动ln结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>这里计算均值和标准差，是把所有channel内的所有特征值放在一起算的，即每个样本只有一个标量的均值和一个标量的标准差。但是仿射变换的时候就每个特征值都有自己的参数。</p>\n<p>手动计算的结果如下，和官方接口一致</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">手动ln结果:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5090</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8527</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4581</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"nlp数据\">NLP数据</h1>\n<p>再看下在NLP场景下的情况。</p>\n<p>先定义输入，N是batch size，S是sequence length，H是hidden size。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义一个随机三维输入，[N,S,H]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">seq_len = <span class=\"number\">3</span></span><br><span class=\"line\">hidden_size = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">6</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">inputs = torch.randn(batch_size, seq_len, hidden_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;三维输入:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-2\">batchnorm</h2>\n<p>用官方接口计算</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的batchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的batchnorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">7</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># # 结果</span></span><br><span class=\"line\">torch_normed = torch_bn(inputs.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>根据官方接口的描述，输入的第二维应该为特征数，第三维为序列长度，因此这里对输入做了transpose，再把结果transpose回来。</p>\n<p>结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.1468</span>,  <span class=\"number\">0.7861</span>,  <span class=\"number\">0.9468</span>, -<span class=\"number\">1.1143</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.6908</span>, -<span class=\"number\">0.8948</span>, -<span class=\"number\">0.3556</span>,  <span class=\"number\">1.2324</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch bn结果:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9949</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>可以看到batchnorm的仿射变化系数形状在各种情况下都保持和特征向量维度相同。</p>\n<p>再来手动计算验证一下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>) , keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;均值:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>), keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;标准差:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动bn结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>这里计算用于归一化均值和方差，是在dim=(0,1)范围内计算的，相当于把[N,\nS, H]的输入拉平为[N×S,\nH]的二维输入，再按二维输入的方式进行batchnorm。</p>\n<p>结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">均值:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.2151</span>,  <span class=\"number\">0.5444</span>, -<span class=\"number\">0.2633</span>, -<span class=\"number\">0.5424</span>]]])</span><br><span class=\"line\">标准差:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.7984</span>, <span class=\"number\">0.3537</span>, <span class=\"number\">0.7799</span>, <span class=\"number\">0.7986</span>]]]) </span><br><span class=\"line\"></span><br><span class=\"line\">手动bn结果:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9950</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-2\">layernorm</h2>\n<p>终于来到NLP数据的layernorm，先确认一下，huggingface中bert是这么使用layernorm的</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BertSelfOutput</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, config</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class=\"line\">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class=\"line\">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, hidden_states: torch.normTensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class=\"line\">        hidden_states = self.dense(hidden_states)</span><br><span class=\"line\">        hidden_states = self.dropout(hidden_states)</span><br><span class=\"line\">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hidden_states</span><br></pre></td></tr></table></figure>\n<p>用我们的数据跑一下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torch自带的layernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的layernorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class=\"line\"><span class=\"comment\"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">8</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln结果:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>仿射变化参数的形状和hidden size一致</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.2713</span>, -<span class=\"number\">1.2729</span>,  <span class=\"number\">0.5027</span>,  <span class=\"number\">0.4181</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.6394</span>, -<span class=\"number\">0.6608</span>, -<span class=\"number\">0.1433</span>, -<span class=\"number\">0.1043</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch ln结果:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5589</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9346</span>, -<span class=\"number\">0.1230</span>]]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>再来手动验证一下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 手动ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;均值:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;标准差:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;手动ln结果:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>得到的均值和标准差如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">均值:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.8469</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.0745</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.3386</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.1364</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7003</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.2831</span>]]])</span><br><span class=\"line\">标准差:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.8578</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.3354</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6505</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"number\">0.4426</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.8448</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6816</span>]]]) </span><br></pre></td></tr></table></figure>\n<p>每个sample中的每个token，都有各自的均值和标准差，用于归一化。</p>\n<p>最终结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">手动ln结果:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5590</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9347</span>, -<span class=\"number\">0.1230</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"归一化的输入能变回原输入吗\">归一化的输入能变回原输入吗</h1>\n<p>既然这些操作是先计算均值和标准差进行归一化，再进行仿射变换，那把仿射变换的参数设置为输入的均值和标准差，是不是就可以把归一化过的数据变回跟原数据一模一样了呢？</p>\n<p>以二维情况为例，看下batchnorm是否能变回去。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义一个随机二维输入</span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># 设置随机种子，方便复现</span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;二维输入:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值和标准差</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;均值:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;标准差:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch自带的batchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 把仿射变换的缩放和平移替换为标准差和均值</span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn结果:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">二维输入:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch bn结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1821</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>确认了batchnorm是可以变回去的。</p>\n<p>再来看下layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;二维输入:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算均值和标准差</span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;均值:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;标准差:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch自带的layernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># 注意完整的layernorm要包括仿射变换</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 把仿射变换的缩放和平移替换为标准差和均值</span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln结果:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;验证结果:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">二维输入:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch ln结果:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.1918</span>, -<span class=\"number\">0.1481</span>, -<span class=\"number\">1.5251</span>,  <span class=\"number\">0.4814</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8146</span>, -<span class=\"number\">1.1451</span>,  <span class=\"number\">0.7512</span>,  <span class=\"number\">1.2086</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.9685</span>, -<span class=\"number\">0.0551</span>, -<span class=\"number\">0.6140</span>,  <span class=\"number\">1.6376</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class=\"line\">验证结果:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>]])</span><br></pre></td></tr></table></figure>\n<p>发现layernorm并不能通过这种方式把归一化的输入变回原始值，因为layernorm归一化是在特征向量内进行的，所有特征值共享一个均值和方差，但是仿射变换的时候每个特征却有单独的系数。</p>\n<p>对于CV数据和NLP数据也有一样的结论。</p>\n<p>可以认为batchnorm的归一化和仿射变换是互为可逆的一对操作，而layernorm的归一化和仿射变换是在不同范围内的操作，是不可逆的。</p>\n<h1 id=\"小结\">小结</h1>\n<p>本篇从各种输入数据对batchnorm和layernorm做了手动复现。</p>\n<p>需要注意到，batchnorm、layernorm等实际都包含两步操作：①归一化②仿射变换。</p>\n<p>基本上，batchnorm可以总结为，对于特征向量中的每一个特征值，在一个\"大范围\"内进行归一化，这个\"大范围\"根据输入数据形状，可能是batch，可能是batch×序列长度，或者batch×feature\nmap大小。并且归一化和仿射变换在同一个方向上进行，因此这两个操作是互为可逆的。</p>\n<p>而layernorm是在每个特征向量内部进行归一化处理，然后在另一个方向上使用仿射变换。由于归一化和仿射变换的方向不同，因此无法通过把仿射变换，把已经归一化的数据变换为原输入数据。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】LAYERNORM\nhttps://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html<br>\n【2】BATCHNORM1D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html<br>\n【3】BATCHNORM2D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html</p>\n"},{"title":"大模型算法题(3)","abbrlink":"1736008","date":"2024-04-05T06:08:31.000Z","_content":"\n![](/images/cover.png)  \n\n【往期文章】\n\n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错漏，欢迎指正~\n\n***  \n\n# 1.旋转位置编码RoPE有什么优缺点？  \n\n优点：RoPE以绝对位置编码的方式实现了相对位置编码，使得能够在不破坏注意力形式的情况下，以“加性编码”的方式让模型学习相对位置。①相比其他相对位置编码来说，实现简单，计算量少。②可以应用于线性注意力。③RoPE具有远程衰减的特性，使得每个位置天然能够更关注到附近的信息。  \n\n缺点：RoPE相比训练式的绝对位置编码具有一定的外推能力，如可以在2k数据长度训练的模型进行略长于2k的推理。但是相比于Alibi等位置编码，其直接外推能力并不算特别好，需要通过线性插值、NTK插值、YaRN等方式来优化外推能力。  \n\n# 2.batchnorm中的momentum怎么影响训练效果  \n\nbatchnorm在训练时计算每个batch内的均值和方差用于normalization，同时统计一个全局均值和方差用于推理。全局均值和方差计算公式为：  \n\nmoving_mean = momentum × moving_mean + (1.0 − momentum) × mean  \n\nmoving_var = momentum × moving_var + (1.0 − momentum) × var  \n\n小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动；如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。如果batch size比较小，每个mini batch和全局差异较大，就不应该用太大的momentum。  \n\n# 3.多头注意力相比单头有什么好处？  \n\n多头注意力使用多个维度较低的子空间分别进行学习。  \n\n一般来说，相比单头的情况，多个头能够分别关注到不同的特征，增强了表达能力。多个头中，会有部分头能够学习到更高级的特征，并减少注意力权重对角线值过大的情况。  \n\n比如部分头关注语法信息，部分头关注知识内容，部分头关注近距离文本，部分头关注远距离文本，这样减少信息缺失，提升模型容量。  \n\n另外虽然多头注意力的整体计算量比单头要大一点，但是并行度也高一些。  \n\n# 4.kv cache为什么能加速推理？  \n\n对于GPT类模型，使用的是单向注意力，每个位置只能看到自己和前面的内容。  \n\n在进行自回归解码的时候，新生成的token会加入序列，一起作为下一次解码的输入。  \n\n由于单向注意力的存在，新加入的token并不会影响前面序列的计算，因此可以把已经计算过的每层的kv值保存起来，这样就节省了和本次生成无关的计算量。  \n\n通过把kv值存储在速度远快于显存的L2缓存中，可以大大减少kv值的保存和读取，这样就极大加快了模型推理的速度。  \n\n# 5.ReLU有什么优缺点？  \n\n优点：（1）计算快，前向只需要进行max(0, x)计算，后向则是直接透传；（2）有激活值的时候，梯度恒定为1，不会爆炸/消失；  \n\n缺点：（1）均值不为0，分布产生偏移（2）输入值小于0时，梯度再也无法回传过来，导致神经元坏死。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  ","source":"_posts/cs/nlp/2024/04/大模型算法题-3.md","raw":"---\ntitle: 大模型算法题(3)\ntags:\n  - NLP\n  - LLM\n  - 算法题\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: '1736008'\ndate: 2024-04-05 14:08:31\n---\n\n![](/images/cover.png)  \n\n【往期文章】\n\n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错漏，欢迎指正~\n\n***  \n\n# 1.旋转位置编码RoPE有什么优缺点？  \n\n优点：RoPE以绝对位置编码的方式实现了相对位置编码，使得能够在不破坏注意力形式的情况下，以“加性编码”的方式让模型学习相对位置。①相比其他相对位置编码来说，实现简单，计算量少。②可以应用于线性注意力。③RoPE具有远程衰减的特性，使得每个位置天然能够更关注到附近的信息。  \n\n缺点：RoPE相比训练式的绝对位置编码具有一定的外推能力，如可以在2k数据长度训练的模型进行略长于2k的推理。但是相比于Alibi等位置编码，其直接外推能力并不算特别好，需要通过线性插值、NTK插值、YaRN等方式来优化外推能力。  \n\n# 2.batchnorm中的momentum怎么影响训练效果  \n\nbatchnorm在训练时计算每个batch内的均值和方差用于normalization，同时统计一个全局均值和方差用于推理。全局均值和方差计算公式为：  \n\nmoving_mean = momentum × moving_mean + (1.0 − momentum) × mean  \n\nmoving_var = momentum × moving_var + (1.0 − momentum) × var  \n\n小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动；如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。如果batch size比较小，每个mini batch和全局差异较大，就不应该用太大的momentum。  \n\n# 3.多头注意力相比单头有什么好处？  \n\n多头注意力使用多个维度较低的子空间分别进行学习。  \n\n一般来说，相比单头的情况，多个头能够分别关注到不同的特征，增强了表达能力。多个头中，会有部分头能够学习到更高级的特征，并减少注意力权重对角线值过大的情况。  \n\n比如部分头关注语法信息，部分头关注知识内容，部分头关注近距离文本，部分头关注远距离文本，这样减少信息缺失，提升模型容量。  \n\n另外虽然多头注意力的整体计算量比单头要大一点，但是并行度也高一些。  \n\n# 4.kv cache为什么能加速推理？  \n\n对于GPT类模型，使用的是单向注意力，每个位置只能看到自己和前面的内容。  \n\n在进行自回归解码的时候，新生成的token会加入序列，一起作为下一次解码的输入。  \n\n由于单向注意力的存在，新加入的token并不会影响前面序列的计算，因此可以把已经计算过的每层的kv值保存起来，这样就节省了和本次生成无关的计算量。  \n\n通过把kv值存储在速度远快于显存的L2缓存中，可以大大减少kv值的保存和读取，这样就极大加快了模型推理的速度。  \n\n# 5.ReLU有什么优缺点？  \n\n优点：（1）计算快，前向只需要进行max(0, x)计算，后向则是直接透传；（2）有激活值的时候，梯度恒定为1，不会爆炸/消失；  \n\n缺点：（1）均值不为0，分布产生偏移（2）输入值小于0时，梯度再也无法回传过来，导致神经元坏死。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  ","slug":"cs/nlp/2024/04/大模型算法题-3","published":1,"updated":"2024-05-10T06:51:01.150Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9t0008314k1t7sfln7","content":"<p><img src=\"/images/cover.png\"></p>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错漏，欢迎指正</sub></p>\n<hr>\n<h1 id=\"旋转位置编码rope有什么优缺点\">1.旋转位置编码RoPE有什么优缺点？</h1>\n<p>优点：RoPE以绝对位置编码的方式实现了相对位置编码，使得能够在不破坏注意力形式的情况下，以“加性编码”的方式让模型学习相对位置。①相比其他相对位置编码来说，实现简单，计算量少。②可以应用于线性注意力。③RoPE具有远程衰减的特性，使得每个位置天然能够更关注到附近的信息。</p>\n<p>缺点：RoPE相比训练式的绝对位置编码具有一定的外推能力，如可以在2k数据长度训练的模型进行略长于2k的推理。但是相比于Alibi等位置编码，其直接外推能力并不算特别好，需要通过线性插值、NTK插值、YaRN等方式来优化外推能力。</p>\n<h1 id=\"batchnorm中的momentum怎么影响训练效果\">2.batchnorm中的momentum怎么影响训练效果</h1>\n<p>batchnorm在训练时计算每个batch内的均值和方差用于normalization，同时统计一个全局均值和方差用于推理。全局均值和方差计算公式为：</p>\n<p>moving_mean = momentum × moving_mean + (1.0 − momentum) × mean</p>\n<p>moving_var = momentum × moving_var + (1.0 − momentum) × var</p>\n<p>小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动；如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。如果batch\nsize比较小，每个mini batch和全局差异较大，就不应该用太大的momentum。</p>\n<h1 id=\"多头注意力相比单头有什么好处\">3.多头注意力相比单头有什么好处？</h1>\n<p>多头注意力使用多个维度较低的子空间分别进行学习。</p>\n<p>一般来说，相比单头的情况，多个头能够分别关注到不同的特征，增强了表达能力。多个头中，会有部分头能够学习到更高级的特征，并减少注意力权重对角线值过大的情况。</p>\n<p>比如部分头关注语法信息，部分头关注知识内容，部分头关注近距离文本，部分头关注远距离文本，这样减少信息缺失，提升模型容量。</p>\n<p>另外虽然多头注意力的整体计算量比单头要大一点，但是并行度也高一些。</p>\n<h1 id=\"kv-cache为什么能加速推理\">4.kv cache为什么能加速推理？</h1>\n<p>对于GPT类模型，使用的是单向注意力，每个位置只能看到自己和前面的内容。</p>\n<p>在进行自回归解码的时候，新生成的token会加入序列，一起作为下一次解码的输入。</p>\n<p>由于单向注意力的存在，新加入的token并不会影响前面序列的计算，因此可以把已经计算过的每层的kv值保存起来，这样就节省了和本次生成无关的计算量。</p>\n<p>通过把kv值存储在速度远快于显存的L2缓存中，可以大大减少kv值的保存和读取，这样就极大加快了模型推理的速度。</p>\n<h1 id=\"relu有什么优缺点\">5.ReLU有什么优缺点？</h1>\n<p>优点：（1）计算快，前向只需要进行max(0,\nx)计算，后向则是直接透传；（2）有激活值的时候，梯度恒定为1，不会爆炸/消失；</p>\n<p>缺点：（1）均值不为0，分布产生偏移（2）输入值小于0时，梯度再也无法回传过来，导致神经元坏死。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n","length":1653,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错漏，欢迎指正</sub></p>\n<hr>\n<h1 id=\"旋转位置编码rope有什么优缺点\">1.旋转位置编码RoPE有什么优缺点？</h1>\n<p>优点：RoPE以绝对位置编码的方式实现了相对位置编码，使得能够在不破坏注意力形式的情况下，以“加性编码”的方式让模型学习相对位置。①相比其他相对位置编码来说，实现简单，计算量少。②可以应用于线性注意力。③RoPE具有远程衰减的特性，使得每个位置天然能够更关注到附近的信息。</p>\n<p>缺点：RoPE相比训练式的绝对位置编码具有一定的外推能力，如可以在2k数据长度训练的模型进行略长于2k的推理。但是相比于Alibi等位置编码，其直接外推能力并不算特别好，需要通过线性插值、NTK插值、YaRN等方式来优化外推能力。</p>\n<h1 id=\"batchnorm中的momentum怎么影响训练效果\">2.batchnorm中的momentum怎么影响训练效果</h1>\n<p>batchnorm在训练时计算每个batch内的均值和方差用于normalization，同时统计一个全局均值和方差用于推理。全局均值和方差计算公式为：</p>\n<p>moving_mean = momentum × moving_mean + (1.0 − momentum) × mean</p>\n<p>moving_var = momentum × moving_var + (1.0 − momentum) × var</p>\n<p>小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动；如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。如果batch\nsize比较小，每个mini batch和全局差异较大，就不应该用太大的momentum。</p>\n<h1 id=\"多头注意力相比单头有什么好处\">3.多头注意力相比单头有什么好处？</h1>\n<p>多头注意力使用多个维度较低的子空间分别进行学习。</p>\n<p>一般来说，相比单头的情况，多个头能够分别关注到不同的特征，增强了表达能力。多个头中，会有部分头能够学习到更高级的特征，并减少注意力权重对角线值过大的情况。</p>\n<p>比如部分头关注语法信息，部分头关注知识内容，部分头关注近距离文本，部分头关注远距离文本，这样减少信息缺失，提升模型容量。</p>\n<p>另外虽然多头注意力的整体计算量比单头要大一点，但是并行度也高一些。</p>\n<h1 id=\"kv-cache为什么能加速推理\">4.kv cache为什么能加速推理？</h1>\n<p>对于GPT类模型，使用的是单向注意力，每个位置只能看到自己和前面的内容。</p>\n<p>在进行自回归解码的时候，新生成的token会加入序列，一起作为下一次解码的输入。</p>\n<p>由于单向注意力的存在，新加入的token并不会影响前面序列的计算，因此可以把已经计算过的每层的kv值保存起来，这样就节省了和本次生成无关的计算量。</p>\n<p>通过把kv值存储在速度远快于显存的L2缓存中，可以大大减少kv值的保存和读取，这样就极大加快了模型推理的速度。</p>\n<h1 id=\"relu有什么优缺点\">5.ReLU有什么优缺点？</h1>\n<p>优点：（1）计算快，前向只需要进行max(0,\nx)计算，后向则是直接透传；（2）有激活值的时候，梯度恒定为1，不会爆炸/消失；</p>\n<p>缺点：（1）均值不为0，分布产生偏移（2）输入值小于0时，梯度再也无法回传过来，导致神经元坏死。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n"},{"title":"大模型算法题(4)","abbrlink":"1736008","date":"2024-04-20T08:56:45.000Z","_content":"\n![](/images/cover.png)  \n\n【往期文章】\n\n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错漏，欢迎指正~\n\n***  \n\n# 1.为什么Transformer用layernorm而不是batchnorm  \n\n首先，NLP数据中由于每条样本可能不一样长，会使用padding，如果对padding部分进行normalization，对效果有负面影响。直观来说，batchnorm会对同一个特征以batch为组进行归一化，而对于文本数据，同一个位置的token很可能是没有关联的两个token，对这样一组数据进行归一化没有什么实际意义。《PowerNorm: Rethinking Batch Normalization in Transformers》论文的实验也表明，在NLP数据使用batchnorm，均值和方差相对layernorm会更加震荡，因此效果欠佳。  \n\n# 2.transformer中，encdoer和decoder是怎么进行交互的？  \n\ndecoder部分的输入，在每层中，先进行一次self-attention；之后用encoder的输出作为attention计算中的K、V，decoder的输入作为Q，进行cross-attention。  \n\n{% asset_img transformer.png transformer %}  \n\n# 3.PyTorch中，Tensor的view()和reshape()两个方法有什么区别？  \n\n1.功能上：view()与reshape()方法都可以用来改变tensor的形状，但是使用条件不同，view()能做的是reshape的子集。  \n\n2.view()方法需要tensor满足连续性，操作后返回一个引用，返回值是视图，没有改变原储存空间的值，多个视图共享同一个物理储存空间的内容。  \n\n3.reshape()方法不需要tensor一定满足连续性。如果tensor不满足连续性的要求，则会使用新的储存空间并返回。如果满足连续性需求，则功能和view()一致。  \n\n4.连续性：比如一个二维张量，如果按行优先展开成一维的结果，和物理储存顺序是一致的，就是连续的。可以用is_contiguous()来判断一个张量是否连续，如果不连续，可以用contiguous()得到一份新空间中的连续副本。  \n\n# 4.RLHF中，PPO需要哪几个模型，分别是什么作用？  \n\n一般来说，PPO需要使用4个模型。  \n\n1.Actor模型：由SFT初始化，就是进行强化学习的主模型，是我们想要最终获得的模型；它不断产生action并被Critic模型所评价，计算loss进行训练。  \n\n2.Reference模型：一般也是从SFT模型初始化，RLHF中Reference模型并不更新参数，只是作为Actor模型的参考使用；通过约束Actor模型和Reference模型的KL penalty等，可以防止Actor模型被训得跑得太偏。  \n\n3.Reward模型：提前训练好的，对SFT模型进行打分的模型，RLHF中参数是冻结的。  \n\n4.Critic模型：一般由Reward模型进行初始化，参数可训练，用于预测Actor模型生成的token的收益。  \n\n# 5.GPT类模型训练过程中，消耗显存的主要有哪些部分？分别是多少？哪部分占用最多？假设模型有L层，词表大小为V，hidden size为H，batch size为B，训练窗口长度为S，使用Adam优化器混合精度训练（需要存一阶和二阶动量），注意力头数为N。  \n\n训练过程中，显存消耗主要有模型参数、梯度、optimizer状态值和中间激活值。  \n\n1.模型参数Φ：词表部分VH，每层参数12H^2+13H，总共有Φ=VH+L(12H^2+13H)，如果是半精度就是2Φ  \n\n2.梯度：每个参数对应有一个梯度，总量为Φ，如果是半精度就是2Φ  \n\n3.optimizer状态值：每个参数有一个对应梯度，每个参数又对应优化器一个一阶动量和二阶动量。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有(Φ+Φ)*2+(Φ+Φ+2Φ)*4=20Φ，除去参数和梯度，优化器占部分16Φ  \n\n4.激活值：保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。总共是34BSH+5BNS^2，如果都是半精度，就乘以2。  \n\n模型参数、梯度和优化器状态和输入长度无关，是固定值，而激活值随着长度增加，以平方速度增长。\n以GPT3（175B）为例，H=12288，L=96，N=96。模型参数量显存越为350G。以B=1计算，如果S=1024，激活值约为90G；如果S=8192，激活值约为3420G。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  ","source":"_posts/cs/nlp/2024/04/大模型算法题-4.md","raw":"---\ntitle: 大模型算法题(4)\nabbrlink: '1736008'\ndate: 2024-04-20 16:56:45\ntags:\n  - NLP\n  - LLM\n  - 算法题\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n![](/images/cover.png)  \n\n【往期文章】\n\n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错漏，欢迎指正~\n\n***  \n\n# 1.为什么Transformer用layernorm而不是batchnorm  \n\n首先，NLP数据中由于每条样本可能不一样长，会使用padding，如果对padding部分进行normalization，对效果有负面影响。直观来说，batchnorm会对同一个特征以batch为组进行归一化，而对于文本数据，同一个位置的token很可能是没有关联的两个token，对这样一组数据进行归一化没有什么实际意义。《PowerNorm: Rethinking Batch Normalization in Transformers》论文的实验也表明，在NLP数据使用batchnorm，均值和方差相对layernorm会更加震荡，因此效果欠佳。  \n\n# 2.transformer中，encdoer和decoder是怎么进行交互的？  \n\ndecoder部分的输入，在每层中，先进行一次self-attention；之后用encoder的输出作为attention计算中的K、V，decoder的输入作为Q，进行cross-attention。  \n\n{% asset_img transformer.png transformer %}  \n\n# 3.PyTorch中，Tensor的view()和reshape()两个方法有什么区别？  \n\n1.功能上：view()与reshape()方法都可以用来改变tensor的形状，但是使用条件不同，view()能做的是reshape的子集。  \n\n2.view()方法需要tensor满足连续性，操作后返回一个引用，返回值是视图，没有改变原储存空间的值，多个视图共享同一个物理储存空间的内容。  \n\n3.reshape()方法不需要tensor一定满足连续性。如果tensor不满足连续性的要求，则会使用新的储存空间并返回。如果满足连续性需求，则功能和view()一致。  \n\n4.连续性：比如一个二维张量，如果按行优先展开成一维的结果，和物理储存顺序是一致的，就是连续的。可以用is_contiguous()来判断一个张量是否连续，如果不连续，可以用contiguous()得到一份新空间中的连续副本。  \n\n# 4.RLHF中，PPO需要哪几个模型，分别是什么作用？  \n\n一般来说，PPO需要使用4个模型。  \n\n1.Actor模型：由SFT初始化，就是进行强化学习的主模型，是我们想要最终获得的模型；它不断产生action并被Critic模型所评价，计算loss进行训练。  \n\n2.Reference模型：一般也是从SFT模型初始化，RLHF中Reference模型并不更新参数，只是作为Actor模型的参考使用；通过约束Actor模型和Reference模型的KL penalty等，可以防止Actor模型被训得跑得太偏。  \n\n3.Reward模型：提前训练好的，对SFT模型进行打分的模型，RLHF中参数是冻结的。  \n\n4.Critic模型：一般由Reward模型进行初始化，参数可训练，用于预测Actor模型生成的token的收益。  \n\n# 5.GPT类模型训练过程中，消耗显存的主要有哪些部分？分别是多少？哪部分占用最多？假设模型有L层，词表大小为V，hidden size为H，batch size为B，训练窗口长度为S，使用Adam优化器混合精度训练（需要存一阶和二阶动量），注意力头数为N。  \n\n训练过程中，显存消耗主要有模型参数、梯度、optimizer状态值和中间激活值。  \n\n1.模型参数Φ：词表部分VH，每层参数12H^2+13H，总共有Φ=VH+L(12H^2+13H)，如果是半精度就是2Φ  \n\n2.梯度：每个参数对应有一个梯度，总量为Φ，如果是半精度就是2Φ  \n\n3.optimizer状态值：每个参数有一个对应梯度，每个参数又对应优化器一个一阶动量和二阶动量。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有(Φ+Φ)*2+(Φ+Φ+2Φ)*4=20Φ，除去参数和梯度，优化器占部分16Φ  \n\n4.激活值：保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。总共是34BSH+5BNS^2，如果都是半精度，就乘以2。  \n\n模型参数、梯度和优化器状态和输入长度无关，是固定值，而激活值随着长度增加，以平方速度增长。\n以GPT3（175B）为例，H=12288，L=96，N=96。模型参数量显存越为350G。以B=1计算，如果S=1024，激活值约为90G；如果S=8192，激活值约为3420G。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  ","slug":"cs/nlp/2024/04/大模型算法题-4","published":1,"updated":"2024-05-10T06:50:46.826Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9t0009314k1qrlaw74","content":"<p><img src=\"/images/cover.png\"></p>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错漏，欢迎指正</sub></p>\n<hr>\n<h1 id=\"为什么transformer用layernorm而不是batchnorm\">1.为什么Transformer用layernorm而不是batchnorm</h1>\n<p>首先，NLP数据中由于每条样本可能不一样长，会使用padding，如果对padding部分进行normalization，对效果有负面影响。直观来说，batchnorm会对同一个特征以batch为组进行归一化，而对于文本数据，同一个位置的token很可能是没有关联的两个token，对这样一组数据进行归一化没有什么实际意义。《PowerNorm:\nRethinking Batch Normalization in\nTransformers》论文的实验也表明，在NLP数据使用batchnorm，均值和方差相对layernorm会更加震荡，因此效果欠佳。</p>\n<h1 id=\"transformer中encdoer和decoder是怎么进行交互的\">2.transformer中，encdoer和decoder是怎么进行交互的？</h1>\n<p>decoder部分的输入，在每层中，先进行一次self-attention；之后用encoder的输出作为attention计算中的K、V，decoder的输入作为Q，进行cross-attention。</p>\n<img src=\"/1736008/transformer.png\" class title=\"transformer\">\n<h1 id=\"pytorch中tensor的view和reshape两个方法有什么区别\">3.PyTorch中，Tensor的view()和reshape()两个方法有什么区别？</h1>\n<p>1.功能上：view()与reshape()方法都可以用来改变tensor的形状，但是使用条件不同，view()能做的是reshape的子集。</p>\n<p>2.view()方法需要tensor满足连续性，操作后返回一个引用，返回值是视图，没有改变原储存空间的值，多个视图共享同一个物理储存空间的内容。</p>\n<p>3.reshape()方法不需要tensor一定满足连续性。如果tensor不满足连续性的要求，则会使用新的储存空间并返回。如果满足连续性需求，则功能和view()一致。</p>\n<p>4.连续性：比如一个二维张量，如果按行优先展开成一维的结果，和物理储存顺序是一致的，就是连续的。可以用is_contiguous()来判断一个张量是否连续，如果不连续，可以用contiguous()得到一份新空间中的连续副本。</p>\n<h1 id=\"rlhf中ppo需要哪几个模型分别是什么作用\">4.RLHF中，PPO需要哪几个模型，分别是什么作用？</h1>\n<p>一般来说，PPO需要使用4个模型。</p>\n<p>1.Actor模型：由SFT初始化，就是进行强化学习的主模型，是我们想要最终获得的模型；它不断产生action并被Critic模型所评价，计算loss进行训练。</p>\n<p>2.Reference模型：一般也是从SFT模型初始化，RLHF中Reference模型并不更新参数，只是作为Actor模型的参考使用；通过约束Actor模型和Reference模型的KL\npenalty等，可以防止Actor模型被训得跑得太偏。</p>\n<p>3.Reward模型：提前训练好的，对SFT模型进行打分的模型，RLHF中参数是冻结的。</p>\n<p>4.Critic模型：一般由Reward模型进行初始化，参数可训练，用于预测Actor模型生成的token的收益。</p>\n<h1 id=\"gpt类模型训练过程中消耗显存的主要有哪些部分分别是多少哪部分占用最多假设模型有l层词表大小为vhidden-size为hbatch-size为b训练窗口长度为s使用adam优化器混合精度训练需要存一阶和二阶动量注意力头数为n\">5.GPT类模型训练过程中，消耗显存的主要有哪些部分？分别是多少？哪部分占用最多？假设模型有L层，词表大小为V，hidden\nsize为H，batch\nsize为B，训练窗口长度为S，使用Adam优化器混合精度训练（需要存一阶和二阶动量），注意力头数为N。</h1>\n<p>训练过程中，显存消耗主要有模型参数、梯度、optimizer状态值和中间激活值。</p>\n<p>1.模型参数Φ：词表部分VH，每层参数12H<sup>2+13H，总共有Φ=VH+L(12H</sup>2+13H)，如果是半精度就是2Φ</p>\n<p>2.梯度：每个参数对应有一个梯度，总量为Φ，如果是半精度就是2Φ</p>\n<p>3.optimizer状态值：每个参数有一个对应梯度，每个参数又对应优化器一个一阶动量和二阶动量。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有(Φ+Φ)<em>2+(Φ+Φ+2Φ)</em>4=20Φ，除去参数和梯度，优化器占部分16Φ</p>\n<p>4.激活值：保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。总共是34BSH+5BNS^2，如果都是半精度，就乘以2。</p>\n<p>模型参数、梯度和优化器状态和输入长度无关，是固定值，而激活值随着长度增加，以平方速度增长。\n以GPT3（175B）为例，H=12288，L=96，N=96。模型参数量显存越为350G。以B=1计算，如果S=1024，激活值约为90G；如果S=8192，激活值约为3420G。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a></p>\n","length":2305,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错漏，欢迎指正</sub></p>\n<hr>\n<h1 id=\"为什么transformer用layernorm而不是batchnorm\">1.为什么Transformer用layernorm而不是batchnorm</h1>\n<p>首先，NLP数据中由于每条样本可能不一样长，会使用padding，如果对padding部分进行normalization，对效果有负面影响。直观来说，batchnorm会对同一个特征以batch为组进行归一化，而对于文本数据，同一个位置的token很可能是没有关联的两个token，对这样一组数据进行归一化没有什么实际意义。《PowerNorm:\nRethinking Batch Normalization in\nTransformers》论文的实验也表明，在NLP数据使用batchnorm，均值和方差相对layernorm会更加震荡，因此效果欠佳。</p>\n<h1 id=\"transformer中encdoer和decoder是怎么进行交互的\">2.transformer中，encdoer和decoder是怎么进行交互的？</h1>\n<p>decoder部分的输入，在每层中，先进行一次self-attention；之后用encoder的输出作为attention计算中的K、V，decoder的输入作为Q，进行cross-attention。</p>\n<img src=\"/1736008/transformer.png\" class title=\"transformer\">\n<h1 id=\"pytorch中tensor的view和reshape两个方法有什么区别\">3.PyTorch中，Tensor的view()和reshape()两个方法有什么区别？</h1>\n<p>1.功能上：view()与reshape()方法都可以用来改变tensor的形状，但是使用条件不同，view()能做的是reshape的子集。</p>\n<p>2.view()方法需要tensor满足连续性，操作后返回一个引用，返回值是视图，没有改变原储存空间的值，多个视图共享同一个物理储存空间的内容。</p>\n<p>3.reshape()方法不需要tensor一定满足连续性。如果tensor不满足连续性的要求，则会使用新的储存空间并返回。如果满足连续性需求，则功能和view()一致。</p>\n<p>4.连续性：比如一个二维张量，如果按行优先展开成一维的结果，和物理储存顺序是一致的，就是连续的。可以用is_contiguous()来判断一个张量是否连续，如果不连续，可以用contiguous()得到一份新空间中的连续副本。</p>\n<h1 id=\"rlhf中ppo需要哪几个模型分别是什么作用\">4.RLHF中，PPO需要哪几个模型，分别是什么作用？</h1>\n<p>一般来说，PPO需要使用4个模型。</p>\n<p>1.Actor模型：由SFT初始化，就是进行强化学习的主模型，是我们想要最终获得的模型；它不断产生action并被Critic模型所评价，计算loss进行训练。</p>\n<p>2.Reference模型：一般也是从SFT模型初始化，RLHF中Reference模型并不更新参数，只是作为Actor模型的参考使用；通过约束Actor模型和Reference模型的KL\npenalty等，可以防止Actor模型被训得跑得太偏。</p>\n<p>3.Reward模型：提前训练好的，对SFT模型进行打分的模型，RLHF中参数是冻结的。</p>\n<p>4.Critic模型：一般由Reward模型进行初始化，参数可训练，用于预测Actor模型生成的token的收益。</p>\n<h1 id=\"gpt类模型训练过程中消耗显存的主要有哪些部分分别是多少哪部分占用最多假设模型有l层词表大小为vhidden-size为hbatch-size为b训练窗口长度为s使用adam优化器混合精度训练需要存一阶和二阶动量注意力头数为n\">5.GPT类模型训练过程中，消耗显存的主要有哪些部分？分别是多少？哪部分占用最多？假设模型有L层，词表大小为V，hidden\nsize为H，batch\nsize为B，训练窗口长度为S，使用Adam优化器混合精度训练（需要存一阶和二阶动量），注意力头数为N。</h1>\n<p>训练过程中，显存消耗主要有模型参数、梯度、optimizer状态值和中间激活值。</p>\n<p>1.模型参数Φ：词表部分VH，每层参数12H<sup>2+13H，总共有Φ=VH+L(12H</sup>2+13H)，如果是半精度就是2Φ</p>\n<p>2.梯度：每个参数对应有一个梯度，总量为Φ，如果是半精度就是2Φ</p>\n<p>3.optimizer状态值：每个参数有一个对应梯度，每个参数又对应优化器一个一阶动量和二阶动量。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有(Φ+Φ)<em>2+(Φ+Φ+2Φ)</em>4=20Φ，除去参数和梯度，优化器占部分16Φ</p>\n<p>4.激活值：保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。总共是34BSH+5BNS^2，如果都是半精度，就乘以2。</p>\n<p>模型参数、梯度和优化器状态和输入长度无关，是固定值，而激活值随着长度增加，以平方速度增长。\n以GPT3（175B）为例，H=12288，L=96，N=96。模型参数量显存越为350G。以B=1计算，如果S=1024，激活值约为90G；如果S=8192，激活值约为3420G。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a></p>\n"},{"title":"MiniCPM","abbrlink":"376db710","date":"2024-06-18T13:51:22.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\nMiniCPM是面壁智能和清华开源的模型，MiniCPM开源系列包括非embedding参数为1.2B和2.4B两个规模的模型，以及对应的MiniCPM-DPO，MiniCPM-MoE和MiniCPM-128K模型。  \n\n简单梳理一下MiniCPM提到的一些内容。  \n\n# 背景  \n\n大模型的训练成本很高，而且很多机制还没搞清楚，训出来的大规模模型在很多设备上也跑不起来，因此现在有不少机构对小一点的模型，即SLM，进行更全面的探索，比如Phi系列、TinyLlama、MobileLLM和Gemma等。  \n\nMiniCPM也是对SLM的一次探索，从中得到的经验也可以推广到更大的模型上。  \n\n# 风洞实验  \n\n为了找到好的模型参数和训练参数，MiniCPM做了很多“风洞实验”（Model Wind Tunnel Experiments）。  \n\n这些风洞实验主要包括三个部分：（1）搜索模型结构的超参（2）探索batch size的scaling（3）寻找最佳的learning rate。\n\n后续风洞实验所用的模型具体参数如下  \n\n{% asset_img exp_model.png 模型结构 %}  \n\n## 模型超参  \n\n预训练资源消耗很大，即使是SLM也不可能把所有参数的排列组合都搜索一遍。  \n\n这里参考Tensor Program的做法（《Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer》和《Tensor programs vi: Feature learning in infinite-depth neural networks》），分别对模型的宽度和深度进行了搜索。  \n\n搜索所用的操作如下表所示，这里没有应用attention softmax的scaling技术。  \n\n{% asset_img param_search_2.png 超参搜索 %}  \n\n关于超参搜索的一些细节：  \n- 用Maximal Update Parametrization的方法进行了调参。  \n- 在一系列预定义的参数空间上进行贝叶斯搜索，所用模型参数为在N=0.009B。发现使用规模为10N和20N大小的数据集进行超参数优化时，超参的有效性表现出了一致性。因此调参的过程就使用D=10N=0.09B个token来进行实验了。  \n- 应用了QK-Norm（《Querykey normalization for transformers》）和independent weight decay（《Decoupled weight decay regularization》）之后，发现模型对learning rate的敏感性显著降低。不过在找到最佳learning rate之后，后面的训练就不用再调整参数了，因此后面的实验就没有继续使用QK-Norm和independent weight decay。  \n\n最终从下图展示的参数搜索，确定了最佳的hyper-parameters为：  \n- scale depth = 1.4  \n- scale emb = 12  \n- init std = 0.1  \n- lr = 0.01  \n\n{% asset_img param_search.png 超参搜索 %}  \n\n## Optimal Batch Size  \n\nbatch size决定了模型收敛速度与计算资源消耗之间的平衡。  \n\n如果batch size太大，会导致很大的数据和计算成本（才能跑到足够的update次数，让模型收敛）。如果batch size太小，将会有大量的update step，并且相比大一些的batch size，loss的减小很有限，训练效率太低。  \n\n这里参考OpenAI的《Scaling laws for neural language models》的方法来寻找最佳的batch size，并做了一些改动。  \n\n《Scaling laws for neural language models》研究的是loss function和token数之间的关系。他们假设了更多的step=更多的训练时间。在这个假设下，OpenAI定义了一个critical batch size，在不消耗过多的step或者token的情况下，能达到一定的loss水平。  \n\n这在无限GPU资源下是合理的。由于GPU资源是无限的，增加batch size不会增加单个step的耗时，但会减少总step数，因而提高了效率。但是实际上我们并没有无限GPU资源，因此将batch size增大相当于增加的每个step的时间，所以实际上通过增加batch size来减小step数，对总训练时间的影响并不大。  \n\n因此MiniCPM放弃了“not consuming too many steps”的目标，转而追求“minimizing the token quantity to achieve the lowest loss”。  \n\n关于optimal batch size与loss之间关系的估计，类似于“先有鸡还是先有蛋”的悖论，因为暂时没法完全搞清楚这二者之间的决定关系。目前的做法是，对于给定的模型大小，通常会有一个初步估计的achievable loss，这是由先前的初步实验得出的经验估计。  \n\n而optimal batch size和optimal learning rate很可能并不是独立的。为了克服这种相关性，MiniCPM首先对learning rate进行了初步研究，然后选择一个最优learning rate来进行batch size实验，并使用batch size缩放再次进行learning rate调整。这有点像Coordinate Descent optimization method。  \n\n细节上，MiniCPM分别对0.009B、0.03B和0.17B的模型进行了实验。每个模型大小都在6种不同的batch size上进行训练，使用了global learning rate=0.01和cosine learning rate scheduler。在C4数据集上，optimal batch size与loss的趋势如下图红线  \n\n{% asset_img batch_size.png 超参搜索 %}  \n\n这三条红线在log空间中很好连成一条直线，如下图。  \n\n{% asset_img batch_size_2.png 超参搜索 %}  \n\n这里就得到了在C4数据集上的训练loss和optimal batch size的关系。  \n\n$$bs=\\frac{1.21\\times10^9}{L^{6.24}}$$  \n\n## Optimal Learning Rate  \n\n由于使用了Tensor Program，optimal learning rate在模型缩放的过程中应该不会有明显变化。为了验证这一点，MiniCPM在0.04B、0.1B、0.3B和0.5B的模型上进行了六组learning rate的实验。  \n\n在下图中，可以发现尽管模型大小增加了十倍，但optimal learning rate并没有明显的偏移，基本上一致保持在0.01左右。  \n\n{% asset_img learning_rate.png 超参搜索 %}  \n\nMiniCPM进一步在2.1B规模的模型上进行了一个简单的验证，最终确认了0.01的learning rate确实实现了最低loss。  \n\n# WSD  \n\n## cosine learning rate scheduler的分析  \n\ncosine scheduler的周期很重要，一般是设置降到最小learning rate的时间T和预训练的总step数S持平。为了验证这个设置的效果，用0.036B的模型的做了实验，按以下公式分别实现cosine和cosine loop两种scheduler。\n\n{% asset_img cos_lr.png LR %}  \n\nloss的变化如下图  \n\n{% asset_img cos_loss.png LR %}  \n\n可以看到确实总是T=S时效果最好。分析原因可能是：  \n- 与T<S的scheduler相比，T=S的scheduler有更长的高learning rate持续时间。而这种高learning rate可能有助于模型找到更好的global optimum。  \n- 与T>S的scheduler相比，T=S的scheduler有更彻底的learning rate decay。这种衰减可能涉及到training dynamics，使模型能够找到更好的 local optimum。  \n\n## Warmup-Stable-Decay  \n\n基于上面的分析，MiniCPM把训练过程显式分成high learning rate stage和learning decay stage，这个scheduler就叫Warmup-Stable-Decay scheduler，公式如下  \n\n$$\\left.WSD(T;s)=\\begin{cases}&\\frac{s}{W}\\eta,\\quad s<W\\\\&\\eta,\\quad W<s<T\\\\&f(s-T)\\eta,\\quad T<s<S\\end{cases}\\right.$$  \n\n其中W是warmup的step数，T是stable training step数，$\\eta$ 是maximum learning rate，$f\\left(s-T\\right)$ 是关于s的 decreasing function，取值在0到1之间。  \n\n一般来说W只要足够，对训练的效果影响就不大，因此所有后面就忽略W了。  \n\n继续做一些实验来探索WSD。  \n\n（1）Loss Decreases Dramatically in Decay Stage  \n\n首先在0.036B的模型上应用了WSD，并设置了不同的T和S（影响decay阶段的长度）。发现在decay阶段，随着learning rate的下降，loss出现了显著的快速下降，并迅速降低到等于或低于T=S时的Cosine LRS的loss，具体loss变化如下图  \n\n{% asset_img wsd_exp1.png WSD %}  \n\n由于stable training阶段learning是保持不变的，所以这里可以重用decay前的模型checkpoint，继续进行的高learning rate的训练。在原设置上增加了更多的stable training step之后，还可以再进行learning rate退火，并且能够实现与Cosine LRS在同样step下相同的loss。这进一步验证了“训练阶段可以明确地分为stable阶段和decay阶段”的假设。  \n\n（2）10% Steps are Enough  \n\n如上图所示，在40N、60N和80N训练数据的实验中，使用总token数的10%的进行learning rate decay就足以获得最好的结果，如果小于10%则效果会比较差。因此，在后续的训练实验中，都使用大约10%的step进行learning rate decay，以确保完全收敛。  \n\n（3）Effective Data Scaling with WSD LRS  \n\n使用WSD可以把模型训练到极致收敛的状态。为了展示WSD训练固定大小模型到收敛的潜力，MiniCPM对0.036B的模型进行持续训练，然后与使用40N数据的0.17B模型进行比较，loss如下图。  \n\n{% asset_img wsd_exp2.png WSD %}  \n\n0.036B模型在使用更多的数据后，超过Chinchilla Optimal，并且仍有收敛趋势，按这个趋势继续训练就能match 0.17B模型的loss水平。  \n\n## Measuring the Scaling Law with WSD LRS  \n\n利用WSD，可以把探索model size和data size的scaling关系的成本变成线性，因为stable stage阶段learning保持不变，可以把decay接在不同的step后面来获取不同数据量下的效果。  \n\n通过训练从0.04B到2B共6种大小的SLM来测量scaling law。每种大小的模型都有从10N到60N数据共6个数据量开始decay的结果。  \n\n这36个模型的训练结果在5个数据集上进行比较。为了可以比较不同tokenizer的模型的损失，按《GPT-4 technical report》里的做法，使用byte数的平均而非token数的平均来进行比较。然后用scipy curvefit function，按下面这个公式拟合model size N and data size D的关系。  \n\n$$L(N,D)=C_NN^{-\\alpha}+C_DD^{-\\beta}+L_0$$  \n\n实验结果和拟合结果如下图  \n\n{% asset_img scaling_law.png scaling law %}  \n\n然后参照《Scaling language models: Methods, analysis & insights from training gopher》、《Training compute-optimal large language models》、《Scaling laws for neural language models》的做法，推算出token数量应该是模型参数量的192倍，这比《Training compute-optimal large language models》中给出的20倍要大得多。  \n\nMiniCPM还把LLAMA2的数据拿出来进行了验证。按LLAMA2报告中给出的数据计算出的token数应是模型参数量的70~100倍，这个值同样比20要大很多。\n\n因此结论是，按照基于WSD的实验结果，语言模型比我们之前想象的可以吸收更多语料数据。  \n\n# Two Stage Pre-training Strategy  \n\n前面观察到WSD的衰减阶段loss有显著的减少，因此MiniCPM认为在learning rate的退火阶段整合高质量SFT数据，混合进预训练数据中可以SFT效果：  \n- 一方面，在退火阶段使用SFT数据能获得预SFT更相关的loss下降  \n- 另一方面，和在整个预训练阶段都使用SFT数据相比，只在learning rate decay阶段使用更不容易过拟合  \n\n为了验证这个猜想，设计以下训练配置：  \n- A-1: 2.4B模型，decay阶段仅用无标签数据，之后进行4B的SFT训练  \n- A-2: 2.4B模型，decay阶段使用无标签数据+SFT数据，之后进行4B的SFT训练  \n- B-1: 1.2B模型，decay阶段仅用无标签数据，之后进行6B的SFT训练  \n- B-2: 1.2B模型，decay阶段仅用无标签数据，之后进行12B的SFT训练  \n- B-3: 1.2B模型，decay阶段使用无标签数据+SFT数据，之后进行6B的SFT训练  \n\n各个模型预训练+SFT之后的效果如下  \n\n{% asset_img 2_stage.png 2阶段训练 %}  \n\n可以看到在预训练learning rate退火阶段加入SFT数据的模型效果更好。  \n\n# MiniCPM  \n\n## 模型  \n\nMiniCPM有2.4B和1.2B两个规模。其中2.4B模型的词表大小为122,753，1.2B模型词表大小为73,440，都是通过BPE进行构建。在测试数据集上评测，MiniCPM的tokenizer的效率是比较高的，具体数值如下  \n\n{% asset_img tokenizer.png tokenizer %}  \n\nMiniCPM模型的输入输出共享了矩阵，因为小模型共享输入输出矩阵可以节省很多参数。\n\n在层数和hidden state的设计上，MiniCPM使用了相比Phi-2等SLM更深更瘦的模型结构，这和《Mobilellm: Optimizing sub-billion parameter language models for on-device use cases》的想法一致。具体的结构参数如下  \n\n{% asset_img layers.png 更深更瘦的结构 %}  \n\n1.2B模型上使用了GQA，可以进一步节省参数量。  \n\n## 训练  \n\n在WSD的stable阶段，使用1T预训练数据，batch size=3.93M，max lr=0.01。  \n\n在decay阶段，decay的策略为 $f(s-T)=0.5^{(s-S)/T}$，其中T=5000 steps (20B tokens)。  \n\nSFT阶段共使用了6B数据，learning rate和预训练阶段结束时的learning rate对齐，同样使用了WSD。  \n\n预训练数据的分布如下  \n\n{% asset_img data.png 训练数据 %}  \n\n1.2B和2.4B模型的预训练loss如下图  \n\n{% asset_img train_loss.png training loss %}  \n\n左图loss的第一次突变是因为增大了batch size，效果相当于减小了learning rate。  \n\n最终SFT模型在下游任务的评测结果如下  \n\n{% asset_img eval.png evaluation %}  \n\n## MiniCPM-DPO  \n\n在SFT的基础上，MiniCPM用UltraFeedback数据集进行DPO训练。  \n\nDPO训练使用了Cosine LRS, max learning rate=1e-5，一共训练了一个epoch。  \n\nDPO使得模型在MT-bench上的得分从6.89提升到7.25，但是在原来通用benchmark的效果有所下降。  \n\n## MiniCPM-128k  \n\n长文本训练把MiniCPM支持的窗口大小从4k拓展到128k。在这一阶段的训练禁用了输入输出矩阵共享，这会使得模型的实际参数略有上升。训练的初始模型用的是预训练中stable阶段的最后一个checkpoint。  \n\nMiniCPM将书籍、维基百科文章和论文分类为“长数据”，其他为“短数据”。那么在这一阶段的训练包含了44%的长数据和 56%的短数据。  \n\n训练时不直接训练到128k，而是使用curriculum learning：先训练32k，再训练128k。4k-32k范围内应用ABF，32K到128K的范围内使用NTK-Aware RoPE scaling。  \n\n如Yi的技术报告和《Zebra: Extending context window with layerwise grouped local-global attention》所指出的那样，使用合成的长QA数据，有助于提高模型在上下文感知任务中的性能，MiniCPM也使用了合成的长QA数据。  \n\nMiniCPM-128k在∞Bench（《∞bench: Extending long context evaluation beyond 100k tokens》）上评测结果如下  \n\n{% asset_img 128k_result.png 128k evaluation %}  \n\n## MiniCPM-MoE  \n\nMiniCPM-MoE使用Sparse Upcycling（《Sparse upcycling: Training mixture-of-experts from dense checkpoints》）进行初始化，使用了stable阶段的checkpoint。router用均值为0、方差为0.01的正态分布进行初始化。  \n\nMiniCPM-MoE共有13.6B参数，激活2个专家，共激活4B参数。\n\n训练时使用switch transformer的负载均衡函数，权重系数为0.01。\n\nlearning rate使用了WSD，在4M的batch size下共进行了130k步预训练，而在SFT阶段batch size减小了到2M。  \n\nMiniCPM-MoE的效果评测如下  \n\n{% asset_img moe_result.png moe evaluation %}  \n\n# 小结  \n\nMiniCPM站在很多前人结果的肩膀上，把目前各种比较先进的做法融合到了1B/2B模型上，获得了不错的效果。其中用到的参数搜索、对scaling law的刷新都挺有参考价值。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM的重复生成和ICL](https://www.linsight.cn/7381cae3.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n[大模型算法题(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n【1】MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies https://arxiv.org/abs/2404.06395  \n","source":"_posts/cs/nlp/2024/06/MiniCPM.md","raw":"---\ntitle: MiniCPM\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 技术报告\n  - 学习率\n  - 预训练\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 376db710\ndate: 2024-06-18 21:51:22\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\nMiniCPM是面壁智能和清华开源的模型，MiniCPM开源系列包括非embedding参数为1.2B和2.4B两个规模的模型，以及对应的MiniCPM-DPO，MiniCPM-MoE和MiniCPM-128K模型。  \n\n简单梳理一下MiniCPM提到的一些内容。  \n\n# 背景  \n\n大模型的训练成本很高，而且很多机制还没搞清楚，训出来的大规模模型在很多设备上也跑不起来，因此现在有不少机构对小一点的模型，即SLM，进行更全面的探索，比如Phi系列、TinyLlama、MobileLLM和Gemma等。  \n\nMiniCPM也是对SLM的一次探索，从中得到的经验也可以推广到更大的模型上。  \n\n# 风洞实验  \n\n为了找到好的模型参数和训练参数，MiniCPM做了很多“风洞实验”（Model Wind Tunnel Experiments）。  \n\n这些风洞实验主要包括三个部分：（1）搜索模型结构的超参（2）探索batch size的scaling（3）寻找最佳的learning rate。\n\n后续风洞实验所用的模型具体参数如下  \n\n{% asset_img exp_model.png 模型结构 %}  \n\n## 模型超参  \n\n预训练资源消耗很大，即使是SLM也不可能把所有参数的排列组合都搜索一遍。  \n\n这里参考Tensor Program的做法（《Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer》和《Tensor programs vi: Feature learning in infinite-depth neural networks》），分别对模型的宽度和深度进行了搜索。  \n\n搜索所用的操作如下表所示，这里没有应用attention softmax的scaling技术。  \n\n{% asset_img param_search_2.png 超参搜索 %}  \n\n关于超参搜索的一些细节：  \n- 用Maximal Update Parametrization的方法进行了调参。  \n- 在一系列预定义的参数空间上进行贝叶斯搜索，所用模型参数为在N=0.009B。发现使用规模为10N和20N大小的数据集进行超参数优化时，超参的有效性表现出了一致性。因此调参的过程就使用D=10N=0.09B个token来进行实验了。  \n- 应用了QK-Norm（《Querykey normalization for transformers》）和independent weight decay（《Decoupled weight decay regularization》）之后，发现模型对learning rate的敏感性显著降低。不过在找到最佳learning rate之后，后面的训练就不用再调整参数了，因此后面的实验就没有继续使用QK-Norm和independent weight decay。  \n\n最终从下图展示的参数搜索，确定了最佳的hyper-parameters为：  \n- scale depth = 1.4  \n- scale emb = 12  \n- init std = 0.1  \n- lr = 0.01  \n\n{% asset_img param_search.png 超参搜索 %}  \n\n## Optimal Batch Size  \n\nbatch size决定了模型收敛速度与计算资源消耗之间的平衡。  \n\n如果batch size太大，会导致很大的数据和计算成本（才能跑到足够的update次数，让模型收敛）。如果batch size太小，将会有大量的update step，并且相比大一些的batch size，loss的减小很有限，训练效率太低。  \n\n这里参考OpenAI的《Scaling laws for neural language models》的方法来寻找最佳的batch size，并做了一些改动。  \n\n《Scaling laws for neural language models》研究的是loss function和token数之间的关系。他们假设了更多的step=更多的训练时间。在这个假设下，OpenAI定义了一个critical batch size，在不消耗过多的step或者token的情况下，能达到一定的loss水平。  \n\n这在无限GPU资源下是合理的。由于GPU资源是无限的，增加batch size不会增加单个step的耗时，但会减少总step数，因而提高了效率。但是实际上我们并没有无限GPU资源，因此将batch size增大相当于增加的每个step的时间，所以实际上通过增加batch size来减小step数，对总训练时间的影响并不大。  \n\n因此MiniCPM放弃了“not consuming too many steps”的目标，转而追求“minimizing the token quantity to achieve the lowest loss”。  \n\n关于optimal batch size与loss之间关系的估计，类似于“先有鸡还是先有蛋”的悖论，因为暂时没法完全搞清楚这二者之间的决定关系。目前的做法是，对于给定的模型大小，通常会有一个初步估计的achievable loss，这是由先前的初步实验得出的经验估计。  \n\n而optimal batch size和optimal learning rate很可能并不是独立的。为了克服这种相关性，MiniCPM首先对learning rate进行了初步研究，然后选择一个最优learning rate来进行batch size实验，并使用batch size缩放再次进行learning rate调整。这有点像Coordinate Descent optimization method。  \n\n细节上，MiniCPM分别对0.009B、0.03B和0.17B的模型进行了实验。每个模型大小都在6种不同的batch size上进行训练，使用了global learning rate=0.01和cosine learning rate scheduler。在C4数据集上，optimal batch size与loss的趋势如下图红线  \n\n{% asset_img batch_size.png 超参搜索 %}  \n\n这三条红线在log空间中很好连成一条直线，如下图。  \n\n{% asset_img batch_size_2.png 超参搜索 %}  \n\n这里就得到了在C4数据集上的训练loss和optimal batch size的关系。  \n\n$$bs=\\frac{1.21\\times10^9}{L^{6.24}}$$  \n\n## Optimal Learning Rate  \n\n由于使用了Tensor Program，optimal learning rate在模型缩放的过程中应该不会有明显变化。为了验证这一点，MiniCPM在0.04B、0.1B、0.3B和0.5B的模型上进行了六组learning rate的实验。  \n\n在下图中，可以发现尽管模型大小增加了十倍，但optimal learning rate并没有明显的偏移，基本上一致保持在0.01左右。  \n\n{% asset_img learning_rate.png 超参搜索 %}  \n\nMiniCPM进一步在2.1B规模的模型上进行了一个简单的验证，最终确认了0.01的learning rate确实实现了最低loss。  \n\n# WSD  \n\n## cosine learning rate scheduler的分析  \n\ncosine scheduler的周期很重要，一般是设置降到最小learning rate的时间T和预训练的总step数S持平。为了验证这个设置的效果，用0.036B的模型的做了实验，按以下公式分别实现cosine和cosine loop两种scheduler。\n\n{% asset_img cos_lr.png LR %}  \n\nloss的变化如下图  \n\n{% asset_img cos_loss.png LR %}  \n\n可以看到确实总是T=S时效果最好。分析原因可能是：  \n- 与T<S的scheduler相比，T=S的scheduler有更长的高learning rate持续时间。而这种高learning rate可能有助于模型找到更好的global optimum。  \n- 与T>S的scheduler相比，T=S的scheduler有更彻底的learning rate decay。这种衰减可能涉及到training dynamics，使模型能够找到更好的 local optimum。  \n\n## Warmup-Stable-Decay  \n\n基于上面的分析，MiniCPM把训练过程显式分成high learning rate stage和learning decay stage，这个scheduler就叫Warmup-Stable-Decay scheduler，公式如下  \n\n$$\\left.WSD(T;s)=\\begin{cases}&\\frac{s}{W}\\eta,\\quad s<W\\\\&\\eta,\\quad W<s<T\\\\&f(s-T)\\eta,\\quad T<s<S\\end{cases}\\right.$$  \n\n其中W是warmup的step数，T是stable training step数，$\\eta$ 是maximum learning rate，$f\\left(s-T\\right)$ 是关于s的 decreasing function，取值在0到1之间。  \n\n一般来说W只要足够，对训练的效果影响就不大，因此所有后面就忽略W了。  \n\n继续做一些实验来探索WSD。  \n\n（1）Loss Decreases Dramatically in Decay Stage  \n\n首先在0.036B的模型上应用了WSD，并设置了不同的T和S（影响decay阶段的长度）。发现在decay阶段，随着learning rate的下降，loss出现了显著的快速下降，并迅速降低到等于或低于T=S时的Cosine LRS的loss，具体loss变化如下图  \n\n{% asset_img wsd_exp1.png WSD %}  \n\n由于stable training阶段learning是保持不变的，所以这里可以重用decay前的模型checkpoint，继续进行的高learning rate的训练。在原设置上增加了更多的stable training step之后，还可以再进行learning rate退火，并且能够实现与Cosine LRS在同样step下相同的loss。这进一步验证了“训练阶段可以明确地分为stable阶段和decay阶段”的假设。  \n\n（2）10% Steps are Enough  \n\n如上图所示，在40N、60N和80N训练数据的实验中，使用总token数的10%的进行learning rate decay就足以获得最好的结果，如果小于10%则效果会比较差。因此，在后续的训练实验中，都使用大约10%的step进行learning rate decay，以确保完全收敛。  \n\n（3）Effective Data Scaling with WSD LRS  \n\n使用WSD可以把模型训练到极致收敛的状态。为了展示WSD训练固定大小模型到收敛的潜力，MiniCPM对0.036B的模型进行持续训练，然后与使用40N数据的0.17B模型进行比较，loss如下图。  \n\n{% asset_img wsd_exp2.png WSD %}  \n\n0.036B模型在使用更多的数据后，超过Chinchilla Optimal，并且仍有收敛趋势，按这个趋势继续训练就能match 0.17B模型的loss水平。  \n\n## Measuring the Scaling Law with WSD LRS  \n\n利用WSD，可以把探索model size和data size的scaling关系的成本变成线性，因为stable stage阶段learning保持不变，可以把decay接在不同的step后面来获取不同数据量下的效果。  \n\n通过训练从0.04B到2B共6种大小的SLM来测量scaling law。每种大小的模型都有从10N到60N数据共6个数据量开始decay的结果。  \n\n这36个模型的训练结果在5个数据集上进行比较。为了可以比较不同tokenizer的模型的损失，按《GPT-4 technical report》里的做法，使用byte数的平均而非token数的平均来进行比较。然后用scipy curvefit function，按下面这个公式拟合model size N and data size D的关系。  \n\n$$L(N,D)=C_NN^{-\\alpha}+C_DD^{-\\beta}+L_0$$  \n\n实验结果和拟合结果如下图  \n\n{% asset_img scaling_law.png scaling law %}  \n\n然后参照《Scaling language models: Methods, analysis & insights from training gopher》、《Training compute-optimal large language models》、《Scaling laws for neural language models》的做法，推算出token数量应该是模型参数量的192倍，这比《Training compute-optimal large language models》中给出的20倍要大得多。  \n\nMiniCPM还把LLAMA2的数据拿出来进行了验证。按LLAMA2报告中给出的数据计算出的token数应是模型参数量的70~100倍，这个值同样比20要大很多。\n\n因此结论是，按照基于WSD的实验结果，语言模型比我们之前想象的可以吸收更多语料数据。  \n\n# Two Stage Pre-training Strategy  \n\n前面观察到WSD的衰减阶段loss有显著的减少，因此MiniCPM认为在learning rate的退火阶段整合高质量SFT数据，混合进预训练数据中可以SFT效果：  \n- 一方面，在退火阶段使用SFT数据能获得预SFT更相关的loss下降  \n- 另一方面，和在整个预训练阶段都使用SFT数据相比，只在learning rate decay阶段使用更不容易过拟合  \n\n为了验证这个猜想，设计以下训练配置：  \n- A-1: 2.4B模型，decay阶段仅用无标签数据，之后进行4B的SFT训练  \n- A-2: 2.4B模型，decay阶段使用无标签数据+SFT数据，之后进行4B的SFT训练  \n- B-1: 1.2B模型，decay阶段仅用无标签数据，之后进行6B的SFT训练  \n- B-2: 1.2B模型，decay阶段仅用无标签数据，之后进行12B的SFT训练  \n- B-3: 1.2B模型，decay阶段使用无标签数据+SFT数据，之后进行6B的SFT训练  \n\n各个模型预训练+SFT之后的效果如下  \n\n{% asset_img 2_stage.png 2阶段训练 %}  \n\n可以看到在预训练learning rate退火阶段加入SFT数据的模型效果更好。  \n\n# MiniCPM  \n\n## 模型  \n\nMiniCPM有2.4B和1.2B两个规模。其中2.4B模型的词表大小为122,753，1.2B模型词表大小为73,440，都是通过BPE进行构建。在测试数据集上评测，MiniCPM的tokenizer的效率是比较高的，具体数值如下  \n\n{% asset_img tokenizer.png tokenizer %}  \n\nMiniCPM模型的输入输出共享了矩阵，因为小模型共享输入输出矩阵可以节省很多参数。\n\n在层数和hidden state的设计上，MiniCPM使用了相比Phi-2等SLM更深更瘦的模型结构，这和《Mobilellm: Optimizing sub-billion parameter language models for on-device use cases》的想法一致。具体的结构参数如下  \n\n{% asset_img layers.png 更深更瘦的结构 %}  \n\n1.2B模型上使用了GQA，可以进一步节省参数量。  \n\n## 训练  \n\n在WSD的stable阶段，使用1T预训练数据，batch size=3.93M，max lr=0.01。  \n\n在decay阶段，decay的策略为 $f(s-T)=0.5^{(s-S)/T}$，其中T=5000 steps (20B tokens)。  \n\nSFT阶段共使用了6B数据，learning rate和预训练阶段结束时的learning rate对齐，同样使用了WSD。  \n\n预训练数据的分布如下  \n\n{% asset_img data.png 训练数据 %}  \n\n1.2B和2.4B模型的预训练loss如下图  \n\n{% asset_img train_loss.png training loss %}  \n\n左图loss的第一次突变是因为增大了batch size，效果相当于减小了learning rate。  \n\n最终SFT模型在下游任务的评测结果如下  \n\n{% asset_img eval.png evaluation %}  \n\n## MiniCPM-DPO  \n\n在SFT的基础上，MiniCPM用UltraFeedback数据集进行DPO训练。  \n\nDPO训练使用了Cosine LRS, max learning rate=1e-5，一共训练了一个epoch。  \n\nDPO使得模型在MT-bench上的得分从6.89提升到7.25，但是在原来通用benchmark的效果有所下降。  \n\n## MiniCPM-128k  \n\n长文本训练把MiniCPM支持的窗口大小从4k拓展到128k。在这一阶段的训练禁用了输入输出矩阵共享，这会使得模型的实际参数略有上升。训练的初始模型用的是预训练中stable阶段的最后一个checkpoint。  \n\nMiniCPM将书籍、维基百科文章和论文分类为“长数据”，其他为“短数据”。那么在这一阶段的训练包含了44%的长数据和 56%的短数据。  \n\n训练时不直接训练到128k，而是使用curriculum learning：先训练32k，再训练128k。4k-32k范围内应用ABF，32K到128K的范围内使用NTK-Aware RoPE scaling。  \n\n如Yi的技术报告和《Zebra: Extending context window with layerwise grouped local-global attention》所指出的那样，使用合成的长QA数据，有助于提高模型在上下文感知任务中的性能，MiniCPM也使用了合成的长QA数据。  \n\nMiniCPM-128k在∞Bench（《∞bench: Extending long context evaluation beyond 100k tokens》）上评测结果如下  \n\n{% asset_img 128k_result.png 128k evaluation %}  \n\n## MiniCPM-MoE  \n\nMiniCPM-MoE使用Sparse Upcycling（《Sparse upcycling: Training mixture-of-experts from dense checkpoints》）进行初始化，使用了stable阶段的checkpoint。router用均值为0、方差为0.01的正态分布进行初始化。  \n\nMiniCPM-MoE共有13.6B参数，激活2个专家，共激活4B参数。\n\n训练时使用switch transformer的负载均衡函数，权重系数为0.01。\n\nlearning rate使用了WSD，在4M的batch size下共进行了130k步预训练，而在SFT阶段batch size减小了到2M。  \n\nMiniCPM-MoE的效果评测如下  \n\n{% asset_img moe_result.png moe evaluation %}  \n\n# 小结  \n\nMiniCPM站在很多前人结果的肩膀上，把目前各种比较先进的做法融合到了1B/2B模型上，获得了不错的效果。其中用到的参数搜索、对scaling law的刷新都挺有参考价值。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM的重复生成和ICL](https://www.linsight.cn/7381cae3.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n[大模型算法题(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n【1】MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies https://arxiv.org/abs/2404.06395  \n","slug":"cs/nlp/2024/06/MiniCPM","published":1,"updated":"2024-06-24T04:09:56.529Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9v000c314k2b930rha","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>MiniCPM是面壁智能和清华开源的模型，MiniCPM开源系列包括非embedding参数为1.2B和2.4B两个规模的模型，以及对应的MiniCPM-DPO，MiniCPM-MoE和MiniCPM-128K模型。</p>\n<p>简单梳理一下MiniCPM提到的一些内容。</p>\n<h1 id=\"背景\">背景</h1>\n<p>大模型的训练成本很高，而且很多机制还没搞清楚，训出来的大规模模型在很多设备上也跑不起来，因此现在有不少机构对小一点的模型，即SLM，进行更全面的探索，比如Phi系列、TinyLlama、MobileLLM和Gemma等。</p>\n<p>MiniCPM也是对SLM的一次探索，从中得到的经验也可以推广到更大的模型上。</p>\n<h1 id=\"风洞实验\">风洞实验</h1>\n<p>为了找到好的模型参数和训练参数，MiniCPM做了很多“风洞实验”（Model Wind\nTunnel Experiments）。</p>\n<p>这些风洞实验主要包括三个部分：（1）搜索模型结构的超参（2）探索batch\nsize的scaling（3）寻找最佳的learning rate。</p>\n<p>后续风洞实验所用的模型具体参数如下</p>\n<img src=\"/376db710/exp_model.png\" class title=\"模型结构\">\n<h2 id=\"模型超参\">模型超参</h2>\n<p>预训练资源消耗很大，即使是SLM也不可能把所有参数的排列组合都搜索一遍。</p>\n<p>这里参考Tensor Program的做法（《Tensor programs v: Tuning large\nneural networks via zero-shot hyperparameter transfer》和《Tensor\nprograms vi: Feature learning in infinite-depth neural\nnetworks》），分别对模型的宽度和深度进行了搜索。</p>\n<p>搜索所用的操作如下表所示，这里没有应用attention\nsoftmax的scaling技术。</p>\n<img src=\"/376db710/param_search_2.png\" class title=\"超参搜索\">\n<p>关于超参搜索的一些细节：<br>\n- 用Maximal Update Parametrization的方法进行了调参。<br>\n-\n在一系列预定义的参数空间上进行贝叶斯搜索，所用模型参数为在N=0.009B。发现使用规模为10N和20N大小的数据集进行超参数优化时，超参的有效性表现出了一致性。因此调参的过程就使用D=10N=0.09B个token来进行实验了。<br>\n- 应用了QK-Norm（《Querykey normalization for\ntransformers》）和independent weight decay（《Decoupled weight decay\nregularization》）之后，发现模型对learning\nrate的敏感性显著降低。不过在找到最佳learning\nrate之后，后面的训练就不用再调整参数了，因此后面的实验就没有继续使用QK-Norm和independent\nweight decay。</p>\n<p>最终从下图展示的参数搜索，确定了最佳的hyper-parameters为：<br>\n- scale depth = 1.4<br>\n- scale emb = 12<br>\n- init std = 0.1<br>\n- lr = 0.01</p>\n<img src=\"/376db710/param_search.png\" class title=\"超参搜索\">\n<h2 id=\"optimal-batch-size\">Optimal Batch Size</h2>\n<p>batch size决定了模型收敛速度与计算资源消耗之间的平衡。</p>\n<p>如果batch\nsize太大，会导致很大的数据和计算成本（才能跑到足够的update次数，让模型收敛）。如果batch\nsize太小，将会有大量的update step，并且相比大一些的batch\nsize，loss的减小很有限，训练效率太低。</p>\n<p>这里参考OpenAI的《Scaling laws for neural language\nmodels》的方法来寻找最佳的batch size，并做了一些改动。</p>\n<p>《Scaling laws for neural language models》研究的是loss\nfunction和token数之间的关系。他们假设了更多的step=更多的训练时间。在这个假设下，OpenAI定义了一个critical\nbatch\nsize，在不消耗过多的step或者token的情况下，能达到一定的loss水平。</p>\n<p>这在无限GPU资源下是合理的。由于GPU资源是无限的，增加batch\nsize不会增加单个step的耗时，但会减少总step数，因而提高了效率。但是实际上我们并没有无限GPU资源，因此将batch\nsize增大相当于增加的每个step的时间，所以实际上通过增加batch\nsize来减小step数，对总训练时间的影响并不大。</p>\n<p>因此MiniCPM放弃了“not consuming too many\nsteps”的目标，转而追求“minimizing the token quantity to achieve the\nlowest loss”。</p>\n<p>关于optimal batch\nsize与loss之间关系的估计，类似于“先有鸡还是先有蛋”的悖论，因为暂时没法完全搞清楚这二者之间的决定关系。目前的做法是，对于给定的模型大小，通常会有一个初步估计的achievable\nloss，这是由先前的初步实验得出的经验估计。</p>\n<p>而optimal batch size和optimal learning\nrate很可能并不是独立的。为了克服这种相关性，MiniCPM首先对learning\nrate进行了初步研究，然后选择一个最优learning rate来进行batch\nsize实验，并使用batch size缩放再次进行learning\nrate调整。这有点像Coordinate Descent optimization method。</p>\n<p>细节上，MiniCPM分别对0.009B、0.03B和0.17B的模型进行了实验。每个模型大小都在6种不同的batch\nsize上进行训练，使用了global learning rate=0.01和cosine learning rate\nscheduler。在C4数据集上，optimal batch size与loss的趋势如下图红线</p>\n<img src=\"/376db710/batch_size.png\" class title=\"超参搜索\">\n<p>这三条红线在log空间中很好连成一条直线，如下图。</p>\n<img src=\"/376db710/batch_size_2.png\" class title=\"超参搜索\">\n<p>这里就得到了在C4数据集上的训练loss和optimal batch size的关系。</p>\n<p><span class=\"math display\">\\[bs=\\frac{1.21\\times10^9}{L^{6.24}}\\]</span></p>\n<h2 id=\"optimal-learning-rate\">Optimal Learning Rate</h2>\n<p>由于使用了Tensor Program，optimal learning\nrate在模型缩放的过程中应该不会有明显变化。为了验证这一点，MiniCPM在0.04B、0.1B、0.3B和0.5B的模型上进行了六组learning\nrate的实验。</p>\n<p>在下图中，可以发现尽管模型大小增加了十倍，但optimal learning\nrate并没有明显的偏移，基本上一致保持在0.01左右。</p>\n<img src=\"/376db710/learning_rate.png\" class title=\"超参搜索\">\n<p>MiniCPM进一步在2.1B规模的模型上进行了一个简单的验证，最终确认了0.01的learning\nrate确实实现了最低loss。</p>\n<h1 id=\"wsd\">WSD</h1>\n<h2 id=\"cosine-learning-rate-scheduler的分析\">cosine learning rate\nscheduler的分析</h2>\n<p>cosine scheduler的周期很重要，一般是设置降到最小learning\nrate的时间T和预训练的总step数S持平。为了验证这个设置的效果，用0.036B的模型的做了实验，按以下公式分别实现cosine和cosine\nloop两种scheduler。</p>\n<img src=\"/376db710/cos_lr.png\" class title=\"LR\">\n<p>loss的变化如下图</p>\n<img src=\"/376db710/cos_loss.png\" class title=\"LR\">\n<p>可以看到确实总是T=S时效果最好。分析原因可能是：<br>\n- 与T&lt;S的scheduler相比，T=S的scheduler有更长的高learning\nrate持续时间。而这种高learning rate可能有助于模型找到更好的global\noptimum。<br>\n- 与T&gt;S的scheduler相比，T=S的scheduler有更彻底的learning rate\ndecay。这种衰减可能涉及到training dynamics，使模型能够找到更好的 local\noptimum。</p>\n<h2 id=\"warmup-stable-decay\">Warmup-Stable-Decay</h2>\n<p>基于上面的分析，MiniCPM把训练过程显式分成high learning rate\nstage和learning decay stage，这个scheduler就叫Warmup-Stable-Decay\nscheduler，公式如下</p>\n<p><span class=\"math display\">\\[\\left.WSD(T;s)=\\begin{cases}&amp;\\frac{s}{W}\\eta,\\quad\ns&lt;W\\\\&amp;\\eta,\\quad W&lt;s&lt;T\\\\&amp;f(s-T)\\eta,\\quad\nT&lt;s&lt;S\\end{cases}\\right.\\]</span></p>\n<p>其中W是warmup的step数，T是stable training step数，<span class=\"math inline\">\\(\\eta\\)</span> 是maximum learning rate，<span class=\"math inline\">\\(f\\left(s-T\\right)\\)</span> 是关于s的 decreasing\nfunction，取值在0到1之间。</p>\n<p>一般来说W只要足够，对训练的效果影响就不大，因此所有后面就忽略W了。</p>\n<p>继续做一些实验来探索WSD。</p>\n<p>（1）Loss Decreases Dramatically in Decay Stage</p>\n<p>首先在0.036B的模型上应用了WSD，并设置了不同的T和S（影响decay阶段的长度）。发现在decay阶段，随着learning\nrate的下降，loss出现了显著的快速下降，并迅速降低到等于或低于T=S时的Cosine\nLRS的loss，具体loss变化如下图</p>\n<img src=\"/376db710/wsd_exp1.png\" class title=\"WSD\">\n<p>由于stable\ntraining阶段learning是保持不变的，所以这里可以重用decay前的模型checkpoint，继续进行的高learning\nrate的训练。在原设置上增加了更多的stable training\nstep之后，还可以再进行learning rate退火，并且能够实现与Cosine\nLRS在同样step下相同的loss。这进一步验证了“训练阶段可以明确地分为stable阶段和decay阶段”的假设。</p>\n<p>（2）10% Steps are Enough</p>\n<p>如上图所示，在40N、60N和80N训练数据的实验中，使用总token数的10%的进行learning\nrate\ndecay就足以获得最好的结果，如果小于10%则效果会比较差。因此，在后续的训练实验中，都使用大约10%的step进行learning\nrate decay，以确保完全收敛。</p>\n<p>（3）Effective Data Scaling with WSD LRS</p>\n<p>使用WSD可以把模型训练到极致收敛的状态。为了展示WSD训练固定大小模型到收敛的潜力，MiniCPM对0.036B的模型进行持续训练，然后与使用40N数据的0.17B模型进行比较，loss如下图。</p>\n<img src=\"/376db710/wsd_exp2.png\" class title=\"WSD\">\n<p>0.036B模型在使用更多的数据后，超过Chinchilla\nOptimal，并且仍有收敛趋势，按这个趋势继续训练就能match\n0.17B模型的loss水平。</p>\n<h2 id=\"measuring-the-scaling-law-with-wsd-lrs\">Measuring the Scaling\nLaw with WSD LRS</h2>\n<p>利用WSD，可以把探索model size和data\nsize的scaling关系的成本变成线性，因为stable\nstage阶段learning保持不变，可以把decay接在不同的step后面来获取不同数据量下的效果。</p>\n<p>通过训练从0.04B到2B共6种大小的SLM来测量scaling\nlaw。每种大小的模型都有从10N到60N数据共6个数据量开始decay的结果。</p>\n<p>这36个模型的训练结果在5个数据集上进行比较。为了可以比较不同tokenizer的模型的损失，按《GPT-4\ntechnical\nreport》里的做法，使用byte数的平均而非token数的平均来进行比较。然后用scipy\ncurvefit function，按下面这个公式拟合model size N and data size\nD的关系。</p>\n<p><span class=\"math display\">\\[L(N,D)=C_NN^{-\\alpha}+C_DD^{-\\beta}+L_0\\]</span></p>\n<p>实验结果和拟合结果如下图</p>\n<img src=\"/376db710/scaling_law.png\" class title=\"scaling law\">\n<p>然后参照《Scaling language models: Methods, analysis &amp; insights\nfrom training gopher》、《Training compute-optimal large language\nmodels》、《Scaling laws for neural language\nmodels》的做法，推算出token数量应该是模型参数量的192倍，这比《Training\ncompute-optimal large language models》中给出的20倍要大得多。</p>\n<p>MiniCPM还把LLAMA2的数据拿出来进行了验证。按LLAMA2报告中给出的数据计算出的token数应是模型参数量的70~100倍，这个值同样比20要大很多。</p>\n<p>因此结论是，按照基于WSD的实验结果，语言模型比我们之前想象的可以吸收更多语料数据。</p>\n<h1 id=\"two-stage-pre-training-strategy\">Two Stage Pre-training\nStrategy</h1>\n<p>前面观察到WSD的衰减阶段loss有显著的减少，因此MiniCPM认为在learning\nrate的退火阶段整合高质量SFT数据，混合进预训练数据中可以SFT效果：<br>\n- 一方面，在退火阶段使用SFT数据能获得预SFT更相关的loss下降<br>\n- 另一方面，和在整个预训练阶段都使用SFT数据相比，只在learning rate\ndecay阶段使用更不容易过拟合</p>\n<p>为了验证这个猜想，设计以下训练配置：<br>\n- A-1: 2.4B模型，decay阶段仅用无标签数据，之后进行4B的SFT训练<br>\n- A-2:\n2.4B模型，decay阶段使用无标签数据+SFT数据，之后进行4B的SFT训练<br>\n- B-1: 1.2B模型，decay阶段仅用无标签数据，之后进行6B的SFT训练<br>\n- B-2: 1.2B模型，decay阶段仅用无标签数据，之后进行12B的SFT训练<br>\n- B-3:\n1.2B模型，decay阶段使用无标签数据+SFT数据，之后进行6B的SFT训练</p>\n<p>各个模型预训练+SFT之后的效果如下</p>\n<img src=\"/376db710/2_stage.png\" class title=\"2阶段训练\">\n<p>可以看到在预训练learning rate退火阶段加入SFT数据的模型效果更好。</p>\n<h1 id=\"minicpm\">MiniCPM</h1>\n<h2 id=\"模型\">模型</h2>\n<p>MiniCPM有2.4B和1.2B两个规模。其中2.4B模型的词表大小为122,753，1.2B模型词表大小为73,440，都是通过BPE进行构建。在测试数据集上评测，MiniCPM的tokenizer的效率是比较高的，具体数值如下</p>\n<img src=\"/376db710/tokenizer.png\" class title=\"tokenizer\">\n<p>MiniCPM模型的输入输出共享了矩阵，因为小模型共享输入输出矩阵可以节省很多参数。</p>\n<p>在层数和hidden\nstate的设计上，MiniCPM使用了相比Phi-2等SLM更深更瘦的模型结构，这和《Mobilellm:\nOptimizing sub-billion parameter language models for on-device use\ncases》的想法一致。具体的结构参数如下</p>\n<img src=\"/376db710/layers.png\" class title=\"更深更瘦的结构\">\n<p>1.2B模型上使用了GQA，可以进一步节省参数量。</p>\n<h2 id=\"训练\">训练</h2>\n<p>在WSD的stable阶段，使用1T预训练数据，batch size=3.93M，max\nlr=0.01。</p>\n<p>在decay阶段，decay的策略为 <span class=\"math inline\">\\(f(s-T)=0.5^{(s-S)/T}\\)</span>，其中T=5000 steps\n(20B tokens)。</p>\n<p>SFT阶段共使用了6B数据，learning rate和预训练阶段结束时的learning\nrate对齐，同样使用了WSD。</p>\n<p>预训练数据的分布如下</p>\n<img src=\"/376db710/data.png\" class title=\"训练数据\">\n<p>1.2B和2.4B模型的预训练loss如下图</p>\n<img src=\"/376db710/train_loss.png\" class title=\"training loss\">\n<p>左图loss的第一次突变是因为增大了batch size，效果相当于减小了learning\nrate。</p>\n<p>最终SFT模型在下游任务的评测结果如下</p>\n<img src=\"/376db710/eval.png\" class title=\"evaluation\">\n<h2 id=\"minicpm-dpo\">MiniCPM-DPO</h2>\n<p>在SFT的基础上，MiniCPM用UltraFeedback数据集进行DPO训练。</p>\n<p>DPO训练使用了Cosine LRS, max learning\nrate=1e-5，一共训练了一个epoch。</p>\n<p>DPO使得模型在MT-bench上的得分从6.89提升到7.25，但是在原来通用benchmark的效果有所下降。</p>\n<h2 id=\"minicpm-128k\">MiniCPM-128k</h2>\n<p>长文本训练把MiniCPM支持的窗口大小从4k拓展到128k。在这一阶段的训练禁用了输入输出矩阵共享，这会使得模型的实际参数略有上升。训练的初始模型用的是预训练中stable阶段的最后一个checkpoint。</p>\n<p>MiniCPM将书籍、维基百科文章和论文分类为“长数据”，其他为“短数据”。那么在这一阶段的训练包含了44%的长数据和\n56%的短数据。</p>\n<p>训练时不直接训练到128k，而是使用curriculum\nlearning：先训练32k，再训练128k。4k-32k范围内应用ABF，32K到128K的范围内使用NTK-Aware\nRoPE scaling。</p>\n<p>如Yi的技术报告和《Zebra: Extending context window with layerwise\ngrouped local-global\nattention》所指出的那样，使用合成的长QA数据，有助于提高模型在上下文感知任务中的性能，MiniCPM也使用了合成的长QA数据。</p>\n<p>MiniCPM-128k在∞Bench（《∞bench: Extending long context evaluation\nbeyond 100k tokens》）上评测结果如下</p>\n<img src=\"/376db710/128k_result.png\" class title=\"128k evaluation\">\n<h2 id=\"minicpm-moe\">MiniCPM-MoE</h2>\n<p>MiniCPM-MoE使用Sparse Upcycling（《Sparse upcycling: Training\nmixture-of-experts from dense\ncheckpoints》）进行初始化，使用了stable阶段的checkpoint。router用均值为0、方差为0.01的正态分布进行初始化。</p>\n<p>MiniCPM-MoE共有13.6B参数，激活2个专家，共激活4B参数。</p>\n<p>训练时使用switch transformer的负载均衡函数，权重系数为0.01。</p>\n<p>learning rate使用了WSD，在4M的batch\nsize下共进行了130k步预训练，而在SFT阶段batch size减小了到2M。</p>\n<p>MiniCPM-MoE的效果评测如下</p>\n<img src=\"/376db710/moe_result.png\" class title=\"moe evaluation\">\n<h1 id=\"小结\">小结</h1>\n<p>MiniCPM站在很多前人结果的肩膀上，把目前各种比较先进的做法融合到了1B/2B模型上，获得了不错的效果。其中用到的参数搜索、对scaling\nlaw的刷新都挺有参考价值。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">从loss视角理解大模型涌现能力</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLM的重复生成和ICL</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">大模型推理加速-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">大模型算法题(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】MiniCPM: Unveiling the Potential of Small Language Models with\nScalable Training Strategies https://arxiv.org/abs/2404.06395</p>\n","length":7850,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>MiniCPM是面壁智能和清华开源的模型，MiniCPM开源系列包括非embedding参数为1.2B和2.4B两个规模的模型，以及对应的MiniCPM-DPO，MiniCPM-MoE和MiniCPM-128K模型。</p>\n<p>简单梳理一下MiniCPM提到的一些内容。</p>\n<h1 id=\"背景\">背景</h1>\n<p>大模型的训练成本很高，而且很多机制还没搞清楚，训出来的大规模模型在很多设备上也跑不起来，因此现在有不少机构对小一点的模型，即SLM，进行更全面的探索，比如Phi系列、TinyLlama、MobileLLM和Gemma等。</p>\n<p>MiniCPM也是对SLM的一次探索，从中得到的经验也可以推广到更大的模型上。</p>\n<h1 id=\"风洞实验\">风洞实验</h1>\n<p>为了找到好的模型参数和训练参数，MiniCPM做了很多“风洞实验”（Model Wind\nTunnel Experiments）。</p>\n<p>这些风洞实验主要包括三个部分：（1）搜索模型结构的超参（2）探索batch\nsize的scaling（3）寻找最佳的learning rate。</p>\n<p>后续风洞实验所用的模型具体参数如下</p>\n<img src=\"/376db710/exp_model.png\" class title=\"模型结构\">\n<h2 id=\"模型超参\">模型超参</h2>\n<p>预训练资源消耗很大，即使是SLM也不可能把所有参数的排列组合都搜索一遍。</p>\n<p>这里参考Tensor Program的做法（《Tensor programs v: Tuning large\nneural networks via zero-shot hyperparameter transfer》和《Tensor\nprograms vi: Feature learning in infinite-depth neural\nnetworks》），分别对模型的宽度和深度进行了搜索。</p>\n<p>搜索所用的操作如下表所示，这里没有应用attention\nsoftmax的scaling技术。</p>\n<img src=\"/376db710/param_search_2.png\" class title=\"超参搜索\">\n<p>关于超参搜索的一些细节：<br>\n- 用Maximal Update Parametrization的方法进行了调参。<br>\n-\n在一系列预定义的参数空间上进行贝叶斯搜索，所用模型参数为在N=0.009B。发现使用规模为10N和20N大小的数据集进行超参数优化时，超参的有效性表现出了一致性。因此调参的过程就使用D=10N=0.09B个token来进行实验了。<br>\n- 应用了QK-Norm（《Querykey normalization for\ntransformers》）和independent weight decay（《Decoupled weight decay\nregularization》）之后，发现模型对learning\nrate的敏感性显著降低。不过在找到最佳learning\nrate之后，后面的训练就不用再调整参数了，因此后面的实验就没有继续使用QK-Norm和independent\nweight decay。</p>\n<p>最终从下图展示的参数搜索，确定了最佳的hyper-parameters为：<br>\n- scale depth = 1.4<br>\n- scale emb = 12<br>\n- init std = 0.1<br>\n- lr = 0.01</p>\n<img src=\"/376db710/param_search.png\" class title=\"超参搜索\">\n<h2 id=\"optimal-batch-size\">Optimal Batch Size</h2>\n<p>batch size决定了模型收敛速度与计算资源消耗之间的平衡。</p>\n<p>如果batch\nsize太大，会导致很大的数据和计算成本（才能跑到足够的update次数，让模型收敛）。如果batch\nsize太小，将会有大量的update step，并且相比大一些的batch\nsize，loss的减小很有限，训练效率太低。</p>\n<p>这里参考OpenAI的《Scaling laws for neural language\nmodels》的方法来寻找最佳的batch size，并做了一些改动。</p>\n<p>《Scaling laws for neural language models》研究的是loss\nfunction和token数之间的关系。他们假设了更多的step=更多的训练时间。在这个假设下，OpenAI定义了一个critical\nbatch\nsize，在不消耗过多的step或者token的情况下，能达到一定的loss水平。</p>\n<p>这在无限GPU资源下是合理的。由于GPU资源是无限的，增加batch\nsize不会增加单个step的耗时，但会减少总step数，因而提高了效率。但是实际上我们并没有无限GPU资源，因此将batch\nsize增大相当于增加的每个step的时间，所以实际上通过增加batch\nsize来减小step数，对总训练时间的影响并不大。</p>\n<p>因此MiniCPM放弃了“not consuming too many\nsteps”的目标，转而追求“minimizing the token quantity to achieve the\nlowest loss”。</p>\n<p>关于optimal batch\nsize与loss之间关系的估计，类似于“先有鸡还是先有蛋”的悖论，因为暂时没法完全搞清楚这二者之间的决定关系。目前的做法是，对于给定的模型大小，通常会有一个初步估计的achievable\nloss，这是由先前的初步实验得出的经验估计。</p>\n<p>而optimal batch size和optimal learning\nrate很可能并不是独立的。为了克服这种相关性，MiniCPM首先对learning\nrate进行了初步研究，然后选择一个最优learning rate来进行batch\nsize实验，并使用batch size缩放再次进行learning\nrate调整。这有点像Coordinate Descent optimization method。</p>\n<p>细节上，MiniCPM分别对0.009B、0.03B和0.17B的模型进行了实验。每个模型大小都在6种不同的batch\nsize上进行训练，使用了global learning rate=0.01和cosine learning rate\nscheduler。在C4数据集上，optimal batch size与loss的趋势如下图红线</p>\n<img src=\"/376db710/batch_size.png\" class title=\"超参搜索\">\n<p>这三条红线在log空间中很好连成一条直线，如下图。</p>\n<img src=\"/376db710/batch_size_2.png\" class title=\"超参搜索\">\n<p>这里就得到了在C4数据集上的训练loss和optimal batch size的关系。</p>\n<p><span class=\"math display\">\\[bs=\\frac{1.21\\times10^9}{L^{6.24}}\\]</span></p>\n<h2 id=\"optimal-learning-rate\">Optimal Learning Rate</h2>\n<p>由于使用了Tensor Program，optimal learning\nrate在模型缩放的过程中应该不会有明显变化。为了验证这一点，MiniCPM在0.04B、0.1B、0.3B和0.5B的模型上进行了六组learning\nrate的实验。</p>\n<p>在下图中，可以发现尽管模型大小增加了十倍，但optimal learning\nrate并没有明显的偏移，基本上一致保持在0.01左右。</p>\n<img src=\"/376db710/learning_rate.png\" class title=\"超参搜索\">\n<p>MiniCPM进一步在2.1B规模的模型上进行了一个简单的验证，最终确认了0.01的learning\nrate确实实现了最低loss。</p>\n<h1 id=\"wsd\">WSD</h1>\n<h2 id=\"cosine-learning-rate-scheduler的分析\">cosine learning rate\nscheduler的分析</h2>\n<p>cosine scheduler的周期很重要，一般是设置降到最小learning\nrate的时间T和预训练的总step数S持平。为了验证这个设置的效果，用0.036B的模型的做了实验，按以下公式分别实现cosine和cosine\nloop两种scheduler。</p>\n<img src=\"/376db710/cos_lr.png\" class title=\"LR\">\n<p>loss的变化如下图</p>\n<img src=\"/376db710/cos_loss.png\" class title=\"LR\">\n<p>可以看到确实总是T=S时效果最好。分析原因可能是：<br>\n- 与T&lt;S的scheduler相比，T=S的scheduler有更长的高learning\nrate持续时间。而这种高learning rate可能有助于模型找到更好的global\noptimum。<br>\n- 与T&gt;S的scheduler相比，T=S的scheduler有更彻底的learning rate\ndecay。这种衰减可能涉及到training dynamics，使模型能够找到更好的 local\noptimum。</p>\n<h2 id=\"warmup-stable-decay\">Warmup-Stable-Decay</h2>\n<p>基于上面的分析，MiniCPM把训练过程显式分成high learning rate\nstage和learning decay stage，这个scheduler就叫Warmup-Stable-Decay\nscheduler，公式如下</p>\n<p><span class=\"math display\">\\[\\left.WSD(T;s)=\\begin{cases}&amp;\\frac{s}{W}\\eta,\\quad\ns&lt;W\\\\&amp;\\eta,\\quad W&lt;s&lt;T\\\\&amp;f(s-T)\\eta,\\quad\nT&lt;s&lt;S\\end{cases}\\right.\\]</span></p>\n<p>其中W是warmup的step数，T是stable training step数，<span class=\"math inline\">\\(\\eta\\)</span> 是maximum learning rate，<span class=\"math inline\">\\(f\\left(s-T\\right)\\)</span> 是关于s的 decreasing\nfunction，取值在0到1之间。</p>\n<p>一般来说W只要足够，对训练的效果影响就不大，因此所有后面就忽略W了。</p>\n<p>继续做一些实验来探索WSD。</p>\n<p>（1）Loss Decreases Dramatically in Decay Stage</p>\n<p>首先在0.036B的模型上应用了WSD，并设置了不同的T和S（影响decay阶段的长度）。发现在decay阶段，随着learning\nrate的下降，loss出现了显著的快速下降，并迅速降低到等于或低于T=S时的Cosine\nLRS的loss，具体loss变化如下图</p>\n<img src=\"/376db710/wsd_exp1.png\" class title=\"WSD\">\n<p>由于stable\ntraining阶段learning是保持不变的，所以这里可以重用decay前的模型checkpoint，继续进行的高learning\nrate的训练。在原设置上增加了更多的stable training\nstep之后，还可以再进行learning rate退火，并且能够实现与Cosine\nLRS在同样step下相同的loss。这进一步验证了“训练阶段可以明确地分为stable阶段和decay阶段”的假设。</p>\n<p>（2）10% Steps are Enough</p>\n<p>如上图所示，在40N、60N和80N训练数据的实验中，使用总token数的10%的进行learning\nrate\ndecay就足以获得最好的结果，如果小于10%则效果会比较差。因此，在后续的训练实验中，都使用大约10%的step进行learning\nrate decay，以确保完全收敛。</p>\n<p>（3）Effective Data Scaling with WSD LRS</p>\n<p>使用WSD可以把模型训练到极致收敛的状态。为了展示WSD训练固定大小模型到收敛的潜力，MiniCPM对0.036B的模型进行持续训练，然后与使用40N数据的0.17B模型进行比较，loss如下图。</p>\n<img src=\"/376db710/wsd_exp2.png\" class title=\"WSD\">\n<p>0.036B模型在使用更多的数据后，超过Chinchilla\nOptimal，并且仍有收敛趋势，按这个趋势继续训练就能match\n0.17B模型的loss水平。</p>\n<h2 id=\"measuring-the-scaling-law-with-wsd-lrs\">Measuring the Scaling\nLaw with WSD LRS</h2>\n<p>利用WSD，可以把探索model size和data\nsize的scaling关系的成本变成线性，因为stable\nstage阶段learning保持不变，可以把decay接在不同的step后面来获取不同数据量下的效果。</p>\n<p>通过训练从0.04B到2B共6种大小的SLM来测量scaling\nlaw。每种大小的模型都有从10N到60N数据共6个数据量开始decay的结果。</p>\n<p>这36个模型的训练结果在5个数据集上进行比较。为了可以比较不同tokenizer的模型的损失，按《GPT-4\ntechnical\nreport》里的做法，使用byte数的平均而非token数的平均来进行比较。然后用scipy\ncurvefit function，按下面这个公式拟合model size N and data size\nD的关系。</p>\n<p><span class=\"math display\">\\[L(N,D)=C_NN^{-\\alpha}+C_DD^{-\\beta}+L_0\\]</span></p>\n<p>实验结果和拟合结果如下图</p>\n<img src=\"/376db710/scaling_law.png\" class title=\"scaling law\">\n<p>然后参照《Scaling language models: Methods, analysis &amp; insights\nfrom training gopher》、《Training compute-optimal large language\nmodels》、《Scaling laws for neural language\nmodels》的做法，推算出token数量应该是模型参数量的192倍，这比《Training\ncompute-optimal large language models》中给出的20倍要大得多。</p>\n<p>MiniCPM还把LLAMA2的数据拿出来进行了验证。按LLAMA2报告中给出的数据计算出的token数应是模型参数量的70~100倍，这个值同样比20要大很多。</p>\n<p>因此结论是，按照基于WSD的实验结果，语言模型比我们之前想象的可以吸收更多语料数据。</p>\n<h1 id=\"two-stage-pre-training-strategy\">Two Stage Pre-training\nStrategy</h1>\n<p>前面观察到WSD的衰减阶段loss有显著的减少，因此MiniCPM认为在learning\nrate的退火阶段整合高质量SFT数据，混合进预训练数据中可以SFT效果：<br>\n- 一方面，在退火阶段使用SFT数据能获得预SFT更相关的loss下降<br>\n- 另一方面，和在整个预训练阶段都使用SFT数据相比，只在learning rate\ndecay阶段使用更不容易过拟合</p>\n<p>为了验证这个猜想，设计以下训练配置：<br>\n- A-1: 2.4B模型，decay阶段仅用无标签数据，之后进行4B的SFT训练<br>\n- A-2:\n2.4B模型，decay阶段使用无标签数据+SFT数据，之后进行4B的SFT训练<br>\n- B-1: 1.2B模型，decay阶段仅用无标签数据，之后进行6B的SFT训练<br>\n- B-2: 1.2B模型，decay阶段仅用无标签数据，之后进行12B的SFT训练<br>\n- B-3:\n1.2B模型，decay阶段使用无标签数据+SFT数据，之后进行6B的SFT训练</p>\n<p>各个模型预训练+SFT之后的效果如下</p>\n<img src=\"/376db710/2_stage.png\" class title=\"2阶段训练\">\n<p>可以看到在预训练learning rate退火阶段加入SFT数据的模型效果更好。</p>\n<h1 id=\"minicpm\">MiniCPM</h1>\n<h2 id=\"模型\">模型</h2>\n<p>MiniCPM有2.4B和1.2B两个规模。其中2.4B模型的词表大小为122,753，1.2B模型词表大小为73,440，都是通过BPE进行构建。在测试数据集上评测，MiniCPM的tokenizer的效率是比较高的，具体数值如下</p>\n<img src=\"/376db710/tokenizer.png\" class title=\"tokenizer\">\n<p>MiniCPM模型的输入输出共享了矩阵，因为小模型共享输入输出矩阵可以节省很多参数。</p>\n<p>在层数和hidden\nstate的设计上，MiniCPM使用了相比Phi-2等SLM更深更瘦的模型结构，这和《Mobilellm:\nOptimizing sub-billion parameter language models for on-device use\ncases》的想法一致。具体的结构参数如下</p>\n<img src=\"/376db710/layers.png\" class title=\"更深更瘦的结构\">\n<p>1.2B模型上使用了GQA，可以进一步节省参数量。</p>\n<h2 id=\"训练\">训练</h2>\n<p>在WSD的stable阶段，使用1T预训练数据，batch size=3.93M，max\nlr=0.01。</p>\n<p>在decay阶段，decay的策略为 <span class=\"math inline\">\\(f(s-T)=0.5^{(s-S)/T}\\)</span>，其中T=5000 steps\n(20B tokens)。</p>\n<p>SFT阶段共使用了6B数据，learning rate和预训练阶段结束时的learning\nrate对齐，同样使用了WSD。</p>\n<p>预训练数据的分布如下</p>\n<img src=\"/376db710/data.png\" class title=\"训练数据\">\n<p>1.2B和2.4B模型的预训练loss如下图</p>\n<img src=\"/376db710/train_loss.png\" class title=\"training loss\">\n<p>左图loss的第一次突变是因为增大了batch size，效果相当于减小了learning\nrate。</p>\n<p>最终SFT模型在下游任务的评测结果如下</p>\n<img src=\"/376db710/eval.png\" class title=\"evaluation\">\n<h2 id=\"minicpm-dpo\">MiniCPM-DPO</h2>\n<p>在SFT的基础上，MiniCPM用UltraFeedback数据集进行DPO训练。</p>\n<p>DPO训练使用了Cosine LRS, max learning\nrate=1e-5，一共训练了一个epoch。</p>\n<p>DPO使得模型在MT-bench上的得分从6.89提升到7.25，但是在原来通用benchmark的效果有所下降。</p>\n<h2 id=\"minicpm-128k\">MiniCPM-128k</h2>\n<p>长文本训练把MiniCPM支持的窗口大小从4k拓展到128k。在这一阶段的训练禁用了输入输出矩阵共享，这会使得模型的实际参数略有上升。训练的初始模型用的是预训练中stable阶段的最后一个checkpoint。</p>\n<p>MiniCPM将书籍、维基百科文章和论文分类为“长数据”，其他为“短数据”。那么在这一阶段的训练包含了44%的长数据和\n56%的短数据。</p>\n<p>训练时不直接训练到128k，而是使用curriculum\nlearning：先训练32k，再训练128k。4k-32k范围内应用ABF，32K到128K的范围内使用NTK-Aware\nRoPE scaling。</p>\n<p>如Yi的技术报告和《Zebra: Extending context window with layerwise\ngrouped local-global\nattention》所指出的那样，使用合成的长QA数据，有助于提高模型在上下文感知任务中的性能，MiniCPM也使用了合成的长QA数据。</p>\n<p>MiniCPM-128k在∞Bench（《∞bench: Extending long context evaluation\nbeyond 100k tokens》）上评测结果如下</p>\n<img src=\"/376db710/128k_result.png\" class title=\"128k evaluation\">\n<h2 id=\"minicpm-moe\">MiniCPM-MoE</h2>\n<p>MiniCPM-MoE使用Sparse Upcycling（《Sparse upcycling: Training\nmixture-of-experts from dense\ncheckpoints》）进行初始化，使用了stable阶段的checkpoint。router用均值为0、方差为0.01的正态分布进行初始化。</p>\n<p>MiniCPM-MoE共有13.6B参数，激活2个专家，共激活4B参数。</p>\n<p>训练时使用switch transformer的负载均衡函数，权重系数为0.01。</p>\n<p>learning rate使用了WSD，在4M的batch\nsize下共进行了130k步预训练，而在SFT阶段batch size减小了到2M。</p>\n<p>MiniCPM-MoE的效果评测如下</p>\n<img src=\"/376db710/moe_result.png\" class title=\"moe evaluation\">\n<h1 id=\"小结\">小结</h1>\n<p>MiniCPM站在很多前人结果的肩膀上，把目前各种比较先进的做法融合到了1B/2B模型上，获得了不错的效果。其中用到的参数搜索、对scaling\nlaw的刷新都挺有参考价值。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">从loss视角理解大模型涌现能力</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLM的重复生成和ICL</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">大模型推理加速-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">大模型算法题(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】MiniCPM: Unveiling the Potential of Small Language Models with\nScalable Training Strategies https://arxiv.org/abs/2404.06395</p>\n"},{"title":"RoPE的远距离衰减","abbrlink":"f0902f1a","date":"2024-06-25T11:12:38.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n有朋友问到了关于RoPE远距离衰减的问题，这里给出几个示例，提供一个直观理解的视角。  \n\n之前对RoPE的梳理参考 [理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)。  \n\n# 公式  \n\n回顾一下RoPE的实现。RoPE通过在q和k上分别乘一个旋转矩阵，实现了相对距离编码的功能。  \n\n对于position为m的q或者k，旋转矩阵如下  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\n实际实现时，高效率的实现如下  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\n也可以让第二项保持输入向量的元素位置，变成\n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\-\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\-\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\-\\sin m\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\nhuggingface的实现中预先把各个位置的cos额sin向量都计算好了，可以重复利用，这样看后面这样实现的效率会更高一点。  \n\n# 远距离衰减  \n\n远距离衰减指的是随着q和k的相对距离的增大，加入位置编码之后的内积应该随着距离增大而减小，这样相当于离得远的token分配到的attention会比较小，而离得近的token会得到更多的注意力。  \n\n这样的特性确实直觉上比较符合人类的注意力机制。  \n\n把各个参数（base、window size、head size）下的内积值画出来看看是怎么衰减的。实现参考下面的代码。这里偷懒没有实现得很高效，勉强能用就行。  \n\n```python  \n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef apply_rope(input_vec, position, base=10000):\n    # 获取维度\n    d = input_vec.shape[0]\n    \n    # 获取theta\n    i = np.arange(1, d // 2 + 1)\n    theta = base ** (-2 * (i - 1) / d)\n    theta = np.repeat(theta, 2)\n    \n    # 计算旋转后的向量\n    reranged_vec = np.empty_like(input_vec)\n    reranged_vec[0::2] = -input_vec[1::2]\n    reranged_vec[1::2] = input_vec[:-1:2]\n    output_vec = input_vec * np.cos(position * theta) + reranged_vec * np.sin(position * theta)\n    \n    return output_vec\n\n\ndef plot(x, y, name=''):\n    plt.plot(x, y, label=name)\n    plt.legend()\n    # 显示图表\n    plt.show()\n    \nbase = 10000\nwindow_size = 4096\nd = 512\n\nq = np.ones(d)\nk = np.ones(d)\n\nrotated_q = apply_rope(input_vec=q, position=0, base=base)\n\ninner_products = []\nfor i in range(window_size):\n    rotated_k = apply_rope(input_vec=k, position=i, base=base)\n    product = np.dot(rotated_q, rotated_k)\n    inner_products.append(product)\n    \nplot(x=range(window_size), y=inner_products, name=f'base={base},window size={window_size},d={d}')\n\n```  \n\n（1）q = k = 1  \n\n假设q和k都是1向量，如果q在位置0，画出k在0~4096位置下和q在位置编码后的内积如下。  \n\n{% asset_img 1.png 衰减 %}  \n\n这里使用了base=10000，d=512。  \n\n可以看到整体趋势是震荡下降的\n\n不过如果把窗口从4096增大到65536，图像会变成这样  \n\n{% asset_img 2.png 衰减 %}  \n\n可以看到图像不再是单纯的衰减，在距离超过大约15000的时候，出现了上升。  \n\n实际上这个包含多个周期函数的内积也具有一定的周期性，并不是在整个域上保持衰减的特性。只要相对距离够大，超过这个周期的1/4，内积就会再次上升。  \n\n而这个内积的周期受base调控，base越大，周期越长，因此现在的长窗口模型起步就是base=5M或者10M。  \n\n我们把base改成5M，图像如下  \n\n{% asset_img 3.png 衰减 %}  \n\n又呈现了震荡衰减的趋势。  \n\n前面画的是q在位置0，k在0~4096/65536的情况，那么把q放在中间看看内积结果怎么样。  \n\n{% asset_img 4.png 衰减 %}  \n\n可以看到在q两边的内积是对称的，同样的远距离衰减属性。  \n\n（2）q、k随机  \n\n前面是把q和k固定为1向量，现在试着把q和k初始化为随机向量，图像如下\n\n{% asset_img 5.png 衰减 %}  \n\n相比1向量出现了更多的震荡，但是大体上还是能保持一定的远距离衰减特性。  \n\n# 小结  \n\n- RoPE的远距离衰减是震荡的，并且整个内积本身也具有一定的周期性，只有把base设得足够大，才能让内积结果在模型窗口大小内保持远距离衰减的特性。  \n- 在q和k的相对距离小的时候，内积差距较大，也就是衰减较快；到了远距离之后，衰减变慢，也就是从内积角度来看，分辨率会变小。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLM的重复生成和ICL](https://www.linsight.cn/7381cae3.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n[大模型算法题(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n【1】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265  \n【2】RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n【3】理解LLM位置编码:RoPE http://www.linsight.cn/c4da56c0.html  \n","source":"_posts/cs/nlp/2024/06/RoPE的远距离衰减.md","raw":"---\ntitle: RoPE的远距离衰减\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - positional encoding\n  - RoPE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: f0902f1a\ndate: 2024-06-25 19:12:38\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n有朋友问到了关于RoPE远距离衰减的问题，这里给出几个示例，提供一个直观理解的视角。  \n\n之前对RoPE的梳理参考 [理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)。  \n\n# 公式  \n\n回顾一下RoPE的实现。RoPE通过在q和k上分别乘一个旋转矩阵，实现了相对距离编码的功能。  \n\n对于position为m的q或者k，旋转矩阵如下  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\n实际实现时，高效率的实现如下  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\n也可以让第二项保持输入向量的元素位置，变成\n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\-\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\-\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\-\\sin m\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\nhuggingface的实现中预先把各个位置的cos额sin向量都计算好了，可以重复利用，这样看后面这样实现的效率会更高一点。  \n\n# 远距离衰减  \n\n远距离衰减指的是随着q和k的相对距离的增大，加入位置编码之后的内积应该随着距离增大而减小，这样相当于离得远的token分配到的attention会比较小，而离得近的token会得到更多的注意力。  \n\n这样的特性确实直觉上比较符合人类的注意力机制。  \n\n把各个参数（base、window size、head size）下的内积值画出来看看是怎么衰减的。实现参考下面的代码。这里偷懒没有实现得很高效，勉强能用就行。  \n\n```python  \n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef apply_rope(input_vec, position, base=10000):\n    # 获取维度\n    d = input_vec.shape[0]\n    \n    # 获取theta\n    i = np.arange(1, d // 2 + 1)\n    theta = base ** (-2 * (i - 1) / d)\n    theta = np.repeat(theta, 2)\n    \n    # 计算旋转后的向量\n    reranged_vec = np.empty_like(input_vec)\n    reranged_vec[0::2] = -input_vec[1::2]\n    reranged_vec[1::2] = input_vec[:-1:2]\n    output_vec = input_vec * np.cos(position * theta) + reranged_vec * np.sin(position * theta)\n    \n    return output_vec\n\n\ndef plot(x, y, name=''):\n    plt.plot(x, y, label=name)\n    plt.legend()\n    # 显示图表\n    plt.show()\n    \nbase = 10000\nwindow_size = 4096\nd = 512\n\nq = np.ones(d)\nk = np.ones(d)\n\nrotated_q = apply_rope(input_vec=q, position=0, base=base)\n\ninner_products = []\nfor i in range(window_size):\n    rotated_k = apply_rope(input_vec=k, position=i, base=base)\n    product = np.dot(rotated_q, rotated_k)\n    inner_products.append(product)\n    \nplot(x=range(window_size), y=inner_products, name=f'base={base},window size={window_size},d={d}')\n\n```  \n\n（1）q = k = 1  \n\n假设q和k都是1向量，如果q在位置0，画出k在0~4096位置下和q在位置编码后的内积如下。  \n\n{% asset_img 1.png 衰减 %}  \n\n这里使用了base=10000，d=512。  \n\n可以看到整体趋势是震荡下降的\n\n不过如果把窗口从4096增大到65536，图像会变成这样  \n\n{% asset_img 2.png 衰减 %}  \n\n可以看到图像不再是单纯的衰减，在距离超过大约15000的时候，出现了上升。  \n\n实际上这个包含多个周期函数的内积也具有一定的周期性，并不是在整个域上保持衰减的特性。只要相对距离够大，超过这个周期的1/4，内积就会再次上升。  \n\n而这个内积的周期受base调控，base越大，周期越长，因此现在的长窗口模型起步就是base=5M或者10M。  \n\n我们把base改成5M，图像如下  \n\n{% asset_img 3.png 衰减 %}  \n\n又呈现了震荡衰减的趋势。  \n\n前面画的是q在位置0，k在0~4096/65536的情况，那么把q放在中间看看内积结果怎么样。  \n\n{% asset_img 4.png 衰减 %}  \n\n可以看到在q两边的内积是对称的，同样的远距离衰减属性。  \n\n（2）q、k随机  \n\n前面是把q和k固定为1向量，现在试着把q和k初始化为随机向量，图像如下\n\n{% asset_img 5.png 衰减 %}  \n\n相比1向量出现了更多的震荡，但是大体上还是能保持一定的远距离衰减特性。  \n\n# 小结  \n\n- RoPE的远距离衰减是震荡的，并且整个内积本身也具有一定的周期性，只有把base设得足够大，才能让内积结果在模型窗口大小内保持远距离衰减的特性。  \n- 在q和k的相对距离小的时候，内积差距较大，也就是衰减较快；到了远距离之后，衰减变慢，也就是从内积角度来看，分辨率会变小。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLM的重复生成和ICL](https://www.linsight.cn/7381cae3.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n[大模型算法题(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n【1】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265  \n【2】RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n【3】理解LLM位置编码:RoPE http://www.linsight.cn/c4da56c0.html  \n","slug":"cs/nlp/2024/06/RoPE的远距离衰减","published":1,"updated":"2024-06-26T02:59:16.930Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9v000d314kfap0aerf","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>有朋友问到了关于RoPE远距离衰减的问题，这里给出几个示例，提供一个直观理解的视角。</p>\n<p>之前对RoPE的梳理参考 <a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a>。</p>\n<h1 id=\"公式\">公式</h1>\n<p>回顾一下RoPE的实现。RoPE通过在q和k上分别乘一个旋转矩阵，实现了相对距离编码的功能。</p>\n<p>对于position为m的q或者k，旋转矩阵如下</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p>实际实现时，高效率的实现如下</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p>也可以让第二项保持输入向量的元素位置，变成</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\-\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\-\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\-\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p>huggingface的实现中预先把各个位置的cos额sin向量都计算好了，可以重复利用，这样看后面这样实现的效率会更高一点。</p>\n<h1 id=\"远距离衰减\">远距离衰减</h1>\n<p>远距离衰减指的是随着q和k的相对距离的增大，加入位置编码之后的内积应该随着距离增大而减小，这样相当于离得远的token分配到的attention会比较小，而离得近的token会得到更多的注意力。</p>\n<p>这样的特性确实直觉上比较符合人类的注意力机制。</p>\n<p>把各个参数（base、window size、head\nsize）下的内积值画出来看看是怎么衰减的。实现参考下面的代码。这里偷懒没有实现得很高效，勉强能用就行。</p>\n<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span id=\"cb1-1\"><a href=\"#cb1-1\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-2\"><a href=\"#cb1-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> random</span>\n<span id=\"cb1-3\"><a href=\"#cb1-3\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> numpy <span class=\"im\">as</span> np</span>\n<span id=\"cb1-4\"><a href=\"#cb1-4\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> matplotlib.pyplot <span class=\"im\">as</span> plt</span>\n<span id=\"cb1-5\"><a href=\"#cb1-5\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-6\"><a href=\"#cb1-6\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"kw\">def</span> apply_rope(input_vec, position, base<span class=\"op\">=</span><span class=\"dv\">10000</span>):</span>\n<span id=\"cb1-7\"><a href=\"#cb1-7\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># 获取维度</span></span>\n<span id=\"cb1-8\"><a href=\"#cb1-8\" aria-hidden=\"true\" tabindex=\"-1\"></a>    d <span class=\"op\">=</span> input_vec.shape[<span class=\"dv\">0</span>]</span>\n<span id=\"cb1-9\"><a href=\"#cb1-9\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-10\"><a href=\"#cb1-10\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># 获取theta</span></span>\n<span id=\"cb1-11\"><a href=\"#cb1-11\" aria-hidden=\"true\" tabindex=\"-1\"></a>    i <span class=\"op\">=</span> np.arange(<span class=\"dv\">1</span>, d <span class=\"op\">//</span> <span class=\"dv\">2</span> <span class=\"op\">+</span> <span class=\"dv\">1</span>)</span>\n<span id=\"cb1-12\"><a href=\"#cb1-12\" aria-hidden=\"true\" tabindex=\"-1\"></a>    theta <span class=\"op\">=</span> base <span class=\"op\">**</span> (<span class=\"op\">-</span><span class=\"dv\">2</span> <span class=\"op\">*</span> (i <span class=\"op\">-</span> <span class=\"dv\">1</span>) <span class=\"op\">/</span> d)</span>\n<span id=\"cb1-13\"><a href=\"#cb1-13\" aria-hidden=\"true\" tabindex=\"-1\"></a>    theta <span class=\"op\">=</span> np.repeat(theta, <span class=\"dv\">2</span>)</span>\n<span id=\"cb1-14\"><a href=\"#cb1-14\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-15\"><a href=\"#cb1-15\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># 计算旋转后的向量</span></span>\n<span id=\"cb1-16\"><a href=\"#cb1-16\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec <span class=\"op\">=</span> np.empty_like(input_vec)</span>\n<span id=\"cb1-17\"><a href=\"#cb1-17\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec[<span class=\"dv\">0</span>::<span class=\"dv\">2</span>] <span class=\"op\">=</span> <span class=\"op\">-</span>input_vec[<span class=\"dv\">1</span>::<span class=\"dv\">2</span>]</span>\n<span id=\"cb1-18\"><a href=\"#cb1-18\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec[<span class=\"dv\">1</span>::<span class=\"dv\">2</span>] <span class=\"op\">=</span> input_vec[:<span class=\"op\">-</span><span class=\"dv\">1</span>:<span class=\"dv\">2</span>]</span>\n<span id=\"cb1-19\"><a href=\"#cb1-19\" aria-hidden=\"true\" tabindex=\"-1\"></a>    output_vec <span class=\"op\">=</span> input_vec <span class=\"op\">*</span> np.cos(position <span class=\"op\">*</span> theta) <span class=\"op\">+</span> reranged_vec <span class=\"op\">*</span> np.sin(position <span class=\"op\">*</span> theta)</span>\n<span id=\"cb1-20\"><a href=\"#cb1-20\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-21\"><a href=\"#cb1-21\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"cf\">return</span> output_vec</span>\n<span id=\"cb1-22\"><a href=\"#cb1-22\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-23\"><a href=\"#cb1-23\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-24\"><a href=\"#cb1-24\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"kw\">def</span> plot(x, y, name<span class=\"op\">=</span><span class=\"st\">&#39;&#39;</span>):</span>\n<span id=\"cb1-25\"><a href=\"#cb1-25\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.plot(x, y, label<span class=\"op\">=</span>name)</span>\n<span id=\"cb1-26\"><a href=\"#cb1-26\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.legend()</span>\n<span id=\"cb1-27\"><a href=\"#cb1-27\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># 显示图表</span></span>\n<span id=\"cb1-28\"><a href=\"#cb1-28\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.show()</span>\n<span id=\"cb1-29\"><a href=\"#cb1-29\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-30\"><a href=\"#cb1-30\" aria-hidden=\"true\" tabindex=\"-1\"></a>base <span class=\"op\">=</span> <span class=\"dv\">10000</span></span>\n<span id=\"cb1-31\"><a href=\"#cb1-31\" aria-hidden=\"true\" tabindex=\"-1\"></a>window_size <span class=\"op\">=</span> <span class=\"dv\">4096</span></span>\n<span id=\"cb1-32\"><a href=\"#cb1-32\" aria-hidden=\"true\" tabindex=\"-1\"></a>d <span class=\"op\">=</span> <span class=\"dv\">512</span></span>\n<span id=\"cb1-33\"><a href=\"#cb1-33\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-34\"><a href=\"#cb1-34\" aria-hidden=\"true\" tabindex=\"-1\"></a>q <span class=\"op\">=</span> np.ones(d)</span>\n<span id=\"cb1-35\"><a href=\"#cb1-35\" aria-hidden=\"true\" tabindex=\"-1\"></a>k <span class=\"op\">=</span> np.ones(d)</span>\n<span id=\"cb1-36\"><a href=\"#cb1-36\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-37\"><a href=\"#cb1-37\" aria-hidden=\"true\" tabindex=\"-1\"></a>rotated_q <span class=\"op\">=</span> apply_rope(input_vec<span class=\"op\">=</span>q, position<span class=\"op\">=</span><span class=\"dv\">0</span>, base<span class=\"op\">=</span>base)</span>\n<span id=\"cb1-38\"><a href=\"#cb1-38\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-39\"><a href=\"#cb1-39\" aria-hidden=\"true\" tabindex=\"-1\"></a>inner_products <span class=\"op\">=</span> []</span>\n<span id=\"cb1-40\"><a href=\"#cb1-40\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(window_size):</span>\n<span id=\"cb1-41\"><a href=\"#cb1-41\" aria-hidden=\"true\" tabindex=\"-1\"></a>    rotated_k <span class=\"op\">=</span> apply_rope(input_vec<span class=\"op\">=</span>k, position<span class=\"op\">=</span>i, base<span class=\"op\">=</span>base)</span>\n<span id=\"cb1-42\"><a href=\"#cb1-42\" aria-hidden=\"true\" tabindex=\"-1\"></a>    product <span class=\"op\">=</span> np.dot(rotated_q, rotated_k)</span>\n<span id=\"cb1-43\"><a href=\"#cb1-43\" aria-hidden=\"true\" tabindex=\"-1\"></a>    inner_products.append(product)</span>\n<span id=\"cb1-44\"><a href=\"#cb1-44\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-45\"><a href=\"#cb1-45\" aria-hidden=\"true\" tabindex=\"-1\"></a>plot(x<span class=\"op\">=</span><span class=\"bu\">range</span>(window_size), y<span class=\"op\">=</span>inner_products, name<span class=\"op\">=</span><span class=\"ss\">f&#39;base=</span><span class=\"sc\">&#123;</span>base<span class=\"sc\">&#125;</span><span class=\"ss\">,window size=</span><span class=\"sc\">&#123;</span>window_size<span class=\"sc\">&#125;</span><span class=\"ss\">,d=</span><span class=\"sc\">&#123;</span>d<span class=\"sc\">&#125;</span><span class=\"ss\">&#39;</span>)</span></code></pre></div>\n<p>（1）q = k = 1</p>\n<p>假设q和k都是1向量，如果q在位置0，画出k在0~4096位置下和q在位置编码后的内积如下。</p>\n<img src=\"/f0902f1a/1.png\" class title=\"衰减\">\n<p>这里使用了base=10000，d=512。</p>\n<p>可以看到整体趋势是震荡下降的</p>\n<p>不过如果把窗口从4096增大到65536，图像会变成这样</p>\n<img src=\"/f0902f1a/2.png\" class title=\"衰减\">\n<p>可以看到图像不再是单纯的衰减，在距离超过大约15000的时候，出现了上升。</p>\n<p>实际上这个包含多个周期函数的内积也具有一定的周期性，并不是在整个域上保持衰减的特性。只要相对距离够大，超过这个周期的1/4，内积就会再次上升。</p>\n<p>而这个内积的周期受base调控，base越大，周期越长，因此现在的长窗口模型起步就是base=5M或者10M。</p>\n<p>我们把base改成5M，图像如下</p>\n<img src=\"/f0902f1a/3.png\" class title=\"衰减\">\n<p>又呈现了震荡衰减的趋势。</p>\n<p>前面画的是q在位置0，k在0~4096/65536的情况，那么把q放在中间看看内积结果怎么样。</p>\n<img src=\"/f0902f1a/4.png\" class title=\"衰减\">\n<p>可以看到在q两边的内积是对称的，同样的远距离衰减属性。</p>\n<p>（2）q、k随机</p>\n<p>前面是把q和k固定为1向量，现在试着把q和k初始化为随机向量，图像如下</p>\n<img src=\"/f0902f1a/5.png\" class title=\"衰减\">\n<p>相比1向量出现了更多的震荡，但是大体上还是能保持一定的远距离衰减特性。</p>\n<h1 id=\"小结\">小结</h1>\n<ul>\n<li>RoPE的远距离衰减是震荡的，并且整个内积本身也具有一定的周期性，只有把base设得足够大，才能让内积结果在模型窗口大小内保持远距离衰减的特性。<br>\n</li>\n<li>在q和k的相对距离小的时候，内积差距较大，也就是衰减较快；到了远距离之后，衰减变慢，也就是从内积角度来看，分辨率会变小。</li>\n</ul>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">从loss视角理解大模型涌现能力</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">大模型推理加速-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLM的重复生成和ICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">大模型算法题(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265<br>\n【2】RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n【3】理解LLM位置编码:RoPE http://www.linsight.cn/c4da56c0.html</p>\n","length":4209,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>有朋友问到了关于RoPE远距离衰减的问题，这里给出几个示例，提供一个直观理解的视角。</p>\n<p>之前对RoPE的梳理参考 <a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a>。</p>\n<h1 id=\"公式\">公式</h1>\n<p>回顾一下RoPE的实现。RoPE通过在q和k上分别乘一个旋转矩阵，实现了相对距离编码的功能。</p>\n<p>对于position为m的q或者k，旋转矩阵如下</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p>实际实现时，高效率的实现如下</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p>也可以让第二项保持输入向量的元素位置，变成</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\-\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\-\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\-\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p>huggingface的实现中预先把各个位置的cos额sin向量都计算好了，可以重复利用，这样看后面这样实现的效率会更高一点。</p>\n<h1 id=\"远距离衰减\">远距离衰减</h1>\n<p>远距离衰减指的是随着q和k的相对距离的增大，加入位置编码之后的内积应该随着距离增大而减小，这样相当于离得远的token分配到的attention会比较小，而离得近的token会得到更多的注意力。</p>\n<p>这样的特性确实直觉上比较符合人类的注意力机制。</p>\n<p>把各个参数（base、window size、head\nsize）下的内积值画出来看看是怎么衰减的。实现参考下面的代码。这里偷懒没有实现得很高效，勉强能用就行。</p>\n<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span id=\"cb1-1\"><a href=\"#cb1-1\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-2\"><a href=\"#cb1-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> random</span>\n<span id=\"cb1-3\"><a href=\"#cb1-3\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> numpy <span class=\"im\">as</span> np</span>\n<span id=\"cb1-4\"><a href=\"#cb1-4\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> matplotlib.pyplot <span class=\"im\">as</span> plt</span>\n<span id=\"cb1-5\"><a href=\"#cb1-5\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-6\"><a href=\"#cb1-6\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"kw\">def</span> apply_rope(input_vec, position, base<span class=\"op\">=</span><span class=\"dv\">10000</span>):</span>\n<span id=\"cb1-7\"><a href=\"#cb1-7\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># 获取维度</span></span>\n<span id=\"cb1-8\"><a href=\"#cb1-8\" aria-hidden=\"true\" tabindex=\"-1\"></a>    d <span class=\"op\">=</span> input_vec.shape[<span class=\"dv\">0</span>]</span>\n<span id=\"cb1-9\"><a href=\"#cb1-9\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-10\"><a href=\"#cb1-10\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># 获取theta</span></span>\n<span id=\"cb1-11\"><a href=\"#cb1-11\" aria-hidden=\"true\" tabindex=\"-1\"></a>    i <span class=\"op\">=</span> np.arange(<span class=\"dv\">1</span>, d <span class=\"op\">//</span> <span class=\"dv\">2</span> <span class=\"op\">+</span> <span class=\"dv\">1</span>)</span>\n<span id=\"cb1-12\"><a href=\"#cb1-12\" aria-hidden=\"true\" tabindex=\"-1\"></a>    theta <span class=\"op\">=</span> base <span class=\"op\">**</span> (<span class=\"op\">-</span><span class=\"dv\">2</span> <span class=\"op\">*</span> (i <span class=\"op\">-</span> <span class=\"dv\">1</span>) <span class=\"op\">/</span> d)</span>\n<span id=\"cb1-13\"><a href=\"#cb1-13\" aria-hidden=\"true\" tabindex=\"-1\"></a>    theta <span class=\"op\">=</span> np.repeat(theta, <span class=\"dv\">2</span>)</span>\n<span id=\"cb1-14\"><a href=\"#cb1-14\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-15\"><a href=\"#cb1-15\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># 计算旋转后的向量</span></span>\n<span id=\"cb1-16\"><a href=\"#cb1-16\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec <span class=\"op\">=</span> np.empty_like(input_vec)</span>\n<span id=\"cb1-17\"><a href=\"#cb1-17\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec[<span class=\"dv\">0</span>::<span class=\"dv\">2</span>] <span class=\"op\">=</span> <span class=\"op\">-</span>input_vec[<span class=\"dv\">1</span>::<span class=\"dv\">2</span>]</span>\n<span id=\"cb1-18\"><a href=\"#cb1-18\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec[<span class=\"dv\">1</span>::<span class=\"dv\">2</span>] <span class=\"op\">=</span> input_vec[:<span class=\"op\">-</span><span class=\"dv\">1</span>:<span class=\"dv\">2</span>]</span>\n<span id=\"cb1-19\"><a href=\"#cb1-19\" aria-hidden=\"true\" tabindex=\"-1\"></a>    output_vec <span class=\"op\">=</span> input_vec <span class=\"op\">*</span> np.cos(position <span class=\"op\">*</span> theta) <span class=\"op\">+</span> reranged_vec <span class=\"op\">*</span> np.sin(position <span class=\"op\">*</span> theta)</span>\n<span id=\"cb1-20\"><a href=\"#cb1-20\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-21\"><a href=\"#cb1-21\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"cf\">return</span> output_vec</span>\n<span id=\"cb1-22\"><a href=\"#cb1-22\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-23\"><a href=\"#cb1-23\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-24\"><a href=\"#cb1-24\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"kw\">def</span> plot(x, y, name<span class=\"op\">=</span><span class=\"st\">&#39;&#39;</span>):</span>\n<span id=\"cb1-25\"><a href=\"#cb1-25\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.plot(x, y, label<span class=\"op\">=</span>name)</span>\n<span id=\"cb1-26\"><a href=\"#cb1-26\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.legend()</span>\n<span id=\"cb1-27\"><a href=\"#cb1-27\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># 显示图表</span></span>\n<span id=\"cb1-28\"><a href=\"#cb1-28\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.show()</span>\n<span id=\"cb1-29\"><a href=\"#cb1-29\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-30\"><a href=\"#cb1-30\" aria-hidden=\"true\" tabindex=\"-1\"></a>base <span class=\"op\">=</span> <span class=\"dv\">10000</span></span>\n<span id=\"cb1-31\"><a href=\"#cb1-31\" aria-hidden=\"true\" tabindex=\"-1\"></a>window_size <span class=\"op\">=</span> <span class=\"dv\">4096</span></span>\n<span id=\"cb1-32\"><a href=\"#cb1-32\" aria-hidden=\"true\" tabindex=\"-1\"></a>d <span class=\"op\">=</span> <span class=\"dv\">512</span></span>\n<span id=\"cb1-33\"><a href=\"#cb1-33\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-34\"><a href=\"#cb1-34\" aria-hidden=\"true\" tabindex=\"-1\"></a>q <span class=\"op\">=</span> np.ones(d)</span>\n<span id=\"cb1-35\"><a href=\"#cb1-35\" aria-hidden=\"true\" tabindex=\"-1\"></a>k <span class=\"op\">=</span> np.ones(d)</span>\n<span id=\"cb1-36\"><a href=\"#cb1-36\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-37\"><a href=\"#cb1-37\" aria-hidden=\"true\" tabindex=\"-1\"></a>rotated_q <span class=\"op\">=</span> apply_rope(input_vec<span class=\"op\">=</span>q, position<span class=\"op\">=</span><span class=\"dv\">0</span>, base<span class=\"op\">=</span>base)</span>\n<span id=\"cb1-38\"><a href=\"#cb1-38\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-39\"><a href=\"#cb1-39\" aria-hidden=\"true\" tabindex=\"-1\"></a>inner_products <span class=\"op\">=</span> []</span>\n<span id=\"cb1-40\"><a href=\"#cb1-40\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(window_size):</span>\n<span id=\"cb1-41\"><a href=\"#cb1-41\" aria-hidden=\"true\" tabindex=\"-1\"></a>    rotated_k <span class=\"op\">=</span> apply_rope(input_vec<span class=\"op\">=</span>k, position<span class=\"op\">=</span>i, base<span class=\"op\">=</span>base)</span>\n<span id=\"cb1-42\"><a href=\"#cb1-42\" aria-hidden=\"true\" tabindex=\"-1\"></a>    product <span class=\"op\">=</span> np.dot(rotated_q, rotated_k)</span>\n<span id=\"cb1-43\"><a href=\"#cb1-43\" aria-hidden=\"true\" tabindex=\"-1\"></a>    inner_products.append(product)</span>\n<span id=\"cb1-44\"><a href=\"#cb1-44\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-45\"><a href=\"#cb1-45\" aria-hidden=\"true\" tabindex=\"-1\"></a>plot(x<span class=\"op\">=</span><span class=\"bu\">range</span>(window_size), y<span class=\"op\">=</span>inner_products, name<span class=\"op\">=</span><span class=\"ss\">f&#39;base=</span><span class=\"sc\">&#123;</span>base<span class=\"sc\">&#125;</span><span class=\"ss\">,window size=</span><span class=\"sc\">&#123;</span>window_size<span class=\"sc\">&#125;</span><span class=\"ss\">,d=</span><span class=\"sc\">&#123;</span>d<span class=\"sc\">&#125;</span><span class=\"ss\">&#39;</span>)</span></code></pre></div>\n<p>（1）q = k = 1</p>\n<p>假设q和k都是1向量，如果q在位置0，画出k在0~4096位置下和q在位置编码后的内积如下。</p>\n<img src=\"/f0902f1a/1.png\" class title=\"衰减\">\n<p>这里使用了base=10000，d=512。</p>\n<p>可以看到整体趋势是震荡下降的</p>\n<p>不过如果把窗口从4096增大到65536，图像会变成这样</p>\n<img src=\"/f0902f1a/2.png\" class title=\"衰减\">\n<p>可以看到图像不再是单纯的衰减，在距离超过大约15000的时候，出现了上升。</p>\n<p>实际上这个包含多个周期函数的内积也具有一定的周期性，并不是在整个域上保持衰减的特性。只要相对距离够大，超过这个周期的1/4，内积就会再次上升。</p>\n<p>而这个内积的周期受base调控，base越大，周期越长，因此现在的长窗口模型起步就是base=5M或者10M。</p>\n<p>我们把base改成5M，图像如下</p>\n<img src=\"/f0902f1a/3.png\" class title=\"衰减\">\n<p>又呈现了震荡衰减的趋势。</p>\n<p>前面画的是q在位置0，k在0~4096/65536的情况，那么把q放在中间看看内积结果怎么样。</p>\n<img src=\"/f0902f1a/4.png\" class title=\"衰减\">\n<p>可以看到在q两边的内积是对称的，同样的远距离衰减属性。</p>\n<p>（2）q、k随机</p>\n<p>前面是把q和k固定为1向量，现在试着把q和k初始化为随机向量，图像如下</p>\n<img src=\"/f0902f1a/5.png\" class title=\"衰减\">\n<p>相比1向量出现了更多的震荡，但是大体上还是能保持一定的远距离衰减特性。</p>\n<h1 id=\"小结\">小结</h1>\n<ul>\n<li>RoPE的远距离衰减是震荡的，并且整个内积本身也具有一定的周期性，只有把base设得足够大，才能让内积结果在模型窗口大小内保持远距离衰减的特性。<br>\n</li>\n<li>在q和k的相对距离小的时候，内积差距较大，也就是衰减较快；到了远距离之后，衰减变慢，也就是从内积角度来看，分辨率会变小。</li>\n</ul>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">从loss视角理解大模型涌现能力</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">大模型推理加速-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLM的重复生成和ICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">大模型算法题(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265<br>\n【2】RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n【3】理解LLM位置编码:RoPE http://www.linsight.cn/c4da56c0.html</p>\n"},{"title":"LLM的重复生成和ICL","abbrlink":"7381cae3","date":"2024-06-17T11:22:22.000Z","_content":"\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\nLLM的重复生成问题，俗称复读机问题。  \n\n对于这个问题的研究很多都和in-context learning相关。  \n\n# 背景\n\n在目前这个时间点，其实已经很少会遇到字符级别的重复生成问题。  \n\n自ChatGPT发布以来，模型规模的增大，训练数据数量和质量的提升，都让LLM的能力不断提升，复读机问题这种基础问题看起来确实不多见了。  \n\n不过在某些场景，比如手机的端侧智能助手，所用的模型相对较小，有时还是能遇到一些句子级别、段落级别的重复生成问题。  \n\n此外，在多轮对话下，随着对话轮次的增多，出现“重复生成/近似重复生成”的概率也会增加。在特定的对话中，用户新增了一些细致的要求或者进行关于细节的追问，这时模型有可能倾向于给出和前面回答几乎相同的答案。  \n\n当然这些情况都可以归因于模型训得不够好。但重复生成的问题显然和知识储备、回复质量这些问题有所不同，也有一些工作进行相关分析。  \n\n模型这种重复生成的特性部分可以为我们所用，但有时也会带来问题。  \n\n# induction heads  \n\nAnthropic在22年的文章《In-context Learning and Induction Heads》中对基于transformer的语言模型进行了ICL相关的分析。他们发现在这些生成模型里，存在着induction heads的机制。  \n\ninduction heads是模型中的一条circuit。简单来说，其功能是回顾当前token前面的内容，找到前面出现当前token的地方，并按照前面出现过的模式来补全当前token后面的内容。\n\n举个例子，比如现在的序列是  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A]  \n</center>  \n\n在生成下一个token的时候，induction heads就会从最后一个 [toekn A] 往前找，发现前面出现过相同的 [toekn A]，那么模型后面就会倾向于按照前面的出现过的 [token A] [token B] [token C] 这样的pattern来补全后面的内容，生成  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]  \n</center>  \n\ninduction heads这样的复制能力也会扩展到“相似”token上。如果当前token在前面没有出现过，那么induction heads就会去前面找和当前token相似的token所在的pattern，以此pattern作为后面生成的参考。比如现在的序列是  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A‘]  \n</center>  \n\n其中 [toekn A‘] 和 [toekn A] 具有相近的特征，那么induction heads就会倾向于把序列补全为  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A‘] [toekn B‘] [toekn C‘]  \n</center>  \n\n其中 [toekn B‘]、[toekn C‘]分别和[token B]、[token C]有相近的特征。  \n\ninduction heads由2个attention head组成。Anthropic在2层的transformer模型上精确地探测到了这样的行为，而且更多层更复杂的transformer模型上，也通过一些手段观察到类似功能的存在。\n\ninduction heads这样的“复制”行为被认为是ICL能力的主要来源。前面的例子中，从[token A] [token B] [toekn A‘] 生成 [toekn B‘]，这就已经是一个ICL的行为。  \n\n同时induction heads这样的“复制”行为某种程度上其实也是一种重复生成行为。这样的“复制”行为很可能和LLM的训练方式有关：预训练next token prediction鼓励模型预测概率最大的token，而在上文出现过相似token/pattern会提升模型复制token的信心，从而加强了重复生成的行为。  \n\n# 重复生成与ICL  \n\n论文：《Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation》，2022年  \n\n这篇文论对生成模型的（句子级）重复生成问题做了一些实验和分析，找到一些和重复生成现象相关的发现，并提出DITTO（pseuDo-repetITion penalizaTiOn）缓解重复生成的问题。（这缩写的方式让人想起 NEural contextualiZed representation for cHinese lAnguage understanding）  \n\n这样的研究基于一个前提：使用maximization-based decoding算法（如greedy decoding）。一些带有随机性的算法本身是具有缓解重复生成问题的能力的。  \n\n发现一：模型倾向于提升前面出现过的句子的生成概率  \n\n并且只要重复一次，这个概率就会飙升很多（下图）。这个发现和induction heads中的类似。  \n\n发现二：重复生成具有self-reinforcement的特点  \n\n重复的次数越多，越容易重复，越难以打破这个循环，如下图，横轴表示重复次数，纵轴红色表示某个token的概率，蓝色表示最大的概率。  \n\n{% asset_img ditto_1.png 重复生成 %}  \n\n发现三：Sentences with higher initial probabilities usually have a\nstronger self-reinforcement effect  \n\n句子本身概率越大（模型认为越通顺），重复的自我加强效应越强。把重复的句子换成随机token，在不同重复次数下解码token的概率变化如下图，增强的趋势比上图（通顺句子）要弱很多  \n\n{% asset_img ditto_2.png 重复生成 %}  \n\n而DITTO的做法简单来说是构造了一些重复句子的训练样本，并在训练时显示加入对重复token的惩罚。  \n\n另一个工作《UNDERSTANDING IN-CONTEXT LEARNING FROM REPETITIONS》，对self-reinforcement进行了进一步的测试，发现：  \n- self-reinforcement是LLM的共同属性，多个模型都具有这样的特点  \n- self-reinforcement强度随着重复token的距离减小而增强，如下图所示  \n\n{% asset_img 3.png 重复生成 %}  \n\n- 即使重复的token个数只有1个，这种self-reinforcement也会出现；而随着重复片段的增长，self-reinforcement强度也在提升，如下图所示  \n\n{% asset_img 4.png 重复生成 %}  \n\n而通过计算token重复的次数和token的概率，发现正是预训练next token prediction的任务赋予了模型self-reinforcement的特点。  \n\n{% asset_img 5.png 重复生成 %}  \n\n模型这种自我加强的作用对我们来说，既有好处又有坏处，可以说令人又爱又恨。  \n\n好处一：constraining output space  \n\n在ICL中，给定多个demonstration，通过利用self-reinforcement的特点，可以让模型的输出不要跑偏。比如多项选择题的ICL，可以让模型输出ABCD，而不是其他无法解析的内容。  \n\n为了验证这个假设，对用ICL数据做了实验，如下图。橙色线是对demonstration的问题内容和答案内容进行mask处理，蓝色线是在橙色的基础上进一步把ABCD替换成别的符号，红色线是在橙色线的基础上把“Answer：”替换成相同意思的内容。  \n\n{% asset_img 6.png 重复生成 %}  \n\n可以看到，如果仅仅对问题和答案内容进行mask，并不影响模型输出ABCD的概率，但是如果把ABCD/“Answer：”这种在demonstration中多次重复的内容替换成意思相近的符号，就会使得模型在答案空间的分布概率降低。  \n\n好处二：learning to follow patterns  \n\n和好处一类似，CoT的成功正是这个好处的一个例子。  \n\n坏处一：spurious connections\n\n对于ICL来说，self-reinforcement的坏处也很明显。不合理的prompt，比如不平均的答案分布，都会影响模型的能力，甚至可能成为用户注入攻击的入口。  \n\n# 缓解复读机问题    \n\n模型重复生成的self-reinforcement可以在ICL中发挥作用，但是也会让模型在生成回复的时候不断重复相同内容，停不下来。  \n\n一个可能的原因是训练数据中存在较多重复内容，这在从网页爬取的预训练数据中还是有一定比例的，因此对数据的清洗需要加入筛选这种重复内容的逻辑。  \n\n但是即使把训练数据中的重复数据比例降到很低，依然不能完全杜绝复读机问题，因此有很多方法是在解码的时候进行处理（decoding-base），缓解复读机问题：  \n- stochastic sampling：通过引入随机性，让模型不要总是选择概率最大的token输出，比如top-k、top-p采样。  \n- 重复惩罚：对已经出现过的token进行惩罚，减少生成结果的重复token。  \n- contrastive decoding：对比采样，降低生成表征相近的token。（实操对效果有比较大的影响）  \n- beam search（比较慢）  \n- locally typical sampling  \n- no repeat ngram：和重复惩罚类似，保证没有重复的ngram出现  \n\n此外也有training-based的方法：\n- 在训练的时候对已经出现过的token进行惩罚  \n- 通过强化学习对模型的重复生成进行惩罚（但强化学习成本和难度都比较高）  \n\n# 小结  \n\n自回归模型的重复生成不仅和数据有关，跟训练方法、模型结构、解码策略都有关。  \n\n这种特性既好处也有坏处，在ICL可以成为我们控制模型效果的抓手，但也有可能带来生成内容的崩溃问题。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n[大模型算法题(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n【1】如何解释大模型的重复生成现象？ https://www.zhihu.com/question/616130636  \n【2】Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation https://arxiv.org/abs/2206.02369  \n【3】Understanding In-Context Learning from Repetitions https://arxiv.org/abs/2310.00297  \n【4】In-context Learning and Induction Heads https://arxiv.org/abs/2209.11895  \n【5】https://zhuanlan.zhihu.com/p/671697479","source":"_posts/cs/nlp/2024/06/关于LLM的重复生成现象.md","raw":"---\ntitle: LLM的重复生成和ICL\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 复读机\n  - 重复生成\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 7381cae3\ndate: 2024-06-17 19:22:22\n---\n\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\nLLM的重复生成问题，俗称复读机问题。  \n\n对于这个问题的研究很多都和in-context learning相关。  \n\n# 背景\n\n在目前这个时间点，其实已经很少会遇到字符级别的重复生成问题。  \n\n自ChatGPT发布以来，模型规模的增大，训练数据数量和质量的提升，都让LLM的能力不断提升，复读机问题这种基础问题看起来确实不多见了。  \n\n不过在某些场景，比如手机的端侧智能助手，所用的模型相对较小，有时还是能遇到一些句子级别、段落级别的重复生成问题。  \n\n此外，在多轮对话下，随着对话轮次的增多，出现“重复生成/近似重复生成”的概率也会增加。在特定的对话中，用户新增了一些细致的要求或者进行关于细节的追问，这时模型有可能倾向于给出和前面回答几乎相同的答案。  \n\n当然这些情况都可以归因于模型训得不够好。但重复生成的问题显然和知识储备、回复质量这些问题有所不同，也有一些工作进行相关分析。  \n\n模型这种重复生成的特性部分可以为我们所用，但有时也会带来问题。  \n\n# induction heads  \n\nAnthropic在22年的文章《In-context Learning and Induction Heads》中对基于transformer的语言模型进行了ICL相关的分析。他们发现在这些生成模型里，存在着induction heads的机制。  \n\ninduction heads是模型中的一条circuit。简单来说，其功能是回顾当前token前面的内容，找到前面出现当前token的地方，并按照前面出现过的模式来补全当前token后面的内容。\n\n举个例子，比如现在的序列是  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A]  \n</center>  \n\n在生成下一个token的时候，induction heads就会从最后一个 [toekn A] 往前找，发现前面出现过相同的 [toekn A]，那么模型后面就会倾向于按照前面的出现过的 [token A] [token B] [token C] 这样的pattern来补全后面的内容，生成  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]  \n</center>  \n\ninduction heads这样的复制能力也会扩展到“相似”token上。如果当前token在前面没有出现过，那么induction heads就会去前面找和当前token相似的token所在的pattern，以此pattern作为后面生成的参考。比如现在的序列是  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A‘]  \n</center>  \n\n其中 [toekn A‘] 和 [toekn A] 具有相近的特征，那么induction heads就会倾向于把序列补全为  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A‘] [toekn B‘] [toekn C‘]  \n</center>  \n\n其中 [toekn B‘]、[toekn C‘]分别和[token B]、[token C]有相近的特征。  \n\ninduction heads由2个attention head组成。Anthropic在2层的transformer模型上精确地探测到了这样的行为，而且更多层更复杂的transformer模型上，也通过一些手段观察到类似功能的存在。\n\ninduction heads这样的“复制”行为被认为是ICL能力的主要来源。前面的例子中，从[token A] [token B] [toekn A‘] 生成 [toekn B‘]，这就已经是一个ICL的行为。  \n\n同时induction heads这样的“复制”行为某种程度上其实也是一种重复生成行为。这样的“复制”行为很可能和LLM的训练方式有关：预训练next token prediction鼓励模型预测概率最大的token，而在上文出现过相似token/pattern会提升模型复制token的信心，从而加强了重复生成的行为。  \n\n# 重复生成与ICL  \n\n论文：《Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation》，2022年  \n\n这篇文论对生成模型的（句子级）重复生成问题做了一些实验和分析，找到一些和重复生成现象相关的发现，并提出DITTO（pseuDo-repetITion penalizaTiOn）缓解重复生成的问题。（这缩写的方式让人想起 NEural contextualiZed representation for cHinese lAnguage understanding）  \n\n这样的研究基于一个前提：使用maximization-based decoding算法（如greedy decoding）。一些带有随机性的算法本身是具有缓解重复生成问题的能力的。  \n\n发现一：模型倾向于提升前面出现过的句子的生成概率  \n\n并且只要重复一次，这个概率就会飙升很多（下图）。这个发现和induction heads中的类似。  \n\n发现二：重复生成具有self-reinforcement的特点  \n\n重复的次数越多，越容易重复，越难以打破这个循环，如下图，横轴表示重复次数，纵轴红色表示某个token的概率，蓝色表示最大的概率。  \n\n{% asset_img ditto_1.png 重复生成 %}  \n\n发现三：Sentences with higher initial probabilities usually have a\nstronger self-reinforcement effect  \n\n句子本身概率越大（模型认为越通顺），重复的自我加强效应越强。把重复的句子换成随机token，在不同重复次数下解码token的概率变化如下图，增强的趋势比上图（通顺句子）要弱很多  \n\n{% asset_img ditto_2.png 重复生成 %}  \n\n而DITTO的做法简单来说是构造了一些重复句子的训练样本，并在训练时显示加入对重复token的惩罚。  \n\n另一个工作《UNDERSTANDING IN-CONTEXT LEARNING FROM REPETITIONS》，对self-reinforcement进行了进一步的测试，发现：  \n- self-reinforcement是LLM的共同属性，多个模型都具有这样的特点  \n- self-reinforcement强度随着重复token的距离减小而增强，如下图所示  \n\n{% asset_img 3.png 重复生成 %}  \n\n- 即使重复的token个数只有1个，这种self-reinforcement也会出现；而随着重复片段的增长，self-reinforcement强度也在提升，如下图所示  \n\n{% asset_img 4.png 重复生成 %}  \n\n而通过计算token重复的次数和token的概率，发现正是预训练next token prediction的任务赋予了模型self-reinforcement的特点。  \n\n{% asset_img 5.png 重复生成 %}  \n\n模型这种自我加强的作用对我们来说，既有好处又有坏处，可以说令人又爱又恨。  \n\n好处一：constraining output space  \n\n在ICL中，给定多个demonstration，通过利用self-reinforcement的特点，可以让模型的输出不要跑偏。比如多项选择题的ICL，可以让模型输出ABCD，而不是其他无法解析的内容。  \n\n为了验证这个假设，对用ICL数据做了实验，如下图。橙色线是对demonstration的问题内容和答案内容进行mask处理，蓝色线是在橙色的基础上进一步把ABCD替换成别的符号，红色线是在橙色线的基础上把“Answer：”替换成相同意思的内容。  \n\n{% asset_img 6.png 重复生成 %}  \n\n可以看到，如果仅仅对问题和答案内容进行mask，并不影响模型输出ABCD的概率，但是如果把ABCD/“Answer：”这种在demonstration中多次重复的内容替换成意思相近的符号，就会使得模型在答案空间的分布概率降低。  \n\n好处二：learning to follow patterns  \n\n和好处一类似，CoT的成功正是这个好处的一个例子。  \n\n坏处一：spurious connections\n\n对于ICL来说，self-reinforcement的坏处也很明显。不合理的prompt，比如不平均的答案分布，都会影响模型的能力，甚至可能成为用户注入攻击的入口。  \n\n# 缓解复读机问题    \n\n模型重复生成的self-reinforcement可以在ICL中发挥作用，但是也会让模型在生成回复的时候不断重复相同内容，停不下来。  \n\n一个可能的原因是训练数据中存在较多重复内容，这在从网页爬取的预训练数据中还是有一定比例的，因此对数据的清洗需要加入筛选这种重复内容的逻辑。  \n\n但是即使把训练数据中的重复数据比例降到很低，依然不能完全杜绝复读机问题，因此有很多方法是在解码的时候进行处理（decoding-base），缓解复读机问题：  \n- stochastic sampling：通过引入随机性，让模型不要总是选择概率最大的token输出，比如top-k、top-p采样。  \n- 重复惩罚：对已经出现过的token进行惩罚，减少生成结果的重复token。  \n- contrastive decoding：对比采样，降低生成表征相近的token。（实操对效果有比较大的影响）  \n- beam search（比较慢）  \n- locally typical sampling  \n- no repeat ngram：和重复惩罚类似，保证没有重复的ngram出现  \n\n此外也有training-based的方法：\n- 在训练的时候对已经出现过的token进行惩罚  \n- 通过强化学习对模型的重复生成进行惩罚（但强化学习成本和难度都比较高）  \n\n# 小结  \n\n自回归模型的重复生成不仅和数据有关，跟训练方法、模型结构、解码策略都有关。  \n\n这种特性既好处也有坏处，在ICL可以成为我们控制模型效果的抓手，但也有可能带来生成内容的崩溃问题。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n[大模型算法题(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n【1】如何解释大模型的重复生成现象？ https://www.zhihu.com/question/616130636  \n【2】Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation https://arxiv.org/abs/2206.02369  \n【3】Understanding In-Context Learning from Repetitions https://arxiv.org/abs/2310.00297  \n【4】In-context Learning and Induction Heads https://arxiv.org/abs/2209.11895  \n【5】https://zhuanlan.zhihu.com/p/671697479","slug":"cs/nlp/2024/06/关于LLM的重复生成现象","published":1,"updated":"2024-06-18T13:36:13.750Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9v000g314kg07s11is","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>LLM的重复生成问题，俗称复读机问题。</p>\n<p>对于这个问题的研究很多都和in-context learning相关。</p>\n<h1 id=\"背景\">背景</h1>\n<p>在目前这个时间点，其实已经很少会遇到字符级别的重复生成问题。</p>\n<p>自ChatGPT发布以来，模型规模的增大，训练数据数量和质量的提升，都让LLM的能力不断提升，复读机问题这种基础问题看起来确实不多见了。</p>\n<p>不过在某些场景，比如手机的端侧智能助手，所用的模型相对较小，有时还是能遇到一些句子级别、段落级别的重复生成问题。</p>\n<p>此外，在多轮对话下，随着对话轮次的增多，出现“重复生成/近似重复生成”的概率也会增加。在特定的对话中，用户新增了一些细致的要求或者进行关于细节的追问，这时模型有可能倾向于给出和前面回答几乎相同的答案。</p>\n<p>当然这些情况都可以归因于模型训得不够好。但重复生成的问题显然和知识储备、回复质量这些问题有所不同，也有一些工作进行相关分析。</p>\n<p>模型这种重复生成的特性部分可以为我们所用，但有时也会带来问题。</p>\n<h1 id=\"induction-heads\">induction heads</h1>\n<p>Anthropic在22年的文章《In-context Learning and Induction\nHeads》中对基于transformer的语言模型进行了ICL相关的分析。他们发现在这些生成模型里，存在着induction\nheads的机制。</p>\n<p>induction\nheads是模型中的一条circuit。简单来说，其功能是回顾当前token前面的内容，找到前面出现当前token的地方，并按照前面出现过的模式来补全当前token后面的内容。</p>\n<p>举个例子，比如现在的序列是</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A]\n</center>\n<p>在生成下一个token的时候，induction heads就会从最后一个 [toekn A]\n往前找，发现前面出现过相同的 [toekn\nA]，那么模型后面就会倾向于按照前面的出现过的 [token A] [token B] [token\nC] 这样的pattern来补全后面的内容，生成</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]\n</center>\n<p>induction\nheads这样的复制能力也会扩展到“相似”token上。如果当前token在前面没有出现过，那么induction\nheads就会去前面找和当前token相似的token所在的pattern，以此pattern作为后面生成的参考。比如现在的序列是</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A‘]\n</center>\n<p>其中 [toekn A‘] 和 [toekn A] 具有相近的特征，那么induction\nheads就会倾向于把序列补全为</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A‘] [toekn B‘] [toekn C‘]\n</center>\n<p>其中 [toekn B‘]、[toekn C‘]分别和[token B]、[token\nC]有相近的特征。</p>\n<p>induction heads由2个attention\nhead组成。Anthropic在2层的transformer模型上精确地探测到了这样的行为，而且更多层更复杂的transformer模型上，也通过一些手段观察到类似功能的存在。</p>\n<p>induction\nheads这样的“复制”行为被认为是ICL能力的主要来源。前面的例子中，从[token\nA] [token B] [toekn A‘] 生成 [toekn B‘]，这就已经是一个ICL的行为。</p>\n<p>同时induction\nheads这样的“复制”行为某种程度上其实也是一种重复生成行为。这样的“复制”行为很可能和LLM的训练方式有关：预训练next\ntoken\nprediction鼓励模型预测概率最大的token，而在上文出现过相似token/pattern会提升模型复制token的信心，从而加强了重复生成的行为。</p>\n<h1 id=\"重复生成与icl\">重复生成与ICL</h1>\n<p>论文：《Learning to Break the Loop: Analyzing and Mitigating\nRepetitions for Neural Text Generation》，2022年</p>\n<p>这篇文论对生成模型的（句子级）重复生成问题做了一些实验和分析，找到一些和重复生成现象相关的发现，并提出DITTO（pseuDo-repetITion\npenalizaTiOn）缓解重复生成的问题。（这缩写的方式让人想起 NEural\ncontextualiZed representation for cHinese lAnguage understanding）</p>\n<p>这样的研究基于一个前提：使用maximization-based decoding算法（如greedy\ndecoding）。一些带有随机性的算法本身是具有缓解重复生成问题的能力的。</p>\n<p>发现一：模型倾向于提升前面出现过的句子的生成概率</p>\n<p>并且只要重复一次，这个概率就会飙升很多（下图）。这个发现和induction\nheads中的类似。</p>\n<p>发现二：重复生成具有self-reinforcement的特点</p>\n<p>重复的次数越多，越容易重复，越难以打破这个循环，如下图，横轴表示重复次数，纵轴红色表示某个token的概率，蓝色表示最大的概率。</p>\n<img src=\"/7381cae3/ditto_1.png\" class title=\"重复生成\">\n<p>发现三：Sentences with higher initial probabilities usually have a\nstronger self-reinforcement effect</p>\n<p>句子本身概率越大（模型认为越通顺），重复的自我加强效应越强。把重复的句子换成随机token，在不同重复次数下解码token的概率变化如下图，增强的趋势比上图（通顺句子）要弱很多</p>\n<img src=\"/7381cae3/ditto_2.png\" class title=\"重复生成\">\n<p>而DITTO的做法简单来说是构造了一些重复句子的训练样本，并在训练时显示加入对重复token的惩罚。</p>\n<p>另一个工作《UNDERSTANDING IN-CONTEXT LEARNING FROM\nREPETITIONS》，对self-reinforcement进行了进一步的测试，发现：<br>\n- self-reinforcement是LLM的共同属性，多个模型都具有这样的特点<br>\n- self-reinforcement强度随着重复token的距离减小而增强，如下图所示</p>\n<img src=\"/7381cae3/3.png\" class title=\"重复生成\">\n<ul>\n<li>即使重复的token个数只有1个，这种self-reinforcement也会出现；而随着重复片段的增长，self-reinforcement强度也在提升，如下图所示</li>\n</ul>\n<img src=\"/7381cae3/4.png\" class title=\"重复生成\">\n<p>而通过计算token重复的次数和token的概率，发现正是预训练next token\nprediction的任务赋予了模型self-reinforcement的特点。</p>\n<img src=\"/7381cae3/5.png\" class title=\"重复生成\">\n<p>模型这种自我加强的作用对我们来说，既有好处又有坏处，可以说令人又爱又恨。</p>\n<p>好处一：constraining output space</p>\n<p>在ICL中，给定多个demonstration，通过利用self-reinforcement的特点，可以让模型的输出不要跑偏。比如多项选择题的ICL，可以让模型输出ABCD，而不是其他无法解析的内容。</p>\n<p>为了验证这个假设，对用ICL数据做了实验，如下图。橙色线是对demonstration的问题内容和答案内容进行mask处理，蓝色线是在橙色的基础上进一步把ABCD替换成别的符号，红色线是在橙色线的基础上把“Answer：”替换成相同意思的内容。</p>\n<img src=\"/7381cae3/6.png\" class title=\"重复生成\">\n<p>可以看到，如果仅仅对问题和答案内容进行mask，并不影响模型输出ABCD的概率，但是如果把ABCD/“Answer：”这种在demonstration中多次重复的内容替换成意思相近的符号，就会使得模型在答案空间的分布概率降低。</p>\n<p>好处二：learning to follow patterns</p>\n<p>和好处一类似，CoT的成功正是这个好处的一个例子。</p>\n<p>坏处一：spurious connections</p>\n<p>对于ICL来说，self-reinforcement的坏处也很明显。不合理的prompt，比如不平均的答案分布，都会影响模型的能力，甚至可能成为用户注入攻击的入口。</p>\n<h1 id=\"缓解复读机问题\">缓解复读机问题</h1>\n<p>模型重复生成的self-reinforcement可以在ICL中发挥作用，但是也会让模型在生成回复的时候不断重复相同内容，停不下来。</p>\n<p>一个可能的原因是训练数据中存在较多重复内容，这在从网页爬取的预训练数据中还是有一定比例的，因此对数据的清洗需要加入筛选这种重复内容的逻辑。</p>\n<p>但是即使把训练数据中的重复数据比例降到很低，依然不能完全杜绝复读机问题，因此有很多方法是在解码的时候进行处理（decoding-base），缓解复读机问题：<br>\n- stochastic\nsampling：通过引入随机性，让模型不要总是选择概率最大的token输出，比如top-k、top-p采样。<br>\n- 重复惩罚：对已经出现过的token进行惩罚，减少生成结果的重复token。<br>\n- contrastive\ndecoding：对比采样，降低生成表征相近的token。（实操对效果有比较大的影响）<br>\n- beam search（比较慢）<br>\n- locally typical sampling<br>\n- no repeat ngram：和重复惩罚类似，保证没有重复的ngram出现</p>\n<p>此外也有training-based的方法： -\n在训练的时候对已经出现过的token进行惩罚<br>\n-\n通过强化学习对模型的重复生成进行惩罚（但强化学习成本和难度都比较高）</p>\n<h1 id=\"小结\">小结</h1>\n<p>自回归模型的重复生成不仅和数据有关，跟训练方法、模型结构、解码策略都有关。</p>\n<p>这种特性既好处也有坏处，在ICL可以成为我们控制模型效果的抓手，但也有可能带来生成内容的崩溃问题。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">从loss视角理解大模型涌现能力</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">大模型推理加速-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">大模型算法题(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】如何解释大模型的重复生成现象？\nhttps://www.zhihu.com/question/616130636<br>\n【2】Learning to Break the Loop: Analyzing and Mitigating Repetitions\nfor Neural Text Generation https://arxiv.org/abs/2206.02369<br>\n【3】Understanding In-Context Learning from Repetitions\nhttps://arxiv.org/abs/2310.00297<br>\n【4】In-context Learning and Induction Heads\nhttps://arxiv.org/abs/2209.11895<br>\n【5】https://zhuanlan.zhihu.com/p/671697479</p>\n","length":4666,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>LLM的重复生成问题，俗称复读机问题。</p>\n<p>对于这个问题的研究很多都和in-context learning相关。</p>\n<h1 id=\"背景\">背景</h1>\n<p>在目前这个时间点，其实已经很少会遇到字符级别的重复生成问题。</p>\n<p>自ChatGPT发布以来，模型规模的增大，训练数据数量和质量的提升，都让LLM的能力不断提升，复读机问题这种基础问题看起来确实不多见了。</p>\n<p>不过在某些场景，比如手机的端侧智能助手，所用的模型相对较小，有时还是能遇到一些句子级别、段落级别的重复生成问题。</p>\n<p>此外，在多轮对话下，随着对话轮次的增多，出现“重复生成/近似重复生成”的概率也会增加。在特定的对话中，用户新增了一些细致的要求或者进行关于细节的追问，这时模型有可能倾向于给出和前面回答几乎相同的答案。</p>\n<p>当然这些情况都可以归因于模型训得不够好。但重复生成的问题显然和知识储备、回复质量这些问题有所不同，也有一些工作进行相关分析。</p>\n<p>模型这种重复生成的特性部分可以为我们所用，但有时也会带来问题。</p>\n<h1 id=\"induction-heads\">induction heads</h1>\n<p>Anthropic在22年的文章《In-context Learning and Induction\nHeads》中对基于transformer的语言模型进行了ICL相关的分析。他们发现在这些生成模型里，存在着induction\nheads的机制。</p>\n<p>induction\nheads是模型中的一条circuit。简单来说，其功能是回顾当前token前面的内容，找到前面出现当前token的地方，并按照前面出现过的模式来补全当前token后面的内容。</p>\n<p>举个例子，比如现在的序列是</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A]\n</center>\n<p>在生成下一个token的时候，induction heads就会从最后一个 [toekn A]\n往前找，发现前面出现过相同的 [toekn\nA]，那么模型后面就会倾向于按照前面的出现过的 [token A] [token B] [token\nC] 这样的pattern来补全后面的内容，生成</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]\n</center>\n<p>induction\nheads这样的复制能力也会扩展到“相似”token上。如果当前token在前面没有出现过，那么induction\nheads就会去前面找和当前token相似的token所在的pattern，以此pattern作为后面生成的参考。比如现在的序列是</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A‘]\n</center>\n<p>其中 [toekn A‘] 和 [toekn A] 具有相近的特征，那么induction\nheads就会倾向于把序列补全为</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A‘] [toekn B‘] [toekn C‘]\n</center>\n<p>其中 [toekn B‘]、[toekn C‘]分别和[token B]、[token\nC]有相近的特征。</p>\n<p>induction heads由2个attention\nhead组成。Anthropic在2层的transformer模型上精确地探测到了这样的行为，而且更多层更复杂的transformer模型上，也通过一些手段观察到类似功能的存在。</p>\n<p>induction\nheads这样的“复制”行为被认为是ICL能力的主要来源。前面的例子中，从[token\nA] [token B] [toekn A‘] 生成 [toekn B‘]，这就已经是一个ICL的行为。</p>\n<p>同时induction\nheads这样的“复制”行为某种程度上其实也是一种重复生成行为。这样的“复制”行为很可能和LLM的训练方式有关：预训练next\ntoken\nprediction鼓励模型预测概率最大的token，而在上文出现过相似token/pattern会提升模型复制token的信心，从而加强了重复生成的行为。</p>\n<h1 id=\"重复生成与icl\">重复生成与ICL</h1>\n<p>论文：《Learning to Break the Loop: Analyzing and Mitigating\nRepetitions for Neural Text Generation》，2022年</p>\n<p>这篇文论对生成模型的（句子级）重复生成问题做了一些实验和分析，找到一些和重复生成现象相关的发现，并提出DITTO（pseuDo-repetITion\npenalizaTiOn）缓解重复生成的问题。（这缩写的方式让人想起 NEural\ncontextualiZed representation for cHinese lAnguage understanding）</p>\n<p>这样的研究基于一个前提：使用maximization-based decoding算法（如greedy\ndecoding）。一些带有随机性的算法本身是具有缓解重复生成问题的能力的。</p>\n<p>发现一：模型倾向于提升前面出现过的句子的生成概率</p>\n<p>并且只要重复一次，这个概率就会飙升很多（下图）。这个发现和induction\nheads中的类似。</p>\n<p>发现二：重复生成具有self-reinforcement的特点</p>\n<p>重复的次数越多，越容易重复，越难以打破这个循环，如下图，横轴表示重复次数，纵轴红色表示某个token的概率，蓝色表示最大的概率。</p>\n<img src=\"/7381cae3/ditto_1.png\" class title=\"重复生成\">\n<p>发现三：Sentences with higher initial probabilities usually have a\nstronger self-reinforcement effect</p>\n<p>句子本身概率越大（模型认为越通顺），重复的自我加强效应越强。把重复的句子换成随机token，在不同重复次数下解码token的概率变化如下图，增强的趋势比上图（通顺句子）要弱很多</p>\n<img src=\"/7381cae3/ditto_2.png\" class title=\"重复生成\">\n<p>而DITTO的做法简单来说是构造了一些重复句子的训练样本，并在训练时显示加入对重复token的惩罚。</p>\n<p>另一个工作《UNDERSTANDING IN-CONTEXT LEARNING FROM\nREPETITIONS》，对self-reinforcement进行了进一步的测试，发现：<br>\n- self-reinforcement是LLM的共同属性，多个模型都具有这样的特点<br>\n- self-reinforcement强度随着重复token的距离减小而增强，如下图所示</p>\n<img src=\"/7381cae3/3.png\" class title=\"重复生成\">\n<ul>\n<li>即使重复的token个数只有1个，这种self-reinforcement也会出现；而随着重复片段的增长，self-reinforcement强度也在提升，如下图所示</li>\n</ul>\n<img src=\"/7381cae3/4.png\" class title=\"重复生成\">\n<p>而通过计算token重复的次数和token的概率，发现正是预训练next token\nprediction的任务赋予了模型self-reinforcement的特点。</p>\n<img src=\"/7381cae3/5.png\" class title=\"重复生成\">\n<p>模型这种自我加强的作用对我们来说，既有好处又有坏处，可以说令人又爱又恨。</p>\n<p>好处一：constraining output space</p>\n<p>在ICL中，给定多个demonstration，通过利用self-reinforcement的特点，可以让模型的输出不要跑偏。比如多项选择题的ICL，可以让模型输出ABCD，而不是其他无法解析的内容。</p>\n<p>为了验证这个假设，对用ICL数据做了实验，如下图。橙色线是对demonstration的问题内容和答案内容进行mask处理，蓝色线是在橙色的基础上进一步把ABCD替换成别的符号，红色线是在橙色线的基础上把“Answer：”替换成相同意思的内容。</p>\n<img src=\"/7381cae3/6.png\" class title=\"重复生成\">\n<p>可以看到，如果仅仅对问题和答案内容进行mask，并不影响模型输出ABCD的概率，但是如果把ABCD/“Answer：”这种在demonstration中多次重复的内容替换成意思相近的符号，就会使得模型在答案空间的分布概率降低。</p>\n<p>好处二：learning to follow patterns</p>\n<p>和好处一类似，CoT的成功正是这个好处的一个例子。</p>\n<p>坏处一：spurious connections</p>\n<p>对于ICL来说，self-reinforcement的坏处也很明显。不合理的prompt，比如不平均的答案分布，都会影响模型的能力，甚至可能成为用户注入攻击的入口。</p>\n<h1 id=\"缓解复读机问题\">缓解复读机问题</h1>\n<p>模型重复生成的self-reinforcement可以在ICL中发挥作用，但是也会让模型在生成回复的时候不断重复相同内容，停不下来。</p>\n<p>一个可能的原因是训练数据中存在较多重复内容，这在从网页爬取的预训练数据中还是有一定比例的，因此对数据的清洗需要加入筛选这种重复内容的逻辑。</p>\n<p>但是即使把训练数据中的重复数据比例降到很低，依然不能完全杜绝复读机问题，因此有很多方法是在解码的时候进行处理（decoding-base），缓解复读机问题：<br>\n- stochastic\nsampling：通过引入随机性，让模型不要总是选择概率最大的token输出，比如top-k、top-p采样。<br>\n- 重复惩罚：对已经出现过的token进行惩罚，减少生成结果的重复token。<br>\n- contrastive\ndecoding：对比采样，降低生成表征相近的token。（实操对效果有比较大的影响）<br>\n- beam search（比较慢）<br>\n- locally typical sampling<br>\n- no repeat ngram：和重复惩罚类似，保证没有重复的ngram出现</p>\n<p>此外也有training-based的方法： -\n在训练的时候对已经出现过的token进行惩罚<br>\n-\n通过强化学习对模型的重复生成进行惩罚（但强化学习成本和难度都比较高）</p>\n<h1 id=\"小结\">小结</h1>\n<p>自回归模型的重复生成不仅和数据有关，跟训练方法、模型结构、解码策略都有关。</p>\n<p>这种特性既好处也有坏处，在ICL可以成为我们控制模型效果的抓手，但也有可能带来生成内容的崩溃问题。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">从loss视角理解大模型涌现能力</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">大模型推理加速-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">大模型算法题(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】如何解释大模型的重复生成现象？\nhttps://www.zhihu.com/question/616130636<br>\n【2】Learning to Break the Loop: Analyzing and Mitigating Repetitions\nfor Neural Text Generation https://arxiv.org/abs/2206.02369<br>\n【3】Understanding In-Context Learning from Repetitions\nhttps://arxiv.org/abs/2310.00297<br>\n【4】In-context Learning and Induction Heads\nhttps://arxiv.org/abs/2209.11895<br>\n【5】https://zhuanlan.zhihu.com/p/671697479</p>\n"},{"title":"从loss视角理解大模型涌现能力","abbrlink":"f5fb75e4","date":"2024-06-15T08:13:55.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n智谱在《Understanding Emergent Abilities of Language Models from the Loss Perspective》中提出一个观察大模型涌现能力的视角 -- 预训练loss，主要内容是通过一系列实验结果来解释一些关于涌现能力的观察。可以作为一个理解大模型的参考角度，也可以用于指导预训练模型的开发和优化。  \n\n# 背景  \n\n《Emergent abilities of large language models》把emergent ability定义为在大规模模型中有，而在参数量较小的模型没有的能力。  \n\n这个看法现在受到一些挑战：  \n- 目前很多在更大规模数据集训练出来的小模型，展现出比之前大规模模型更强的能力，比如LLaMA3在大部分评测指标上就比GPT-3强，很多以前千亿模型才能做到的任务，现在百亿甚至十亿的模型也能做好。  \n- 《Are emergent abilities of large language models a mirage?》认为产生涌现能力现象的因为是数据评测指标的非线性和不连续性带来的，如果使用更细粒度的连续指标，就能观察到指标的平滑提升。  \n\n而《Training compute-optimal large language models》指出，相同的计算量下，不同的模型规模和数据量的组合会产生不同的效果。这说明单纯的模型规模或者数据规模并不是一个好的下游任务能力的indicator，预训练loss才是更合适的指标。\n\n但是训练loss和下游任务表现具体是什么关系却还没有确定的说法，智谱针对这个问题做了一些预训练实验，并从预训练loss角度定义了emergent ability。  \n\n# pretraining loss和下游任务表现的关系  \n\n## 设置  \n\n后续所有预训练实验使用相同的模型结构和同一份预训练数据（但是训练数据量可能有区别），一些通用设置如下：  \n- 分词用BPE  \n- 模型结构在LLaMA基础上，全部使用GQA，而RoPE只在一半的Q/K上应用  \n- 使用AdamW优化器，$\\beta_1=0.9$，$\\beta_2=0.95$  \n- 训练窗口长度为2048  \n- 所有模型都在中英文比例为1:4的预训练数据集上训练，英文数据集分布如下  \n\n{% asset_img eng_data.png 英文数据 %}  \n\n所有模型都是从零开始预训练。  \n\n评测模型的下游任务共有6类12个数据集，具体信息如下  \n\n{% asset_img downstream_dataset.png 下游任务 %}  \n\n{% asset_img downstream_dataset_num.png 下游任务 %}  \n\n## 实验一：pretraining loss vs. performance  \n\n第一个实验训练了3个规模的模型：1.5B、6B、32B，训练数据量分别为3T、3T、2.5T。具体设置如下  \n\n{% asset_img exp1_param.png 实验设置 %}  \n\n大约每训练43B token就会保存一次checkpoint。把3个模型所有checkpoint下，对应的预训练loss和下游任务评测结果画出来，如下所示  \n\n{% asset_img exp1_plot.png loss vs. performance %}  \n\n从上图可以观察到3个现象：  \n- 无论模型规模如何，下游任务评测结果都随着预训练loss的降低而提升。从提升的具体情况可以分成两类，这个后面部分再分析。  \n- 各个规模的模型所画出的点都落在了同一条曲线上，这说明下游任务的评测结果和预训练loss高度相关，而和模型规模没有直接关系。这点很重要。  \n- 预训练loss对下游任务指标的表征能力同时适用于中英文，这说明中英文token在多语言预训练中具有相似的learning dynamics。  \n\n而把计算量和下游任务指标的关系画出来，则有如下结果  \n\n{% asset_img exp1_compute.png 下游任务效果和预训练计算量的关系 %}  \n\n可以看到各个规模的模型所画出的点并没有落在同一条曲线上，这说明计算量并不是表征下游任务效果的好指标。  \n\n## 实验二：training token count vs. performance  \n\n第二个实验使用了不同的数据量训练了28个小一些的模型，具体设置如下  \n\n{% asset_img exp2_param.png 实验设置 %}  \n\n第一个实验中，每个规模的模型设置了一个固定的训练token数，然后取中间checkpoint进行评测。第二个实验是对每个规模的模型设置了多个不同的总训练token数。二者的区别在于，预训练的最后阶段会逐渐把学习率decay到最小值，而这样的学习率退火策略对效果有很大的影响。\n\n取28个模型的最终checkpoint，画出对应的预训练loss和下游任务评测结果如下  \n\n{% asset_img exp2_plot.png token count vs. performance %}  \n\n结果和实验一类似，各个模型的点都落在了同一条曲线上。说明无论模型规模和训练量如何，只要loss相同，在下游任务上就有相同的表现。  \n\n由于这28个模型相比实验一的较小，在图中最后一排的任务上效果都接近于随机。这个现象后续分析。  \n\n## LLaMA’s loss vs. performance  \n\n实验一和二是在从零开始训练的模型上评测的，这里用LLaMA来验证前面得到的结论。  \n\n由于LLaMA没有放出中间checkpoint，这里直接从LLaMA的报告里抽出相应的数据点，在6个下游任务上的结果如下图\n\n{% asset_img exp3_plot.png loss vs. performance %}  \n\n可以看到基本上各个模型的点也是落在同一条曲线上。LLaMA和实验一实验二的训练框架、模型结构、训练数据都有所不同，但是也有相同的结论，说明这样的结论是具有普遍性的。  \n\n> pre-training loss is a good indicator of LMs’ performance on downstream tasks, independent of model sizes, training tokens, languages, and pretraining frameworks  \n\n# 进一步分析  \n\n## 不同任务的趋势  \n\n12个下游任务可以分为2类：  \n- 第一类：TriviaQA, HellaSwag, RACE, WinoGrande, NLPCC-KBQA, ClozeT, CLUEWSC, C3。这些任务的效果随着预训练loss的下降，平滑上升。  \n- 第二类：MMLU, C-Eval, GSM8K, GSM8K-Chinese。这些任务上，只有当预训练loss低于一定阈值，评测结果才开始提升。可以观察到，在实验一实验二的配置下，大概在预训练loss小于2.2这个阈值之后，下游任务表现开始提升。整体来说，第二类任务难度是大于第一类的。所以虽然第一类中有些任务的prompt或者形式与第二类中的任务有些相似，但是依然有不同的表现。  \n\n第二类任务这个现象和《Grokking: Generalization beyond overfitting on small algorithmic datasets》提出的grokking有关联。  \n\ngrokking描述了下游任务的效果从随机水平（乱猜）提高到perfect generalization的improvement。这种improvement只有在过拟合到一定程度才会发生。在预训练中，模型整体上通常是欠拟合的。不过由于预训练语料库是不同文档的混合，因此模型可能在某些能力上过拟合（比如数值计算的能力，情感分析的能力），而在整体上依然欠拟合。  \n\n当然第二类任务这个现象也和emergent ability有关联。按scaling law的说法，在训练token数固定的情况下，预训练loss与模型规模呈幂律关系。也就是说，模型大小和预训练损失之间存在单调关系。对于第二类任务，存在一个与预训练loss中的临界点相对应的模型规模阈值。当模型大小超过这个阈值时，模型就可以展现出超过随机猜测的能力。  \n\n## 评测指标的影响  \n\n前面提到，emergent ability这个现象有可能是因为评测指标的非线性和不连续性带来的。比如 MMLU这样的多项选择题，打分结果只能是0分或者满分。  \n\n现在把这个评测指标换成两个连续的指标：  \n- 一个是probability of the correct answer (CorrectChoiceProb)  \n- 第二个是《Are emergent abilities of large language models a mirage?》中提出的Brier Score：  \n\n$$\\text{BrierScore}=\\frac1N\\sum_{i=1}^N\\sum_{j=1}^C(y_{ij}-\\hat{y}_{ij})^2$$  \n\nN是样本数，C的类别数。  \n\n把MMLU和C-Eval在这两个新指标上的评测结果画出来，如下所示  \n\n{% asset_img metrics.png 指标 %}  \n\n可以发现涌现能力的现象依然存在。  \n\n值得注意的是，Brier Score的下降并不总是表示下游任务效果的提升。  \n\n比如对于有A/B/C/D四个选项的多项选择题任务，假设正确答案的分布是均匀的。现在有两个模型，一个总是预测A，即（1，0，0，0），另一个总是给出平均分布的预测，即（0.25，0.25，0.25，0.25，0.25）。  \n\n那么前者的Brier Score是1.5，而后者是0.75，但这并不能说明后者就更好。对于这个任务，实际上高于0.75的Brier Score都说明比随机猜测差。而低于随机猜测的指标变化并不能当做真正的提升，比如Brier Score从1.5提升到1.0并不能算作提升。  \n\n另外《Training trajectories of language models across scales》提出用perplexity of correct options来作为评测，可以看到平滑的提升。但perplexity of correct options其实不能作为一个合适的指标。  \n\n比如对于多项选择题，区分各个答案的能力才是我们想要的。而随着预训练进行，正确答案和错误答案的perplexity都在下降，只有当训练到二者的perplexity差异开始变大的时候，才能算是有提升。因此单纯的正确答案perplexity下降也能作为能力提升的指标，因为错误答案的perplexity可能下降更多。  \n\n# 从loss角度定义emergent abilities  \n\n基于前面的实验和分析，现在从预训练loss角度重新定义emergent ability：  \n\n> Definition. An ability is emergent if it is not present in models with higher pre-training loss but is present in models with lower pre-training loss.  \n\n一个emergent ability的normalized performance（比如多项选择题随机猜测的得分是0.25分，那这个任务原始的0.25分在normalized performance下就是0分）是预训练loss $L$ 的函数  \n\n$$\\begin{cases}f(L)&\\mathrm{if~}L<\\eta\\\\0&\\mathrm{otherwise}&\\end{cases}$$  \n\n其中f是一个单调递减函数，$\\eta$ 是阈值。  \n\n《Scaling laws for autoregressive generative modeling》中提出，在固定的训练token数 $D$ 下，模型规模 $N$ 和预训练损失的关系是  \n\n$$L(N)=L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}$$  \n\n其中 $L_{\\infty}$ 是irreducible loss，$\\alpha_{N}$ 是固定的系数。  \n\n把上面两个式子结合起来，就有  \n\n$$\\begin{cases}f\\left(L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\right)&\\text{if }N\\geq N_0\\cdot\\left(\\eta-L_\\infty\\right)^{-\\frac1{\\alpha_N}}\\\\0&\\text{otherwise}&\\end{cases}$$  \n\n当模型规模小于 $N_0\\cdot(\\eta-L_\\infty)^{-1/\\alpha_N}$ 这个阈值时，normalized performance为0；当模型规模超过这个阈值时，模型规模的增长带来了预训练loss的下降，从而带来了normalized performance的提升。  \n\n# 小结  \n\n通过预训练loss来预测下游任务的提升，这点用在预训练模型的分析和优化上还是有些帮助的。比如在loss较高的时候，在下游任务上的效果的变化可能更多是随机波动而不是真正的提升。  \n\n不过文中只对一个model family做了实验，而loss和模型结构，词表等都有关系，因此还需要进一步的探索。  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n[大模型算法题(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n【1】Understanding Emergent Abilities of Language Models from the Loss Perspective https://arxiv.org/abs/2403.15796  ","source":"_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力.md","raw":"---\ntitle: 从loss视角理解大模型涌现能力\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 涌现能力\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: f5fb75e4\ndate: 2024-06-15 16:13:55\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n智谱在《Understanding Emergent Abilities of Language Models from the Loss Perspective》中提出一个观察大模型涌现能力的视角 -- 预训练loss，主要内容是通过一系列实验结果来解释一些关于涌现能力的观察。可以作为一个理解大模型的参考角度，也可以用于指导预训练模型的开发和优化。  \n\n# 背景  \n\n《Emergent abilities of large language models》把emergent ability定义为在大规模模型中有，而在参数量较小的模型没有的能力。  \n\n这个看法现在受到一些挑战：  \n- 目前很多在更大规模数据集训练出来的小模型，展现出比之前大规模模型更强的能力，比如LLaMA3在大部分评测指标上就比GPT-3强，很多以前千亿模型才能做到的任务，现在百亿甚至十亿的模型也能做好。  \n- 《Are emergent abilities of large language models a mirage?》认为产生涌现能力现象的因为是数据评测指标的非线性和不连续性带来的，如果使用更细粒度的连续指标，就能观察到指标的平滑提升。  \n\n而《Training compute-optimal large language models》指出，相同的计算量下，不同的模型规模和数据量的组合会产生不同的效果。这说明单纯的模型规模或者数据规模并不是一个好的下游任务能力的indicator，预训练loss才是更合适的指标。\n\n但是训练loss和下游任务表现具体是什么关系却还没有确定的说法，智谱针对这个问题做了一些预训练实验，并从预训练loss角度定义了emergent ability。  \n\n# pretraining loss和下游任务表现的关系  \n\n## 设置  \n\n后续所有预训练实验使用相同的模型结构和同一份预训练数据（但是训练数据量可能有区别），一些通用设置如下：  \n- 分词用BPE  \n- 模型结构在LLaMA基础上，全部使用GQA，而RoPE只在一半的Q/K上应用  \n- 使用AdamW优化器，$\\beta_1=0.9$，$\\beta_2=0.95$  \n- 训练窗口长度为2048  \n- 所有模型都在中英文比例为1:4的预训练数据集上训练，英文数据集分布如下  \n\n{% asset_img eng_data.png 英文数据 %}  \n\n所有模型都是从零开始预训练。  \n\n评测模型的下游任务共有6类12个数据集，具体信息如下  \n\n{% asset_img downstream_dataset.png 下游任务 %}  \n\n{% asset_img downstream_dataset_num.png 下游任务 %}  \n\n## 实验一：pretraining loss vs. performance  \n\n第一个实验训练了3个规模的模型：1.5B、6B、32B，训练数据量分别为3T、3T、2.5T。具体设置如下  \n\n{% asset_img exp1_param.png 实验设置 %}  \n\n大约每训练43B token就会保存一次checkpoint。把3个模型所有checkpoint下，对应的预训练loss和下游任务评测结果画出来，如下所示  \n\n{% asset_img exp1_plot.png loss vs. performance %}  \n\n从上图可以观察到3个现象：  \n- 无论模型规模如何，下游任务评测结果都随着预训练loss的降低而提升。从提升的具体情况可以分成两类，这个后面部分再分析。  \n- 各个规模的模型所画出的点都落在了同一条曲线上，这说明下游任务的评测结果和预训练loss高度相关，而和模型规模没有直接关系。这点很重要。  \n- 预训练loss对下游任务指标的表征能力同时适用于中英文，这说明中英文token在多语言预训练中具有相似的learning dynamics。  \n\n而把计算量和下游任务指标的关系画出来，则有如下结果  \n\n{% asset_img exp1_compute.png 下游任务效果和预训练计算量的关系 %}  \n\n可以看到各个规模的模型所画出的点并没有落在同一条曲线上，这说明计算量并不是表征下游任务效果的好指标。  \n\n## 实验二：training token count vs. performance  \n\n第二个实验使用了不同的数据量训练了28个小一些的模型，具体设置如下  \n\n{% asset_img exp2_param.png 实验设置 %}  \n\n第一个实验中，每个规模的模型设置了一个固定的训练token数，然后取中间checkpoint进行评测。第二个实验是对每个规模的模型设置了多个不同的总训练token数。二者的区别在于，预训练的最后阶段会逐渐把学习率decay到最小值，而这样的学习率退火策略对效果有很大的影响。\n\n取28个模型的最终checkpoint，画出对应的预训练loss和下游任务评测结果如下  \n\n{% asset_img exp2_plot.png token count vs. performance %}  \n\n结果和实验一类似，各个模型的点都落在了同一条曲线上。说明无论模型规模和训练量如何，只要loss相同，在下游任务上就有相同的表现。  \n\n由于这28个模型相比实验一的较小，在图中最后一排的任务上效果都接近于随机。这个现象后续分析。  \n\n## LLaMA’s loss vs. performance  \n\n实验一和二是在从零开始训练的模型上评测的，这里用LLaMA来验证前面得到的结论。  \n\n由于LLaMA没有放出中间checkpoint，这里直接从LLaMA的报告里抽出相应的数据点，在6个下游任务上的结果如下图\n\n{% asset_img exp3_plot.png loss vs. performance %}  \n\n可以看到基本上各个模型的点也是落在同一条曲线上。LLaMA和实验一实验二的训练框架、模型结构、训练数据都有所不同，但是也有相同的结论，说明这样的结论是具有普遍性的。  \n\n> pre-training loss is a good indicator of LMs’ performance on downstream tasks, independent of model sizes, training tokens, languages, and pretraining frameworks  \n\n# 进一步分析  \n\n## 不同任务的趋势  \n\n12个下游任务可以分为2类：  \n- 第一类：TriviaQA, HellaSwag, RACE, WinoGrande, NLPCC-KBQA, ClozeT, CLUEWSC, C3。这些任务的效果随着预训练loss的下降，平滑上升。  \n- 第二类：MMLU, C-Eval, GSM8K, GSM8K-Chinese。这些任务上，只有当预训练loss低于一定阈值，评测结果才开始提升。可以观察到，在实验一实验二的配置下，大概在预训练loss小于2.2这个阈值之后，下游任务表现开始提升。整体来说，第二类任务难度是大于第一类的。所以虽然第一类中有些任务的prompt或者形式与第二类中的任务有些相似，但是依然有不同的表现。  \n\n第二类任务这个现象和《Grokking: Generalization beyond overfitting on small algorithmic datasets》提出的grokking有关联。  \n\ngrokking描述了下游任务的效果从随机水平（乱猜）提高到perfect generalization的improvement。这种improvement只有在过拟合到一定程度才会发生。在预训练中，模型整体上通常是欠拟合的。不过由于预训练语料库是不同文档的混合，因此模型可能在某些能力上过拟合（比如数值计算的能力，情感分析的能力），而在整体上依然欠拟合。  \n\n当然第二类任务这个现象也和emergent ability有关联。按scaling law的说法，在训练token数固定的情况下，预训练loss与模型规模呈幂律关系。也就是说，模型大小和预训练损失之间存在单调关系。对于第二类任务，存在一个与预训练loss中的临界点相对应的模型规模阈值。当模型大小超过这个阈值时，模型就可以展现出超过随机猜测的能力。  \n\n## 评测指标的影响  \n\n前面提到，emergent ability这个现象有可能是因为评测指标的非线性和不连续性带来的。比如 MMLU这样的多项选择题，打分结果只能是0分或者满分。  \n\n现在把这个评测指标换成两个连续的指标：  \n- 一个是probability of the correct answer (CorrectChoiceProb)  \n- 第二个是《Are emergent abilities of large language models a mirage?》中提出的Brier Score：  \n\n$$\\text{BrierScore}=\\frac1N\\sum_{i=1}^N\\sum_{j=1}^C(y_{ij}-\\hat{y}_{ij})^2$$  \n\nN是样本数，C的类别数。  \n\n把MMLU和C-Eval在这两个新指标上的评测结果画出来，如下所示  \n\n{% asset_img metrics.png 指标 %}  \n\n可以发现涌现能力的现象依然存在。  \n\n值得注意的是，Brier Score的下降并不总是表示下游任务效果的提升。  \n\n比如对于有A/B/C/D四个选项的多项选择题任务，假设正确答案的分布是均匀的。现在有两个模型，一个总是预测A，即（1，0，0，0），另一个总是给出平均分布的预测，即（0.25，0.25，0.25，0.25，0.25）。  \n\n那么前者的Brier Score是1.5，而后者是0.75，但这并不能说明后者就更好。对于这个任务，实际上高于0.75的Brier Score都说明比随机猜测差。而低于随机猜测的指标变化并不能当做真正的提升，比如Brier Score从1.5提升到1.0并不能算作提升。  \n\n另外《Training trajectories of language models across scales》提出用perplexity of correct options来作为评测，可以看到平滑的提升。但perplexity of correct options其实不能作为一个合适的指标。  \n\n比如对于多项选择题，区分各个答案的能力才是我们想要的。而随着预训练进行，正确答案和错误答案的perplexity都在下降，只有当训练到二者的perplexity差异开始变大的时候，才能算是有提升。因此单纯的正确答案perplexity下降也能作为能力提升的指标，因为错误答案的perplexity可能下降更多。  \n\n# 从loss角度定义emergent abilities  \n\n基于前面的实验和分析，现在从预训练loss角度重新定义emergent ability：  \n\n> Definition. An ability is emergent if it is not present in models with higher pre-training loss but is present in models with lower pre-training loss.  \n\n一个emergent ability的normalized performance（比如多项选择题随机猜测的得分是0.25分，那这个任务原始的0.25分在normalized performance下就是0分）是预训练loss $L$ 的函数  \n\n$$\\begin{cases}f(L)&\\mathrm{if~}L<\\eta\\\\0&\\mathrm{otherwise}&\\end{cases}$$  \n\n其中f是一个单调递减函数，$\\eta$ 是阈值。  \n\n《Scaling laws for autoregressive generative modeling》中提出，在固定的训练token数 $D$ 下，模型规模 $N$ 和预训练损失的关系是  \n\n$$L(N)=L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}$$  \n\n其中 $L_{\\infty}$ 是irreducible loss，$\\alpha_{N}$ 是固定的系数。  \n\n把上面两个式子结合起来，就有  \n\n$$\\begin{cases}f\\left(L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\right)&\\text{if }N\\geq N_0\\cdot\\left(\\eta-L_\\infty\\right)^{-\\frac1{\\alpha_N}}\\\\0&\\text{otherwise}&\\end{cases}$$  \n\n当模型规模小于 $N_0\\cdot(\\eta-L_\\infty)^{-1/\\alpha_N}$ 这个阈值时，normalized performance为0；当模型规模超过这个阈值时，模型规模的增长带来了预训练loss的下降，从而带来了normalized performance的提升。  \n\n# 小结  \n\n通过预训练loss来预测下游任务的提升，这点用在预训练模型的分析和优化上还是有些帮助的。比如在loss较高的时候，在下游任务上的效果的变化可能更多是随机波动而不是真正的提升。  \n\n不过文中只对一个model family做了实验，而loss和模型结构，词表等都有关系，因此还需要进一步的探索。  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n[大模型算法题(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n【1】Understanding Emergent Abilities of Language Models from the Loss Perspective https://arxiv.org/abs/2403.15796  ","slug":"cs/nlp/2024/06/从loss视角理解大模型涌现能力","published":1,"updated":"2024-06-16T09:16:55.156Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9w000h314k4r62a89s","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>智谱在《Understanding Emergent Abilities of Language Models from the\nLoss Perspective》中提出一个观察大模型涌现能力的视角 --\n预训练loss，主要内容是通过一系列实验结果来解释一些关于涌现能力的观察。可以作为一个理解大模型的参考角度，也可以用于指导预训练模型的开发和优化。</p>\n<h1 id=\"背景\">背景</h1>\n<p>《Emergent abilities of large language models》把emergent\nability定义为在大规模模型中有，而在参数量较小的模型没有的能力。</p>\n<p>这个看法现在受到一些挑战：<br>\n-\n目前很多在更大规模数据集训练出来的小模型，展现出比之前大规模模型更强的能力，比如LLaMA3在大部分评测指标上就比GPT-3强，很多以前千亿模型才能做到的任务，现在百亿甚至十亿的模型也能做好。<br>\n- 《Are emergent abilities of large language models a\nmirage?》认为产生涌现能力现象的因为是数据评测指标的非线性和不连续性带来的，如果使用更细粒度的连续指标，就能观察到指标的平滑提升。</p>\n<p>而《Training compute-optimal large language\nmodels》指出，相同的计算量下，不同的模型规模和数据量的组合会产生不同的效果。这说明单纯的模型规模或者数据规模并不是一个好的下游任务能力的indicator，预训练loss才是更合适的指标。</p>\n<p>但是训练loss和下游任务表现具体是什么关系却还没有确定的说法，智谱针对这个问题做了一些预训练实验，并从预训练loss角度定义了emergent\nability。</p>\n<h1 id=\"pretraining-loss和下游任务表现的关系\">pretraining\nloss和下游任务表现的关系</h1>\n<h2 id=\"设置\">设置</h2>\n<p>后续所有预训练实验使用相同的模型结构和同一份预训练数据（但是训练数据量可能有区别），一些通用设置如下：<br>\n- 分词用BPE<br>\n- 模型结构在LLaMA基础上，全部使用GQA，而RoPE只在一半的Q/K上应用<br>\n- 使用AdamW优化器，<span class=\"math inline\">\\(\\beta_1=0.9\\)</span>，<span class=\"math inline\">\\(\\beta_2=0.95\\)</span><br>\n- 训练窗口长度为2048<br>\n-\n所有模型都在中英文比例为1:4的预训练数据集上训练，英文数据集分布如下</p>\n<img src=\"/f5fb75e4/eng_data.png\" class title=\"英文数据\">\n<p>所有模型都是从零开始预训练。</p>\n<p>评测模型的下游任务共有6类12个数据集，具体信息如下</p>\n<img src=\"/f5fb75e4/downstream_dataset.png\" class title=\"下游任务\">\n<img src=\"/f5fb75e4/downstream_dataset_num.png\" class title=\"下游任务\">\n<h2 id=\"实验一pretraining-loss-vs.-performance\">实验一：pretraining loss\nvs. performance</h2>\n<p>第一个实验训练了3个规模的模型：1.5B、6B、32B，训练数据量分别为3T、3T、2.5T。具体设置如下</p>\n<img src=\"/f5fb75e4/exp1_param.png\" class title=\"实验设置\">\n<p>大约每训练43B\ntoken就会保存一次checkpoint。把3个模型所有checkpoint下，对应的预训练loss和下游任务评测结果画出来，如下所示</p>\n<img src=\"/f5fb75e4/exp1_plot.png\" class title=\"loss vs. performance\">\n<p>从上图可以观察到3个现象：<br>\n-\n无论模型规模如何，下游任务评测结果都随着预训练loss的降低而提升。从提升的具体情况可以分成两类，这个后面部分再分析。<br>\n-\n各个规模的模型所画出的点都落在了同一条曲线上，这说明下游任务的评测结果和预训练loss高度相关，而和模型规模没有直接关系。这点很重要。<br>\n-\n预训练loss对下游任务指标的表征能力同时适用于中英文，这说明中英文token在多语言预训练中具有相似的learning\ndynamics。</p>\n<p>而把计算量和下游任务指标的关系画出来，则有如下结果</p>\n<img src=\"/f5fb75e4/exp1_compute.png\" class title=\"下游任务效果和预训练计算量的关系\">\n<p>可以看到各个规模的模型所画出的点并没有落在同一条曲线上，这说明计算量并不是表征下游任务效果的好指标。</p>\n<h2 id=\"实验二training-token-count-vs.-performance\">实验二：training\ntoken count vs. performance</h2>\n<p>第二个实验使用了不同的数据量训练了28个小一些的模型，具体设置如下</p>\n<img src=\"/f5fb75e4/exp2_param.png\" class title=\"实验设置\">\n<p>第一个实验中，每个规模的模型设置了一个固定的训练token数，然后取中间checkpoint进行评测。第二个实验是对每个规模的模型设置了多个不同的总训练token数。二者的区别在于，预训练的最后阶段会逐渐把学习率decay到最小值，而这样的学习率退火策略对效果有很大的影响。</p>\n<p>取28个模型的最终checkpoint，画出对应的预训练loss和下游任务评测结果如下</p>\n<img src=\"/f5fb75e4/exp2_plot.png\" class title=\"token count vs. performance\">\n<p>结果和实验一类似，各个模型的点都落在了同一条曲线上。说明无论模型规模和训练量如何，只要loss相同，在下游任务上就有相同的表现。</p>\n<p>由于这28个模型相比实验一的较小，在图中最后一排的任务上效果都接近于随机。这个现象后续分析。</p>\n<h2 id=\"llamas-loss-vs.-performance\">LLaMA’s loss vs. performance</h2>\n<p>实验一和二是在从零开始训练的模型上评测的，这里用LLaMA来验证前面得到的结论。</p>\n<p>由于LLaMA没有放出中间checkpoint，这里直接从LLaMA的报告里抽出相应的数据点，在6个下游任务上的结果如下图</p>\n<img src=\"/f5fb75e4/exp3_plot.png\" class title=\"loss vs. performance\">\n<p>可以看到基本上各个模型的点也是落在同一条曲线上。LLaMA和实验一实验二的训练框架、模型结构、训练数据都有所不同，但是也有相同的结论，说明这样的结论是具有普遍性的。</p>\n<blockquote>\n<p>pre-training loss is a good indicator of LMs’ performance on\ndownstream tasks, independent of model sizes, training tokens,\nlanguages, and pretraining frameworks</p>\n</blockquote>\n<h1 id=\"进一步分析\">进一步分析</h1>\n<h2 id=\"不同任务的趋势\">不同任务的趋势</h2>\n<p>12个下游任务可以分为2类：<br>\n- 第一类：TriviaQA, HellaSwag, RACE, WinoGrande, NLPCC-KBQA, ClozeT,\nCLUEWSC, C3。这些任务的效果随着预训练loss的下降，平滑上升。<br>\n- 第二类：MMLU, C-Eval, GSM8K,\nGSM8K-Chinese。这些任务上，只有当预训练loss低于一定阈值，评测结果才开始提升。可以观察到，在实验一实验二的配置下，大概在预训练loss小于2.2这个阈值之后，下游任务表现开始提升。整体来说，第二类任务难度是大于第一类的。所以虽然第一类中有些任务的prompt或者形式与第二类中的任务有些相似，但是依然有不同的表现。</p>\n<p>第二类任务这个现象和《Grokking: Generalization beyond overfitting on\nsmall algorithmic datasets》提出的grokking有关联。</p>\n<p>grokking描述了下游任务的效果从随机水平（乱猜）提高到perfect\ngeneralization的improvement。这种improvement只有在过拟合到一定程度才会发生。在预训练中，模型整体上通常是欠拟合的。不过由于预训练语料库是不同文档的混合，因此模型可能在某些能力上过拟合（比如数值计算的能力，情感分析的能力），而在整体上依然欠拟合。</p>\n<p>当然第二类任务这个现象也和emergent ability有关联。按scaling\nlaw的说法，在训练token数固定的情况下，预训练loss与模型规模呈幂律关系。也就是说，模型大小和预训练损失之间存在单调关系。对于第二类任务，存在一个与预训练loss中的临界点相对应的模型规模阈值。当模型大小超过这个阈值时，模型就可以展现出超过随机猜测的能力。</p>\n<h2 id=\"评测指标的影响\">评测指标的影响</h2>\n<p>前面提到，emergent\nability这个现象有可能是因为评测指标的非线性和不连续性带来的。比如\nMMLU这样的多项选择题，打分结果只能是0分或者满分。</p>\n<p>现在把这个评测指标换成两个连续的指标：<br>\n- 一个是probability of the correct answer (CorrectChoiceProb)<br>\n- 第二个是《Are emergent abilities of large language models a\nmirage?》中提出的Brier Score：</p>\n<p><span class=\"math display\">\\[\\text{BrierScore}=\\frac1N\\sum_{i=1}^N\\sum_{j=1}^C(y_{ij}-\\hat{y}_{ij})^2\\]</span></p>\n<p>N是样本数，C的类别数。</p>\n<p>把MMLU和C-Eval在这两个新指标上的评测结果画出来，如下所示</p>\n<img src=\"/f5fb75e4/metrics.png\" class title=\"指标\">\n<p>可以发现涌现能力的现象依然存在。</p>\n<p>值得注意的是，Brier Score的下降并不总是表示下游任务效果的提升。</p>\n<p>比如对于有A/B/C/D四个选项的多项选择题任务，假设正确答案的分布是均匀的。现在有两个模型，一个总是预测A，即（1，0，0，0），另一个总是给出平均分布的预测，即（0.25，0.25，0.25，0.25，0.25）。</p>\n<p>那么前者的Brier\nScore是1.5，而后者是0.75，但这并不能说明后者就更好。对于这个任务，实际上高于0.75的Brier\nScore都说明比随机猜测差。而低于随机猜测的指标变化并不能当做真正的提升，比如Brier\nScore从1.5提升到1.0并不能算作提升。</p>\n<p>另外《Training trajectories of language models across\nscales》提出用perplexity of correct\noptions来作为评测，可以看到平滑的提升。但perplexity of correct\noptions其实不能作为一个合适的指标。</p>\n<p>比如对于多项选择题，区分各个答案的能力才是我们想要的。而随着预训练进行，正确答案和错误答案的perplexity都在下降，只有当训练到二者的perplexity差异开始变大的时候，才能算是有提升。因此单纯的正确答案perplexity下降也能作为能力提升的指标，因为错误答案的perplexity可能下降更多。</p>\n<h1 id=\"从loss角度定义emergent-abilities\">从loss角度定义emergent\nabilities</h1>\n<p>基于前面的实验和分析，现在从预训练loss角度重新定义emergent\nability：</p>\n<blockquote>\n<p>Definition. An ability is emergent if it is not present in models\nwith higher pre-training loss but is present in models with lower\npre-training loss.</p>\n</blockquote>\n<p>一个emergent ability的normalized\nperformance（比如多项选择题随机猜测的得分是0.25分，那这个任务原始的0.25分在normalized\nperformance下就是0分）是预训练loss <span class=\"math inline\">\\(L\\)</span> 的函数</p>\n<p><span class=\"math display\">\\[\\begin{cases}f(L)&amp;\\mathrm{if~}L&lt;\\eta\\\\0&amp;\\mathrm{otherwise}&amp;\\end{cases}\\]</span></p>\n<p>其中f是一个单调递减函数，<span class=\"math inline\">\\(\\eta\\)</span>\n是阈值。</p>\n<p>《Scaling laws for autoregressive generative\nmodeling》中提出，在固定的训练token数 <span class=\"math inline\">\\(D\\)</span> 下，模型规模 <span class=\"math inline\">\\(N\\)</span> 和预训练损失的关系是</p>\n<p><span class=\"math display\">\\[L(N)=L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(L_{\\infty}\\)</span> 是irreducible\nloss，<span class=\"math inline\">\\(\\alpha_{N}\\)</span> 是固定的系数。</p>\n<p>把上面两个式子结合起来，就有</p>\n<p><span class=\"math display\">\\[\\begin{cases}f\\left(L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\right)&amp;\\text{if\n}N\\geq\nN_0\\cdot\\left(\\eta-L_\\infty\\right)^{-\\frac1{\\alpha_N}}\\\\0&amp;\\text{otherwise}&amp;\\end{cases}\\]</span></p>\n<p>当模型规模小于 <span class=\"math inline\">\\(N_0\\cdot(\\eta-L_\\infty)^{-1/\\alpha_N}\\)</span>\n这个阈值时，normalized\nperformance为0；当模型规模超过这个阈值时，模型规模的增长带来了预训练loss的下降，从而带来了normalized\nperformance的提升。</p>\n<h1 id=\"小结\">小结</h1>\n<p>通过预训练loss来预测下游任务的提升，这点用在预训练模型的分析和优化上还是有些帮助的。比如在loss较高的时候，在下游任务上的效果的变化可能更多是随机波动而不是真正的提升。</p>\n<p>不过文中只对一个model\nfamily做了实验，而loss和模型结构，词表等都有关系，因此还需要进一步的探索。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">大模型推理加速-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">大模型算法题(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Understanding Emergent Abilities of Language Models from the\nLoss Perspective https://arxiv.org/abs/2403.15796</p>\n","length":5448,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>智谱在《Understanding Emergent Abilities of Language Models from the\nLoss Perspective》中提出一个观察大模型涌现能力的视角 --\n预训练loss，主要内容是通过一系列实验结果来解释一些关于涌现能力的观察。可以作为一个理解大模型的参考角度，也可以用于指导预训练模型的开发和优化。</p>\n<h1 id=\"背景\">背景</h1>\n<p>《Emergent abilities of large language models》把emergent\nability定义为在大规模模型中有，而在参数量较小的模型没有的能力。</p>\n<p>这个看法现在受到一些挑战：<br>\n-\n目前很多在更大规模数据集训练出来的小模型，展现出比之前大规模模型更强的能力，比如LLaMA3在大部分评测指标上就比GPT-3强，很多以前千亿模型才能做到的任务，现在百亿甚至十亿的模型也能做好。<br>\n- 《Are emergent abilities of large language models a\nmirage?》认为产生涌现能力现象的因为是数据评测指标的非线性和不连续性带来的，如果使用更细粒度的连续指标，就能观察到指标的平滑提升。</p>\n<p>而《Training compute-optimal large language\nmodels》指出，相同的计算量下，不同的模型规模和数据量的组合会产生不同的效果。这说明单纯的模型规模或者数据规模并不是一个好的下游任务能力的indicator，预训练loss才是更合适的指标。</p>\n<p>但是训练loss和下游任务表现具体是什么关系却还没有确定的说法，智谱针对这个问题做了一些预训练实验，并从预训练loss角度定义了emergent\nability。</p>\n<h1 id=\"pretraining-loss和下游任务表现的关系\">pretraining\nloss和下游任务表现的关系</h1>\n<h2 id=\"设置\">设置</h2>\n<p>后续所有预训练实验使用相同的模型结构和同一份预训练数据（但是训练数据量可能有区别），一些通用设置如下：<br>\n- 分词用BPE<br>\n- 模型结构在LLaMA基础上，全部使用GQA，而RoPE只在一半的Q/K上应用<br>\n- 使用AdamW优化器，<span class=\"math inline\">\\(\\beta_1=0.9\\)</span>，<span class=\"math inline\">\\(\\beta_2=0.95\\)</span><br>\n- 训练窗口长度为2048<br>\n-\n所有模型都在中英文比例为1:4的预训练数据集上训练，英文数据集分布如下</p>\n<img src=\"/f5fb75e4/eng_data.png\" class title=\"英文数据\">\n<p>所有模型都是从零开始预训练。</p>\n<p>评测模型的下游任务共有6类12个数据集，具体信息如下</p>\n<img src=\"/f5fb75e4/downstream_dataset.png\" class title=\"下游任务\">\n<img src=\"/f5fb75e4/downstream_dataset_num.png\" class title=\"下游任务\">\n<h2 id=\"实验一pretraining-loss-vs.-performance\">实验一：pretraining loss\nvs. performance</h2>\n<p>第一个实验训练了3个规模的模型：1.5B、6B、32B，训练数据量分别为3T、3T、2.5T。具体设置如下</p>\n<img src=\"/f5fb75e4/exp1_param.png\" class title=\"实验设置\">\n<p>大约每训练43B\ntoken就会保存一次checkpoint。把3个模型所有checkpoint下，对应的预训练loss和下游任务评测结果画出来，如下所示</p>\n<img src=\"/f5fb75e4/exp1_plot.png\" class title=\"loss vs. performance\">\n<p>从上图可以观察到3个现象：<br>\n-\n无论模型规模如何，下游任务评测结果都随着预训练loss的降低而提升。从提升的具体情况可以分成两类，这个后面部分再分析。<br>\n-\n各个规模的模型所画出的点都落在了同一条曲线上，这说明下游任务的评测结果和预训练loss高度相关，而和模型规模没有直接关系。这点很重要。<br>\n-\n预训练loss对下游任务指标的表征能力同时适用于中英文，这说明中英文token在多语言预训练中具有相似的learning\ndynamics。</p>\n<p>而把计算量和下游任务指标的关系画出来，则有如下结果</p>\n<img src=\"/f5fb75e4/exp1_compute.png\" class title=\"下游任务效果和预训练计算量的关系\">\n<p>可以看到各个规模的模型所画出的点并没有落在同一条曲线上，这说明计算量并不是表征下游任务效果的好指标。</p>\n<h2 id=\"实验二training-token-count-vs.-performance\">实验二：training\ntoken count vs. performance</h2>\n<p>第二个实验使用了不同的数据量训练了28个小一些的模型，具体设置如下</p>\n<img src=\"/f5fb75e4/exp2_param.png\" class title=\"实验设置\">\n<p>第一个实验中，每个规模的模型设置了一个固定的训练token数，然后取中间checkpoint进行评测。第二个实验是对每个规模的模型设置了多个不同的总训练token数。二者的区别在于，预训练的最后阶段会逐渐把学习率decay到最小值，而这样的学习率退火策略对效果有很大的影响。</p>\n<p>取28个模型的最终checkpoint，画出对应的预训练loss和下游任务评测结果如下</p>\n<img src=\"/f5fb75e4/exp2_plot.png\" class title=\"token count vs. performance\">\n<p>结果和实验一类似，各个模型的点都落在了同一条曲线上。说明无论模型规模和训练量如何，只要loss相同，在下游任务上就有相同的表现。</p>\n<p>由于这28个模型相比实验一的较小，在图中最后一排的任务上效果都接近于随机。这个现象后续分析。</p>\n<h2 id=\"llamas-loss-vs.-performance\">LLaMA’s loss vs. performance</h2>\n<p>实验一和二是在从零开始训练的模型上评测的，这里用LLaMA来验证前面得到的结论。</p>\n<p>由于LLaMA没有放出中间checkpoint，这里直接从LLaMA的报告里抽出相应的数据点，在6个下游任务上的结果如下图</p>\n<img src=\"/f5fb75e4/exp3_plot.png\" class title=\"loss vs. performance\">\n<p>可以看到基本上各个模型的点也是落在同一条曲线上。LLaMA和实验一实验二的训练框架、模型结构、训练数据都有所不同，但是也有相同的结论，说明这样的结论是具有普遍性的。</p>\n<blockquote>\n<p>pre-training loss is a good indicator of LMs’ performance on\ndownstream tasks, independent of model sizes, training tokens,\nlanguages, and pretraining frameworks</p>\n</blockquote>\n<h1 id=\"进一步分析\">进一步分析</h1>\n<h2 id=\"不同任务的趋势\">不同任务的趋势</h2>\n<p>12个下游任务可以分为2类：<br>\n- 第一类：TriviaQA, HellaSwag, RACE, WinoGrande, NLPCC-KBQA, ClozeT,\nCLUEWSC, C3。这些任务的效果随着预训练loss的下降，平滑上升。<br>\n- 第二类：MMLU, C-Eval, GSM8K,\nGSM8K-Chinese。这些任务上，只有当预训练loss低于一定阈值，评测结果才开始提升。可以观察到，在实验一实验二的配置下，大概在预训练loss小于2.2这个阈值之后，下游任务表现开始提升。整体来说，第二类任务难度是大于第一类的。所以虽然第一类中有些任务的prompt或者形式与第二类中的任务有些相似，但是依然有不同的表现。</p>\n<p>第二类任务这个现象和《Grokking: Generalization beyond overfitting on\nsmall algorithmic datasets》提出的grokking有关联。</p>\n<p>grokking描述了下游任务的效果从随机水平（乱猜）提高到perfect\ngeneralization的improvement。这种improvement只有在过拟合到一定程度才会发生。在预训练中，模型整体上通常是欠拟合的。不过由于预训练语料库是不同文档的混合，因此模型可能在某些能力上过拟合（比如数值计算的能力，情感分析的能力），而在整体上依然欠拟合。</p>\n<p>当然第二类任务这个现象也和emergent ability有关联。按scaling\nlaw的说法，在训练token数固定的情况下，预训练loss与模型规模呈幂律关系。也就是说，模型大小和预训练损失之间存在单调关系。对于第二类任务，存在一个与预训练loss中的临界点相对应的模型规模阈值。当模型大小超过这个阈值时，模型就可以展现出超过随机猜测的能力。</p>\n<h2 id=\"评测指标的影响\">评测指标的影响</h2>\n<p>前面提到，emergent\nability这个现象有可能是因为评测指标的非线性和不连续性带来的。比如\nMMLU这样的多项选择题，打分结果只能是0分或者满分。</p>\n<p>现在把这个评测指标换成两个连续的指标：<br>\n- 一个是probability of the correct answer (CorrectChoiceProb)<br>\n- 第二个是《Are emergent abilities of large language models a\nmirage?》中提出的Brier Score：</p>\n<p><span class=\"math display\">\\[\\text{BrierScore}=\\frac1N\\sum_{i=1}^N\\sum_{j=1}^C(y_{ij}-\\hat{y}_{ij})^2\\]</span></p>\n<p>N是样本数，C的类别数。</p>\n<p>把MMLU和C-Eval在这两个新指标上的评测结果画出来，如下所示</p>\n<img src=\"/f5fb75e4/metrics.png\" class title=\"指标\">\n<p>可以发现涌现能力的现象依然存在。</p>\n<p>值得注意的是，Brier Score的下降并不总是表示下游任务效果的提升。</p>\n<p>比如对于有A/B/C/D四个选项的多项选择题任务，假设正确答案的分布是均匀的。现在有两个模型，一个总是预测A，即（1，0，0，0），另一个总是给出平均分布的预测，即（0.25，0.25，0.25，0.25，0.25）。</p>\n<p>那么前者的Brier\nScore是1.5，而后者是0.75，但这并不能说明后者就更好。对于这个任务，实际上高于0.75的Brier\nScore都说明比随机猜测差。而低于随机猜测的指标变化并不能当做真正的提升，比如Brier\nScore从1.5提升到1.0并不能算作提升。</p>\n<p>另外《Training trajectories of language models across\nscales》提出用perplexity of correct\noptions来作为评测，可以看到平滑的提升。但perplexity of correct\noptions其实不能作为一个合适的指标。</p>\n<p>比如对于多项选择题，区分各个答案的能力才是我们想要的。而随着预训练进行，正确答案和错误答案的perplexity都在下降，只有当训练到二者的perplexity差异开始变大的时候，才能算是有提升。因此单纯的正确答案perplexity下降也能作为能力提升的指标，因为错误答案的perplexity可能下降更多。</p>\n<h1 id=\"从loss角度定义emergent-abilities\">从loss角度定义emergent\nabilities</h1>\n<p>基于前面的实验和分析，现在从预训练loss角度重新定义emergent\nability：</p>\n<blockquote>\n<p>Definition. An ability is emergent if it is not present in models\nwith higher pre-training loss but is present in models with lower\npre-training loss.</p>\n</blockquote>\n<p>一个emergent ability的normalized\nperformance（比如多项选择题随机猜测的得分是0.25分，那这个任务原始的0.25分在normalized\nperformance下就是0分）是预训练loss <span class=\"math inline\">\\(L\\)</span> 的函数</p>\n<p><span class=\"math display\">\\[\\begin{cases}f(L)&amp;\\mathrm{if~}L&lt;\\eta\\\\0&amp;\\mathrm{otherwise}&amp;\\end{cases}\\]</span></p>\n<p>其中f是一个单调递减函数，<span class=\"math inline\">\\(\\eta\\)</span>\n是阈值。</p>\n<p>《Scaling laws for autoregressive generative\nmodeling》中提出，在固定的训练token数 <span class=\"math inline\">\\(D\\)</span> 下，模型规模 <span class=\"math inline\">\\(N\\)</span> 和预训练损失的关系是</p>\n<p><span class=\"math display\">\\[L(N)=L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(L_{\\infty}\\)</span> 是irreducible\nloss，<span class=\"math inline\">\\(\\alpha_{N}\\)</span> 是固定的系数。</p>\n<p>把上面两个式子结合起来，就有</p>\n<p><span class=\"math display\">\\[\\begin{cases}f\\left(L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\right)&amp;\\text{if\n}N\\geq\nN_0\\cdot\\left(\\eta-L_\\infty\\right)^{-\\frac1{\\alpha_N}}\\\\0&amp;\\text{otherwise}&amp;\\end{cases}\\]</span></p>\n<p>当模型规模小于 <span class=\"math inline\">\\(N_0\\cdot(\\eta-L_\\infty)^{-1/\\alpha_N}\\)</span>\n这个阈值时，normalized\nperformance为0；当模型规模超过这个阈值时，模型规模的增长带来了预训练loss的下降，从而带来了normalized\nperformance的提升。</p>\n<h1 id=\"小结\">小结</h1>\n<p>通过预训练loss来预测下游任务的提升，这点用在预训练模型的分析和优化上还是有些帮助的。比如在loss较高的时候，在下游任务上的效果的变化可能更多是随机波动而不是真正的提升。</p>\n<p>不过文中只对一个model\nfamily做了实验，而loss和模型结构，词表等都有关系，因此还需要进一步的探索。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">大模型推理加速-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">大模型算法题(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Understanding Emergent Abilities of Language Models from the\nLoss Perspective https://arxiv.org/abs/2403.15796</p>\n"},{"title":"大模型偏好对齐-IPO","abbrlink":"4fe7b810","date":"2024-06-02T03:58:52.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n前面我们对DPO、ODPO、simPO的思路做了整理：[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)，[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)，[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)。  \n\n而《A General Theoretical Paradigm to Understand Learning from Human Preferences》提出了可以将RLHF和DPO的目标函数视为其中一个特例的更general的目标函数ΨPO，并对ΨPO的一些问题进行了分析，最终设计了Identity-PO (IPO)来绕过这些问题。  \n\n# ΨPO  \n\n回顾一下RLHF，它的目标函数是  \n\n$$\\mathbb{E}_\\pi[r(x,y)]-\\beta D_{\\text{KL}}(\\pi\\mid\\mid\\pi_{\\text{ref}})$$  \n\n而DPO从等价的目标函数推导出DPO的损失函数如下  \n\n$$\\begin{aligned}\\min_{\\pi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\Bigg[-\\log\\sigma\\Bigg(\\beta\\log\\Bigg(\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}\\Bigg)-\\beta\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(y_w|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\Bigg)\\Bigg)\\right]\\end{aligned}$$  \n\nIPO这篇论文则提出一个general的目标函数。考虑一个对preference probability进行非线性变换的non-decreasing function Ψ  \n\n$$\\Psi:\\begin{bmatrix}0,1\\end{bmatrix}\\to\\mathbb{R}$$  \n\nΨ-preference optimisation objective定义为  \n\n$$\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[\\Psi(p^*(y\\succ y'|x))]-\\beta D_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})$$  \n\n如果我们给Ψ一个具体定义，如下式  \n\n$$\\Psi(q)=\\log(q/(1-q))$$  \n\n那么在Bradley-Terry model的假设下，我们有  \n\n$$\\begin{aligned}\n\\mathbb{E}_{y'\\thicksim\\mu}[\\Psi(p^*(y\\succ y'))]& =\\underset{y'\\thicksim\\mu}{\\operatorname*{\\mathbb{E}}}\\left[\\Psi\\left(\\frac{e^{r(y)}}{e^{r(y)}+e^{r(y')}}\\right)\\right]  \\\\\n&=\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\log(e^{r(y)}/e^{r(y^{\\prime})})] \\\\\n&=\\mathbb{E}_{y'\\thicksim\\mu}[r(y)-r(y')] \\\\\n&=r(y)-\\underset{y'\\thicksim\\mu}{\\mathbb{E}}[r(y')]\n\\end{aligned}$$  \n\n右边最终结果里的第二项可视为常数。除去这个常数，ΨPO的优化目标和RLHF的优化目标是等价的，同时也就和DPO的目标是等价的。  \n\n同DPO的做法一样，这里我们可以推出ΨPO在Bradley-Terry model下的解析解  \n\n$$\\pi^*(y)\\propto\\pi_{\\mathrm{ref}}(y)\\exp\\left(\\beta^{-1}\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\Psi(p^*(y\\succ y^{\\prime}))]\\right)$$  \n\n我们把Ψ(q)的图像画出来，如下所示  \n\n{% asset_img curve.png log %}  \n\n可以看到在两端，Ψ(q)的曲线有很强的非线性化特征，并且值会趋向于无穷大。  \n\n那么当我们对一对质量差异很大的样本，即  \n\n$$p^*(y\\succ y')=1$$  \n\n进行学习时，在BT模型的假设下，就有  \n\n$$(r(y)-r(y'))\\to+\\infty$$  \n\n把 $(r(y)-r(y'))\\to+\\infty$ 代入到ΨPO上面退出来的解析解里，有  \n\n$$\\begin{aligned}\n&\\frac{\\pi^*(y_l)}{\\pi^*(y_w)}\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}\\left(\\beta^{-1}\\sum_{y^{\\prime}}[\\Psi(p(y_l\\succ y^{\\prime}))-\\Psi(p(y_w\\succ y^{\\prime}))]\\right)\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[r(y_l)-r(y_w)])\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[-\\infty])\\\\\n=&0\n\\end{aligned}$$  \n\n那么此时无论 $\\beta$ 取什么值，都有 $\\pi^*(y_l)=0$。说明当偏好越确定，KL项的约束能力越弱，模型就很容易摆脱KL项的约束，过度追求reward的最大化，最终导致过拟合。  \n\n不过RLHF在实践上并没有表现出如这里推算结果一样特别容易过拟合的特性，原因是因为训练出来的reward模型通常由于欠拟合，没有给出那么极端的偏好概率。反而是DPO因为节省了reward模型的训练，因此更加容易受到这种过拟合的困扰。  \n\n# IPO  \n\n既然高度非线性化（且极值无限大）的Ψ(q)会导致DPO容易过拟合，那么一个自然的想法就是把Ψ(q)替换成一个有界的函数，identity mapping恒等变换就是一个符合要求的选择。这样就得到IPO的目标函数  \n\n$$\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[p^*(y\\succ y'|x)]-\\beta D_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})$$  \n\n根据这个，可以推导出IPO的损失函数为  \n\n$$\\mathbb{E}_{(y_w,y_l,x)\\thicksim D}\\left(h_\\pi(y_w,y_l,x)-\\frac{\\beta^{-1}}2\\right)^2$$  \n\n$$h_\\pi(y,y',x)=\\log\\left(\\frac{\\pi(y|x)\\pi_{\\text{ref}}(y'|x)}{\\pi(y'|x)\\pi_{\\text{ref}}(y|x)}\\right)$$  \n\n# 小结  \n\nΨPO/IPO从理论上对DPO进行了一系列的分析，也推出了一个相对更不容易过拟合的偏好学习方法。不过在实践上的证明没有完善，可以作为一个理解的DPO的角度来参考吧。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】A General Theoretical Paradigm to Understand Learning from Human Preferences https://arxiv.org/abs/2310.12036  \n","source":"_posts/cs/nlp/2024/06/大模型偏好对齐-IPO.md","raw":"---\ntitle: 大模型偏好对齐-IPO\nabbrlink: 4fe7b810\ndate: 2024-06-02 11:58:52\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 强化学习\n  - 微调\n  - SFT\n  - 偏好对齐\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n前面我们对DPO、ODPO、simPO的思路做了整理：[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)，[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)，[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)。  \n\n而《A General Theoretical Paradigm to Understand Learning from Human Preferences》提出了可以将RLHF和DPO的目标函数视为其中一个特例的更general的目标函数ΨPO，并对ΨPO的一些问题进行了分析，最终设计了Identity-PO (IPO)来绕过这些问题。  \n\n# ΨPO  \n\n回顾一下RLHF，它的目标函数是  \n\n$$\\mathbb{E}_\\pi[r(x,y)]-\\beta D_{\\text{KL}}(\\pi\\mid\\mid\\pi_{\\text{ref}})$$  \n\n而DPO从等价的目标函数推导出DPO的损失函数如下  \n\n$$\\begin{aligned}\\min_{\\pi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\Bigg[-\\log\\sigma\\Bigg(\\beta\\log\\Bigg(\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}\\Bigg)-\\beta\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(y_w|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\Bigg)\\Bigg)\\right]\\end{aligned}$$  \n\nIPO这篇论文则提出一个general的目标函数。考虑一个对preference probability进行非线性变换的non-decreasing function Ψ  \n\n$$\\Psi:\\begin{bmatrix}0,1\\end{bmatrix}\\to\\mathbb{R}$$  \n\nΨ-preference optimisation objective定义为  \n\n$$\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[\\Psi(p^*(y\\succ y'|x))]-\\beta D_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})$$  \n\n如果我们给Ψ一个具体定义，如下式  \n\n$$\\Psi(q)=\\log(q/(1-q))$$  \n\n那么在Bradley-Terry model的假设下，我们有  \n\n$$\\begin{aligned}\n\\mathbb{E}_{y'\\thicksim\\mu}[\\Psi(p^*(y\\succ y'))]& =\\underset{y'\\thicksim\\mu}{\\operatorname*{\\mathbb{E}}}\\left[\\Psi\\left(\\frac{e^{r(y)}}{e^{r(y)}+e^{r(y')}}\\right)\\right]  \\\\\n&=\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\log(e^{r(y)}/e^{r(y^{\\prime})})] \\\\\n&=\\mathbb{E}_{y'\\thicksim\\mu}[r(y)-r(y')] \\\\\n&=r(y)-\\underset{y'\\thicksim\\mu}{\\mathbb{E}}[r(y')]\n\\end{aligned}$$  \n\n右边最终结果里的第二项可视为常数。除去这个常数，ΨPO的优化目标和RLHF的优化目标是等价的，同时也就和DPO的目标是等价的。  \n\n同DPO的做法一样，这里我们可以推出ΨPO在Bradley-Terry model下的解析解  \n\n$$\\pi^*(y)\\propto\\pi_{\\mathrm{ref}}(y)\\exp\\left(\\beta^{-1}\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\Psi(p^*(y\\succ y^{\\prime}))]\\right)$$  \n\n我们把Ψ(q)的图像画出来，如下所示  \n\n{% asset_img curve.png log %}  \n\n可以看到在两端，Ψ(q)的曲线有很强的非线性化特征，并且值会趋向于无穷大。  \n\n那么当我们对一对质量差异很大的样本，即  \n\n$$p^*(y\\succ y')=1$$  \n\n进行学习时，在BT模型的假设下，就有  \n\n$$(r(y)-r(y'))\\to+\\infty$$  \n\n把 $(r(y)-r(y'))\\to+\\infty$ 代入到ΨPO上面退出来的解析解里，有  \n\n$$\\begin{aligned}\n&\\frac{\\pi^*(y_l)}{\\pi^*(y_w)}\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}\\left(\\beta^{-1}\\sum_{y^{\\prime}}[\\Psi(p(y_l\\succ y^{\\prime}))-\\Psi(p(y_w\\succ y^{\\prime}))]\\right)\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[r(y_l)-r(y_w)])\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[-\\infty])\\\\\n=&0\n\\end{aligned}$$  \n\n那么此时无论 $\\beta$ 取什么值，都有 $\\pi^*(y_l)=0$。说明当偏好越确定，KL项的约束能力越弱，模型就很容易摆脱KL项的约束，过度追求reward的最大化，最终导致过拟合。  \n\n不过RLHF在实践上并没有表现出如这里推算结果一样特别容易过拟合的特性，原因是因为训练出来的reward模型通常由于欠拟合，没有给出那么极端的偏好概率。反而是DPO因为节省了reward模型的训练，因此更加容易受到这种过拟合的困扰。  \n\n# IPO  \n\n既然高度非线性化（且极值无限大）的Ψ(q)会导致DPO容易过拟合，那么一个自然的想法就是把Ψ(q)替换成一个有界的函数，identity mapping恒等变换就是一个符合要求的选择。这样就得到IPO的目标函数  \n\n$$\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[p^*(y\\succ y'|x)]-\\beta D_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})$$  \n\n根据这个，可以推导出IPO的损失函数为  \n\n$$\\mathbb{E}_{(y_w,y_l,x)\\thicksim D}\\left(h_\\pi(y_w,y_l,x)-\\frac{\\beta^{-1}}2\\right)^2$$  \n\n$$h_\\pi(y,y',x)=\\log\\left(\\frac{\\pi(y|x)\\pi_{\\text{ref}}(y'|x)}{\\pi(y'|x)\\pi_{\\text{ref}}(y|x)}\\right)$$  \n\n# 小结  \n\nΨPO/IPO从理论上对DPO进行了一系列的分析，也推出了一个相对更不容易过拟合的偏好学习方法。不过在实践上的证明没有完善，可以作为一个理解的DPO的角度来参考吧。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】A General Theoretical Paradigm to Understand Learning from Human Preferences https://arxiv.org/abs/2310.12036  \n","slug":"cs/nlp/2024/06/大模型偏好对齐-IPO","published":1,"updated":"2024-06-06T14:52:00.000Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9w000k314kedlv11uf","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>前面我们对DPO、ODPO、simPO的思路做了整理：<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a>，<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a>，<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a>。</p>\n<p>而《A General Theoretical Paradigm to Understand Learning from Human\nPreferences》提出了可以将RLHF和DPO的目标函数视为其中一个特例的更general的目标函数ΨPO，并对ΨPO的一些问题进行了分析，最终设计了Identity-PO\n(IPO)来绕过这些问题。</p>\n<h1 id=\"ψpo\">ΨPO</h1>\n<p>回顾一下RLHF，它的目标函数是</p>\n<p><span class=\"math display\">\\[\\mathbb{E}_\\pi[r(x,y)]-\\beta\nD_{\\text{KL}}(\\pi\\mid\\mid\\pi_{\\text{ref}})\\]</span></p>\n<p>而DPO从等价的目标函数推导出DPO的损失函数如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_{\\pi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\Bigg[-\\log\\sigma\\Bigg(\\beta\\log\\Bigg(\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}\\Bigg)-\\beta\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(y_w|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\Bigg)\\Bigg)\\right]\\end{aligned}\\]</span></p>\n<p>IPO这篇论文则提出一个general的目标函数。考虑一个对preference\nprobability进行非线性变换的non-decreasing function Ψ</p>\n<p><span class=\"math display\">\\[\\Psi:\\begin{bmatrix}0,1\\end{bmatrix}\\to\\mathbb{R}\\]</span></p>\n<p>Ψ-preference optimisation objective定义为</p>\n<p><span class=\"math display\">\\[\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[\\Psi(p^*(y\\succ\ny&#39;|x))]-\\beta\nD_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})\\]</span></p>\n<p>如果我们给Ψ一个具体定义，如下式</p>\n<p><span class=\"math display\">\\[\\Psi(q)=\\log(q/(1-q))\\]</span></p>\n<p>那么在Bradley-Terry model的假设下，我们有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathbb{E}_{y&#39;\\thicksim\\mu}[\\Psi(p^*(y\\succ y&#39;))]&amp;\n=\\underset{y&#39;\\thicksim\\mu}{\\operatorname*{\\mathbb{E}}}\\left[\\Psi\\left(\\frac{e^{r(y)}}{e^{r(y)}+e^{r(y&#39;)}}\\right)\\right]  \\\\\n&amp;=\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\log(e^{r(y)}/e^{r(y^{\\prime})})]\n\\\\\n&amp;=\\mathbb{E}_{y&#39;\\thicksim\\mu}[r(y)-r(y&#39;)] \\\\\n&amp;=r(y)-\\underset{y&#39;\\thicksim\\mu}{\\mathbb{E}}[r(y&#39;)]\n\\end{aligned}\\]</span></p>\n<p>右边最终结果里的第二项可视为常数。除去这个常数，ΨPO的优化目标和RLHF的优化目标是等价的，同时也就和DPO的目标是等价的。</p>\n<p>同DPO的做法一样，这里我们可以推出ΨPO在Bradley-Terry\nmodel下的解析解</p>\n<p><span class=\"math display\">\\[\\pi^*(y)\\propto\\pi_{\\mathrm{ref}}(y)\\exp\\left(\\beta^{-1}\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\Psi(p^*(y\\succ\ny^{\\prime}))]\\right)\\]</span></p>\n<p>我们把Ψ(q)的图像画出来，如下所示</p>\n<img src=\"/4fe7b810/curve.png\" class title=\"log\">\n<p>可以看到在两端，Ψ(q)的曲线有很强的非线性化特征，并且值会趋向于无穷大。</p>\n<p>那么当我们对一对质量差异很大的样本，即</p>\n<p><span class=\"math display\">\\[p^*(y\\succ y&#39;)=1\\]</span></p>\n<p>进行学习时，在BT模型的假设下，就有</p>\n<p><span class=\"math display\">\\[(r(y)-r(y&#39;))\\to+\\infty\\]</span></p>\n<p>把 <span class=\"math inline\">\\((r(y)-r(y&#39;))\\to+\\infty\\)</span>\n代入到ΨPO上面退出来的解析解里，有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\frac{\\pi^*(y_l)}{\\pi^*(y_w)}\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}\\left(\\beta^{-1}\\sum_{y^{\\prime}}[\\Psi(p(y_l\\succ\ny^{\\prime}))-\\Psi(p(y_w\\succ y^{\\prime}))]\\right)\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[r(y_l)-r(y_w)])\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[-\\infty])\\\\\n=&amp;0\n\\end{aligned}\\]</span></p>\n<p>那么此时无论 <span class=\"math inline\">\\(\\beta\\)</span>\n取什么值，都有 <span class=\"math inline\">\\(\\pi^*(y_l)=0\\)</span>。说明当偏好越确定，KL项的约束能力越弱，模型就很容易摆脱KL项的约束，过度追求reward的最大化，最终导致过拟合。</p>\n<p>不过RLHF在实践上并没有表现出如这里推算结果一样特别容易过拟合的特性，原因是因为训练出来的reward模型通常由于欠拟合，没有给出那么极端的偏好概率。反而是DPO因为节省了reward模型的训练，因此更加容易受到这种过拟合的困扰。</p>\n<h1 id=\"ipo\">IPO</h1>\n<p>既然高度非线性化（且极值无限大）的Ψ(q)会导致DPO容易过拟合，那么一个自然的想法就是把Ψ(q)替换成一个有界的函数，identity\nmapping恒等变换就是一个符合要求的选择。这样就得到IPO的目标函数</p>\n<p><span class=\"math display\">\\[\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[p^*(y\\succ\ny&#39;|x)]-\\beta\nD_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})\\]</span></p>\n<p>根据这个，可以推导出IPO的损失函数为</p>\n<p><span class=\"math display\">\\[\\mathbb{E}_{(y_w,y_l,x)\\thicksim\nD}\\left(h_\\pi(y_w,y_l,x)-\\frac{\\beta^{-1}}2\\right)^2\\]</span></p>\n<p><span class=\"math display\">\\[h_\\pi(y,y&#39;,x)=\\log\\left(\\frac{\\pi(y|x)\\pi_{\\text{ref}}(y&#39;|x)}{\\pi(y&#39;|x)\\pi_{\\text{ref}}(y|x)}\\right)\\]</span></p>\n<h1 id=\"小结\">小结</h1>\n<p>ΨPO/IPO从理论上对DPO进行了一系列的分析，也推出了一个相对更不容易过拟合的偏好学习方法。不过在实践上的证明没有完善，可以作为一个理解的DPO的角度来参考吧。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】A General Theoretical Paradigm to Understand Learning from Human\nPreferences https://arxiv.org/abs/2310.12036</p>\n","length":3544,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>前面我们对DPO、ODPO、simPO的思路做了整理：<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a>，<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a>，<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a>。</p>\n<p>而《A General Theoretical Paradigm to Understand Learning from Human\nPreferences》提出了可以将RLHF和DPO的目标函数视为其中一个特例的更general的目标函数ΨPO，并对ΨPO的一些问题进行了分析，最终设计了Identity-PO\n(IPO)来绕过这些问题。</p>\n<h1 id=\"ψpo\">ΨPO</h1>\n<p>回顾一下RLHF，它的目标函数是</p>\n<p><span class=\"math display\">\\[\\mathbb{E}_\\pi[r(x,y)]-\\beta\nD_{\\text{KL}}(\\pi\\mid\\mid\\pi_{\\text{ref}})\\]</span></p>\n<p>而DPO从等价的目标函数推导出DPO的损失函数如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_{\\pi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\Bigg[-\\log\\sigma\\Bigg(\\beta\\log\\Bigg(\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}\\Bigg)-\\beta\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(y_w|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\Bigg)\\Bigg)\\right]\\end{aligned}\\]</span></p>\n<p>IPO这篇论文则提出一个general的目标函数。考虑一个对preference\nprobability进行非线性变换的non-decreasing function Ψ</p>\n<p><span class=\"math display\">\\[\\Psi:\\begin{bmatrix}0,1\\end{bmatrix}\\to\\mathbb{R}\\]</span></p>\n<p>Ψ-preference optimisation objective定义为</p>\n<p><span class=\"math display\">\\[\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[\\Psi(p^*(y\\succ\ny&#39;|x))]-\\beta\nD_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})\\]</span></p>\n<p>如果我们给Ψ一个具体定义，如下式</p>\n<p><span class=\"math display\">\\[\\Psi(q)=\\log(q/(1-q))\\]</span></p>\n<p>那么在Bradley-Terry model的假设下，我们有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathbb{E}_{y&#39;\\thicksim\\mu}[\\Psi(p^*(y\\succ y&#39;))]&amp;\n=\\underset{y&#39;\\thicksim\\mu}{\\operatorname*{\\mathbb{E}}}\\left[\\Psi\\left(\\frac{e^{r(y)}}{e^{r(y)}+e^{r(y&#39;)}}\\right)\\right]  \\\\\n&amp;=\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\log(e^{r(y)}/e^{r(y^{\\prime})})]\n\\\\\n&amp;=\\mathbb{E}_{y&#39;\\thicksim\\mu}[r(y)-r(y&#39;)] \\\\\n&amp;=r(y)-\\underset{y&#39;\\thicksim\\mu}{\\mathbb{E}}[r(y&#39;)]\n\\end{aligned}\\]</span></p>\n<p>右边最终结果里的第二项可视为常数。除去这个常数，ΨPO的优化目标和RLHF的优化目标是等价的，同时也就和DPO的目标是等价的。</p>\n<p>同DPO的做法一样，这里我们可以推出ΨPO在Bradley-Terry\nmodel下的解析解</p>\n<p><span class=\"math display\">\\[\\pi^*(y)\\propto\\pi_{\\mathrm{ref}}(y)\\exp\\left(\\beta^{-1}\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\Psi(p^*(y\\succ\ny^{\\prime}))]\\right)\\]</span></p>\n<p>我们把Ψ(q)的图像画出来，如下所示</p>\n<img src=\"/4fe7b810/curve.png\" class title=\"log\">\n<p>可以看到在两端，Ψ(q)的曲线有很强的非线性化特征，并且值会趋向于无穷大。</p>\n<p>那么当我们对一对质量差异很大的样本，即</p>\n<p><span class=\"math display\">\\[p^*(y\\succ y&#39;)=1\\]</span></p>\n<p>进行学习时，在BT模型的假设下，就有</p>\n<p><span class=\"math display\">\\[(r(y)-r(y&#39;))\\to+\\infty\\]</span></p>\n<p>把 <span class=\"math inline\">\\((r(y)-r(y&#39;))\\to+\\infty\\)</span>\n代入到ΨPO上面退出来的解析解里，有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\frac{\\pi^*(y_l)}{\\pi^*(y_w)}\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}\\left(\\beta^{-1}\\sum_{y^{\\prime}}[\\Psi(p(y_l\\succ\ny^{\\prime}))-\\Psi(p(y_w\\succ y^{\\prime}))]\\right)\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[r(y_l)-r(y_w)])\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[-\\infty])\\\\\n=&amp;0\n\\end{aligned}\\]</span></p>\n<p>那么此时无论 <span class=\"math inline\">\\(\\beta\\)</span>\n取什么值，都有 <span class=\"math inline\">\\(\\pi^*(y_l)=0\\)</span>。说明当偏好越确定，KL项的约束能力越弱，模型就很容易摆脱KL项的约束，过度追求reward的最大化，最终导致过拟合。</p>\n<p>不过RLHF在实践上并没有表现出如这里推算结果一样特别容易过拟合的特性，原因是因为训练出来的reward模型通常由于欠拟合，没有给出那么极端的偏好概率。反而是DPO因为节省了reward模型的训练，因此更加容易受到这种过拟合的困扰。</p>\n<h1 id=\"ipo\">IPO</h1>\n<p>既然高度非线性化（且极值无限大）的Ψ(q)会导致DPO容易过拟合，那么一个自然的想法就是把Ψ(q)替换成一个有界的函数，identity\nmapping恒等变换就是一个符合要求的选择。这样就得到IPO的目标函数</p>\n<p><span class=\"math display\">\\[\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[p^*(y\\succ\ny&#39;|x)]-\\beta\nD_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})\\]</span></p>\n<p>根据这个，可以推导出IPO的损失函数为</p>\n<p><span class=\"math display\">\\[\\mathbb{E}_{(y_w,y_l,x)\\thicksim\nD}\\left(h_\\pi(y_w,y_l,x)-\\frac{\\beta^{-1}}2\\right)^2\\]</span></p>\n<p><span class=\"math display\">\\[h_\\pi(y,y&#39;,x)=\\log\\left(\\frac{\\pi(y|x)\\pi_{\\text{ref}}(y&#39;|x)}{\\pi(y&#39;|x)\\pi_{\\text{ref}}(y|x)}\\right)\\]</span></p>\n<h1 id=\"小结\">小结</h1>\n<p>ΨPO/IPO从理论上对DPO进行了一系列的分析，也推出了一个相对更不容易过拟合的偏好学习方法。不过在实践上的证明没有完善，可以作为一个理解的DPO的角度来参考吧。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】A General Theoretical Paradigm to Understand Learning from Human\nPreferences https://arxiv.org/abs/2310.12036</p>\n"},{"title":"大模型算法题(7)","abbrlink":"dd614e12","date":"2024-06-12T14:13:46.000Z","_content":"\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~  \n\n如有错漏，欢迎指正~\n\n***  \n\n# 1.MoE模型训练中，如果不对专家的路由进行适当干预，可能会遇到什么问题，有什么解决方法？  \n\nMoE使用多个并行的expert，每次推理只选择其中的一小部分expert使用。  \n\n如果让模型完全自行学习，有可能出现routing collapse的问题，也就是模型倾向于总是选择那几个常用的专家。  \n\n而这些常用的专家由于使用得更多，训练得更好，又会提升被路由到的概率，导致大部分模型参数几乎没有被用上。  \n\n一般可以通过增加一个负载平衡的loss来缓解。负载平衡loss有不同的设计和计算方式，但是大致的思路都是迫使模型均匀地使用不同的专家，如果出现某些专家被选中的概率过高，就会进行惩罚。  \n\n# 2.Bert的预训练方式是MLM，通过[Mask] token对部分输入进行掩盖，要求模型预测。为什么要使用[Mask] token而不直接修改attention mask矩阵？  \n\n直接修改attention mask矩阵也可以让模型看不到对应位置的输入，但是相比使用[Mask] token缺少了位置编码的信息。  \n\n另外使用[Mask] token掩盖要预测的值这种做法在实现上相对方便，只需要对输入数据进行处理即可，而不需要修改modeling的内容，更加通用。  \n\n\n# 3.为什么大模型训练的时候需要warmup？  \n\n在训练前期，刚随机初始化的模型参数离收敛值很远，此时的loss会比较大，梯度也会很大。如果直接使用固定的较大learning rate，模型容易过拟合到早期step见过的这些数据。  \n\n依照ResNet的实验结果，如果一开始模型就跑偏了，那后面再怎么训练，收敛效果都会比较差，说明早期太大的学习率导致模型过早收敛到不太好的局部最优了。  \n\n另外，模型刚开始训练的时候，大学习率带来的大更新值，会导致模型参数的震荡会很大，使得模型学到的参数很不稳定，这也不利于训练。  \n\n使用少数step让模型进行热身，可以很大程度规避这些问题。  \n\n# 4.Roberta在Bert的基础上做了什么优化？  \n\n1. Bert在MLM训练中，提早把要mask的token处理好，在训练时训了多个epoch。这样在不同的epoch中，同一条sample总是使用相同的mask进行训练。Roberta使用了dynamic mask，即每条数据在训练前才随机决定进行mask的位置，这样不同的epoch之间同一条样本也有不同的mask结果，提升了数据多样性。  \n\n2. Bert使用了MLM和NSP两个任务，而Roberta通过实验发现NSP的作用不大，因此直接取消了NSP任务。  \n\n3. Roberta增大了batch size，提高训练效率，以获取更好的训练结果。  \n\n4. Roberta增大了训练数据和训练step数，实验表明模型继续训练还能进一步收敛。  \n\n5. Bert使用WordPiece分词，而Roberta使用BBPE，增大了词表。  \n\n6. 再后来，Google发布了WWM全词mask，改进了mask方式。Roberta-WWM也成了最广泛使用的版本。  \n\n# 5.LoRA的参数是怎么初始化的？  \n\nLoRA包含一个降维矩阵A，和一个升维矩阵B。矩阵A用随机高斯分布初始化，而矩阵B用初始化为0。这样可以使得训练开始的时候，LoRA的参数不产生效果，模型能够保持增加LoRA前的输出。但是A、B矩阵不能同时为0，这样会有对称性问题，降低了模型的表达能力。  \n\n{% asset_img lora.png lora %}  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  ","source":"_posts/cs/nlp/2024/06/大模型算法题-7.md","raw":"---\ntitle: 大模型算法题(7)\ntags:\n  - NLP\n  - LLM\n  - 算法题\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: dd614e12\ndate: 2024-06-12 22:13:46\n---\n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~  \n\n如有错漏，欢迎指正~\n\n***  \n\n# 1.MoE模型训练中，如果不对专家的路由进行适当干预，可能会遇到什么问题，有什么解决方法？  \n\nMoE使用多个并行的expert，每次推理只选择其中的一小部分expert使用。  \n\n如果让模型完全自行学习，有可能出现routing collapse的问题，也就是模型倾向于总是选择那几个常用的专家。  \n\n而这些常用的专家由于使用得更多，训练得更好，又会提升被路由到的概率，导致大部分模型参数几乎没有被用上。  \n\n一般可以通过增加一个负载平衡的loss来缓解。负载平衡loss有不同的设计和计算方式，但是大致的思路都是迫使模型均匀地使用不同的专家，如果出现某些专家被选中的概率过高，就会进行惩罚。  \n\n# 2.Bert的预训练方式是MLM，通过[Mask] token对部分输入进行掩盖，要求模型预测。为什么要使用[Mask] token而不直接修改attention mask矩阵？  \n\n直接修改attention mask矩阵也可以让模型看不到对应位置的输入，但是相比使用[Mask] token缺少了位置编码的信息。  \n\n另外使用[Mask] token掩盖要预测的值这种做法在实现上相对方便，只需要对输入数据进行处理即可，而不需要修改modeling的内容，更加通用。  \n\n\n# 3.为什么大模型训练的时候需要warmup？  \n\n在训练前期，刚随机初始化的模型参数离收敛值很远，此时的loss会比较大，梯度也会很大。如果直接使用固定的较大learning rate，模型容易过拟合到早期step见过的这些数据。  \n\n依照ResNet的实验结果，如果一开始模型就跑偏了，那后面再怎么训练，收敛效果都会比较差，说明早期太大的学习率导致模型过早收敛到不太好的局部最优了。  \n\n另外，模型刚开始训练的时候，大学习率带来的大更新值，会导致模型参数的震荡会很大，使得模型学到的参数很不稳定，这也不利于训练。  \n\n使用少数step让模型进行热身，可以很大程度规避这些问题。  \n\n# 4.Roberta在Bert的基础上做了什么优化？  \n\n1. Bert在MLM训练中，提早把要mask的token处理好，在训练时训了多个epoch。这样在不同的epoch中，同一条sample总是使用相同的mask进行训练。Roberta使用了dynamic mask，即每条数据在训练前才随机决定进行mask的位置，这样不同的epoch之间同一条样本也有不同的mask结果，提升了数据多样性。  \n\n2. Bert使用了MLM和NSP两个任务，而Roberta通过实验发现NSP的作用不大，因此直接取消了NSP任务。  \n\n3. Roberta增大了batch size，提高训练效率，以获取更好的训练结果。  \n\n4. Roberta增大了训练数据和训练step数，实验表明模型继续训练还能进一步收敛。  \n\n5. Bert使用WordPiece分词，而Roberta使用BBPE，增大了词表。  \n\n6. 再后来，Google发布了WWM全词mask，改进了mask方式。Roberta-WWM也成了最广泛使用的版本。  \n\n# 5.LoRA的参数是怎么初始化的？  \n\nLoRA包含一个降维矩阵A，和一个升维矩阵B。矩阵A用随机高斯分布初始化，而矩阵B用初始化为0。这样可以使得训练开始的时候，LoRA的参数不产生效果，模型能够保持增加LoRA前的输出。但是A、B矩阵不能同时为0，这样会有对称性问题，降低了模型的表达能力。  \n\n{% asset_img lora.png lora %}  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  ","slug":"cs/nlp/2024/06/大模型算法题-7","published":1,"updated":"2024-06-12T14:21:01.594Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9w000l314kboo50hi0","content":"<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~</p>\n<p>如有错漏，欢迎指正~</p>\n<hr>\n<h1 id=\"moe模型训练中如果不对专家的路由进行适当干预可能会遇到什么问题有什么解决方法\">1.MoE模型训练中，如果不对专家的路由进行适当干预，可能会遇到什么问题，有什么解决方法？</h1>\n<p>MoE使用多个并行的expert，每次推理只选择其中的一小部分expert使用。</p>\n<p>如果让模型完全自行学习，有可能出现routing\ncollapse的问题，也就是模型倾向于总是选择那几个常用的专家。</p>\n<p>而这些常用的专家由于使用得更多，训练得更好，又会提升被路由到的概率，导致大部分模型参数几乎没有被用上。</p>\n<p>一般可以通过增加一个负载平衡的loss来缓解。负载平衡loss有不同的设计和计算方式，但是大致的思路都是迫使模型均匀地使用不同的专家，如果出现某些专家被选中的概率过高，就会进行惩罚。</p>\n<h1 id=\"bert的预训练方式是mlm通过mask-token对部分输入进行掩盖要求模型预测为什么要使用mask-token而不直接修改attention-mask矩阵\">2.Bert的预训练方式是MLM，通过[Mask]\ntoken对部分输入进行掩盖，要求模型预测。为什么要使用[Mask]\ntoken而不直接修改attention mask矩阵？</h1>\n<p>直接修改attention\nmask矩阵也可以让模型看不到对应位置的输入，但是相比使用[Mask]\ntoken缺少了位置编码的信息。</p>\n<p>另外使用[Mask]\ntoken掩盖要预测的值这种做法在实现上相对方便，只需要对输入数据进行处理即可，而不需要修改modeling的内容，更加通用。</p>\n<h1 id=\"为什么大模型训练的时候需要warmup\">3.为什么大模型训练的时候需要warmup？</h1>\n<p>在训练前期，刚随机初始化的模型参数离收敛值很远，此时的loss会比较大，梯度也会很大。如果直接使用固定的较大learning\nrate，模型容易过拟合到早期step见过的这些数据。</p>\n<p>依照ResNet的实验结果，如果一开始模型就跑偏了，那后面再怎么训练，收敛效果都会比较差，说明早期太大的学习率导致模型过早收敛到不太好的局部最优了。</p>\n<p>另外，模型刚开始训练的时候，大学习率带来的大更新值，会导致模型参数的震荡会很大，使得模型学到的参数很不稳定，这也不利于训练。</p>\n<p>使用少数step让模型进行热身，可以很大程度规避这些问题。</p>\n<h1 id=\"roberta在bert的基础上做了什么优化\">4.Roberta在Bert的基础上做了什么优化？</h1>\n<ol type=\"1\">\n<li><p>Bert在MLM训练中，提早把要mask的token处理好，在训练时训了多个epoch。这样在不同的epoch中，同一条sample总是使用相同的mask进行训练。Roberta使用了dynamic\nmask，即每条数据在训练前才随机决定进行mask的位置，这样不同的epoch之间同一条样本也有不同的mask结果，提升了数据多样性。</p></li>\n<li><p>Bert使用了MLM和NSP两个任务，而Roberta通过实验发现NSP的作用不大，因此直接取消了NSP任务。</p></li>\n<li><p>Roberta增大了batch\nsize，提高训练效率，以获取更好的训练结果。</p></li>\n<li><p>Roberta增大了训练数据和训练step数，实验表明模型继续训练还能进一步收敛。</p></li>\n<li><p>Bert使用WordPiece分词，而Roberta使用BBPE，增大了词表。</p></li>\n<li><p>再后来，Google发布了WWM全词mask，改进了mask方式。Roberta-WWM也成了最广泛使用的版本。</p></li>\n</ol>\n<h1 id=\"lora的参数是怎么初始化的\">5.LoRA的参数是怎么初始化的？</h1>\n<p>LoRA包含一个降维矩阵A，和一个升维矩阵B。矩阵A用随机高斯分布初始化，而矩阵B用初始化为0。这样可以使得训练开始的时候，LoRA的参数不产生效果，模型能够保持增加LoRA前的输出。但是A、B矩阵不能同时为0，这样会有对称性问题，降低了模型的表达能力。</p>\n<img src=\"/dd614e12/lora.png\" class title=\"lora\">\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n","length":2131,"excerpt":"","more":"<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~</p>\n<p>如有错漏，欢迎指正~</p>\n<hr>\n<h1 id=\"moe模型训练中如果不对专家的路由进行适当干预可能会遇到什么问题有什么解决方法\">1.MoE模型训练中，如果不对专家的路由进行适当干预，可能会遇到什么问题，有什么解决方法？</h1>\n<p>MoE使用多个并行的expert，每次推理只选择其中的一小部分expert使用。</p>\n<p>如果让模型完全自行学习，有可能出现routing\ncollapse的问题，也就是模型倾向于总是选择那几个常用的专家。</p>\n<p>而这些常用的专家由于使用得更多，训练得更好，又会提升被路由到的概率，导致大部分模型参数几乎没有被用上。</p>\n<p>一般可以通过增加一个负载平衡的loss来缓解。负载平衡loss有不同的设计和计算方式，但是大致的思路都是迫使模型均匀地使用不同的专家，如果出现某些专家被选中的概率过高，就会进行惩罚。</p>\n<h1 id=\"bert的预训练方式是mlm通过mask-token对部分输入进行掩盖要求模型预测为什么要使用mask-token而不直接修改attention-mask矩阵\">2.Bert的预训练方式是MLM，通过[Mask]\ntoken对部分输入进行掩盖，要求模型预测。为什么要使用[Mask]\ntoken而不直接修改attention mask矩阵？</h1>\n<p>直接修改attention\nmask矩阵也可以让模型看不到对应位置的输入，但是相比使用[Mask]\ntoken缺少了位置编码的信息。</p>\n<p>另外使用[Mask]\ntoken掩盖要预测的值这种做法在实现上相对方便，只需要对输入数据进行处理即可，而不需要修改modeling的内容，更加通用。</p>\n<h1 id=\"为什么大模型训练的时候需要warmup\">3.为什么大模型训练的时候需要warmup？</h1>\n<p>在训练前期，刚随机初始化的模型参数离收敛值很远，此时的loss会比较大，梯度也会很大。如果直接使用固定的较大learning\nrate，模型容易过拟合到早期step见过的这些数据。</p>\n<p>依照ResNet的实验结果，如果一开始模型就跑偏了，那后面再怎么训练，收敛效果都会比较差，说明早期太大的学习率导致模型过早收敛到不太好的局部最优了。</p>\n<p>另外，模型刚开始训练的时候，大学习率带来的大更新值，会导致模型参数的震荡会很大，使得模型学到的参数很不稳定，这也不利于训练。</p>\n<p>使用少数step让模型进行热身，可以很大程度规避这些问题。</p>\n<h1 id=\"roberta在bert的基础上做了什么优化\">4.Roberta在Bert的基础上做了什么优化？</h1>\n<ol type=\"1\">\n<li><p>Bert在MLM训练中，提早把要mask的token处理好，在训练时训了多个epoch。这样在不同的epoch中，同一条sample总是使用相同的mask进行训练。Roberta使用了dynamic\nmask，即每条数据在训练前才随机决定进行mask的位置，这样不同的epoch之间同一条样本也有不同的mask结果，提升了数据多样性。</p></li>\n<li><p>Bert使用了MLM和NSP两个任务，而Roberta通过实验发现NSP的作用不大，因此直接取消了NSP任务。</p></li>\n<li><p>Roberta增大了batch\nsize，提高训练效率，以获取更好的训练结果。</p></li>\n<li><p>Roberta增大了训练数据和训练step数，实验表明模型继续训练还能进一步收敛。</p></li>\n<li><p>Bert使用WordPiece分词，而Roberta使用BBPE，增大了词表。</p></li>\n<li><p>再后来，Google发布了WWM全词mask，改进了mask方式。Roberta-WWM也成了最广泛使用的版本。</p></li>\n</ol>\n<h1 id=\"lora的参数是怎么初始化的\">5.LoRA的参数是怎么初始化的？</h1>\n<p>LoRA包含一个降维矩阵A，和一个升维矩阵B。矩阵A用随机高斯分布初始化，而矩阵B用初始化为0。这样可以使得训练开始的时候，LoRA的参数不产生效果，模型能够保持增加LoRA前的输出。但是A、B矩阵不能同时为0，这样会有对称性问题，降低了模型的表达能力。</p>\n<img src=\"/dd614e12/lora.png\" class title=\"lora\">\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n"},{"title":"成本10w刀的JetMoE","abbrlink":"f3acf042","date":"2024-06-26T03:22:35.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\nJetMoE是由MIT、Princeton等几个学术机构发布的MoE模型，其总参数量为8B，激活参数量为2B。  \n\n训练JetMoE的总花费约为10w美元，而JetMoE在各个benchmark上都有不错的效果，这样看训练这个模型算是比较经济实惠的了。  \n\nMoE的基础内容可以看之前梳理的 [MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)。  \n\n# 模型设计  \n\n## 结构  \n\n在模型结构上，和目前一些主流的模型如Deepseek MoE、Mixtral 8x7B、Qwen-MoE等有点不同，JetMoE不仅在FFN层应用Sparsely-gated Mixtureof-Experts（SMOE）的设计，而且参考了《Moduleformer: Learning modular large language models from uncurated data》的做法，把attention层也设计成了混合专家的结构，如下图所示。  \n\n{% asset_img structure.png 结构 %}  \n\nattention层的混合专家结构也叫MoA（ Mixture of Attention heads (MoA)，是由《Mixture of Attention Heads: Selecting Attention Heads Per Token》提出的。  \n\nMoA和FFN层的MoE一样，每个attention层包含多个attention expert。而每个attention expert e包括4个形状为 $\\mathbf{R}^{D_{emb}\\times D_{att}}$ 的矩阵： $\\mathbf{W}_q^e,\\mathbf{W}_k,\\mathbf{W}_v,\\mathbf{W}_o^e$。其中 $D_{att}=H\\times D_{head}$，H是每个attention expert的attention head数量。每个attention expert内部和常规的注意力层是一样的。  \n\n每层attention expert中的 $\\mathbf{W}_k$ 和 $\\mathbf{W}_v$ 这两个矩阵的参数在同个attention层的多个expert之间共享，这样可以减少一些参数量和计算量，提升计算效率。而每个attention expert保留各自的 $\\mathbf{W}_q^e$ 和 $\\mathbf{W}_o^e$。  \n\n对于一个输入的vector x，首先用2个共享的矩阵获得k和v  \n\n$$\\begin{aligned}\\mathbf{k}&=\\mathbf{W}_{k}\\mathbf{x}\\\\\\mathbf{v}&=\\mathbf{W}_{v}\\mathbf{x}\\end{aligned}$$  \n\n而在gating function选择了expert之后，再在attention expert内部进行标准的attention计算：  \n\n$$\\begin{aligned}&\\mathbf{q}_{e}=\\mathbf{W}_{q}^{e}\\mathbf{x}\\\\&\\mathbf{a}_{e}=\\mathrm{МНА}\\left(\\mathbf{q}_{e},\\mathbf{k},\\mathbf{v}\\right)\\\\&\\mathbf{o}_{e}=\\mathbf{W}_{o}^{e}\\mathbf{a}\\end{aligned}$$  \n\nJetMoE的FFN层的设计和gating的设计就是常规的top-k gating MoE，就不再赘述。  \n\nJetMoE的具体模型参数如下  \n\n{% asset_img model_param.png 模型参数 %}  \n\n每层有8个expert，每个token激活2个expert。  \n\n## 负载均衡  \n\n在负载均衡上，参考Switch Transformer，加入了frequency-based auxiliary loss：  \n\n$$loss_b=N\\sum_{i=1}^Nf_iP_i$$  \n\n其中N是expert数量，$f_i$ 是分配给expert i的token占比，$P_i$ 是router分配给expert i的概率占比。  \n\n此外还加入了ST-MoE中的z-loss来提升训练稳定性：  \n\n$$loss_z=\\frac1B\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^N\\exp(x_j^i)\\right)^2$$  \n\nx是router给出的logits，B是token数。  \n\n通过两个超参把这两个负载平衡的loss加入到训练loss中  \n\n$$loss=loss_{lm}+\\alpha loss_b+\\beta loss_z$$  \n\n训练中 $\\alpha=0.01$，$\\beta=0.001$。  \n\n# 训练数据  \n\nJetMoE预训练数据使用了真实数据和合成数据两种。  \n\n真实数据：  \n- RefinedWeb：从总共5T的token里抽取了600B来训练  \n- StarCoder：包含86种代码语言  \n- Dolma：包含3T token的英文数据集  \n- The Pile：825GB的英文数据集  \n- 其他：还使用了Proof-Pile-2、OpenWebMath、StackMathQA、OpenAssistant、xP3x、CommitPackFT这些规模比较小、质量比较高的数据集。  \n\n合成数据：  \n- OpenHermes 2.5  \n- UltraTextbooks  \n- UltraChat 200k  \n- 其他：还使用了TemplateGSM、Magicoder-Evol-110K、Evol-Code Alpaca、Code-290k-ShareGPT这些规模比较小、质量比较高的数据集。  \n\n# 训练  \n\nJetMoE基于Megatron框架进行训练，仅使用pipeline parallelism而不expert parallelism。训练过程用了96个H100，消耗约30,000个GPU hour，训练了大概1.25T token的数据。  \n\n一些训练设置：  \n- 使用AdamW优化器  \n- maximum learning rate = 5e-4  \n- batch size = 4M  \n- sequence length = 4096  \n- learning rate schedule = WSD，warmup = 10B token，decay = 250B token  \n\n参考MiniCPM的做法，把训练分为两个阶段：\n- phase1：warmup and stable learning rate；使用的数据集包括RefinedWeb, Starcoder, The Pile, peS2o from Dolma, and OpenWebMath  \n- phase2:decay learning rate；使用了更多的高质量数据。  \n\nphase1和phase2的具体数据混合情况如下  \n\n{% asset_img data1.png 数据 %}  \n\n{% asset_img data2.png 数据 %}  \n\n# Alignment  \n\nJetMoE用Distilled Supervised Fine-Tuning（dSFT）的方法对模型进行微调。dSFT就是用prompt获取更强模型的应答结果，用来训练别的模型。  \n\nJetMoE使用Zephyr-7b-beta的chat template获取GPT-4和Claude的答案用来训练JetMoE，所用的数据有：  \n- UltraChat 200k  \n- Airoboros-3.2  \n- Code-Feedback  \n- Orca-math-word-problems-200k  \n- SystemChat  \n- Capybara  \n\n训练配置：  \n- lr = 2e-5  \n- batch size = 128  \n- epoch = 3  \n\n此外，在SFT的基础上，还用了Distilled Direct Preference Optimization (dDPO)进一步优化模型。  \n\n所用的数据集是UltraFeedback，包含了preference数据对。  \n\n训练配置：  \n- lr = 5e-7  \n- batch size = 128  \n- epoch = 1  \n\n# 效果  \n\n在各个benchmark的效果如下  \n\n{% asset_img evaluation.png 评测 %}  \n\n{% asset_img mtbench.png 评测 %}  \n\n# 小结  \n\nJetMoE算是一次比较低成本的MoE训练实践，其中大部分的训练设置都是沿用了之前多个工作总结下来的经验。这些经验基本上可以保证训练不出什么大问题了，是相对比较成熟的了。  \n\n常规的内容之外，attention expert可能是一个可以探索的方向。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLM的重复生成和ICL](https://www.linsight.cn/7381cae3.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE的远距离衰减](https://www.linsight.cn/f0902f1a.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n[大模型算法题(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n【1】JetMoE: Reaching Llama2 Performance with 0.1M Dollars https://arxiv.org/abs/2404.07413  \n","source":"_posts/cs/nlp/2024/06/成本10w刀的JetMoE.md","raw":"---\ntitle: 成本10w刀的JetMoE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: f3acf042\ndate: 2024-06-26 11:22:35\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\nJetMoE是由MIT、Princeton等几个学术机构发布的MoE模型，其总参数量为8B，激活参数量为2B。  \n\n训练JetMoE的总花费约为10w美元，而JetMoE在各个benchmark上都有不错的效果，这样看训练这个模型算是比较经济实惠的了。  \n\nMoE的基础内容可以看之前梳理的 [MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)。  \n\n# 模型设计  \n\n## 结构  \n\n在模型结构上，和目前一些主流的模型如Deepseek MoE、Mixtral 8x7B、Qwen-MoE等有点不同，JetMoE不仅在FFN层应用Sparsely-gated Mixtureof-Experts（SMOE）的设计，而且参考了《Moduleformer: Learning modular large language models from uncurated data》的做法，把attention层也设计成了混合专家的结构，如下图所示。  \n\n{% asset_img structure.png 结构 %}  \n\nattention层的混合专家结构也叫MoA（ Mixture of Attention heads (MoA)，是由《Mixture of Attention Heads: Selecting Attention Heads Per Token》提出的。  \n\nMoA和FFN层的MoE一样，每个attention层包含多个attention expert。而每个attention expert e包括4个形状为 $\\mathbf{R}^{D_{emb}\\times D_{att}}$ 的矩阵： $\\mathbf{W}_q^e,\\mathbf{W}_k,\\mathbf{W}_v,\\mathbf{W}_o^e$。其中 $D_{att}=H\\times D_{head}$，H是每个attention expert的attention head数量。每个attention expert内部和常规的注意力层是一样的。  \n\n每层attention expert中的 $\\mathbf{W}_k$ 和 $\\mathbf{W}_v$ 这两个矩阵的参数在同个attention层的多个expert之间共享，这样可以减少一些参数量和计算量，提升计算效率。而每个attention expert保留各自的 $\\mathbf{W}_q^e$ 和 $\\mathbf{W}_o^e$。  \n\n对于一个输入的vector x，首先用2个共享的矩阵获得k和v  \n\n$$\\begin{aligned}\\mathbf{k}&=\\mathbf{W}_{k}\\mathbf{x}\\\\\\mathbf{v}&=\\mathbf{W}_{v}\\mathbf{x}\\end{aligned}$$  \n\n而在gating function选择了expert之后，再在attention expert内部进行标准的attention计算：  \n\n$$\\begin{aligned}&\\mathbf{q}_{e}=\\mathbf{W}_{q}^{e}\\mathbf{x}\\\\&\\mathbf{a}_{e}=\\mathrm{МНА}\\left(\\mathbf{q}_{e},\\mathbf{k},\\mathbf{v}\\right)\\\\&\\mathbf{o}_{e}=\\mathbf{W}_{o}^{e}\\mathbf{a}\\end{aligned}$$  \n\nJetMoE的FFN层的设计和gating的设计就是常规的top-k gating MoE，就不再赘述。  \n\nJetMoE的具体模型参数如下  \n\n{% asset_img model_param.png 模型参数 %}  \n\n每层有8个expert，每个token激活2个expert。  \n\n## 负载均衡  \n\n在负载均衡上，参考Switch Transformer，加入了frequency-based auxiliary loss：  \n\n$$loss_b=N\\sum_{i=1}^Nf_iP_i$$  \n\n其中N是expert数量，$f_i$ 是分配给expert i的token占比，$P_i$ 是router分配给expert i的概率占比。  \n\n此外还加入了ST-MoE中的z-loss来提升训练稳定性：  \n\n$$loss_z=\\frac1B\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^N\\exp(x_j^i)\\right)^2$$  \n\nx是router给出的logits，B是token数。  \n\n通过两个超参把这两个负载平衡的loss加入到训练loss中  \n\n$$loss=loss_{lm}+\\alpha loss_b+\\beta loss_z$$  \n\n训练中 $\\alpha=0.01$，$\\beta=0.001$。  \n\n# 训练数据  \n\nJetMoE预训练数据使用了真实数据和合成数据两种。  \n\n真实数据：  \n- RefinedWeb：从总共5T的token里抽取了600B来训练  \n- StarCoder：包含86种代码语言  \n- Dolma：包含3T token的英文数据集  \n- The Pile：825GB的英文数据集  \n- 其他：还使用了Proof-Pile-2、OpenWebMath、StackMathQA、OpenAssistant、xP3x、CommitPackFT这些规模比较小、质量比较高的数据集。  \n\n合成数据：  \n- OpenHermes 2.5  \n- UltraTextbooks  \n- UltraChat 200k  \n- 其他：还使用了TemplateGSM、Magicoder-Evol-110K、Evol-Code Alpaca、Code-290k-ShareGPT这些规模比较小、质量比较高的数据集。  \n\n# 训练  \n\nJetMoE基于Megatron框架进行训练，仅使用pipeline parallelism而不expert parallelism。训练过程用了96个H100，消耗约30,000个GPU hour，训练了大概1.25T token的数据。  \n\n一些训练设置：  \n- 使用AdamW优化器  \n- maximum learning rate = 5e-4  \n- batch size = 4M  \n- sequence length = 4096  \n- learning rate schedule = WSD，warmup = 10B token，decay = 250B token  \n\n参考MiniCPM的做法，把训练分为两个阶段：\n- phase1：warmup and stable learning rate；使用的数据集包括RefinedWeb, Starcoder, The Pile, peS2o from Dolma, and OpenWebMath  \n- phase2:decay learning rate；使用了更多的高质量数据。  \n\nphase1和phase2的具体数据混合情况如下  \n\n{% asset_img data1.png 数据 %}  \n\n{% asset_img data2.png 数据 %}  \n\n# Alignment  \n\nJetMoE用Distilled Supervised Fine-Tuning（dSFT）的方法对模型进行微调。dSFT就是用prompt获取更强模型的应答结果，用来训练别的模型。  \n\nJetMoE使用Zephyr-7b-beta的chat template获取GPT-4和Claude的答案用来训练JetMoE，所用的数据有：  \n- UltraChat 200k  \n- Airoboros-3.2  \n- Code-Feedback  \n- Orca-math-word-problems-200k  \n- SystemChat  \n- Capybara  \n\n训练配置：  \n- lr = 2e-5  \n- batch size = 128  \n- epoch = 3  \n\n此外，在SFT的基础上，还用了Distilled Direct Preference Optimization (dDPO)进一步优化模型。  \n\n所用的数据集是UltraFeedback，包含了preference数据对。  \n\n训练配置：  \n- lr = 5e-7  \n- batch size = 128  \n- epoch = 1  \n\n# 效果  \n\n在各个benchmark的效果如下  \n\n{% asset_img evaluation.png 评测 %}  \n\n{% asset_img mtbench.png 评测 %}  \n\n# 小结  \n\nJetMoE算是一次比较低成本的MoE训练实践，其中大部分的训练设置都是沿用了之前多个工作总结下来的经验。这些经验基本上可以保证训练不出什么大问题了，是相对比较成熟的了。  \n\n常规的内容之外，attention expert可能是一个可以探索的方向。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLM的重复生成和ICL](https://www.linsight.cn/7381cae3.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE的远距离衰减](https://www.linsight.cn/f0902f1a.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n[大模型算法题(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n【1】JetMoE: Reaching Llama2 Performance with 0.1M Dollars https://arxiv.org/abs/2404.07413  \n","slug":"cs/nlp/2024/06/成本10w刀的JetMoE","published":1,"updated":"2024-06-26T12:49:15.240Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9w000o314k8lspc9l4","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>JetMoE是由MIT、Princeton等几个学术机构发布的MoE模型，其总参数量为8B，激活参数量为2B。</p>\n<p>训练JetMoE的总花费约为10w美元，而JetMoE在各个benchmark上都有不错的效果，这样看训练这个模型算是比较经济实惠的了。</p>\n<p>MoE的基础内容可以看之前梳理的 <a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a>。</p>\n<h1 id=\"模型设计\">模型设计</h1>\n<h2 id=\"结构\">结构</h2>\n<p>在模型结构上，和目前一些主流的模型如Deepseek MoE、Mixtral\n8x7B、Qwen-MoE等有点不同，JetMoE不仅在FFN层应用Sparsely-gated\nMixtureof-Experts（SMOE）的设计，而且参考了《Moduleformer: Learning\nmodular large language models from uncurated\ndata》的做法，把attention层也设计成了混合专家的结构，如下图所示。</p>\n<img src=\"/f3acf042/structure.png\" class title=\"结构\">\n<p>attention层的混合专家结构也叫MoA（ Mixture of Attention heads\n(MoA)，是由《Mixture of Attention Heads: Selecting Attention Heads Per\nToken》提出的。</p>\n<p>MoA和FFN层的MoE一样，每个attention层包含多个attention\nexpert。而每个attention expert e包括4个形状为 <span class=\"math inline\">\\(\\mathbf{R}^{D_{emb}\\times D_{att}}\\)</span>\n的矩阵： <span class=\"math inline\">\\(\\mathbf{W}_q^e,\\mathbf{W}_k,\\mathbf{W}_v,\\mathbf{W}_o^e\\)</span>。其中\n<span class=\"math inline\">\\(D_{att}=H\\times\nD_{head}\\)</span>，H是每个attention expert的attention\nhead数量。每个attention expert内部和常规的注意力层是一样的。</p>\n<p>每层attention expert中的 <span class=\"math inline\">\\(\\mathbf{W}_k\\)</span> 和 <span class=\"math inline\">\\(\\mathbf{W}_v\\)</span>\n这两个矩阵的参数在同个attention层的多个expert之间共享，这样可以减少一些参数量和计算量，提升计算效率。而每个attention\nexpert保留各自的 <span class=\"math inline\">\\(\\mathbf{W}_q^e\\)</span> 和\n<span class=\"math inline\">\\(\\mathbf{W}_o^e\\)</span>。</p>\n<p>对于一个输入的vector x，首先用2个共享的矩阵获得k和v</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\mathbf{k}&amp;=\\mathbf{W}_{k}\\mathbf{x}\\\\\\mathbf{v}&amp;=\\mathbf{W}_{v}\\mathbf{x}\\end{aligned}\\]</span></p>\n<p>而在gating function选择了expert之后，再在attention\nexpert内部进行标准的attention计算：</p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;\\mathbf{q}_{e}=\\mathbf{W}_{q}^{e}\\mathbf{x}\\\\&amp;\\mathbf{a}_{e}=\\mathrm{МНА}\\left(\\mathbf{q}_{e},\\mathbf{k},\\mathbf{v}\\right)\\\\&amp;\\mathbf{o}_{e}=\\mathbf{W}_{o}^{e}\\mathbf{a}\\end{aligned}\\]</span></p>\n<p>JetMoE的FFN层的设计和gating的设计就是常规的top-k gating\nMoE，就不再赘述。</p>\n<p>JetMoE的具体模型参数如下</p>\n<img src=\"/f3acf042/model_param.png\" class title=\"模型参数\">\n<p>每层有8个expert，每个token激活2个expert。</p>\n<h2 id=\"负载均衡\">负载均衡</h2>\n<p>在负载均衡上，参考Switch Transformer，加入了frequency-based auxiliary\nloss：</p>\n<p><span class=\"math display\">\\[loss_b=N\\sum_{i=1}^Nf_iP_i\\]</span></p>\n<p>其中N是expert数量，<span class=\"math inline\">\\(f_i\\)</span>\n是分配给expert i的token占比，<span class=\"math inline\">\\(P_i\\)</span>\n是router分配给expert i的概率占比。</p>\n<p>此外还加入了ST-MoE中的z-loss来提升训练稳定性：</p>\n<p><span class=\"math display\">\\[loss_z=\\frac1B\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^N\\exp(x_j^i)\\right)^2\\]</span></p>\n<p>x是router给出的logits，B是token数。</p>\n<p>通过两个超参把这两个负载平衡的loss加入到训练loss中</p>\n<p><span class=\"math display\">\\[loss=loss_{lm}+\\alpha loss_b+\\beta\nloss_z\\]</span></p>\n<p>训练中 <span class=\"math inline\">\\(\\alpha=0.01\\)</span>，<span class=\"math inline\">\\(\\beta=0.001\\)</span>。</p>\n<h1 id=\"训练数据\">训练数据</h1>\n<p>JetMoE预训练数据使用了真实数据和合成数据两种。</p>\n<p>真实数据：<br>\n- RefinedWeb：从总共5T的token里抽取了600B来训练<br>\n- StarCoder：包含86种代码语言<br>\n- Dolma：包含3T token的英文数据集<br>\n- The Pile：825GB的英文数据集<br>\n-\n其他：还使用了Proof-Pile-2、OpenWebMath、StackMathQA、OpenAssistant、xP3x、CommitPackFT这些规模比较小、质量比较高的数据集。</p>\n<p>合成数据：<br>\n- OpenHermes 2.5<br>\n- UltraTextbooks<br>\n- UltraChat 200k<br>\n- 其他：还使用了TemplateGSM、Magicoder-Evol-110K、Evol-Code\nAlpaca、Code-290k-ShareGPT这些规模比较小、质量比较高的数据集。</p>\n<h1 id=\"训练\">训练</h1>\n<p>JetMoE基于Megatron框架进行训练，仅使用pipeline parallelism而不expert\nparallelism。训练过程用了96个H100，消耗约30,000个GPU\nhour，训练了大概1.25T token的数据。</p>\n<p>一些训练设置：<br>\n- 使用AdamW优化器<br>\n- maximum learning rate = 5e-4<br>\n- batch size = 4M<br>\n- sequence length = 4096<br>\n- learning rate schedule = WSD，warmup = 10B token，decay = 250B\ntoken</p>\n<p>参考MiniCPM的做法，把训练分为两个阶段： - phase1：warmup and stable\nlearning rate；使用的数据集包括RefinedWeb, Starcoder, The Pile, peS2o\nfrom Dolma, and OpenWebMath<br>\n- phase2:decay learning rate；使用了更多的高质量数据。</p>\n<p>phase1和phase2的具体数据混合情况如下</p>\n<img src=\"/f3acf042/data1.png\" class title=\"数据\">\n<img src=\"/f3acf042/data2.png\" class title=\"数据\">\n<h1 id=\"alignment\">Alignment</h1>\n<p>JetMoE用Distilled Supervised\nFine-Tuning（dSFT）的方法对模型进行微调。dSFT就是用prompt获取更强模型的应答结果，用来训练别的模型。</p>\n<p>JetMoE使用Zephyr-7b-beta的chat\ntemplate获取GPT-4和Claude的答案用来训练JetMoE，所用的数据有：<br>\n- UltraChat 200k<br>\n- Airoboros-3.2<br>\n- Code-Feedback<br>\n- Orca-math-word-problems-200k<br>\n- SystemChat<br>\n- Capybara</p>\n<p>训练配置：<br>\n- lr = 2e-5<br>\n- batch size = 128<br>\n- epoch = 3</p>\n<p>此外，在SFT的基础上，还用了Distilled Direct Preference Optimization\n(dDPO)进一步优化模型。</p>\n<p>所用的数据集是UltraFeedback，包含了preference数据对。</p>\n<p>训练配置：<br>\n- lr = 5e-7<br>\n- batch size = 128<br>\n- epoch = 1</p>\n<h1 id=\"效果\">效果</h1>\n<p>在各个benchmark的效果如下</p>\n<img src=\"/f3acf042/evaluation.png\" class title=\"评测\">\n<img src=\"/f3acf042/mtbench.png\" class title=\"评测\">\n<h1 id=\"小结\">小结</h1>\n<p>JetMoE算是一次比较低成本的MoE训练实践，其中大部分的训练设置都是沿用了之前多个工作总结下来的经验。这些经验基本上可以保证训练不出什么大问题了，是相对比较成熟的了。</p>\n<p>常规的内容之外，attention expert可能是一个可以探索的方向。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">从loss视角理解大模型涌现能力</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">大模型推理加速-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLM的重复生成和ICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE的远距离衰减</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">大模型算法题(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】JetMoE: Reaching Llama2 Performance with 0.1M Dollars\nhttps://arxiv.org/abs/2404.07413</p>\n","length":3801,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>JetMoE是由MIT、Princeton等几个学术机构发布的MoE模型，其总参数量为8B，激活参数量为2B。</p>\n<p>训练JetMoE的总花费约为10w美元，而JetMoE在各个benchmark上都有不错的效果，这样看训练这个模型算是比较经济实惠的了。</p>\n<p>MoE的基础内容可以看之前梳理的 <a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a>。</p>\n<h1 id=\"模型设计\">模型设计</h1>\n<h2 id=\"结构\">结构</h2>\n<p>在模型结构上，和目前一些主流的模型如Deepseek MoE、Mixtral\n8x7B、Qwen-MoE等有点不同，JetMoE不仅在FFN层应用Sparsely-gated\nMixtureof-Experts（SMOE）的设计，而且参考了《Moduleformer: Learning\nmodular large language models from uncurated\ndata》的做法，把attention层也设计成了混合专家的结构，如下图所示。</p>\n<img src=\"/f3acf042/structure.png\" class title=\"结构\">\n<p>attention层的混合专家结构也叫MoA（ Mixture of Attention heads\n(MoA)，是由《Mixture of Attention Heads: Selecting Attention Heads Per\nToken》提出的。</p>\n<p>MoA和FFN层的MoE一样，每个attention层包含多个attention\nexpert。而每个attention expert e包括4个形状为 <span class=\"math inline\">\\(\\mathbf{R}^{D_{emb}\\times D_{att}}\\)</span>\n的矩阵： <span class=\"math inline\">\\(\\mathbf{W}_q^e,\\mathbf{W}_k,\\mathbf{W}_v,\\mathbf{W}_o^e\\)</span>。其中\n<span class=\"math inline\">\\(D_{att}=H\\times\nD_{head}\\)</span>，H是每个attention expert的attention\nhead数量。每个attention expert内部和常规的注意力层是一样的。</p>\n<p>每层attention expert中的 <span class=\"math inline\">\\(\\mathbf{W}_k\\)</span> 和 <span class=\"math inline\">\\(\\mathbf{W}_v\\)</span>\n这两个矩阵的参数在同个attention层的多个expert之间共享，这样可以减少一些参数量和计算量，提升计算效率。而每个attention\nexpert保留各自的 <span class=\"math inline\">\\(\\mathbf{W}_q^e\\)</span> 和\n<span class=\"math inline\">\\(\\mathbf{W}_o^e\\)</span>。</p>\n<p>对于一个输入的vector x，首先用2个共享的矩阵获得k和v</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\mathbf{k}&amp;=\\mathbf{W}_{k}\\mathbf{x}\\\\\\mathbf{v}&amp;=\\mathbf{W}_{v}\\mathbf{x}\\end{aligned}\\]</span></p>\n<p>而在gating function选择了expert之后，再在attention\nexpert内部进行标准的attention计算：</p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;\\mathbf{q}_{e}=\\mathbf{W}_{q}^{e}\\mathbf{x}\\\\&amp;\\mathbf{a}_{e}=\\mathrm{МНА}\\left(\\mathbf{q}_{e},\\mathbf{k},\\mathbf{v}\\right)\\\\&amp;\\mathbf{o}_{e}=\\mathbf{W}_{o}^{e}\\mathbf{a}\\end{aligned}\\]</span></p>\n<p>JetMoE的FFN层的设计和gating的设计就是常规的top-k gating\nMoE，就不再赘述。</p>\n<p>JetMoE的具体模型参数如下</p>\n<img src=\"/f3acf042/model_param.png\" class title=\"模型参数\">\n<p>每层有8个expert，每个token激活2个expert。</p>\n<h2 id=\"负载均衡\">负载均衡</h2>\n<p>在负载均衡上，参考Switch Transformer，加入了frequency-based auxiliary\nloss：</p>\n<p><span class=\"math display\">\\[loss_b=N\\sum_{i=1}^Nf_iP_i\\]</span></p>\n<p>其中N是expert数量，<span class=\"math inline\">\\(f_i\\)</span>\n是分配给expert i的token占比，<span class=\"math inline\">\\(P_i\\)</span>\n是router分配给expert i的概率占比。</p>\n<p>此外还加入了ST-MoE中的z-loss来提升训练稳定性：</p>\n<p><span class=\"math display\">\\[loss_z=\\frac1B\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^N\\exp(x_j^i)\\right)^2\\]</span></p>\n<p>x是router给出的logits，B是token数。</p>\n<p>通过两个超参把这两个负载平衡的loss加入到训练loss中</p>\n<p><span class=\"math display\">\\[loss=loss_{lm}+\\alpha loss_b+\\beta\nloss_z\\]</span></p>\n<p>训练中 <span class=\"math inline\">\\(\\alpha=0.01\\)</span>，<span class=\"math inline\">\\(\\beta=0.001\\)</span>。</p>\n<h1 id=\"训练数据\">训练数据</h1>\n<p>JetMoE预训练数据使用了真实数据和合成数据两种。</p>\n<p>真实数据：<br>\n- RefinedWeb：从总共5T的token里抽取了600B来训练<br>\n- StarCoder：包含86种代码语言<br>\n- Dolma：包含3T token的英文数据集<br>\n- The Pile：825GB的英文数据集<br>\n-\n其他：还使用了Proof-Pile-2、OpenWebMath、StackMathQA、OpenAssistant、xP3x、CommitPackFT这些规模比较小、质量比较高的数据集。</p>\n<p>合成数据：<br>\n- OpenHermes 2.5<br>\n- UltraTextbooks<br>\n- UltraChat 200k<br>\n- 其他：还使用了TemplateGSM、Magicoder-Evol-110K、Evol-Code\nAlpaca、Code-290k-ShareGPT这些规模比较小、质量比较高的数据集。</p>\n<h1 id=\"训练\">训练</h1>\n<p>JetMoE基于Megatron框架进行训练，仅使用pipeline parallelism而不expert\nparallelism。训练过程用了96个H100，消耗约30,000个GPU\nhour，训练了大概1.25T token的数据。</p>\n<p>一些训练设置：<br>\n- 使用AdamW优化器<br>\n- maximum learning rate = 5e-4<br>\n- batch size = 4M<br>\n- sequence length = 4096<br>\n- learning rate schedule = WSD，warmup = 10B token，decay = 250B\ntoken</p>\n<p>参考MiniCPM的做法，把训练分为两个阶段： - phase1：warmup and stable\nlearning rate；使用的数据集包括RefinedWeb, Starcoder, The Pile, peS2o\nfrom Dolma, and OpenWebMath<br>\n- phase2:decay learning rate；使用了更多的高质量数据。</p>\n<p>phase1和phase2的具体数据混合情况如下</p>\n<img src=\"/f3acf042/data1.png\" class title=\"数据\">\n<img src=\"/f3acf042/data2.png\" class title=\"数据\">\n<h1 id=\"alignment\">Alignment</h1>\n<p>JetMoE用Distilled Supervised\nFine-Tuning（dSFT）的方法对模型进行微调。dSFT就是用prompt获取更强模型的应答结果，用来训练别的模型。</p>\n<p>JetMoE使用Zephyr-7b-beta的chat\ntemplate获取GPT-4和Claude的答案用来训练JetMoE，所用的数据有：<br>\n- UltraChat 200k<br>\n- Airoboros-3.2<br>\n- Code-Feedback<br>\n- Orca-math-word-problems-200k<br>\n- SystemChat<br>\n- Capybara</p>\n<p>训练配置：<br>\n- lr = 2e-5<br>\n- batch size = 128<br>\n- epoch = 3</p>\n<p>此外，在SFT的基础上，还用了Distilled Direct Preference Optimization\n(dDPO)进一步优化模型。</p>\n<p>所用的数据集是UltraFeedback，包含了preference数据对。</p>\n<p>训练配置：<br>\n- lr = 5e-7<br>\n- batch size = 128<br>\n- epoch = 1</p>\n<h1 id=\"效果\">效果</h1>\n<p>在各个benchmark的效果如下</p>\n<img src=\"/f3acf042/evaluation.png\" class title=\"评测\">\n<img src=\"/f3acf042/mtbench.png\" class title=\"评测\">\n<h1 id=\"小结\">小结</h1>\n<p>JetMoE算是一次比较低成本的MoE训练实践，其中大部分的训练设置都是沿用了之前多个工作总结下来的经验。这些经验基本上可以保证训练不出什么大问题了，是相对比较成熟的了。</p>\n<p>常规的内容之外，attention expert可能是一个可以探索的方向。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">从loss视角理解大模型涌现能力</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">大模型推理加速-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLM的重复生成和ICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE的远距离衰减</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">大模型算法题(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】JetMoE: Reaching Llama2 Performance with 0.1M Dollars\nhttps://arxiv.org/abs/2404.07413</p>\n"},{"title":"大模型推理加速-MEDUSA","abbrlink":"7bbe2df6","date":"2024-06-11T14:13:04.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n之前对speculative decoding的做法做了介绍：[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)。  \n\n本篇介绍一下另外一个热门的解码加速算法，MEDUSA。MEDUSA在不同的训练方法下能提供×2.2~×2.8的解码加速效果。  \n\n# 背景  \n\n自回归大模型推理下一个token的时候，需要依赖前面的结果。而在实际使用GPU进行计算时，需要将相关矩阵移至片上内存进行运算，而一般来说片上内存带宽比计算性能要低两个数量级，这就使得大模型推理是memory-bandwidth-bound的。  \n\n要解决这个问题，一个思路是increasing the arithmetic intensity，即提高“浮点数计算量/数据传输量”这个比值，让数据传输不要成为瓶颈。另一个思路是reducing the number of decoding steps。投机解码就属于后者。  \n\n不过投机解码有几个问题：  \n- 一个好的draft model不容易获取：draft模型和原模型存在distribution shift  \n- 推理时有多个模型参与，在分布式系统上的部署难度增大  \n\n而MEDUSA相比投机解码，不需要新增一个模型，而是基于原模型进行并行推理，这样训练难度更低，部署也更容易。  \n\n# MEDUSA  \n\nMEDUSA的大致思路是和投机解码类似：  \n- 首先生成各个位置的候选token；MEDUSA通过接在原模型的多个解码头来获取多个位置的候选token  \n- 把各个位置的候选token进行处理，选出一些候选序列，进行验证；MEDUSA通过tree attention来处理  \n- 最后通过typical acceptance选择最终输出的结果  \n\nMEDUSA的pipeline如下图所示  \n\n{% asset_img intro.png introduction %}  \n\nMEDUSA的这些分类头需要经过训练才能有比较好的预测效果。针对不同的条件，可以选择不同的训练方式：  \n- MEDUSA-1：冻结原模型的backbone（包括原模型的解码头），只训练增加的解码头。这种方案适用于计算资源比较少，或者不想影响原模型的效果的情况。还可以使用QLoRA对解码头进行训练，进一步节省memory和计算资源。  \n- MEDUSA-2：原模型和MEDUSA的解码头一起训练。MEDUSA-1这样的训练方法虽然可以节省资源，但是并不能最大程度发挥多个解码头的加速效果，而MEDUSA-2则可以进一步发挥MEDUSA解码头的提速能力。MEDUSA-2适用于计算资源充足，或者从Base模型进行SFT的场景。  \n\n另外，如果原模型的SFT数据集是available的，那可以直接进行训练。如果不能获得原模型的SFT数据，或者原模型是经过RLHF训练的，则可以通过self-distillation来获取MEDUSA head的训练数据。  \n\n# 模型设计：MEDUSA HEADS  \n\n先来看下第一步，MEDUSA的多个解码头是怎么给出各个位置的候选token的。  \n\n假设原始模型最后一层的hidden state在时间 $t$ 的输出是 $h_{t}$，我们给模型额外加上 $K$ 个解码头。那么第 $k$ 个头就可以用来预测位置 $t+k+1$ 的输出token（这里 $k$ 的取值为 $1$ ~ $K$）。这里注意原模型自己还有一个解码头，它依然用来预测位置 $t+1$ 的输出，相当于 $k=0$。  \n\n把第 $k$ 个解码头在vocabulary上的输出分布写作 $p_t^{(k)}$，其计算方式如下  \n\n$$\\begin{aligned}p_t^{(k)}=\\text{softmax}\\left(W_2^{(k)}\\cdot\\left(\\text{SiLU}(W_1^{(k)}\\cdot h_t)+h_t\\right)\\right),\\\\\\mathrm{where~}W_2^{(k)}\\in\\mathbb{R}^{d\\times V},W_1^{(k)}\\in\\mathbb{R}^{d\\times d}.\\end{aligned}$$  \n\n$d$ 是hidden state的输出维度，$V$ 是词表大小。每个解码头其实就是一个FFN网络，实践上发现这样简单的设计已经有足够好的效果。  \n\n在初始化各个解码头的参数时，把 $W_2^{(k)}$ 初始化成和原模型的解码头一样，而把 $W_1^{(k)}$ 设置成0。这样能使得在一开始训练的时候，增加的这些解码头就有一定的预测能力。  \n\n这 $K$ 个新增的解码头直接在原模型的基础上进行训练，因此相比投机解码的draft model，MEDUSA的训练方式缓解了distribution shift的问题。  \n\n# 候选校验：TREE ATTENTION  \n\n## Cartesian product  \n\n增加额外的解码头之后，模型每次前向推理都会给出 $K+1$ 个位置的候选token。  \n\n投机解码里是直接选出draft model最有信心的一个候选序列给原模型进行验证。  \n\n显然，如果增加候选序列的数量，那么最终接受token的命中率就会提升，acceleration rate（即每个decoding step能获得的token数，不是实际解码时间）也就更高，但是验证更多候选序列也会带来额外的计算消耗。为了获得一个效果和性能比较好的平衡，MEDUSA使用tree attention来同时对多个候选序列进行处理。  \n\n假设第 $k$ 个解码头给出的候选token数量是 $s_k$ 个，那么可以通过Cartesian product来获取多个解码头组成的所有可能的候选序列，然后用tree attention对所有候选序列进行验证。  \n\n对于两个解码头的情况，tree attention验证的示意图如下  \n\n{% asset_img tree_attention.png tree attention %}  \n\n通过使用这样的mask，我们可以在不扩大batch size的情况下同时处理多个候选序列。（注意，这里要对各个候选token的位置编码进行处理。）  \n\n## 更高效的tree attention构建  \n\n上面这个例子使用了Cartesian product对两个解码头的结果进行处理，获得所有候选序列。  \n\n但是如果解码头数量数量比较多，每个头给出的候选token也比较多，那么实际要验证的序列数量会极大地增长。  \n\n直觉上，这些解码头应该有不同的准确率，因此可以利用这一点来构建tree attention，而不需要使用所有可能的排列组合。  \n\n具体来说，可以使用一个calibration dataset（比如Alpaca-eval dataset）来获取不同解码头给出的各个token的准确率：把第 $k$ 个解码头给出的第 $i$ 个token的准确率记为 $a_k^{(i)}$。  \n\n假设各个token的准确率之间是独立的，那么一个由 $[i_1,i_2,\\cdots,i_k]$ 构成的候选序列的准确率可以写作 $\\prod_{j=1}^ka_j^{(i_j)}$。  \n\n每个候选序列可以表示所构建的tree上的一条路径上所有的node（而不只是leaf node，因为tree attention验证的时候会把路径上所有token都进行验证）。用 $I$ 表示候选序列的集合，那么集合里的候选序列的expectation of acceptance length就表示为  \n\n$$\\sum_{[i_1,i_2,\\cdots,i_k]\\in I}\\prod_{j=1}^ka_j^{(i_j)}$$  \n\n在构建tree的时候，优先加入当前有最大准确率的候选序列，直到tree的节点数量达到上限，这样能最大化expectation of acceptance length，也就能最大化acceleration rate。  \n\n下图是一个按这种方法构建的tree的例子。可以看到这棵树向左偏，这是因为这个方法倾向于使用更高准确率的token。  \n\n{% asset_img construct_tree.png tree attention %}  \n\n# 训练策略  \n\nMEDUSA的解码头需要进行训练。训练策略根据是否有“与模型输出分布对齐的训练数据”而有所不同。  \n\n## 有训练数据  \n\nMEDUSA-1冻结了原模型的参数，而只对新增的解码头进行训练。  \n\n第 $k$ 个解码头的训练loss可以写作  \n\n$$\\mathcal{L}_k=-\\log p_t^{(k)}(y_{t+k+1})$$  \n\n总的训练loss为  \n\n$$\\mathcal{L}_{\\text{MEDUSA-l}}=\\sum_{k=1}^K-\\lambda_k\\log p_t^{(k)}(y_{t+k+1})$$  \n\n这里的 $\\lambda_{k}$ 是每个解码头的缩放系数，是一系列超参。因为 $k$ 越大，对应解码头的预测难度越大，loss也就越大，为了防止靠后的解码头过分主导训练，因此使用一个缩放系数进行调整。  \n\n实际使用中，$\\lambda_{k}=0.8^{k}$。  \n\n训练时，由于冻结了原模型，因此可以对原模型的参数进行量化而不会对训练效果有明显影响，比如使用QLoRA。  \n\nMEDUSA-1冻结了原模型，比较适用于计算资源有限，或者希望保持原模型能力的场景。如果要进一步发挥MEDUSA多个解码头的加速效果，那就需要使用MEDUSA-2。  \n\nMEDUSA-2把原模型和多个解码头一起训练，因此各个解码头的准确率能达到更高的水平，acceleration rate也更高。但是为了保持原模型的输出质量，需要使用以下三个措施。  \n\n（1）Combined loss  \n\n首先是加入原模型next-token prediction的loss，即把原模型解码头的loss也加上，如下式  \n\n$$\\mathcal{L}_{\\text{MEDUSA-}2}=\\mathcal{L}_{\\text{LM}}+\\lambda_0\\mathcal{L}_{\\text{MEDUSA-}1}$$  \n\n$$\\mathcal{L}_{\\text{LM}}=-\\log p_t^{(0)}(y_{t+1})$$  \n\n实际使用中，直接训练时 $\\lambda_0=0.2$，使用self-distillation时$\\lambda_0=0.01$。  \n\n（2）Differential learning rates  \n\n原模型已经是训练好了的，因此和新加入的解码头使用相同的学习率并不合适，因此可以让新的解码头使用更大的学习率，而原模型参数使用相对小的学习率。实践中把学习率差距设为4倍，比如分别使用2e-3和5e-4。  \n\n（3）Heads warmup  \n\n新加入的解码头在一开始训练会有比较大的loss，从而导致更大的梯度，有可能损害原模型的能力。  \n\n针对这个问题，可以使用two-stage training的方式，先在MEDUSA-1的策略下训练解码头，然后再进行MEDUSA-2的训练。这其实相当于把 $\\lambda_0$ 在训练过程中逐渐增大。two-stage training和逐渐增大 $\\lambda_0$ 的方法在实践中都是可行的。  \n\n## SELF-DISTILLATION  \n\n前面讲的这些训练方式都有一个前提，那就是有与模型输出分布对齐的训练数据可供使用。但是实际上这个前提并不总是成立。比如大部分开源模型并没有发布相应的SFT数据，或者模型使用了RLHF等对齐方式，而不是直接SFT。  \n\n解决方法是使用self-distillation：通过原模型为MEDUSA解码头生成训练数据。  \n\n首先选择一个和target model的domain相近的数据集，然后把prompt输入给原模型，获得原模型的输出。对于对话模型，需要生成多轮对话，可以使用self-talk。  \n\n对于MEDUSA-1，这样生成的数据集已经够用，但是对于MEDUSA-2，这样的训练会降低生成质量。  \n \n实际上，即使不进行MEDUSA解码头的训练，只用生成的数据训练原模型，原模型的效果也会变差。因此MEDUSA-2训练的时候，原模型的训练不应该直接使用ground truth进行训练，而是进行蒸馏，按下式计算损失  \n\n$$\\mathcal{L}_{\\text{LM-distill}}=KL(p_{\\text{original},t}^{(0)}||p_t^{(0)})$$  \n\n# 接受策略：TYPICAL ACCEPTANCE  \n\n投机解码随着temperature的提升，命中率会降低。因为temperature提升，draft model所选择的候选token的多样性就增大，也就降低了命中原模型token，从而被接受的概率。  \n\n但是这种特性并不合理。通常更高的temperature参数一般对应更强的creativity特性，因此合理的情况应该是随着温度提高，候选序列有更大的概率被接受。这和投机解码的情况是相反的。  \n\n另外，MEDUSA认为候选序列的分布没有必要完全match原模型的分布。我们要做的应该是选出typical的候选，也就是只要候选序列不是极不可能的结果，就可以被接受。  \n\n给定context $x_1,x_2,\\cdots,x_n$，候选序列 $(x_{n+1},x_{n+2},\\cdots,x_{n+K+1})$，我们按以下这个条件来接受候选token  \n\n$$\\begin{aligned}p_{\\text{original}}(x_{n+k}|x_1,x_2,\\cdots,x_{n+k-1})&>\\\\\\min\\left(\\epsilon,\\delta\\exp\\left(-H(p_{\\text{original}}(\\cdot|x_1,x_2,\\cdots,x_{n+k-1})))\\right)\\right),\\end{aligned}$$  \n\n其中 $H(\\cdot)$ 表示entropy function，$\\epsilon,\\delta$ 分别是hard threshold和entropy-dependent threshold。  \n\n两个threshold的解释：（1）$\\epsilon$ 保证所选的token的概率不能低于特定值，保证不选出可能性很低的结果（2）当一个位置的多个候选token的entropy较高时，表示多个候选都是reasonable的，那么 $\\delta$ 和exp(entropy)的乘积会更小，各个token都有更大的机会被接受。  \n\n当temperatrue为0的时候，相当于贪心解码，这个时候只有概率最大那个token有非0概率。随着温度提升，其他token的概率也提升，因此它们也有一定的机会被接受。随着温度提升，这些token被接受的概率会增大。  \n\n最后选择被接受的解码长度最长的候选序列作为最终结果。  \n\n# 消融实验  \n\n## CONFIGURATION OF TREE ATTENTION  \n\n对比通过准确率构建tree attention的方式，和随机构建tree attention的方式，结果如下  \n\n{% asset_img tree_attention_exp.png 消融实验 %}  \n\n基于准确率构建的tree attention有更高的acceleration rate。  \n\n但随着候选token数量的增加，两种方式的实际速度反而有所下降，因为更多的候选token引入了额外的计算成本。  \n\n## THRESHOLDS OF TYPICAL ACCEPTANCE  \n\n随着 $\\epsilon $ 增加，输出质量得到提升，但代价是acceleration rate降低，如下图  \n\n{% asset_img threshold.png 消融实验 %}  \n\n## 各环节对速度的影响  \n\n各个技术优化点对速度的影响如下表  \n\n{% asset_img speed.png 消融实验 %}  \n\n# 小结  \n\nMEDUSA引入了tree attention、typical acceptance的做法，在加速比上相比投机解码有进一步提升。  \n\n但是MEDUSA不保证解码结果和原模型一致，因此应该更适用于对模型生成质量的没有那么严格要求的场景。  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】MEDUSA: Simple LLM Inference Acceleration Framework with Multiple\nDecoding Heads https://arxiv.org/abs/2401.10774  \n","source":"_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA.md","raw":"---\ntitle: 大模型推理加速-MEDUSA\nabbrlink: 7bbe2df6\ndate: 2024-06-11 22:13:04\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 推理加速\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n之前对speculative decoding的做法做了介绍：[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)。  \n\n本篇介绍一下另外一个热门的解码加速算法，MEDUSA。MEDUSA在不同的训练方法下能提供×2.2~×2.8的解码加速效果。  \n\n# 背景  \n\n自回归大模型推理下一个token的时候，需要依赖前面的结果。而在实际使用GPU进行计算时，需要将相关矩阵移至片上内存进行运算，而一般来说片上内存带宽比计算性能要低两个数量级，这就使得大模型推理是memory-bandwidth-bound的。  \n\n要解决这个问题，一个思路是increasing the arithmetic intensity，即提高“浮点数计算量/数据传输量”这个比值，让数据传输不要成为瓶颈。另一个思路是reducing the number of decoding steps。投机解码就属于后者。  \n\n不过投机解码有几个问题：  \n- 一个好的draft model不容易获取：draft模型和原模型存在distribution shift  \n- 推理时有多个模型参与，在分布式系统上的部署难度增大  \n\n而MEDUSA相比投机解码，不需要新增一个模型，而是基于原模型进行并行推理，这样训练难度更低，部署也更容易。  \n\n# MEDUSA  \n\nMEDUSA的大致思路是和投机解码类似：  \n- 首先生成各个位置的候选token；MEDUSA通过接在原模型的多个解码头来获取多个位置的候选token  \n- 把各个位置的候选token进行处理，选出一些候选序列，进行验证；MEDUSA通过tree attention来处理  \n- 最后通过typical acceptance选择最终输出的结果  \n\nMEDUSA的pipeline如下图所示  \n\n{% asset_img intro.png introduction %}  \n\nMEDUSA的这些分类头需要经过训练才能有比较好的预测效果。针对不同的条件，可以选择不同的训练方式：  \n- MEDUSA-1：冻结原模型的backbone（包括原模型的解码头），只训练增加的解码头。这种方案适用于计算资源比较少，或者不想影响原模型的效果的情况。还可以使用QLoRA对解码头进行训练，进一步节省memory和计算资源。  \n- MEDUSA-2：原模型和MEDUSA的解码头一起训练。MEDUSA-1这样的训练方法虽然可以节省资源，但是并不能最大程度发挥多个解码头的加速效果，而MEDUSA-2则可以进一步发挥MEDUSA解码头的提速能力。MEDUSA-2适用于计算资源充足，或者从Base模型进行SFT的场景。  \n\n另外，如果原模型的SFT数据集是available的，那可以直接进行训练。如果不能获得原模型的SFT数据，或者原模型是经过RLHF训练的，则可以通过self-distillation来获取MEDUSA head的训练数据。  \n\n# 模型设计：MEDUSA HEADS  \n\n先来看下第一步，MEDUSA的多个解码头是怎么给出各个位置的候选token的。  \n\n假设原始模型最后一层的hidden state在时间 $t$ 的输出是 $h_{t}$，我们给模型额外加上 $K$ 个解码头。那么第 $k$ 个头就可以用来预测位置 $t+k+1$ 的输出token（这里 $k$ 的取值为 $1$ ~ $K$）。这里注意原模型自己还有一个解码头，它依然用来预测位置 $t+1$ 的输出，相当于 $k=0$。  \n\n把第 $k$ 个解码头在vocabulary上的输出分布写作 $p_t^{(k)}$，其计算方式如下  \n\n$$\\begin{aligned}p_t^{(k)}=\\text{softmax}\\left(W_2^{(k)}\\cdot\\left(\\text{SiLU}(W_1^{(k)}\\cdot h_t)+h_t\\right)\\right),\\\\\\mathrm{where~}W_2^{(k)}\\in\\mathbb{R}^{d\\times V},W_1^{(k)}\\in\\mathbb{R}^{d\\times d}.\\end{aligned}$$  \n\n$d$ 是hidden state的输出维度，$V$ 是词表大小。每个解码头其实就是一个FFN网络，实践上发现这样简单的设计已经有足够好的效果。  \n\n在初始化各个解码头的参数时，把 $W_2^{(k)}$ 初始化成和原模型的解码头一样，而把 $W_1^{(k)}$ 设置成0。这样能使得在一开始训练的时候，增加的这些解码头就有一定的预测能力。  \n\n这 $K$ 个新增的解码头直接在原模型的基础上进行训练，因此相比投机解码的draft model，MEDUSA的训练方式缓解了distribution shift的问题。  \n\n# 候选校验：TREE ATTENTION  \n\n## Cartesian product  \n\n增加额外的解码头之后，模型每次前向推理都会给出 $K+1$ 个位置的候选token。  \n\n投机解码里是直接选出draft model最有信心的一个候选序列给原模型进行验证。  \n\n显然，如果增加候选序列的数量，那么最终接受token的命中率就会提升，acceleration rate（即每个decoding step能获得的token数，不是实际解码时间）也就更高，但是验证更多候选序列也会带来额外的计算消耗。为了获得一个效果和性能比较好的平衡，MEDUSA使用tree attention来同时对多个候选序列进行处理。  \n\n假设第 $k$ 个解码头给出的候选token数量是 $s_k$ 个，那么可以通过Cartesian product来获取多个解码头组成的所有可能的候选序列，然后用tree attention对所有候选序列进行验证。  \n\n对于两个解码头的情况，tree attention验证的示意图如下  \n\n{% asset_img tree_attention.png tree attention %}  \n\n通过使用这样的mask，我们可以在不扩大batch size的情况下同时处理多个候选序列。（注意，这里要对各个候选token的位置编码进行处理。）  \n\n## 更高效的tree attention构建  \n\n上面这个例子使用了Cartesian product对两个解码头的结果进行处理，获得所有候选序列。  \n\n但是如果解码头数量数量比较多，每个头给出的候选token也比较多，那么实际要验证的序列数量会极大地增长。  \n\n直觉上，这些解码头应该有不同的准确率，因此可以利用这一点来构建tree attention，而不需要使用所有可能的排列组合。  \n\n具体来说，可以使用一个calibration dataset（比如Alpaca-eval dataset）来获取不同解码头给出的各个token的准确率：把第 $k$ 个解码头给出的第 $i$ 个token的准确率记为 $a_k^{(i)}$。  \n\n假设各个token的准确率之间是独立的，那么一个由 $[i_1,i_2,\\cdots,i_k]$ 构成的候选序列的准确率可以写作 $\\prod_{j=1}^ka_j^{(i_j)}$。  \n\n每个候选序列可以表示所构建的tree上的一条路径上所有的node（而不只是leaf node，因为tree attention验证的时候会把路径上所有token都进行验证）。用 $I$ 表示候选序列的集合，那么集合里的候选序列的expectation of acceptance length就表示为  \n\n$$\\sum_{[i_1,i_2,\\cdots,i_k]\\in I}\\prod_{j=1}^ka_j^{(i_j)}$$  \n\n在构建tree的时候，优先加入当前有最大准确率的候选序列，直到tree的节点数量达到上限，这样能最大化expectation of acceptance length，也就能最大化acceleration rate。  \n\n下图是一个按这种方法构建的tree的例子。可以看到这棵树向左偏，这是因为这个方法倾向于使用更高准确率的token。  \n\n{% asset_img construct_tree.png tree attention %}  \n\n# 训练策略  \n\nMEDUSA的解码头需要进行训练。训练策略根据是否有“与模型输出分布对齐的训练数据”而有所不同。  \n\n## 有训练数据  \n\nMEDUSA-1冻结了原模型的参数，而只对新增的解码头进行训练。  \n\n第 $k$ 个解码头的训练loss可以写作  \n\n$$\\mathcal{L}_k=-\\log p_t^{(k)}(y_{t+k+1})$$  \n\n总的训练loss为  \n\n$$\\mathcal{L}_{\\text{MEDUSA-l}}=\\sum_{k=1}^K-\\lambda_k\\log p_t^{(k)}(y_{t+k+1})$$  \n\n这里的 $\\lambda_{k}$ 是每个解码头的缩放系数，是一系列超参。因为 $k$ 越大，对应解码头的预测难度越大，loss也就越大，为了防止靠后的解码头过分主导训练，因此使用一个缩放系数进行调整。  \n\n实际使用中，$\\lambda_{k}=0.8^{k}$。  \n\n训练时，由于冻结了原模型，因此可以对原模型的参数进行量化而不会对训练效果有明显影响，比如使用QLoRA。  \n\nMEDUSA-1冻结了原模型，比较适用于计算资源有限，或者希望保持原模型能力的场景。如果要进一步发挥MEDUSA多个解码头的加速效果，那就需要使用MEDUSA-2。  \n\nMEDUSA-2把原模型和多个解码头一起训练，因此各个解码头的准确率能达到更高的水平，acceleration rate也更高。但是为了保持原模型的输出质量，需要使用以下三个措施。  \n\n（1）Combined loss  \n\n首先是加入原模型next-token prediction的loss，即把原模型解码头的loss也加上，如下式  \n\n$$\\mathcal{L}_{\\text{MEDUSA-}2}=\\mathcal{L}_{\\text{LM}}+\\lambda_0\\mathcal{L}_{\\text{MEDUSA-}1}$$  \n\n$$\\mathcal{L}_{\\text{LM}}=-\\log p_t^{(0)}(y_{t+1})$$  \n\n实际使用中，直接训练时 $\\lambda_0=0.2$，使用self-distillation时$\\lambda_0=0.01$。  \n\n（2）Differential learning rates  \n\n原模型已经是训练好了的，因此和新加入的解码头使用相同的学习率并不合适，因此可以让新的解码头使用更大的学习率，而原模型参数使用相对小的学习率。实践中把学习率差距设为4倍，比如分别使用2e-3和5e-4。  \n\n（3）Heads warmup  \n\n新加入的解码头在一开始训练会有比较大的loss，从而导致更大的梯度，有可能损害原模型的能力。  \n\n针对这个问题，可以使用two-stage training的方式，先在MEDUSA-1的策略下训练解码头，然后再进行MEDUSA-2的训练。这其实相当于把 $\\lambda_0$ 在训练过程中逐渐增大。two-stage training和逐渐增大 $\\lambda_0$ 的方法在实践中都是可行的。  \n\n## SELF-DISTILLATION  \n\n前面讲的这些训练方式都有一个前提，那就是有与模型输出分布对齐的训练数据可供使用。但是实际上这个前提并不总是成立。比如大部分开源模型并没有发布相应的SFT数据，或者模型使用了RLHF等对齐方式，而不是直接SFT。  \n\n解决方法是使用self-distillation：通过原模型为MEDUSA解码头生成训练数据。  \n\n首先选择一个和target model的domain相近的数据集，然后把prompt输入给原模型，获得原模型的输出。对于对话模型，需要生成多轮对话，可以使用self-talk。  \n\n对于MEDUSA-1，这样生成的数据集已经够用，但是对于MEDUSA-2，这样的训练会降低生成质量。  \n \n实际上，即使不进行MEDUSA解码头的训练，只用生成的数据训练原模型，原模型的效果也会变差。因此MEDUSA-2训练的时候，原模型的训练不应该直接使用ground truth进行训练，而是进行蒸馏，按下式计算损失  \n\n$$\\mathcal{L}_{\\text{LM-distill}}=KL(p_{\\text{original},t}^{(0)}||p_t^{(0)})$$  \n\n# 接受策略：TYPICAL ACCEPTANCE  \n\n投机解码随着temperature的提升，命中率会降低。因为temperature提升，draft model所选择的候选token的多样性就增大，也就降低了命中原模型token，从而被接受的概率。  \n\n但是这种特性并不合理。通常更高的temperature参数一般对应更强的creativity特性，因此合理的情况应该是随着温度提高，候选序列有更大的概率被接受。这和投机解码的情况是相反的。  \n\n另外，MEDUSA认为候选序列的分布没有必要完全match原模型的分布。我们要做的应该是选出typical的候选，也就是只要候选序列不是极不可能的结果，就可以被接受。  \n\n给定context $x_1,x_2,\\cdots,x_n$，候选序列 $(x_{n+1},x_{n+2},\\cdots,x_{n+K+1})$，我们按以下这个条件来接受候选token  \n\n$$\\begin{aligned}p_{\\text{original}}(x_{n+k}|x_1,x_2,\\cdots,x_{n+k-1})&>\\\\\\min\\left(\\epsilon,\\delta\\exp\\left(-H(p_{\\text{original}}(\\cdot|x_1,x_2,\\cdots,x_{n+k-1})))\\right)\\right),\\end{aligned}$$  \n\n其中 $H(\\cdot)$ 表示entropy function，$\\epsilon,\\delta$ 分别是hard threshold和entropy-dependent threshold。  \n\n两个threshold的解释：（1）$\\epsilon$ 保证所选的token的概率不能低于特定值，保证不选出可能性很低的结果（2）当一个位置的多个候选token的entropy较高时，表示多个候选都是reasonable的，那么 $\\delta$ 和exp(entropy)的乘积会更小，各个token都有更大的机会被接受。  \n\n当temperatrue为0的时候，相当于贪心解码，这个时候只有概率最大那个token有非0概率。随着温度提升，其他token的概率也提升，因此它们也有一定的机会被接受。随着温度提升，这些token被接受的概率会增大。  \n\n最后选择被接受的解码长度最长的候选序列作为最终结果。  \n\n# 消融实验  \n\n## CONFIGURATION OF TREE ATTENTION  \n\n对比通过准确率构建tree attention的方式，和随机构建tree attention的方式，结果如下  \n\n{% asset_img tree_attention_exp.png 消融实验 %}  \n\n基于准确率构建的tree attention有更高的acceleration rate。  \n\n但随着候选token数量的增加，两种方式的实际速度反而有所下降，因为更多的候选token引入了额外的计算成本。  \n\n## THRESHOLDS OF TYPICAL ACCEPTANCE  \n\n随着 $\\epsilon $ 增加，输出质量得到提升，但代价是acceleration rate降低，如下图  \n\n{% asset_img threshold.png 消融实验 %}  \n\n## 各环节对速度的影响  \n\n各个技术优化点对速度的影响如下表  \n\n{% asset_img speed.png 消融实验 %}  \n\n# 小结  \n\nMEDUSA引入了tree attention、typical acceptance的做法，在加速比上相比投机解码有进一步提升。  \n\n但是MEDUSA不保证解码结果和原模型一致，因此应该更适用于对模型生成质量的没有那么严格要求的场景。  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】MEDUSA: Simple LLM Inference Acceleration Framework with Multiple\nDecoding Heads https://arxiv.org/abs/2401.10774  \n","slug":"cs/nlp/2024/06/大模型推理加速-MEDUSA","published":1,"updated":"2024-06-15T08:19:24.061Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9x000p314k00fi58q0","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>之前对speculative decoding的做法做了介绍：<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a>。</p>\n<p>本篇介绍一下另外一个热门的解码加速算法，MEDUSA。MEDUSA在不同的训练方法下能提供×2.2~×2.8的解码加速效果。</p>\n<h1 id=\"背景\">背景</h1>\n<p>自回归大模型推理下一个token的时候，需要依赖前面的结果。而在实际使用GPU进行计算时，需要将相关矩阵移至片上内存进行运算，而一般来说片上内存带宽比计算性能要低两个数量级，这就使得大模型推理是memory-bandwidth-bound的。</p>\n<p>要解决这个问题，一个思路是increasing the arithmetic\nintensity，即提高“浮点数计算量/数据传输量”这个比值，让数据传输不要成为瓶颈。另一个思路是reducing\nthe number of decoding steps。投机解码就属于后者。</p>\n<p>不过投机解码有几个问题：<br>\n- 一个好的draft model不容易获取：draft模型和原模型存在distribution\nshift<br>\n- 推理时有多个模型参与，在分布式系统上的部署难度增大</p>\n<p>而MEDUSA相比投机解码，不需要新增一个模型，而是基于原模型进行并行推理，这样训练难度更低，部署也更容易。</p>\n<h1 id=\"medusa\">MEDUSA</h1>\n<p>MEDUSA的大致思路是和投机解码类似：<br>\n-\n首先生成各个位置的候选token；MEDUSA通过接在原模型的多个解码头来获取多个位置的候选token<br>\n-\n把各个位置的候选token进行处理，选出一些候选序列，进行验证；MEDUSA通过tree\nattention来处理<br>\n- 最后通过typical acceptance选择最终输出的结果</p>\n<p>MEDUSA的pipeline如下图所示</p>\n<img src=\"/7bbe2df6/intro.png\" class title=\"introduction\">\n<p>MEDUSA的这些分类头需要经过训练才能有比较好的预测效果。针对不同的条件，可以选择不同的训练方式：<br>\n-\nMEDUSA-1：冻结原模型的backbone（包括原模型的解码头），只训练增加的解码头。这种方案适用于计算资源比较少，或者不想影响原模型的效果的情况。还可以使用QLoRA对解码头进行训练，进一步节省memory和计算资源。<br>\n-\nMEDUSA-2：原模型和MEDUSA的解码头一起训练。MEDUSA-1这样的训练方法虽然可以节省资源，但是并不能最大程度发挥多个解码头的加速效果，而MEDUSA-2则可以进一步发挥MEDUSA解码头的提速能力。MEDUSA-2适用于计算资源充足，或者从Base模型进行SFT的场景。</p>\n<p>另外，如果原模型的SFT数据集是available的，那可以直接进行训练。如果不能获得原模型的SFT数据，或者原模型是经过RLHF训练的，则可以通过self-distillation来获取MEDUSA\nhead的训练数据。</p>\n<h1 id=\"模型设计medusa-heads\">模型设计：MEDUSA HEADS</h1>\n<p>先来看下第一步，MEDUSA的多个解码头是怎么给出各个位置的候选token的。</p>\n<p>假设原始模型最后一层的hidden state在时间 <span class=\"math inline\">\\(t\\)</span> 的输出是 <span class=\"math inline\">\\(h_{t}\\)</span>，我们给模型额外加上 <span class=\"math inline\">\\(K\\)</span> 个解码头。那么第 <span class=\"math inline\">\\(k\\)</span> 个头就可以用来预测位置 <span class=\"math inline\">\\(t+k+1\\)</span> 的输出token（这里 <span class=\"math inline\">\\(k\\)</span> 的取值为 <span class=\"math inline\">\\(1\\)</span> ~ <span class=\"math inline\">\\(K\\)</span>）。这里注意原模型自己还有一个解码头，它依然用来预测位置\n<span class=\"math inline\">\\(t+1\\)</span> 的输出，相当于 <span class=\"math inline\">\\(k=0\\)</span>。</p>\n<p>把第 <span class=\"math inline\">\\(k\\)</span>\n个解码头在vocabulary上的输出分布写作 <span class=\"math inline\">\\(p_t^{(k)}\\)</span>，其计算方式如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_t^{(k)}=\\text{softmax}\\left(W_2^{(k)}\\cdot\\left(\\text{SiLU}(W_1^{(k)}\\cdot\nh_t)+h_t\\right)\\right),\\\\\\mathrm{where~}W_2^{(k)}\\in\\mathbb{R}^{d\\times\nV},W_1^{(k)}\\in\\mathbb{R}^{d\\times d}.\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span> 是hidden\nstate的输出维度，<span class=\"math inline\">\\(V\\)</span>\n是词表大小。每个解码头其实就是一个FFN网络，实践上发现这样简单的设计已经有足够好的效果。</p>\n<p>在初始化各个解码头的参数时，把 <span class=\"math inline\">\\(W_2^{(k)}\\)</span>\n初始化成和原模型的解码头一样，而把 <span class=\"math inline\">\\(W_1^{(k)}\\)</span>\n设置成0。这样能使得在一开始训练的时候，增加的这些解码头就有一定的预测能力。</p>\n<p>这 <span class=\"math inline\">\\(K\\)</span>\n个新增的解码头直接在原模型的基础上进行训练，因此相比投机解码的draft\nmodel，MEDUSA的训练方式缓解了distribution shift的问题。</p>\n<h1 id=\"候选校验tree-attention\">候选校验：TREE ATTENTION</h1>\n<h2 id=\"cartesian-product\">Cartesian product</h2>\n<p>增加额外的解码头之后，模型每次前向推理都会给出 <span class=\"math inline\">\\(K+1\\)</span> 个位置的候选token。</p>\n<p>投机解码里是直接选出draft\nmodel最有信心的一个候选序列给原模型进行验证。</p>\n<p>显然，如果增加候选序列的数量，那么最终接受token的命中率就会提升，acceleration\nrate（即每个decoding\nstep能获得的token数，不是实际解码时间）也就更高，但是验证更多候选序列也会带来额外的计算消耗。为了获得一个效果和性能比较好的平衡，MEDUSA使用tree\nattention来同时对多个候选序列进行处理。</p>\n<p>假设第 <span class=\"math inline\">\\(k\\)</span>\n个解码头给出的候选token数量是 <span class=\"math inline\">\\(s_k\\)</span>\n个，那么可以通过Cartesian\nproduct来获取多个解码头组成的所有可能的候选序列，然后用tree\nattention对所有候选序列进行验证。</p>\n<p>对于两个解码头的情况，tree attention验证的示意图如下</p>\n<img src=\"/7bbe2df6/tree_attention.png\" class title=\"tree attention\">\n<p>通过使用这样的mask，我们可以在不扩大batch\nsize的情况下同时处理多个候选序列。（注意，这里要对各个候选token的位置编码进行处理。）</p>\n<h2 id=\"更高效的tree-attention构建\">更高效的tree attention构建</h2>\n<p>上面这个例子使用了Cartesian\nproduct对两个解码头的结果进行处理，获得所有候选序列。</p>\n<p>但是如果解码头数量数量比较多，每个头给出的候选token也比较多，那么实际要验证的序列数量会极大地增长。</p>\n<p>直觉上，这些解码头应该有不同的准确率，因此可以利用这一点来构建tree\nattention，而不需要使用所有可能的排列组合。</p>\n<p>具体来说，可以使用一个calibration dataset（比如Alpaca-eval\ndataset）来获取不同解码头给出的各个token的准确率：把第 <span class=\"math inline\">\\(k\\)</span> 个解码头给出的第 <span class=\"math inline\">\\(i\\)</span> 个token的准确率记为 <span class=\"math inline\">\\(a_k^{(i)}\\)</span>。</p>\n<p>假设各个token的准确率之间是独立的，那么一个由 <span class=\"math inline\">\\([i_1,i_2,\\cdots,i_k]\\)</span>\n构成的候选序列的准确率可以写作 <span class=\"math inline\">\\(\\prod_{j=1}^ka_j^{(i_j)}\\)</span>。</p>\n<p>每个候选序列可以表示所构建的tree上的一条路径上所有的node（而不只是leaf\nnode，因为tree attention验证的时候会把路径上所有token都进行验证）。用\n<span class=\"math inline\">\\(I\\)</span>\n表示候选序列的集合，那么集合里的候选序列的expectation of acceptance\nlength就表示为</p>\n<p><span class=\"math display\">\\[\\sum_{[i_1,i_2,\\cdots,i_k]\\in\nI}\\prod_{j=1}^ka_j^{(i_j)}\\]</span></p>\n<p>在构建tree的时候，优先加入当前有最大准确率的候选序列，直到tree的节点数量达到上限，这样能最大化expectation\nof acceptance length，也就能最大化acceleration rate。</p>\n<p>下图是一个按这种方法构建的tree的例子。可以看到这棵树向左偏，这是因为这个方法倾向于使用更高准确率的token。</p>\n<img src=\"/7bbe2df6/construct_tree.png\" class title=\"tree attention\">\n<h1 id=\"训练策略\">训练策略</h1>\n<p>MEDUSA的解码头需要进行训练。训练策略根据是否有“与模型输出分布对齐的训练数据”而有所不同。</p>\n<h2 id=\"有训练数据\">有训练数据</h2>\n<p>MEDUSA-1冻结了原模型的参数，而只对新增的解码头进行训练。</p>\n<p>第 <span class=\"math inline\">\\(k\\)</span>\n个解码头的训练loss可以写作</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_k=-\\log\np_t^{(k)}(y_{t+k+1})\\]</span></p>\n<p>总的训练loss为</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{MEDUSA-l}}=\\sum_{k=1}^K-\\lambda_k\\log\np_t^{(k)}(y_{t+k+1})\\]</span></p>\n<p>这里的 <span class=\"math inline\">\\(\\lambda_{k}\\)</span>\n是每个解码头的缩放系数，是一系列超参。因为 <span class=\"math inline\">\\(k\\)</span>\n越大，对应解码头的预测难度越大，loss也就越大，为了防止靠后的解码头过分主导训练，因此使用一个缩放系数进行调整。</p>\n<p>实际使用中，<span class=\"math inline\">\\(\\lambda_{k}=0.8^{k}\\)</span>。</p>\n<p>训练时，由于冻结了原模型，因此可以对原模型的参数进行量化而不会对训练效果有明显影响，比如使用QLoRA。</p>\n<p>MEDUSA-1冻结了原模型，比较适用于计算资源有限，或者希望保持原模型能力的场景。如果要进一步发挥MEDUSA多个解码头的加速效果，那就需要使用MEDUSA-2。</p>\n<p>MEDUSA-2把原模型和多个解码头一起训练，因此各个解码头的准确率能达到更高的水平，acceleration\nrate也更高。但是为了保持原模型的输出质量，需要使用以下三个措施。</p>\n<p>（1）Combined loss</p>\n<p>首先是加入原模型next-token\nprediction的loss，即把原模型解码头的loss也加上，如下式</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{MEDUSA-}2}=\\mathcal{L}_{\\text{LM}}+\\lambda_0\\mathcal{L}_{\\text{MEDUSA-}1}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{LM}}=-\\log\np_t^{(0)}(y_{t+1})\\]</span></p>\n<p>实际使用中，直接训练时 <span class=\"math inline\">\\(\\lambda_0=0.2\\)</span>，使用self-distillation时<span class=\"math inline\">\\(\\lambda_0=0.01\\)</span>。</p>\n<p>（2）Differential learning rates</p>\n<p>原模型已经是训练好了的，因此和新加入的解码头使用相同的学习率并不合适，因此可以让新的解码头使用更大的学习率，而原模型参数使用相对小的学习率。实践中把学习率差距设为4倍，比如分别使用2e-3和5e-4。</p>\n<p>（3）Heads warmup</p>\n<p>新加入的解码头在一开始训练会有比较大的loss，从而导致更大的梯度，有可能损害原模型的能力。</p>\n<p>针对这个问题，可以使用two-stage\ntraining的方式，先在MEDUSA-1的策略下训练解码头，然后再进行MEDUSA-2的训练。这其实相当于把\n<span class=\"math inline\">\\(\\lambda_0\\)</span>\n在训练过程中逐渐增大。two-stage training和逐渐增大 <span class=\"math inline\">\\(\\lambda_0\\)</span> 的方法在实践中都是可行的。</p>\n<h2 id=\"self-distillation\">SELF-DISTILLATION</h2>\n<p>前面讲的这些训练方式都有一个前提，那就是有与模型输出分布对齐的训练数据可供使用。但是实际上这个前提并不总是成立。比如大部分开源模型并没有发布相应的SFT数据，或者模型使用了RLHF等对齐方式，而不是直接SFT。</p>\n<p>解决方法是使用self-distillation：通过原模型为MEDUSA解码头生成训练数据。</p>\n<p>首先选择一个和target\nmodel的domain相近的数据集，然后把prompt输入给原模型，获得原模型的输出。对于对话模型，需要生成多轮对话，可以使用self-talk。</p>\n<p>对于MEDUSA-1，这样生成的数据集已经够用，但是对于MEDUSA-2，这样的训练会降低生成质量。</p>\n<p>实际上，即使不进行MEDUSA解码头的训练，只用生成的数据训练原模型，原模型的效果也会变差。因此MEDUSA-2训练的时候，原模型的训练不应该直接使用ground\ntruth进行训练，而是进行蒸馏，按下式计算损失</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{LM-distill}}=KL(p_{\\text{original},t}^{(0)}||p_t^{(0)})\\]</span></p>\n<h1 id=\"接受策略typical-acceptance\">接受策略：TYPICAL ACCEPTANCE</h1>\n<p>投机解码随着temperature的提升，命中率会降低。因为temperature提升，draft\nmodel所选择的候选token的多样性就增大，也就降低了命中原模型token，从而被接受的概率。</p>\n<p>但是这种特性并不合理。通常更高的temperature参数一般对应更强的creativity特性，因此合理的情况应该是随着温度提高，候选序列有更大的概率被接受。这和投机解码的情况是相反的。</p>\n<p>另外，MEDUSA认为候选序列的分布没有必要完全match原模型的分布。我们要做的应该是选出typical的候选，也就是只要候选序列不是极不可能的结果，就可以被接受。</p>\n<p>给定context <span class=\"math inline\">\\(x_1,x_2,\\cdots,x_n\\)</span>，候选序列 <span class=\"math inline\">\\((x_{n+1},x_{n+2},\\cdots,x_{n+K+1})\\)</span>，我们按以下这个条件来接受候选token</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_{\\text{original}}(x_{n+k}|x_1,x_2,\\cdots,x_{n+k-1})&amp;&gt;\\\\\\min\\left(\\epsilon,\\delta\\exp\\left(-H(p_{\\text{original}}(\\cdot|x_1,x_2,\\cdots,x_{n+k-1})))\\right)\\right),\\end{aligned}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(H(\\cdot)\\)</span> 表示entropy\nfunction，<span class=\"math inline\">\\(\\epsilon,\\delta\\)</span>\n分别是hard threshold和entropy-dependent threshold。</p>\n<p>两个threshold的解释：（1）<span class=\"math inline\">\\(\\epsilon\\)</span>\n保证所选的token的概率不能低于特定值，保证不选出可能性很低的结果（2）当一个位置的多个候选token的entropy较高时，表示多个候选都是reasonable的，那么\n<span class=\"math inline\">\\(\\delta\\)</span>\n和exp(entropy)的乘积会更小，各个token都有更大的机会被接受。</p>\n<p>当temperatrue为0的时候，相当于贪心解码，这个时候只有概率最大那个token有非0概率。随着温度提升，其他token的概率也提升，因此它们也有一定的机会被接受。随着温度提升，这些token被接受的概率会增大。</p>\n<p>最后选择被接受的解码长度最长的候选序列作为最终结果。</p>\n<h1 id=\"消融实验\">消融实验</h1>\n<h2 id=\"configuration-of-tree-attention\">CONFIGURATION OF TREE\nATTENTION</h2>\n<p>对比通过准确率构建tree attention的方式，和随机构建tree\nattention的方式，结果如下</p>\n<img src=\"/7bbe2df6/tree_attention_exp.png\" class title=\"消融实验\">\n<p>基于准确率构建的tree attention有更高的acceleration rate。</p>\n<p>但随着候选token数量的增加，两种方式的实际速度反而有所下降，因为更多的候选token引入了额外的计算成本。</p>\n<h2 id=\"thresholds-of-typical-acceptance\">THRESHOLDS OF TYPICAL\nACCEPTANCE</h2>\n<p>随着 $$ 增加，输出质量得到提升，但代价是acceleration\nrate降低，如下图</p>\n<img src=\"/7bbe2df6/threshold.png\" class title=\"消融实验\">\n<h2 id=\"各环节对速度的影响\">各环节对速度的影响</h2>\n<p>各个技术优化点对速度的影响如下表</p>\n<img src=\"/7bbe2df6/speed.png\" class title=\"消融实验\">\n<h1 id=\"小结\">小结</h1>\n<p>MEDUSA引入了tree attention、typical\nacceptance的做法，在加速比上相比投机解码有进一步提升。</p>\n<p>但是MEDUSA不保证解码结果和原模型一致，因此应该更适用于对模型生成质量的没有那么严格要求的场景。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】MEDUSA: Simple LLM Inference Acceleration Framework with\nMultiple Decoding Heads https://arxiv.org/abs/2401.10774</p>\n","length":6717,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>之前对speculative decoding的做法做了介绍：<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a>。</p>\n<p>本篇介绍一下另外一个热门的解码加速算法，MEDUSA。MEDUSA在不同的训练方法下能提供×2.2~×2.8的解码加速效果。</p>\n<h1 id=\"背景\">背景</h1>\n<p>自回归大模型推理下一个token的时候，需要依赖前面的结果。而在实际使用GPU进行计算时，需要将相关矩阵移至片上内存进行运算，而一般来说片上内存带宽比计算性能要低两个数量级，这就使得大模型推理是memory-bandwidth-bound的。</p>\n<p>要解决这个问题，一个思路是increasing the arithmetic\nintensity，即提高“浮点数计算量/数据传输量”这个比值，让数据传输不要成为瓶颈。另一个思路是reducing\nthe number of decoding steps。投机解码就属于后者。</p>\n<p>不过投机解码有几个问题：<br>\n- 一个好的draft model不容易获取：draft模型和原模型存在distribution\nshift<br>\n- 推理时有多个模型参与，在分布式系统上的部署难度增大</p>\n<p>而MEDUSA相比投机解码，不需要新增一个模型，而是基于原模型进行并行推理，这样训练难度更低，部署也更容易。</p>\n<h1 id=\"medusa\">MEDUSA</h1>\n<p>MEDUSA的大致思路是和投机解码类似：<br>\n-\n首先生成各个位置的候选token；MEDUSA通过接在原模型的多个解码头来获取多个位置的候选token<br>\n-\n把各个位置的候选token进行处理，选出一些候选序列，进行验证；MEDUSA通过tree\nattention来处理<br>\n- 最后通过typical acceptance选择最终输出的结果</p>\n<p>MEDUSA的pipeline如下图所示</p>\n<img src=\"/7bbe2df6/intro.png\" class title=\"introduction\">\n<p>MEDUSA的这些分类头需要经过训练才能有比较好的预测效果。针对不同的条件，可以选择不同的训练方式：<br>\n-\nMEDUSA-1：冻结原模型的backbone（包括原模型的解码头），只训练增加的解码头。这种方案适用于计算资源比较少，或者不想影响原模型的效果的情况。还可以使用QLoRA对解码头进行训练，进一步节省memory和计算资源。<br>\n-\nMEDUSA-2：原模型和MEDUSA的解码头一起训练。MEDUSA-1这样的训练方法虽然可以节省资源，但是并不能最大程度发挥多个解码头的加速效果，而MEDUSA-2则可以进一步发挥MEDUSA解码头的提速能力。MEDUSA-2适用于计算资源充足，或者从Base模型进行SFT的场景。</p>\n<p>另外，如果原模型的SFT数据集是available的，那可以直接进行训练。如果不能获得原模型的SFT数据，或者原模型是经过RLHF训练的，则可以通过self-distillation来获取MEDUSA\nhead的训练数据。</p>\n<h1 id=\"模型设计medusa-heads\">模型设计：MEDUSA HEADS</h1>\n<p>先来看下第一步，MEDUSA的多个解码头是怎么给出各个位置的候选token的。</p>\n<p>假设原始模型最后一层的hidden state在时间 <span class=\"math inline\">\\(t\\)</span> 的输出是 <span class=\"math inline\">\\(h_{t}\\)</span>，我们给模型额外加上 <span class=\"math inline\">\\(K\\)</span> 个解码头。那么第 <span class=\"math inline\">\\(k\\)</span> 个头就可以用来预测位置 <span class=\"math inline\">\\(t+k+1\\)</span> 的输出token（这里 <span class=\"math inline\">\\(k\\)</span> 的取值为 <span class=\"math inline\">\\(1\\)</span> ~ <span class=\"math inline\">\\(K\\)</span>）。这里注意原模型自己还有一个解码头，它依然用来预测位置\n<span class=\"math inline\">\\(t+1\\)</span> 的输出，相当于 <span class=\"math inline\">\\(k=0\\)</span>。</p>\n<p>把第 <span class=\"math inline\">\\(k\\)</span>\n个解码头在vocabulary上的输出分布写作 <span class=\"math inline\">\\(p_t^{(k)}\\)</span>，其计算方式如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_t^{(k)}=\\text{softmax}\\left(W_2^{(k)}\\cdot\\left(\\text{SiLU}(W_1^{(k)}\\cdot\nh_t)+h_t\\right)\\right),\\\\\\mathrm{where~}W_2^{(k)}\\in\\mathbb{R}^{d\\times\nV},W_1^{(k)}\\in\\mathbb{R}^{d\\times d}.\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span> 是hidden\nstate的输出维度，<span class=\"math inline\">\\(V\\)</span>\n是词表大小。每个解码头其实就是一个FFN网络，实践上发现这样简单的设计已经有足够好的效果。</p>\n<p>在初始化各个解码头的参数时，把 <span class=\"math inline\">\\(W_2^{(k)}\\)</span>\n初始化成和原模型的解码头一样，而把 <span class=\"math inline\">\\(W_1^{(k)}\\)</span>\n设置成0。这样能使得在一开始训练的时候，增加的这些解码头就有一定的预测能力。</p>\n<p>这 <span class=\"math inline\">\\(K\\)</span>\n个新增的解码头直接在原模型的基础上进行训练，因此相比投机解码的draft\nmodel，MEDUSA的训练方式缓解了distribution shift的问题。</p>\n<h1 id=\"候选校验tree-attention\">候选校验：TREE ATTENTION</h1>\n<h2 id=\"cartesian-product\">Cartesian product</h2>\n<p>增加额外的解码头之后，模型每次前向推理都会给出 <span class=\"math inline\">\\(K+1\\)</span> 个位置的候选token。</p>\n<p>投机解码里是直接选出draft\nmodel最有信心的一个候选序列给原模型进行验证。</p>\n<p>显然，如果增加候选序列的数量，那么最终接受token的命中率就会提升，acceleration\nrate（即每个decoding\nstep能获得的token数，不是实际解码时间）也就更高，但是验证更多候选序列也会带来额外的计算消耗。为了获得一个效果和性能比较好的平衡，MEDUSA使用tree\nattention来同时对多个候选序列进行处理。</p>\n<p>假设第 <span class=\"math inline\">\\(k\\)</span>\n个解码头给出的候选token数量是 <span class=\"math inline\">\\(s_k\\)</span>\n个，那么可以通过Cartesian\nproduct来获取多个解码头组成的所有可能的候选序列，然后用tree\nattention对所有候选序列进行验证。</p>\n<p>对于两个解码头的情况，tree attention验证的示意图如下</p>\n<img src=\"/7bbe2df6/tree_attention.png\" class title=\"tree attention\">\n<p>通过使用这样的mask，我们可以在不扩大batch\nsize的情况下同时处理多个候选序列。（注意，这里要对各个候选token的位置编码进行处理。）</p>\n<h2 id=\"更高效的tree-attention构建\">更高效的tree attention构建</h2>\n<p>上面这个例子使用了Cartesian\nproduct对两个解码头的结果进行处理，获得所有候选序列。</p>\n<p>但是如果解码头数量数量比较多，每个头给出的候选token也比较多，那么实际要验证的序列数量会极大地增长。</p>\n<p>直觉上，这些解码头应该有不同的准确率，因此可以利用这一点来构建tree\nattention，而不需要使用所有可能的排列组合。</p>\n<p>具体来说，可以使用一个calibration dataset（比如Alpaca-eval\ndataset）来获取不同解码头给出的各个token的准确率：把第 <span class=\"math inline\">\\(k\\)</span> 个解码头给出的第 <span class=\"math inline\">\\(i\\)</span> 个token的准确率记为 <span class=\"math inline\">\\(a_k^{(i)}\\)</span>。</p>\n<p>假设各个token的准确率之间是独立的，那么一个由 <span class=\"math inline\">\\([i_1,i_2,\\cdots,i_k]\\)</span>\n构成的候选序列的准确率可以写作 <span class=\"math inline\">\\(\\prod_{j=1}^ka_j^{(i_j)}\\)</span>。</p>\n<p>每个候选序列可以表示所构建的tree上的一条路径上所有的node（而不只是leaf\nnode，因为tree attention验证的时候会把路径上所有token都进行验证）。用\n<span class=\"math inline\">\\(I\\)</span>\n表示候选序列的集合，那么集合里的候选序列的expectation of acceptance\nlength就表示为</p>\n<p><span class=\"math display\">\\[\\sum_{[i_1,i_2,\\cdots,i_k]\\in\nI}\\prod_{j=1}^ka_j^{(i_j)}\\]</span></p>\n<p>在构建tree的时候，优先加入当前有最大准确率的候选序列，直到tree的节点数量达到上限，这样能最大化expectation\nof acceptance length，也就能最大化acceleration rate。</p>\n<p>下图是一个按这种方法构建的tree的例子。可以看到这棵树向左偏，这是因为这个方法倾向于使用更高准确率的token。</p>\n<img src=\"/7bbe2df6/construct_tree.png\" class title=\"tree attention\">\n<h1 id=\"训练策略\">训练策略</h1>\n<p>MEDUSA的解码头需要进行训练。训练策略根据是否有“与模型输出分布对齐的训练数据”而有所不同。</p>\n<h2 id=\"有训练数据\">有训练数据</h2>\n<p>MEDUSA-1冻结了原模型的参数，而只对新增的解码头进行训练。</p>\n<p>第 <span class=\"math inline\">\\(k\\)</span>\n个解码头的训练loss可以写作</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_k=-\\log\np_t^{(k)}(y_{t+k+1})\\]</span></p>\n<p>总的训练loss为</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{MEDUSA-l}}=\\sum_{k=1}^K-\\lambda_k\\log\np_t^{(k)}(y_{t+k+1})\\]</span></p>\n<p>这里的 <span class=\"math inline\">\\(\\lambda_{k}\\)</span>\n是每个解码头的缩放系数，是一系列超参。因为 <span class=\"math inline\">\\(k\\)</span>\n越大，对应解码头的预测难度越大，loss也就越大，为了防止靠后的解码头过分主导训练，因此使用一个缩放系数进行调整。</p>\n<p>实际使用中，<span class=\"math inline\">\\(\\lambda_{k}=0.8^{k}\\)</span>。</p>\n<p>训练时，由于冻结了原模型，因此可以对原模型的参数进行量化而不会对训练效果有明显影响，比如使用QLoRA。</p>\n<p>MEDUSA-1冻结了原模型，比较适用于计算资源有限，或者希望保持原模型能力的场景。如果要进一步发挥MEDUSA多个解码头的加速效果，那就需要使用MEDUSA-2。</p>\n<p>MEDUSA-2把原模型和多个解码头一起训练，因此各个解码头的准确率能达到更高的水平，acceleration\nrate也更高。但是为了保持原模型的输出质量，需要使用以下三个措施。</p>\n<p>（1）Combined loss</p>\n<p>首先是加入原模型next-token\nprediction的loss，即把原模型解码头的loss也加上，如下式</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{MEDUSA-}2}=\\mathcal{L}_{\\text{LM}}+\\lambda_0\\mathcal{L}_{\\text{MEDUSA-}1}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{LM}}=-\\log\np_t^{(0)}(y_{t+1})\\]</span></p>\n<p>实际使用中，直接训练时 <span class=\"math inline\">\\(\\lambda_0=0.2\\)</span>，使用self-distillation时<span class=\"math inline\">\\(\\lambda_0=0.01\\)</span>。</p>\n<p>（2）Differential learning rates</p>\n<p>原模型已经是训练好了的，因此和新加入的解码头使用相同的学习率并不合适，因此可以让新的解码头使用更大的学习率，而原模型参数使用相对小的学习率。实践中把学习率差距设为4倍，比如分别使用2e-3和5e-4。</p>\n<p>（3）Heads warmup</p>\n<p>新加入的解码头在一开始训练会有比较大的loss，从而导致更大的梯度，有可能损害原模型的能力。</p>\n<p>针对这个问题，可以使用two-stage\ntraining的方式，先在MEDUSA-1的策略下训练解码头，然后再进行MEDUSA-2的训练。这其实相当于把\n<span class=\"math inline\">\\(\\lambda_0\\)</span>\n在训练过程中逐渐增大。two-stage training和逐渐增大 <span class=\"math inline\">\\(\\lambda_0\\)</span> 的方法在实践中都是可行的。</p>\n<h2 id=\"self-distillation\">SELF-DISTILLATION</h2>\n<p>前面讲的这些训练方式都有一个前提，那就是有与模型输出分布对齐的训练数据可供使用。但是实际上这个前提并不总是成立。比如大部分开源模型并没有发布相应的SFT数据，或者模型使用了RLHF等对齐方式，而不是直接SFT。</p>\n<p>解决方法是使用self-distillation：通过原模型为MEDUSA解码头生成训练数据。</p>\n<p>首先选择一个和target\nmodel的domain相近的数据集，然后把prompt输入给原模型，获得原模型的输出。对于对话模型，需要生成多轮对话，可以使用self-talk。</p>\n<p>对于MEDUSA-1，这样生成的数据集已经够用，但是对于MEDUSA-2，这样的训练会降低生成质量。</p>\n<p>实际上，即使不进行MEDUSA解码头的训练，只用生成的数据训练原模型，原模型的效果也会变差。因此MEDUSA-2训练的时候，原模型的训练不应该直接使用ground\ntruth进行训练，而是进行蒸馏，按下式计算损失</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{LM-distill}}=KL(p_{\\text{original},t}^{(0)}||p_t^{(0)})\\]</span></p>\n<h1 id=\"接受策略typical-acceptance\">接受策略：TYPICAL ACCEPTANCE</h1>\n<p>投机解码随着temperature的提升，命中率会降低。因为temperature提升，draft\nmodel所选择的候选token的多样性就增大，也就降低了命中原模型token，从而被接受的概率。</p>\n<p>但是这种特性并不合理。通常更高的temperature参数一般对应更强的creativity特性，因此合理的情况应该是随着温度提高，候选序列有更大的概率被接受。这和投机解码的情况是相反的。</p>\n<p>另外，MEDUSA认为候选序列的分布没有必要完全match原模型的分布。我们要做的应该是选出typical的候选，也就是只要候选序列不是极不可能的结果，就可以被接受。</p>\n<p>给定context <span class=\"math inline\">\\(x_1,x_2,\\cdots,x_n\\)</span>，候选序列 <span class=\"math inline\">\\((x_{n+1},x_{n+2},\\cdots,x_{n+K+1})\\)</span>，我们按以下这个条件来接受候选token</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_{\\text{original}}(x_{n+k}|x_1,x_2,\\cdots,x_{n+k-1})&amp;&gt;\\\\\\min\\left(\\epsilon,\\delta\\exp\\left(-H(p_{\\text{original}}(\\cdot|x_1,x_2,\\cdots,x_{n+k-1})))\\right)\\right),\\end{aligned}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(H(\\cdot)\\)</span> 表示entropy\nfunction，<span class=\"math inline\">\\(\\epsilon,\\delta\\)</span>\n分别是hard threshold和entropy-dependent threshold。</p>\n<p>两个threshold的解释：（1）<span class=\"math inline\">\\(\\epsilon\\)</span>\n保证所选的token的概率不能低于特定值，保证不选出可能性很低的结果（2）当一个位置的多个候选token的entropy较高时，表示多个候选都是reasonable的，那么\n<span class=\"math inline\">\\(\\delta\\)</span>\n和exp(entropy)的乘积会更小，各个token都有更大的机会被接受。</p>\n<p>当temperatrue为0的时候，相当于贪心解码，这个时候只有概率最大那个token有非0概率。随着温度提升，其他token的概率也提升，因此它们也有一定的机会被接受。随着温度提升，这些token被接受的概率会增大。</p>\n<p>最后选择被接受的解码长度最长的候选序列作为最终结果。</p>\n<h1 id=\"消融实验\">消融实验</h1>\n<h2 id=\"configuration-of-tree-attention\">CONFIGURATION OF TREE\nATTENTION</h2>\n<p>对比通过准确率构建tree attention的方式，和随机构建tree\nattention的方式，结果如下</p>\n<img src=\"/7bbe2df6/tree_attention_exp.png\" class title=\"消融实验\">\n<p>基于准确率构建的tree attention有更高的acceleration rate。</p>\n<p>但随着候选token数量的增加，两种方式的实际速度反而有所下降，因为更多的候选token引入了额外的计算成本。</p>\n<h2 id=\"thresholds-of-typical-acceptance\">THRESHOLDS OF TYPICAL\nACCEPTANCE</h2>\n<p>随着 $$ 增加，输出质量得到提升，但代价是acceleration\nrate降低，如下图</p>\n<img src=\"/7bbe2df6/threshold.png\" class title=\"消融实验\">\n<h2 id=\"各环节对速度的影响\">各环节对速度的影响</h2>\n<p>各个技术优化点对速度的影响如下表</p>\n<img src=\"/7bbe2df6/speed.png\" class title=\"消融实验\">\n<h1 id=\"小结\">小结</h1>\n<p>MEDUSA引入了tree attention、typical\nacceptance的做法，在加速比上相比投机解码有进一步提升。</p>\n<p>但是MEDUSA不保证解码结果和原模型一致，因此应该更适用于对模型生成质量的没有那么严格要求的场景。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">昆仑万维-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">大模型偏好对齐-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】MEDUSA: Simple LLM Inference Acceleration Framework with\nMultiple Decoding Heads https://arxiv.org/abs/2401.10774</p>\n"},{"title":"大模型偏好对齐-DPO","abbrlink":"473f2b43","date":"2024-05-26T14:01:48.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n要对齐大模型偏好并不容易，从预训练的数据内容、模型的结构到SFT数据配比甚至数据格式等都会影响最终结果。  \n\n按ChatGPT的技术路线，用SFT+RLHF PPO强化学习确实可以获得一定的提升，但是PPO比较复杂，训练过程不稳定，对微调后的模型、PPO的超参、reward模型的质量等都很敏感，且数据收集和训练的成本都较高，跑通大规模PPO有一定的成本门槛，因此PPO并没有被很广泛地应用。  \n\n而DPO，Direct Preference Optimization，就是PPO的一个简化替代方案。DPO不需要训练reward模型，把PPO的两阶段训练变成一阶段训练，让模型可以直接从偏好数据里学习。  \n\nDPO公式有点多，但是并不算太复杂，一步一步理解即可。  \n\n# 对齐  \n\n大模型在预训练中学到很多知识和技能，但是并不是所有知识和技能都是我们想要的。  \n\n比如有一个常见的错误知识，有超过80%的人会有这样的错误认知，那么这个错误知识在预训练数据里也会经常出现。虽然数据集里也会有关于这个知识的正确认知，但是比例相对会比较低。  \n\n如果让模型直接用在预训练中学到的知识进行回答，那么模型就有可能给出错误的知识。  \n\n这不是我们所希望的。因此需要通过一些方法，让模型给出的结果能对齐人类的偏好，比如最基础的偏好，正确性。  \n\n从模型非常广泛的知识和技能中选出我们所需的response和action是构建安全、高效、可控的AI系统的关键。  \n\nSFT是最直接的偏好学习方法，而RLHF/RLAIF是上限更高的偏好对齐方案。但RLHF比较复杂，训练不稳定，成本也高。  \n\n而DPO的优化目标和RLHF一样，但是实现更简单。  \n\n# RLHF\n\n先回顾下RLHF的三个阶段。  \n\n1. SFT Phase  \n\n基于预训练模型，在高质量的下游任务数据上训练，获得 $\\pi^{\\mathrm{SFT}}$。  \n\n2. Reward Modelling Phase  \n\n首先给定prompt $x$，生成两个答案 $(y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)$，并通过人工标注对比 $y_1,y_2$，获得偏好结果(preference) $y_w\\succ y_l\\mid x$，其中w和l表示win和lose。  \n\n假设在这些偏好结果中，有一个我们无法直接访问的latent reward model $r^*(y,x)$，对每对 $(x,y)$ 进行打分，这个 $r^*(y,x)$ 就是RLHF里reward model的拟合目标。  \n\n基于 $r^*(y,x)$，有很多方法对preference进行建模，Bradley-Terry model就是一个常用的选择。（当然在多个ranked answers的情况下，可以使用Plackett-Luce ranking models）  \n\n基于Bradley-Terry model，人类偏好的分布 $p^{*}$ 写作  \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\n看起来不复杂，就是把两个答案的reward通过softmax归一化成概率。  \n\n假设我们从 $p^{*}$ 采样到一个静态的偏好对比数据集 $\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N$ ，那我们就可以用基于 $\\pi^{\\mathrm{SFT}}$ 初始化得到的reward模型 $r_\\phi(x,y)$，通过maximum likelihood来拟合 $r^*(y,x)$。将这个问题表述为二元分类问题，我们就得到negative log-likelihood loss：  \n\n$$\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}$$  \n\n为了确保reward function有较低的方差，一般会对reward进行归一化，使得对于所有的 $x$，有 $\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0$。  \n\n3. RL Fine-Tuning Phase  \n\n在强化学习阶段，我们用上一步中得到的reward给目标模型提供反馈，优化如下目标  \n\n$$\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid x)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}$$  \n\n上式中第一项是reward模型对目标模型（即RLHF中的actor model）给出的答案的reward打分，这一项是越高越好。  \n\n而第二项是目标模型和参考模型之间的KL散度，用来限制经过训练后的目标模型，不要偏离参考模型（即 $\\pi^{\\mathrm{SFT}}$）太多。这样可以保证reward模型能在经过充分训练的区间工作，同时避免目标模型因过分向高reward分数优化而出现mode-collapse，失去回复的多样性。$\\beta$ 用来控制这个限制项的比重。  \n\n由于语言生成是离散的，因此上面这个优化目标是不可导的，需要通过RL优化。  \n\n标准的RL把reward fucntion构建成  \n\n$$r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid x)-\\log\\pi_\\text{ref}(y\\mid x))$$  \n\n并通过PPO优化。  \n\n# Direct Preference Optimization  \n\nDPO的目标是推导出一种简单的方法，直接使用偏好来进行policy optimization，而省去训练reward模型的训练。  \n\n{% asset_img intro.png DPO %}  \n\n## DPO优化目标的推导  \n\n首先，DPO起始的优化目标和RL是相同的：对于任意的reward function $r(x,y)$，reference model $\\pi_{\\mathrm{ref}}$  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\n由KL散度的定义，把上式中的第二项展开  \n\n$$\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}$$  \n\n这里的条件概率求和其实就是期望值，因此有  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\n$$\\begin{aligned}&=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n然后我们把最大化问题转化成最小化问题  \n\n$$\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n在这里我们用配分函数，归一一下分母。令  \n\n$$Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta r(x,y)\\right)$$  \n\n那我们就得到了一个新的有效的概率分布  \n\n$$\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n那么就有  \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n由于 $Z(x)$ 不是 $y$ 的函数，我们可以把它拿出来  \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log Z(x)\\right]$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log Z(x)\\right]$$  \n\n$Z(x)$ 和 $\\pi$ 无关，因此最小化这个式子只要最小化第一项KL散度。而当且仅当两个分布完全相同的时候，KL散度取得最小值0，因此有  \n\n$$\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n虽然得到了显示解，但是这里的 $Z(x)$ 没法求解，因为排列组合数太多，我们不可能去遍历。  \n\n继续对这个式子做一些变换  \n\n$$\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n$$\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x) +\\frac{1}{\\beta}r(x,y)\n\\end{aligned}$$  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n这里我们开始用上Bradley-Terry model了。前面我们提到了Bradley-Terry model是如下形式  \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\n在这个基础上做一点变换  \n\n$$\\begin{aligned}\np^*(y_1\\succ y_2\\mid x)&=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}$$  \n\n然后我们把 $r$ 代入进去，就得到  \n\n$$p^*(y_1\\succ y_2\\mid x)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}$$  \n\n到这里，我们就有了关于optimal policy的人类偏好数据的概率，而无需经过reward模型。我们可以用MLE直接在这个概率模型上优化目标模型  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\nDPO loss的实现如下  \n\n{% asset_img dpo_loss_code.png DPO实现 %}  \n\n## 理解DPO损失函数  \n\n首先我们了解一下DPO的loss在做什么，对DPO的损失函数求个导。  \n\n方便起见，令  \n\n$$u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}$$  \n\n那么原损失函数可以写成  \n\n$$L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim D}[\\log\\sigma(u)]$$  \n\n对sigmoid求导，有  \n\n$$\\frac\\partial{\\partial u}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)$$  \n\n由sigmoid函数性质，有  \n\n$$1-\\sigma(u)=\\sigma(-u)$$  \n\n对 $u$ 求导  \n\n$$\\frac{\\partial u}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)$$  \n\n第一项对数求导，由于 $\\pi_{\\mathrm{ref}}$ 不依赖 $\\theta$，可以视作常数，因此有  \n\n$$\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}$$  \n\n类似地，第二项求导  \n\n$$\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid x)$$  \n\n因此，DPO损失的导数是  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)–\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n再令  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\n那么DPO损失的梯度可以写作  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)–\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n梯度各项的意义如下  \n\n{% asset_img gradient.png DPO梯度 %}  \n\n$\\hat{r}_\\theta(x,y)$ 相当于 $\\pi_{\\theta}$ 和 $\\pi_{\\mathrm{ref}}$ 共同确定的隐式reward。  \n\n## DPO流程  \n\nDPO的一般流程是：  \n- 对于每个prompt $x$，采样 $y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid x)$，然后进行人工标注构建偏好数据集 $\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N$  \n- 基于 $\\mathcal{L}_{\\mathrm{DPO}}$，在已有的 $\\pi_{\\mathrm{ref}}$、$\\mathcal{D}$ 和 $\\beta$ 上优化 $\\pi\\theta $  \n\n但是收集偏好数据的成本还是比较高的，因此实际使用中，人们更愿意使用开源的偏好数据集。  \n\n当我们的偏好数据是来自 $\\pi^{\\mathrm{SFT}}$ 的时候，我们直接让 $\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}$。如果我们使用开源偏好数据集的话，就可能没法直接使用生成这些数据的模型，这时可以用偏好数据集里 $(x,y_w)$ 数据对 $\\pi_{\\mathrm{ref}}$ 进行微调，即  \n\n$$\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid x)\\right]$$  \n\n这个微调步骤有助于缓解 $\\pi_{\\mathrm{ref}}$ 和真实 reference distribution 之间的distribution shift。  \n\n## Your Language Model Is Secretly a Reward Model  \n\n在前面推导DPO的loss函数的时候，我们把reward的公式显示表达成  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n但是这里 $Z(x)$ 的组合空间太大，实际上没法求解。  \n\n好在\"在Plackett-Luce/Bradley-Terry模型框架下，同一等价类中的两个reward function有相同的preference distribution\"  \n\n> Under the Plackett-Luce preference framework, and in particular the BradleyTerry framework, two reward functions from the same equivalence class induce the same preference distribution  \n\n如果两个reward function $r(x,y)$ 和 $r^{\\prime}(x,y)$ 可以写成  \n\n$$r'(x,y)=r(x,y)+f(x)$$  \n\n即表示这两个reward function来自同一等价类(equivalence class)。  \n\n对于prompt $x$ 和 answer $y_1,\\ldots,y_K$，以及对应的ranking $\\tau$，在Plackett-Luce framework（Bradley–Terry也是其中一个特例）下的证明如下  \n\n$$\\begin{aligned}\np_{r'}(\\tau|y_1,\\ldots,y_K,x)& =\\prod_{k=1}^K\\frac{\\exp(r'(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r'(x,y_{\\tau(j)}))}  \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}$$  \n\n基于此，我们可以把上面的 $\\beta\\log Z(x)$ 项忽略掉，也就是说下面两个reward function是具有相同的preference distribution的  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\n更进一步地，两个来自同一等价类的reward function在相同的RL问题下会导向相同的optimal policy。  \n\n在推导DPO的loss的部分中，我们得到了optimal policy的显式解  \n\n$$\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n这里证明一下两个reward function可以导向相同的optimal policy。假设$r'(x,y)=r(x,y)+f(x)$，$\\pi_r$ 和 $\\pi_{r'}$ 分别是它们对应的optimal policy，有  \n\n$$\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)& \\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)\\end{aligned}  \\\\\n&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right) \\\\\n&\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned} \\\\\n&\\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned} \\\\\n&=\\pi_r(y|x)\n\\end{aligned}$$  \n\n那么，与Plackett-Luce（特别是Bradley-Terry）模型一致的所有reward类别，都可以被某个模型 $\\pi(y\\mid x)$ 和 一个给定的reference model $\\pi_{ref}(y\\mid x)$ 所表示：  \n\n$$r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}$$  \n\n也就是我们的语言模型都天然具有reward model的功能。  \n\n## 实验  \n\n实际训练中，论文中所使用的超参和设置：  \n- $\\beta=0.1$（对于TL;DR summarization，设为0.5）  \n- batch size = 64  \n- RMSprop optimizer  \n- learning rate = 1e-6  \n- linearly warmup 0 to 1e-6 over 150 steps  \n\n论文在对话、摘要等任务进行的效果评测，主要对比了PPO、SFT和DPO的效果。  \n\nDPO即使在没有精细调参的情况下，也有比价好的效果  \n\n{% asset_img result_1.png 对比1 %}  \n\n{% asset_img result_2.png 对比2 %}  \n\n{% asset_img result_3.png 对比3 %}  \n\n{% asset_img result_4.png 对比4 %}  \n\n# 小结  \n\n- DPO在RLHF PPO相同的优化问题下，推导出了新的优化形式，省去了reward模型的部分，从而可以直接用偏好数据优化模型  \n- DPO在效果和效率上相比PPO都有优势  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】Direct Preference Optimization: Your Language Model is Secretly a Reward Model https://arxiv.org/abs/2305.18290v2  ","source":"_posts/cs/nlp/2024/05/大模型偏好对齐-DPO.md","raw":"---\ntitle: 大模型偏好对齐-DPO\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 强化学习\n  - 微调\n  - SFT\n  - 偏好对齐\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 473f2b43\ndate: 2024-05-26 22:01:48\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n要对齐大模型偏好并不容易，从预训练的数据内容、模型的结构到SFT数据配比甚至数据格式等都会影响最终结果。  \n\n按ChatGPT的技术路线，用SFT+RLHF PPO强化学习确实可以获得一定的提升，但是PPO比较复杂，训练过程不稳定，对微调后的模型、PPO的超参、reward模型的质量等都很敏感，且数据收集和训练的成本都较高，跑通大规模PPO有一定的成本门槛，因此PPO并没有被很广泛地应用。  \n\n而DPO，Direct Preference Optimization，就是PPO的一个简化替代方案。DPO不需要训练reward模型，把PPO的两阶段训练变成一阶段训练，让模型可以直接从偏好数据里学习。  \n\nDPO公式有点多，但是并不算太复杂，一步一步理解即可。  \n\n# 对齐  \n\n大模型在预训练中学到很多知识和技能，但是并不是所有知识和技能都是我们想要的。  \n\n比如有一个常见的错误知识，有超过80%的人会有这样的错误认知，那么这个错误知识在预训练数据里也会经常出现。虽然数据集里也会有关于这个知识的正确认知，但是比例相对会比较低。  \n\n如果让模型直接用在预训练中学到的知识进行回答，那么模型就有可能给出错误的知识。  \n\n这不是我们所希望的。因此需要通过一些方法，让模型给出的结果能对齐人类的偏好，比如最基础的偏好，正确性。  \n\n从模型非常广泛的知识和技能中选出我们所需的response和action是构建安全、高效、可控的AI系统的关键。  \n\nSFT是最直接的偏好学习方法，而RLHF/RLAIF是上限更高的偏好对齐方案。但RLHF比较复杂，训练不稳定，成本也高。  \n\n而DPO的优化目标和RLHF一样，但是实现更简单。  \n\n# RLHF\n\n先回顾下RLHF的三个阶段。  \n\n1. SFT Phase  \n\n基于预训练模型，在高质量的下游任务数据上训练，获得 $\\pi^{\\mathrm{SFT}}$。  \n\n2. Reward Modelling Phase  \n\n首先给定prompt $x$，生成两个答案 $(y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)$，并通过人工标注对比 $y_1,y_2$，获得偏好结果(preference) $y_w\\succ y_l\\mid x$，其中w和l表示win和lose。  \n\n假设在这些偏好结果中，有一个我们无法直接访问的latent reward model $r^*(y,x)$，对每对 $(x,y)$ 进行打分，这个 $r^*(y,x)$ 就是RLHF里reward model的拟合目标。  \n\n基于 $r^*(y,x)$，有很多方法对preference进行建模，Bradley-Terry model就是一个常用的选择。（当然在多个ranked answers的情况下，可以使用Plackett-Luce ranking models）  \n\n基于Bradley-Terry model，人类偏好的分布 $p^{*}$ 写作  \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\n看起来不复杂，就是把两个答案的reward通过softmax归一化成概率。  \n\n假设我们从 $p^{*}$ 采样到一个静态的偏好对比数据集 $\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N$ ，那我们就可以用基于 $\\pi^{\\mathrm{SFT}}$ 初始化得到的reward模型 $r_\\phi(x,y)$，通过maximum likelihood来拟合 $r^*(y,x)$。将这个问题表述为二元分类问题，我们就得到negative log-likelihood loss：  \n\n$$\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}$$  \n\n为了确保reward function有较低的方差，一般会对reward进行归一化，使得对于所有的 $x$，有 $\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0$。  \n\n3. RL Fine-Tuning Phase  \n\n在强化学习阶段，我们用上一步中得到的reward给目标模型提供反馈，优化如下目标  \n\n$$\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid x)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}$$  \n\n上式中第一项是reward模型对目标模型（即RLHF中的actor model）给出的答案的reward打分，这一项是越高越好。  \n\n而第二项是目标模型和参考模型之间的KL散度，用来限制经过训练后的目标模型，不要偏离参考模型（即 $\\pi^{\\mathrm{SFT}}$）太多。这样可以保证reward模型能在经过充分训练的区间工作，同时避免目标模型因过分向高reward分数优化而出现mode-collapse，失去回复的多样性。$\\beta$ 用来控制这个限制项的比重。  \n\n由于语言生成是离散的，因此上面这个优化目标是不可导的，需要通过RL优化。  \n\n标准的RL把reward fucntion构建成  \n\n$$r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid x)-\\log\\pi_\\text{ref}(y\\mid x))$$  \n\n并通过PPO优化。  \n\n# Direct Preference Optimization  \n\nDPO的目标是推导出一种简单的方法，直接使用偏好来进行policy optimization，而省去训练reward模型的训练。  \n\n{% asset_img intro.png DPO %}  \n\n## DPO优化目标的推导  \n\n首先，DPO起始的优化目标和RL是相同的：对于任意的reward function $r(x,y)$，reference model $\\pi_{\\mathrm{ref}}$  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\n由KL散度的定义，把上式中的第二项展开  \n\n$$\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}$$  \n\n这里的条件概率求和其实就是期望值，因此有  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\n$$\\begin{aligned}&=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n然后我们把最大化问题转化成最小化问题  \n\n$$\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n在这里我们用配分函数，归一一下分母。令  \n\n$$Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta r(x,y)\\right)$$  \n\n那我们就得到了一个新的有效的概率分布  \n\n$$\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n那么就有  \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n由于 $Z(x)$ 不是 $y$ 的函数，我们可以把它拿出来  \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log Z(x)\\right]$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log Z(x)\\right]$$  \n\n$Z(x)$ 和 $\\pi$ 无关，因此最小化这个式子只要最小化第一项KL散度。而当且仅当两个分布完全相同的时候，KL散度取得最小值0，因此有  \n\n$$\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n虽然得到了显示解，但是这里的 $Z(x)$ 没法求解，因为排列组合数太多，我们不可能去遍历。  \n\n继续对这个式子做一些变换  \n\n$$\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n$$\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x) +\\frac{1}{\\beta}r(x,y)\n\\end{aligned}$$  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n这里我们开始用上Bradley-Terry model了。前面我们提到了Bradley-Terry model是如下形式  \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\n在这个基础上做一点变换  \n\n$$\\begin{aligned}\np^*(y_1\\succ y_2\\mid x)&=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}$$  \n\n然后我们把 $r$ 代入进去，就得到  \n\n$$p^*(y_1\\succ y_2\\mid x)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}$$  \n\n到这里，我们就有了关于optimal policy的人类偏好数据的概率，而无需经过reward模型。我们可以用MLE直接在这个概率模型上优化目标模型  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\nDPO loss的实现如下  \n\n{% asset_img dpo_loss_code.png DPO实现 %}  \n\n## 理解DPO损失函数  \n\n首先我们了解一下DPO的loss在做什么，对DPO的损失函数求个导。  \n\n方便起见，令  \n\n$$u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}$$  \n\n那么原损失函数可以写成  \n\n$$L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim D}[\\log\\sigma(u)]$$  \n\n对sigmoid求导，有  \n\n$$\\frac\\partial{\\partial u}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)$$  \n\n由sigmoid函数性质，有  \n\n$$1-\\sigma(u)=\\sigma(-u)$$  \n\n对 $u$ 求导  \n\n$$\\frac{\\partial u}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)$$  \n\n第一项对数求导，由于 $\\pi_{\\mathrm{ref}}$ 不依赖 $\\theta$，可以视作常数，因此有  \n\n$$\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}$$  \n\n类似地，第二项求导  \n\n$$\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid x)$$  \n\n因此，DPO损失的导数是  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)–\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n再令  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\n那么DPO损失的梯度可以写作  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)–\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n梯度各项的意义如下  \n\n{% asset_img gradient.png DPO梯度 %}  \n\n$\\hat{r}_\\theta(x,y)$ 相当于 $\\pi_{\\theta}$ 和 $\\pi_{\\mathrm{ref}}$ 共同确定的隐式reward。  \n\n## DPO流程  \n\nDPO的一般流程是：  \n- 对于每个prompt $x$，采样 $y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid x)$，然后进行人工标注构建偏好数据集 $\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N$  \n- 基于 $\\mathcal{L}_{\\mathrm{DPO}}$，在已有的 $\\pi_{\\mathrm{ref}}$、$\\mathcal{D}$ 和 $\\beta$ 上优化 $\\pi\\theta $  \n\n但是收集偏好数据的成本还是比较高的，因此实际使用中，人们更愿意使用开源的偏好数据集。  \n\n当我们的偏好数据是来自 $\\pi^{\\mathrm{SFT}}$ 的时候，我们直接让 $\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}$。如果我们使用开源偏好数据集的话，就可能没法直接使用生成这些数据的模型，这时可以用偏好数据集里 $(x,y_w)$ 数据对 $\\pi_{\\mathrm{ref}}$ 进行微调，即  \n\n$$\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid x)\\right]$$  \n\n这个微调步骤有助于缓解 $\\pi_{\\mathrm{ref}}$ 和真实 reference distribution 之间的distribution shift。  \n\n## Your Language Model Is Secretly a Reward Model  \n\n在前面推导DPO的loss函数的时候，我们把reward的公式显示表达成  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n但是这里 $Z(x)$ 的组合空间太大，实际上没法求解。  \n\n好在\"在Plackett-Luce/Bradley-Terry模型框架下，同一等价类中的两个reward function有相同的preference distribution\"  \n\n> Under the Plackett-Luce preference framework, and in particular the BradleyTerry framework, two reward functions from the same equivalence class induce the same preference distribution  \n\n如果两个reward function $r(x,y)$ 和 $r^{\\prime}(x,y)$ 可以写成  \n\n$$r'(x,y)=r(x,y)+f(x)$$  \n\n即表示这两个reward function来自同一等价类(equivalence class)。  \n\n对于prompt $x$ 和 answer $y_1,\\ldots,y_K$，以及对应的ranking $\\tau$，在Plackett-Luce framework（Bradley–Terry也是其中一个特例）下的证明如下  \n\n$$\\begin{aligned}\np_{r'}(\\tau|y_1,\\ldots,y_K,x)& =\\prod_{k=1}^K\\frac{\\exp(r'(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r'(x,y_{\\tau(j)}))}  \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}$$  \n\n基于此，我们可以把上面的 $\\beta\\log Z(x)$ 项忽略掉，也就是说下面两个reward function是具有相同的preference distribution的  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\n更进一步地，两个来自同一等价类的reward function在相同的RL问题下会导向相同的optimal policy。  \n\n在推导DPO的loss的部分中，我们得到了optimal policy的显式解  \n\n$$\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n这里证明一下两个reward function可以导向相同的optimal policy。假设$r'(x,y)=r(x,y)+f(x)$，$\\pi_r$ 和 $\\pi_{r'}$ 分别是它们对应的optimal policy，有  \n\n$$\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)& \\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)\\end{aligned}  \\\\\n&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right) \\\\\n&\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned} \\\\\n&\\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned} \\\\\n&=\\pi_r(y|x)\n\\end{aligned}$$  \n\n那么，与Plackett-Luce（特别是Bradley-Terry）模型一致的所有reward类别，都可以被某个模型 $\\pi(y\\mid x)$ 和 一个给定的reference model $\\pi_{ref}(y\\mid x)$ 所表示：  \n\n$$r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}$$  \n\n也就是我们的语言模型都天然具有reward model的功能。  \n\n## 实验  \n\n实际训练中，论文中所使用的超参和设置：  \n- $\\beta=0.1$（对于TL;DR summarization，设为0.5）  \n- batch size = 64  \n- RMSprop optimizer  \n- learning rate = 1e-6  \n- linearly warmup 0 to 1e-6 over 150 steps  \n\n论文在对话、摘要等任务进行的效果评测，主要对比了PPO、SFT和DPO的效果。  \n\nDPO即使在没有精细调参的情况下，也有比价好的效果  \n\n{% asset_img result_1.png 对比1 %}  \n\n{% asset_img result_2.png 对比2 %}  \n\n{% asset_img result_3.png 对比3 %}  \n\n{% asset_img result_4.png 对比4 %}  \n\n# 小结  \n\n- DPO在RLHF PPO相同的优化问题下，推导出了新的优化形式，省去了reward模型的部分，从而可以直接用偏好数据优化模型  \n- DPO在效果和效率上相比PPO都有优势  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】Direct Preference Optimization: Your Language Model is Secretly a Reward Model https://arxiv.org/abs/2305.18290v2  ","slug":"cs/nlp/2024/05/大模型偏好对齐-DPO","published":1,"updated":"2024-05-29T12:33:13.225Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9x000r314k99vv31xz","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>要对齐大模型偏好并不容易，从预训练的数据内容、模型的结构到SFT数据配比甚至数据格式等都会影响最终结果。</p>\n<p>按ChatGPT的技术路线，用SFT+RLHF\nPPO强化学习确实可以获得一定的提升，但是PPO比较复杂，训练过程不稳定，对微调后的模型、PPO的超参、reward模型的质量等都很敏感，且数据收集和训练的成本都较高，跑通大规模PPO有一定的成本门槛，因此PPO并没有被很广泛地应用。</p>\n<p>而DPO，Direct Preference\nOptimization，就是PPO的一个简化替代方案。DPO不需要训练reward模型，把PPO的两阶段训练变成一阶段训练，让模型可以直接从偏好数据里学习。</p>\n<p>DPO公式有点多，但是并不算太复杂，一步一步理解即可。</p>\n<h1 id=\"对齐\">对齐</h1>\n<p>大模型在预训练中学到很多知识和技能，但是并不是所有知识和技能都是我们想要的。</p>\n<p>比如有一个常见的错误知识，有超过80%的人会有这样的错误认知，那么这个错误知识在预训练数据里也会经常出现。虽然数据集里也会有关于这个知识的正确认知，但是比例相对会比较低。</p>\n<p>如果让模型直接用在预训练中学到的知识进行回答，那么模型就有可能给出错误的知识。</p>\n<p>这不是我们所希望的。因此需要通过一些方法，让模型给出的结果能对齐人类的偏好，比如最基础的偏好，正确性。</p>\n<p>从模型非常广泛的知识和技能中选出我们所需的response和action是构建安全、高效、可控的AI系统的关键。</p>\n<p>SFT是最直接的偏好学习方法，而RLHF/RLAIF是上限更高的偏好对齐方案。但RLHF比较复杂，训练不稳定，成本也高。</p>\n<p>而DPO的优化目标和RLHF一样，但是实现更简单。</p>\n<h1 id=\"rlhf\">RLHF</h1>\n<p>先回顾下RLHF的三个阶段。</p>\n<ol type=\"1\">\n<li>SFT Phase</li>\n</ol>\n<p>基于预训练模型，在高质量的下游任务数据上训练，获得 <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span>。</p>\n<ol start=\"2\" type=\"1\">\n<li>Reward Modelling Phase</li>\n</ol>\n<p>首先给定prompt <span class=\"math inline\">\\(x\\)</span>，生成两个答案\n<span class=\"math inline\">\\((y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)\\)</span>，并通过人工标注对比\n<span class=\"math inline\">\\(y_1,y_2\\)</span>，获得偏好结果(preference)\n<span class=\"math inline\">\\(y_w\\succ y_l\\mid\nx\\)</span>，其中w和l表示win和lose。</p>\n<p>假设在这些偏好结果中，有一个我们无法直接访问的latent reward model\n<span class=\"math inline\">\\(r^*(y,x)\\)</span>，对每对 <span class=\"math inline\">\\((x,y)\\)</span> 进行打分，这个 <span class=\"math inline\">\\(r^*(y,x)\\)</span> 就是RLHF里reward\nmodel的拟合目标。</p>\n<p>基于 <span class=\"math inline\">\\(r^*(y,x)\\)</span>，有很多方法对preference进行建模，Bradley-Terry\nmodel就是一个常用的选择。（当然在多个ranked\nanswers的情况下，可以使用Plackett-Luce ranking models）</p>\n<p>基于Bradley-Terry model，人类偏好的分布 <span class=\"math inline\">\\(p^{*}\\)</span> 写作</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p>看起来不复杂，就是把两个答案的reward通过softmax归一化成概率。</p>\n<p>假设我们从 <span class=\"math inline\">\\(p^{*}\\)</span>\n采样到一个静态的偏好对比数据集 <span class=\"math inline\">\\(\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N\\)</span>\n，那我们就可以用基于 <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> 初始化得到的reward模型\n<span class=\"math inline\">\\(r_\\phi(x,y)\\)</span>，通过maximum\nlikelihood来拟合 <span class=\"math inline\">\\(r^*(y,x)\\)</span>。将这个问题表述为二元分类问题，我们就得到negative\nlog-likelihood loss：</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}\\]</span></p>\n<p>为了确保reward\nfunction有较低的方差，一般会对reward进行归一化，使得对于所有的 <span class=\"math inline\">\\(x\\)</span>，有 <span class=\"math inline\">\\(\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0\\)</span>。</p>\n<ol start=\"3\" type=\"1\">\n<li>RL Fine-Tuning Phase</li>\n</ol>\n<p>在强化学习阶段，我们用上一步中得到的reward给目标模型提供反馈，优化如下目标</p>\n<p><span class=\"math display\">\\[\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid\nx)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}\\]</span></p>\n<p>上式中第一项是reward模型对目标模型（即RLHF中的actor\nmodel）给出的答案的reward打分，这一项是越高越好。</p>\n<p>而第二项是目标模型和参考模型之间的KL散度，用来限制经过训练后的目标模型，不要偏离参考模型（即\n<span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span>）太多。这样可以保证reward模型能在经过充分训练的区间工作，同时避免目标模型因过分向高reward分数优化而出现mode-collapse，失去回复的多样性。<span class=\"math inline\">\\(\\beta\\)</span> 用来控制这个限制项的比重。</p>\n<p>由于语言生成是离散的，因此上面这个优化目标是不可导的，需要通过RL优化。</p>\n<p>标准的RL把reward fucntion构建成</p>\n<p><span class=\"math display\">\\[r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid\nx)-\\log\\pi_\\text{ref}(y\\mid x))\\]</span></p>\n<p>并通过PPO优化。</p>\n<h1 id=\"direct-preference-optimization\">Direct Preference\nOptimization</h1>\n<p>DPO的目标是推导出一种简单的方法，直接使用偏好来进行policy\noptimization，而省去训练reward模型的训练。</p>\n<img src=\"/473f2b43/intro.png\" class title=\"DPO\">\n<h2 id=\"dpo优化目标的推导\">DPO优化目标的推导</h2>\n<p>首先，DPO起始的优化目标和RL是相同的：对于任意的reward function <span class=\"math inline\">\\(r(x,y)\\)</span>，reference model <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span></p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p>由KL散度的定义，把上式中的第二项展开</p>\n<p><span class=\"math display\">\\[\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\]</span></p>\n<p>这里的条件概率求和其实就是期望值，因此有</p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&amp;\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p>然后我们把最大化问题转化成最小化问题</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p>在这里我们用配分函数，归一一下分母。令</p>\n<p><span class=\"math display\">\\[Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta\nr(x,y)\\right)\\]</span></p>\n<p>那我们就得到了一个新的有效的概率分布</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p>那么就有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p>由于 <span class=\"math inline\">\\(Z(x)\\)</span> 不是 <span class=\"math inline\">\\(y\\)</span> 的函数，我们可以把它拿出来</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math inline\">\\(Z(x)\\)</span> 和 <span class=\"math inline\">\\(\\pi\\)</span>\n无关，因此最小化这个式子只要最小化第一项KL散度。而当且仅当两个分布完全相同的时候，KL散度取得最小值0，因此有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p>虽然得到了显示解，但是这里的 <span class=\"math inline\">\\(Z(x)\\)</span>\n没法求解，因为排列组合数太多，我们不可能去遍历。</p>\n<p>继续对这个式子做一些变换</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x)\n+\\frac{1}{\\beta}r(x,y)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>这里我们开始用上Bradley-Terry model了。前面我们提到了Bradley-Terry\nmodel是如下形式</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p>在这个基础上做一点变换</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np^*(y_1\\succ y_2\\mid\nx)&amp;=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&amp;=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&amp;=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}\\]</span></p>\n<p>然后我们把 <span class=\"math inline\">\\(r\\)</span>\n代入进去，就得到</p>\n<p><span class=\"math display\">\\[p^*(y_1\\succ y_2\\mid\nx)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}\\]</span></p>\n<p>到这里，我们就有了关于optimal\npolicy的人类偏好数据的概率，而无需经过reward模型。我们可以用MLE直接在这个概率模型上优化目标模型</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>DPO loss的实现如下</p>\n<img src=\"/473f2b43/dpo_loss_code.png\" class title=\"DPO实现\">\n<h2 id=\"理解dpo损失函数\">理解DPO损失函数</h2>\n<p>首先我们了解一下DPO的loss在做什么，对DPO的损失函数求个导。</p>\n<p>方便起见，令</p>\n<p><span class=\"math display\">\\[u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}\\]</span></p>\n<p>那么原损失函数可以写成</p>\n<p><span class=\"math display\">\\[L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim\nD}[\\log\\sigma(u)]\\]</span></p>\n<p>对sigmoid求导，有</p>\n<p><span class=\"math display\">\\[\\frac\\partial{\\partial\nu}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)\\]</span></p>\n<p>由sigmoid函数性质，有</p>\n<p><span class=\"math display\">\\[1-\\sigma(u)=\\sigma(-u)\\]</span></p>\n<p>对 <span class=\"math inline\">\\(u\\)</span> 求导</p>\n<p><span class=\"math display\">\\[\\frac{\\partial\nu}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)\\]</span></p>\n<p>第一项对数求导，由于 <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span> 不依赖 <span class=\"math inline\">\\(\\theta\\)</span>，可以视作常数，因此有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&amp;\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&amp;\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&amp;\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}\\]</span></p>\n<p>类似地，第二项求导</p>\n<p><span class=\"math display\">\\[\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid\nx)\\]</span></p>\n<p>因此，DPO损失的导数是</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)–\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p>再令</p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>那么DPO损失的梯度可以写作</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)–\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p>梯度各项的意义如下</p>\n<img src=\"/473f2b43/gradient.png\" class title=\"DPO梯度\">\n<p><span class=\"math inline\">\\(\\hat{r}_\\theta(x,y)\\)</span> 相当于 <span class=\"math inline\">\\(\\pi_{\\theta}\\)</span> 和 <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>\n共同确定的隐式reward。</p>\n<h2 id=\"dpo流程\">DPO流程</h2>\n<p>DPO的一般流程是：<br>\n- 对于每个prompt <span class=\"math inline\">\\(x\\)</span>，采样 <span class=\"math inline\">\\(y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid\nx)\\)</span>，然后进行人工标注构建偏好数据集 <span class=\"math inline\">\\(\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N\\)</span><br>\n- 基于 <span class=\"math inline\">\\(\\mathcal{L}_{\\mathrm{DPO}}\\)</span>，在已有的\n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>、<span class=\"math inline\">\\(\\mathcal{D}\\)</span> 和 <span class=\"math inline\">\\(\\beta\\)</span> 上优化 $$</p>\n<p>但是收集偏好数据的成本还是比较高的，因此实际使用中，人们更愿意使用开源的偏好数据集。</p>\n<p>当我们的偏好数据是来自 <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> 的时候，我们直接让\n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}\\)</span>。如果我们使用开源偏好数据集的话，就可能没法直接使用生成这些数据的模型，这时可以用偏好数据集里\n<span class=\"math inline\">\\((x,y_w)\\)</span> 数据对 <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span> 进行微调，即</p>\n<p><span class=\"math display\">\\[\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid\nx)\\right]\\]</span></p>\n<p>这个微调步骤有助于缓解 <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span> 和真实 reference\ndistribution 之间的distribution shift。</p>\n<h2 id=\"your-language-model-is-secretly-a-reward-model\">Your Language\nModel Is Secretly a Reward Model</h2>\n<p>在前面推导DPO的loss函数的时候，我们把reward的公式显示表达成</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>但是这里 <span class=\"math inline\">\\(Z(x)\\)</span>\n的组合空间太大，实际上没法求解。</p>\n<p>好在\"在Plackett-Luce/Bradley-Terry模型框架下，同一等价类中的两个reward\nfunction有相同的preference distribution\"</p>\n<blockquote>\n<p>Under the Plackett-Luce preference framework, and in particular the\nBradleyTerry framework, two reward functions from the same equivalence\nclass induce the same preference distribution</p>\n</blockquote>\n<p>如果两个reward function <span class=\"math inline\">\\(r(x,y)\\)</span>\n和 <span class=\"math inline\">\\(r^{\\prime}(x,y)\\)</span> 可以写成</p>\n<p><span class=\"math display\">\\[r&#39;(x,y)=r(x,y)+f(x)\\]</span></p>\n<p>即表示这两个reward function来自同一等价类(equivalence class)。</p>\n<p>对于prompt <span class=\"math inline\">\\(x\\)</span> 和 answer <span class=\"math inline\">\\(y_1,\\ldots,y_K\\)</span>，以及对应的ranking <span class=\"math inline\">\\(\\tau\\)</span>，在Plackett-Luce\nframework（Bradley–Terry也是其中一个特例）下的证明如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np_{r&#39;}(\\tau|y_1,\\ldots,y_K,x)&amp;\n=\\prod_{k=1}^K\\frac{\\exp(r&#39;(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r&#39;(x,y_{\\tau(j)}))}  \\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}\\]</span></p>\n<p>基于此，我们可以把上面的 <span class=\"math inline\">\\(\\beta\\log\nZ(x)\\)</span> 项忽略掉，也就是说下面两个reward\nfunction是具有相同的preference distribution的</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>更进一步地，两个来自同一等价类的reward\nfunction在相同的RL问题下会导向相同的optimal policy。</p>\n<p>在推导DPO的loss的部分中，我们得到了optimal policy的显式解</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p>这里证明一下两个reward function可以导向相同的optimal\npolicy。假设<span class=\"math inline\">\\(r&#39;(x,y)=r(x,y)+f(x)\\)</span>，<span class=\"math inline\">\\(\\pi_r\\)</span> 和 <span class=\"math inline\">\\(\\pi_{r&#39;}\\)</span> 分别是它们对应的optimal\npolicy，有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)&amp;\n\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)\\end{aligned}  \\\\\n&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)\n\\\\\n&amp;\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned}\n\\\\\n&amp;\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\n\\\\\n&amp;=\\pi_r(y|x)\n\\end{aligned}\\]</span></p>\n<p>那么，与Plackett-Luce（特别是Bradley-Terry）模型一致的所有reward类别，都可以被某个模型\n<span class=\"math inline\">\\(\\pi(y\\mid x)\\)</span> 和 一个给定的reference\nmodel <span class=\"math inline\">\\(\\pi_{ref}(y\\mid x)\\)</span>\n所表示：</p>\n<p><span class=\"math display\">\\[r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\]</span></p>\n<p>也就是我们的语言模型都天然具有reward model的功能。</p>\n<h2 id=\"实验\">实验</h2>\n<p>实际训练中，论文中所使用的超参和设置：<br>\n- <span class=\"math inline\">\\(\\beta=0.1\\)</span>（对于TL;DR\nsummarization，设为0.5）<br>\n- batch size = 64<br>\n- RMSprop optimizer<br>\n- learning rate = 1e-6<br>\n- linearly warmup 0 to 1e-6 over 150 steps</p>\n<p>论文在对话、摘要等任务进行的效果评测，主要对比了PPO、SFT和DPO的效果。</p>\n<p>DPO即使在没有精细调参的情况下，也有比价好的效果</p>\n<img src=\"/473f2b43/result_1.png\" class title=\"对比1\">\n<img src=\"/473f2b43/result_2.png\" class title=\"对比2\">\n<img src=\"/473f2b43/result_3.png\" class title=\"对比3\">\n<img src=\"/473f2b43/result_4.png\" class title=\"对比4\">\n<h1 id=\"小结\">小结</h1>\n<ul>\n<li>DPO在RLHF\nPPO相同的优化问题下，推导出了新的优化形式，省去了reward模型的部分，从而可以直接用偏好数据优化模型<br>\n</li>\n<li>DPO在效果和效率上相比PPO都有优势</li>\n</ul>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Direct Preference Optimization: Your Language Model is Secretly\na Reward Model https://arxiv.org/abs/2305.18290v2</p>\n","length":13170,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>要对齐大模型偏好并不容易，从预训练的数据内容、模型的结构到SFT数据配比甚至数据格式等都会影响最终结果。</p>\n<p>按ChatGPT的技术路线，用SFT+RLHF\nPPO强化学习确实可以获得一定的提升，但是PPO比较复杂，训练过程不稳定，对微调后的模型、PPO的超参、reward模型的质量等都很敏感，且数据收集和训练的成本都较高，跑通大规模PPO有一定的成本门槛，因此PPO并没有被很广泛地应用。</p>\n<p>而DPO，Direct Preference\nOptimization，就是PPO的一个简化替代方案。DPO不需要训练reward模型，把PPO的两阶段训练变成一阶段训练，让模型可以直接从偏好数据里学习。</p>\n<p>DPO公式有点多，但是并不算太复杂，一步一步理解即可。</p>\n<h1 id=\"对齐\">对齐</h1>\n<p>大模型在预训练中学到很多知识和技能，但是并不是所有知识和技能都是我们想要的。</p>\n<p>比如有一个常见的错误知识，有超过80%的人会有这样的错误认知，那么这个错误知识在预训练数据里也会经常出现。虽然数据集里也会有关于这个知识的正确认知，但是比例相对会比较低。</p>\n<p>如果让模型直接用在预训练中学到的知识进行回答，那么模型就有可能给出错误的知识。</p>\n<p>这不是我们所希望的。因此需要通过一些方法，让模型给出的结果能对齐人类的偏好，比如最基础的偏好，正确性。</p>\n<p>从模型非常广泛的知识和技能中选出我们所需的response和action是构建安全、高效、可控的AI系统的关键。</p>\n<p>SFT是最直接的偏好学习方法，而RLHF/RLAIF是上限更高的偏好对齐方案。但RLHF比较复杂，训练不稳定，成本也高。</p>\n<p>而DPO的优化目标和RLHF一样，但是实现更简单。</p>\n<h1 id=\"rlhf\">RLHF</h1>\n<p>先回顾下RLHF的三个阶段。</p>\n<ol type=\"1\">\n<li>SFT Phase</li>\n</ol>\n<p>基于预训练模型，在高质量的下游任务数据上训练，获得 <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span>。</p>\n<ol start=\"2\" type=\"1\">\n<li>Reward Modelling Phase</li>\n</ol>\n<p>首先给定prompt <span class=\"math inline\">\\(x\\)</span>，生成两个答案\n<span class=\"math inline\">\\((y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)\\)</span>，并通过人工标注对比\n<span class=\"math inline\">\\(y_1,y_2\\)</span>，获得偏好结果(preference)\n<span class=\"math inline\">\\(y_w\\succ y_l\\mid\nx\\)</span>，其中w和l表示win和lose。</p>\n<p>假设在这些偏好结果中，有一个我们无法直接访问的latent reward model\n<span class=\"math inline\">\\(r^*(y,x)\\)</span>，对每对 <span class=\"math inline\">\\((x,y)\\)</span> 进行打分，这个 <span class=\"math inline\">\\(r^*(y,x)\\)</span> 就是RLHF里reward\nmodel的拟合目标。</p>\n<p>基于 <span class=\"math inline\">\\(r^*(y,x)\\)</span>，有很多方法对preference进行建模，Bradley-Terry\nmodel就是一个常用的选择。（当然在多个ranked\nanswers的情况下，可以使用Plackett-Luce ranking models）</p>\n<p>基于Bradley-Terry model，人类偏好的分布 <span class=\"math inline\">\\(p^{*}\\)</span> 写作</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p>看起来不复杂，就是把两个答案的reward通过softmax归一化成概率。</p>\n<p>假设我们从 <span class=\"math inline\">\\(p^{*}\\)</span>\n采样到一个静态的偏好对比数据集 <span class=\"math inline\">\\(\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N\\)</span>\n，那我们就可以用基于 <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> 初始化得到的reward模型\n<span class=\"math inline\">\\(r_\\phi(x,y)\\)</span>，通过maximum\nlikelihood来拟合 <span class=\"math inline\">\\(r^*(y,x)\\)</span>。将这个问题表述为二元分类问题，我们就得到negative\nlog-likelihood loss：</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}\\]</span></p>\n<p>为了确保reward\nfunction有较低的方差，一般会对reward进行归一化，使得对于所有的 <span class=\"math inline\">\\(x\\)</span>，有 <span class=\"math inline\">\\(\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0\\)</span>。</p>\n<ol start=\"3\" type=\"1\">\n<li>RL Fine-Tuning Phase</li>\n</ol>\n<p>在强化学习阶段，我们用上一步中得到的reward给目标模型提供反馈，优化如下目标</p>\n<p><span class=\"math display\">\\[\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid\nx)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}\\]</span></p>\n<p>上式中第一项是reward模型对目标模型（即RLHF中的actor\nmodel）给出的答案的reward打分，这一项是越高越好。</p>\n<p>而第二项是目标模型和参考模型之间的KL散度，用来限制经过训练后的目标模型，不要偏离参考模型（即\n<span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span>）太多。这样可以保证reward模型能在经过充分训练的区间工作，同时避免目标模型因过分向高reward分数优化而出现mode-collapse，失去回复的多样性。<span class=\"math inline\">\\(\\beta\\)</span> 用来控制这个限制项的比重。</p>\n<p>由于语言生成是离散的，因此上面这个优化目标是不可导的，需要通过RL优化。</p>\n<p>标准的RL把reward fucntion构建成</p>\n<p><span class=\"math display\">\\[r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid\nx)-\\log\\pi_\\text{ref}(y\\mid x))\\]</span></p>\n<p>并通过PPO优化。</p>\n<h1 id=\"direct-preference-optimization\">Direct Preference\nOptimization</h1>\n<p>DPO的目标是推导出一种简单的方法，直接使用偏好来进行policy\noptimization，而省去训练reward模型的训练。</p>\n<img src=\"/473f2b43/intro.png\" class title=\"DPO\">\n<h2 id=\"dpo优化目标的推导\">DPO优化目标的推导</h2>\n<p>首先，DPO起始的优化目标和RL是相同的：对于任意的reward function <span class=\"math inline\">\\(r(x,y)\\)</span>，reference model <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span></p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p>由KL散度的定义，把上式中的第二项展开</p>\n<p><span class=\"math display\">\\[\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\]</span></p>\n<p>这里的条件概率求和其实就是期望值，因此有</p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&amp;\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p>然后我们把最大化问题转化成最小化问题</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p>在这里我们用配分函数，归一一下分母。令</p>\n<p><span class=\"math display\">\\[Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta\nr(x,y)\\right)\\]</span></p>\n<p>那我们就得到了一个新的有效的概率分布</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p>那么就有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p>由于 <span class=\"math inline\">\\(Z(x)\\)</span> 不是 <span class=\"math inline\">\\(y\\)</span> 的函数，我们可以把它拿出来</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math inline\">\\(Z(x)\\)</span> 和 <span class=\"math inline\">\\(\\pi\\)</span>\n无关，因此最小化这个式子只要最小化第一项KL散度。而当且仅当两个分布完全相同的时候，KL散度取得最小值0，因此有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p>虽然得到了显示解，但是这里的 <span class=\"math inline\">\\(Z(x)\\)</span>\n没法求解，因为排列组合数太多，我们不可能去遍历。</p>\n<p>继续对这个式子做一些变换</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x)\n+\\frac{1}{\\beta}r(x,y)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>这里我们开始用上Bradley-Terry model了。前面我们提到了Bradley-Terry\nmodel是如下形式</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p>在这个基础上做一点变换</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np^*(y_1\\succ y_2\\mid\nx)&amp;=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&amp;=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&amp;=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}\\]</span></p>\n<p>然后我们把 <span class=\"math inline\">\\(r\\)</span>\n代入进去，就得到</p>\n<p><span class=\"math display\">\\[p^*(y_1\\succ y_2\\mid\nx)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}\\]</span></p>\n<p>到这里，我们就有了关于optimal\npolicy的人类偏好数据的概率，而无需经过reward模型。我们可以用MLE直接在这个概率模型上优化目标模型</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>DPO loss的实现如下</p>\n<img src=\"/473f2b43/dpo_loss_code.png\" class title=\"DPO实现\">\n<h2 id=\"理解dpo损失函数\">理解DPO损失函数</h2>\n<p>首先我们了解一下DPO的loss在做什么，对DPO的损失函数求个导。</p>\n<p>方便起见，令</p>\n<p><span class=\"math display\">\\[u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}\\]</span></p>\n<p>那么原损失函数可以写成</p>\n<p><span class=\"math display\">\\[L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim\nD}[\\log\\sigma(u)]\\]</span></p>\n<p>对sigmoid求导，有</p>\n<p><span class=\"math display\">\\[\\frac\\partial{\\partial\nu}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)\\]</span></p>\n<p>由sigmoid函数性质，有</p>\n<p><span class=\"math display\">\\[1-\\sigma(u)=\\sigma(-u)\\]</span></p>\n<p>对 <span class=\"math inline\">\\(u\\)</span> 求导</p>\n<p><span class=\"math display\">\\[\\frac{\\partial\nu}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)\\]</span></p>\n<p>第一项对数求导，由于 <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span> 不依赖 <span class=\"math inline\">\\(\\theta\\)</span>，可以视作常数，因此有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&amp;\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&amp;\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&amp;\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}\\]</span></p>\n<p>类似地，第二项求导</p>\n<p><span class=\"math display\">\\[\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid\nx)\\]</span></p>\n<p>因此，DPO损失的导数是</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)–\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p>再令</p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>那么DPO损失的梯度可以写作</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)–\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p>梯度各项的意义如下</p>\n<img src=\"/473f2b43/gradient.png\" class title=\"DPO梯度\">\n<p><span class=\"math inline\">\\(\\hat{r}_\\theta(x,y)\\)</span> 相当于 <span class=\"math inline\">\\(\\pi_{\\theta}\\)</span> 和 <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>\n共同确定的隐式reward。</p>\n<h2 id=\"dpo流程\">DPO流程</h2>\n<p>DPO的一般流程是：<br>\n- 对于每个prompt <span class=\"math inline\">\\(x\\)</span>，采样 <span class=\"math inline\">\\(y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid\nx)\\)</span>，然后进行人工标注构建偏好数据集 <span class=\"math inline\">\\(\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N\\)</span><br>\n- 基于 <span class=\"math inline\">\\(\\mathcal{L}_{\\mathrm{DPO}}\\)</span>，在已有的\n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>、<span class=\"math inline\">\\(\\mathcal{D}\\)</span> 和 <span class=\"math inline\">\\(\\beta\\)</span> 上优化 $$</p>\n<p>但是收集偏好数据的成本还是比较高的，因此实际使用中，人们更愿意使用开源的偏好数据集。</p>\n<p>当我们的偏好数据是来自 <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> 的时候，我们直接让\n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}\\)</span>。如果我们使用开源偏好数据集的话，就可能没法直接使用生成这些数据的模型，这时可以用偏好数据集里\n<span class=\"math inline\">\\((x,y_w)\\)</span> 数据对 <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span> 进行微调，即</p>\n<p><span class=\"math display\">\\[\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid\nx)\\right]\\]</span></p>\n<p>这个微调步骤有助于缓解 <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span> 和真实 reference\ndistribution 之间的distribution shift。</p>\n<h2 id=\"your-language-model-is-secretly-a-reward-model\">Your Language\nModel Is Secretly a Reward Model</h2>\n<p>在前面推导DPO的loss函数的时候，我们把reward的公式显示表达成</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>但是这里 <span class=\"math inline\">\\(Z(x)\\)</span>\n的组合空间太大，实际上没法求解。</p>\n<p>好在\"在Plackett-Luce/Bradley-Terry模型框架下，同一等价类中的两个reward\nfunction有相同的preference distribution\"</p>\n<blockquote>\n<p>Under the Plackett-Luce preference framework, and in particular the\nBradleyTerry framework, two reward functions from the same equivalence\nclass induce the same preference distribution</p>\n</blockquote>\n<p>如果两个reward function <span class=\"math inline\">\\(r(x,y)\\)</span>\n和 <span class=\"math inline\">\\(r^{\\prime}(x,y)\\)</span> 可以写成</p>\n<p><span class=\"math display\">\\[r&#39;(x,y)=r(x,y)+f(x)\\]</span></p>\n<p>即表示这两个reward function来自同一等价类(equivalence class)。</p>\n<p>对于prompt <span class=\"math inline\">\\(x\\)</span> 和 answer <span class=\"math inline\">\\(y_1,\\ldots,y_K\\)</span>，以及对应的ranking <span class=\"math inline\">\\(\\tau\\)</span>，在Plackett-Luce\nframework（Bradley–Terry也是其中一个特例）下的证明如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np_{r&#39;}(\\tau|y_1,\\ldots,y_K,x)&amp;\n=\\prod_{k=1}^K\\frac{\\exp(r&#39;(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r&#39;(x,y_{\\tau(j)}))}  \\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}\\]</span></p>\n<p>基于此，我们可以把上面的 <span class=\"math inline\">\\(\\beta\\log\nZ(x)\\)</span> 项忽略掉，也就是说下面两个reward\nfunction是具有相同的preference distribution的</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>更进一步地，两个来自同一等价类的reward\nfunction在相同的RL问题下会导向相同的optimal policy。</p>\n<p>在推导DPO的loss的部分中，我们得到了optimal policy的显式解</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p>这里证明一下两个reward function可以导向相同的optimal\npolicy。假设<span class=\"math inline\">\\(r&#39;(x,y)=r(x,y)+f(x)\\)</span>，<span class=\"math inline\">\\(\\pi_r\\)</span> 和 <span class=\"math inline\">\\(\\pi_{r&#39;}\\)</span> 分别是它们对应的optimal\npolicy，有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)&amp;\n\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)\\end{aligned}  \\\\\n&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)\n\\\\\n&amp;\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned}\n\\\\\n&amp;\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\n\\\\\n&amp;=\\pi_r(y|x)\n\\end{aligned}\\]</span></p>\n<p>那么，与Plackett-Luce（特别是Bradley-Terry）模型一致的所有reward类别，都可以被某个模型\n<span class=\"math inline\">\\(\\pi(y\\mid x)\\)</span> 和 一个给定的reference\nmodel <span class=\"math inline\">\\(\\pi_{ref}(y\\mid x)\\)</span>\n所表示：</p>\n<p><span class=\"math display\">\\[r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\]</span></p>\n<p>也就是我们的语言模型都天然具有reward model的功能。</p>\n<h2 id=\"实验\">实验</h2>\n<p>实际训练中，论文中所使用的超参和设置：<br>\n- <span class=\"math inline\">\\(\\beta=0.1\\)</span>（对于TL;DR\nsummarization，设为0.5）<br>\n- batch size = 64<br>\n- RMSprop optimizer<br>\n- learning rate = 1e-6<br>\n- linearly warmup 0 to 1e-6 over 150 steps</p>\n<p>论文在对话、摘要等任务进行的效果评测，主要对比了PPO、SFT和DPO的效果。</p>\n<p>DPO即使在没有精细调参的情况下，也有比价好的效果</p>\n<img src=\"/473f2b43/result_1.png\" class title=\"对比1\">\n<img src=\"/473f2b43/result_2.png\" class title=\"对比2\">\n<img src=\"/473f2b43/result_3.png\" class title=\"对比3\">\n<img src=\"/473f2b43/result_4.png\" class title=\"对比4\">\n<h1 id=\"小结\">小结</h1>\n<ul>\n<li>DPO在RLHF\nPPO相同的优化问题下，推导出了新的优化形式，省去了reward模型的部分，从而可以直接用偏好数据优化模型<br>\n</li>\n<li>DPO在效果和效率上相比PPO都有优势</li>\n</ul>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Direct Preference Optimization: Your Language Model is Secretly\na Reward Model https://arxiv.org/abs/2305.18290v2</p>\n"},{"title":"昆仑万维-SkyworkMoE","abbrlink":"1d5bcd45","date":"2024-06-04T12:51:02.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n之前我们对比较热门的十个MoE工作进行了整理：[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)。  \n\n最近昆仑万维开源了Skywork-MoE，一个总参数量为146B，激活参数量为22B的MoE模型。  \n\nSkywork-MoE技术报告中针对几个实操会遇到的问题做了一些实验，还是挺有借鉴意义的。  \n\n# Skywork-MoE模型  \n\n分析之前，先看下Skywork-MoE的模型设计：  \n- Llama-like architecture  \n- RoPE  \n- RMSNorm  \n- SwiGLU activation function  \n\n其他参数如下表  \n\n{% asset_img structure.png 模型结构 %}  \n\nSkywork-MoE共有146B参数，16个专家，激活参数量为22B。  \n\n训练集群用到了192个NVIDIAHGX-A800节点，共1536个A800-80G显卡。  \n\n训练框架是基于Megatron搭建的，data parallelism开了ZeRO-1优化，训练速度能达到690token/GPU/second，GPU利用率是38%。  \n\n# 训练路线选择  \n\n在当前的情况下，要训练一个MoE模型有两条路线可以选择：  \n- upcycling：用一个dense模型做MoE模型的初始化，进行一定的继续预训练。这样的好处是MoE模型能在一个比较好的初始化点开始训练，直觉上这样的模型应该收敛得相对比较快，成本也比较低。存在的问题是dense模型的选择可能存在一些权衡取舍，且从dense进行初始化可能对最终效果存在负面影响。  \n- from scratch：直接随机初始化一个MoE模型，从零开始训练。这样成本相比upcycling就比较高，但是效果可能比upcycling更好。  \n\n当然还有一种方法是，先从零训一个dense模型，再从这个dense模型训练一个MoE模型。但是后面的实验告诉我们，如果这个dense模型纯粹是为最终的MoE模型服务的话，那这种方法是费力不讨好的。  \n\n要决定是upcycling还是from scratch，需要看现有的dense模型的水平，以及MoE模型的训练预算。首先如果预算根本支持不了MoE模型这个规模的训练，那我们当然只能选择upcycling。只有当预算充足，我们才有机会选择from scratch这条路。而如果没有可用的dense模型，那就只能选择from scratch。  \n\n前面我们从直觉上认为from scratch效果会更好，下面就从实验上来验证这个想法。  \n\n首先，在300B token的数据上训练一个0.3B的dense模型，并分别取100B和300B时的checkpoint作为后续实验的起始点。这两个checkpoint起个名字叫\"checkpoint-100B\"和\"checkpoint-300B\"。  \n\n然后在相同结构下，把dense模型扩成有8个专家的MoE模型，并使用3种不同的初始化策略：from-scratch / checkpoint-100B / checkpoint-300B。  \n\n假设我们现在有两种MoE模型的训练预算，100B和300B（token）。  \n\n对于100B训练预算，对比以下几个模型  \n\n{% asset_img 100B.png 100B %}  \n\n同样地，对于300B预算的情况，训练了init_scratch-decay_300b和init_100b-decay_300b。另外还训练了一个init_300b-3xLR，相比init_300b-const提升了3倍的学习率，用于验证学习率的影响。  \n\n各个模型的训练结果如下图所示  \n\n{% asset_img exp_1.png 实验 %}  \n\n左图：在100B的训练预算下，from scratch已经可以和从dense初始化的MoE模型loss持平，甚至比init_300b-const好。报告认为init_300b-const效果不好有一部分原因是学习率太小了。  \n\n中图：在300B的训练预算下，from scratch模型已经超越所有其他模型。另外学习率最小的模型表现最差。  \n\n右图：把中图几个模型的expert similarity画出来，发现expert similarity越低的模型，表现越好，并且对于upcycling的模型，expert similarity在训练过程中越来越低，对应着模型效果越来越好。而from scratch的模型的expert similarity基本上一直保持为0，这也说明从dense模型初始化会使得专家多样性比较弱，从而使得模型收敛到suboptimal的点。  \n\n据此，报告给出路线选择的经验法则。假设 $C_{\\mathrm{dense}}$ 是dense模型的训练成本，$C_{\\mathrm{MoE}}$ 是MoE模型的训练预算，那么：  \n- 如果 $C_{\\mathrm{MoE}}\\ll C_{\\mathrm{dense}}$，选择upcycling，upcycling能更好利用上dense模型已投入的成本。  \n- 如果 $C_{\\mathrm{MoE}}\\geq2C_{\\mathrm{dense}}$，选择from scratch，能获得更好的效果。  \n\n另外，学习率的影响很大，这个要仔细设置。  \n\n# 模型设计  \n\n模型设计上，Skywork-MoE提出了两个主要的改进：gating logit normalization和adaptive auxiliary loss coefficients。  \n\n## gating logit normalization  \n\n研究人员在训练过程中发现有一个现象，那就是有时gating layer会输出熵很高的分布，也就是分配给各个专家的概率接近平均分布。这样的结果就是MoE层的输出基本上相当于是各个专家的平均值，而不是一个weighted average。  \n\n而出现这种现象说明gating layer没有很好地区分各个专家，无法把相应的输入分配给最合适的专家。  \n\n针对这个问题，Skywork-MoE给出的方法就是在gating layer的softmax之前引入一个normalization step，如下式  \n\n$$\\begin{aligned}&z=Wx+b\\\\&\\tilde{z}=\\lambda\\cdot\\frac{z-\\mu}{\\sigma}\\\\&g=\\operatorname{softmax}(\\tilde{z})\\end{aligned}$$  \n\n其中 $\\lambda$ 是一个超参。  \n\n这样归一化之后我们就得到一个均值为0，而方差受 $\\lambda$ 控制的向量。大的 $\\lambda$ 值会使得softmax之后的分布更显著，更不均匀。这就相当于给softmax加上一个放大器，把原本不显著的差异进行放大。  \n\n为了验证这个设计的有效性，Skywork-MoE在2.5B参数16个专家的MoE模型上，分别使用和不使用gating logit normalization进行了训练。  \n\n两个模型的gating分布差异如下图所示，normalization确实可以增大各个专家分配到的概率的差异。  \n\n{% asset_img gate_dist.png gating distribution %}  \n\n使用了normalization的模型在training loss和token drop rate上都有更好的表现，如下图所示。  \n\n{% asset_img normaization.png gating logit normalization %}  \n\n而统计gating layer输出的分布中的Max1/Max2和Max2/Max3比值也同样说明了各个expert被更有效地区分开了。  \n\n在千亿Skywork-MoE模型的训练中，使用了 $\\lambda=1$。  \n\n## adaptive auxiliary loss coefficients  \n\n一般来说，MoE模型在训练中都会加入一个auxiliary loss，帮助平衡专家的选择分布，提升训练效率，也增强专家的多样性。对于有M个MoE层的模型，最终loss如下式所示。  \n\n$$\\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{ce}}+\\sum_{l=1}^M\\alpha\\mathcal{L}_{\\mathrm{aux}}^{(l)}$$  \n\n每个MoE层都有对应的auxiliary loss。  \n\nSkywork-MoE认为每层的auxiliary loss的系数 $\\alpha$ 不一定要相同，并且随着训练进行，在gating的平衡已经比较好的时候，可以放宽auxiliary loss的限制强度，避免影响模型的最终效果。  \n\n基于这两个想法，Skywork-MoE提出adaptive auxiliary loss coefficients。  \n\n每个MoE层的auxiliary loss有自己的系数，而这个系数和当前这个MoE层的token drop rate联系了起来。大的token drop rate表示gating的分配不平衡，因此要加强auxiliary loss的约束，反之则可以减小约束。  \n\n对于第l个MoE层，在第i个step的时候，auxiliary loss的系数计算如下  \n\n$$\\begin{array}{rcl}\\hat\\alpha_{i+1}^{(l)}&=&f(d_i^{(l)})\\\\\\alpha_{i+1}^{(l)}&=&\\beta\\alpha_i^{(l)}+(1-\\beta)\\hat\\alpha_{i+1}^{(l)}\\end{array}$$  \n\n其中d表示token drop rate，f是一个单调递增函数。$\\alpha$ 会随着训练，通过moving average更新。$\\beta$ 是moving average的权重，是一个超参。  \n\n实际实现中，f设计成：  \n\n$$f(d)=\\left\\{\\begin{array}{ll}\\xi d&\\text{if }d\\leq\\alpha_{\\text{max}}/\\xi\\\\\\alpha_{\\text{max}}&\\text{if }d>\\alpha_{\\text{max}}/\\xi\\end{array}\\right.$$  \n\n$\\xi$ 表示auxiliary loss coefficient对token drop rate的敏感程度。  \n\n最终训练中，各个超参的设置为：  \n- $\\xi=1/5$  \n- $\\alpha_{\\max}=0.01$  \n- $\\beta=0.99$  \n\n# 其他尝试  \n\n报告中还给出了训练中一些其他尝试，虽然没有直接效果，但是也有参考意义。  \n\n## 学习率  \n\nMoE模型由于路由策略的存在，每个专家平均接受到的输入token数比global batch size要小。  \n\n假设共有n个专家，激活专家数为k，那么平均每个专家接受到的输入只有模型输入的k/n。  \n\n而有效batch size的减小意味着更容易引入noise，对此一般的应对方案就是减小learning rate，可以进行linear scaling（$k/n$），或者square root scaling（$\\sqrt{k/n}$）。  \n\n那么减小learning rate是否能提升效果呢？Skywork-MoE用一个1.8B参数，共32个专家，激活专家数为2的模型，按square root scaling，进行了以下3个实验  \n\n{% asset_img lr_exp.png lr实验 %}  \n\n所有模型在训了300B数据之后，lr会降到peak lr的10%，然后会再继续训10B，在这个过程里lr逐渐降为0。  \n\n训练的loss如下图  \n\n{% asset_img lr_result.png lr实验 %}  \n\n虽然在300B的训练量下，减小lr有一点收益，但是随着最后10B的训练，三个模型都收敛到同样的loss。这说明前面的loss差异并不是不可弥补的，更可能只是因为在300B时三个模型的lr decay到不同的绝对值而已。  \n\n这也说明根据专家数量减少MoE模型的训练学习率并没有太大必要。  \n\n## 多样化初始化  \n\n前面提到，用一个dense模型进行初始化，会导致各个专家相似度过高，从而损害MoE模型的效果。那么我们自然想到用多样化的几个dense模型进行MoE的初始化，效果是不是会更好。  \n\nSkywork-MoE对此进行了实验。把原始dense模型分别用不同的100B数据进行训练，从而获得多个dense模型，并用这些多样化的dense模型初始化MoE模型。  \n\n具体来说，基于原始dense模型 $M_{\\mathrm{base}}$，用了中文、英文、代码三个不同的100B数据集进行训练，获得 $M_{\\mathrm{cn}},M_{\\mathrm{en}},M_{\\mathrm{code}}$ 三个dense模型。之后把 $M_{\\mathrm{cn}}$ 复制3份，$M_{\\mathrm{en}}$ 复制3份，$M_{\\mathrm{code}}$ 复制1份，$M_{\\mathrm{base}}$ 复制1份，共同初始化一个有8个专家的MoE模型。  \n\n多样化和无多样化的初始化方法，训练loss对比如下  \n\n{% asset_img diff_dense.png 初始化实验 %}  \n\n可以看到多样化的初始化方法确实有一点收益，不过随着训练进行，差异在逐渐减小。  \n\n经过90B数据的训练之后，二者的loss只有不到0.01的差距。相较于dense模型的多次继续预训练成本，这个收益并不明显，因此Skywork-MoE最终没有采用多样化的初始化方法。  \n\n# 效果  \n\n146B参数的Skywork-MoE是从Skywork-13B初始化而来的。  \n\n训练数据使用了SkyPile中的一部分数据，再加上一批合成数据。  \n\n中文、英文、代码数据的比例为7:2:1。  \n\nSkywork-MoE在和一些主流模型，在一些benchmark上的对比如下  \n\n{% asset_img perf.png 效果 %}  \n\n基本上达到了同归模型比较好的效果。  \n\n# 小结  \n\nSkywork-MoE开源了一个效果不错的MoE模型，同时对于初始化策略的探索也颇有借鉴意义。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】Skywork-MoE: A Deep Dive into Training Techniques for\nMixture-of-Experts Language Models https://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf  \n","source":"_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型.md","raw":"---\ntitle: 昆仑万维-SkyworkMoE\nabbrlink: 1d5bcd45\ndate: 2024-06-04 20:51:02\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n之前我们对比较热门的十个MoE工作进行了整理：[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)。  \n\n最近昆仑万维开源了Skywork-MoE，一个总参数量为146B，激活参数量为22B的MoE模型。  \n\nSkywork-MoE技术报告中针对几个实操会遇到的问题做了一些实验，还是挺有借鉴意义的。  \n\n# Skywork-MoE模型  \n\n分析之前，先看下Skywork-MoE的模型设计：  \n- Llama-like architecture  \n- RoPE  \n- RMSNorm  \n- SwiGLU activation function  \n\n其他参数如下表  \n\n{% asset_img structure.png 模型结构 %}  \n\nSkywork-MoE共有146B参数，16个专家，激活参数量为22B。  \n\n训练集群用到了192个NVIDIAHGX-A800节点，共1536个A800-80G显卡。  \n\n训练框架是基于Megatron搭建的，data parallelism开了ZeRO-1优化，训练速度能达到690token/GPU/second，GPU利用率是38%。  \n\n# 训练路线选择  \n\n在当前的情况下，要训练一个MoE模型有两条路线可以选择：  \n- upcycling：用一个dense模型做MoE模型的初始化，进行一定的继续预训练。这样的好处是MoE模型能在一个比较好的初始化点开始训练，直觉上这样的模型应该收敛得相对比较快，成本也比较低。存在的问题是dense模型的选择可能存在一些权衡取舍，且从dense进行初始化可能对最终效果存在负面影响。  \n- from scratch：直接随机初始化一个MoE模型，从零开始训练。这样成本相比upcycling就比较高，但是效果可能比upcycling更好。  \n\n当然还有一种方法是，先从零训一个dense模型，再从这个dense模型训练一个MoE模型。但是后面的实验告诉我们，如果这个dense模型纯粹是为最终的MoE模型服务的话，那这种方法是费力不讨好的。  \n\n要决定是upcycling还是from scratch，需要看现有的dense模型的水平，以及MoE模型的训练预算。首先如果预算根本支持不了MoE模型这个规模的训练，那我们当然只能选择upcycling。只有当预算充足，我们才有机会选择from scratch这条路。而如果没有可用的dense模型，那就只能选择from scratch。  \n\n前面我们从直觉上认为from scratch效果会更好，下面就从实验上来验证这个想法。  \n\n首先，在300B token的数据上训练一个0.3B的dense模型，并分别取100B和300B时的checkpoint作为后续实验的起始点。这两个checkpoint起个名字叫\"checkpoint-100B\"和\"checkpoint-300B\"。  \n\n然后在相同结构下，把dense模型扩成有8个专家的MoE模型，并使用3种不同的初始化策略：from-scratch / checkpoint-100B / checkpoint-300B。  \n\n假设我们现在有两种MoE模型的训练预算，100B和300B（token）。  \n\n对于100B训练预算，对比以下几个模型  \n\n{% asset_img 100B.png 100B %}  \n\n同样地，对于300B预算的情况，训练了init_scratch-decay_300b和init_100b-decay_300b。另外还训练了一个init_300b-3xLR，相比init_300b-const提升了3倍的学习率，用于验证学习率的影响。  \n\n各个模型的训练结果如下图所示  \n\n{% asset_img exp_1.png 实验 %}  \n\n左图：在100B的训练预算下，from scratch已经可以和从dense初始化的MoE模型loss持平，甚至比init_300b-const好。报告认为init_300b-const效果不好有一部分原因是学习率太小了。  \n\n中图：在300B的训练预算下，from scratch模型已经超越所有其他模型。另外学习率最小的模型表现最差。  \n\n右图：把中图几个模型的expert similarity画出来，发现expert similarity越低的模型，表现越好，并且对于upcycling的模型，expert similarity在训练过程中越来越低，对应着模型效果越来越好。而from scratch的模型的expert similarity基本上一直保持为0，这也说明从dense模型初始化会使得专家多样性比较弱，从而使得模型收敛到suboptimal的点。  \n\n据此，报告给出路线选择的经验法则。假设 $C_{\\mathrm{dense}}$ 是dense模型的训练成本，$C_{\\mathrm{MoE}}$ 是MoE模型的训练预算，那么：  \n- 如果 $C_{\\mathrm{MoE}}\\ll C_{\\mathrm{dense}}$，选择upcycling，upcycling能更好利用上dense模型已投入的成本。  \n- 如果 $C_{\\mathrm{MoE}}\\geq2C_{\\mathrm{dense}}$，选择from scratch，能获得更好的效果。  \n\n另外，学习率的影响很大，这个要仔细设置。  \n\n# 模型设计  \n\n模型设计上，Skywork-MoE提出了两个主要的改进：gating logit normalization和adaptive auxiliary loss coefficients。  \n\n## gating logit normalization  \n\n研究人员在训练过程中发现有一个现象，那就是有时gating layer会输出熵很高的分布，也就是分配给各个专家的概率接近平均分布。这样的结果就是MoE层的输出基本上相当于是各个专家的平均值，而不是一个weighted average。  \n\n而出现这种现象说明gating layer没有很好地区分各个专家，无法把相应的输入分配给最合适的专家。  \n\n针对这个问题，Skywork-MoE给出的方法就是在gating layer的softmax之前引入一个normalization step，如下式  \n\n$$\\begin{aligned}&z=Wx+b\\\\&\\tilde{z}=\\lambda\\cdot\\frac{z-\\mu}{\\sigma}\\\\&g=\\operatorname{softmax}(\\tilde{z})\\end{aligned}$$  \n\n其中 $\\lambda$ 是一个超参。  \n\n这样归一化之后我们就得到一个均值为0，而方差受 $\\lambda$ 控制的向量。大的 $\\lambda$ 值会使得softmax之后的分布更显著，更不均匀。这就相当于给softmax加上一个放大器，把原本不显著的差异进行放大。  \n\n为了验证这个设计的有效性，Skywork-MoE在2.5B参数16个专家的MoE模型上，分别使用和不使用gating logit normalization进行了训练。  \n\n两个模型的gating分布差异如下图所示，normalization确实可以增大各个专家分配到的概率的差异。  \n\n{% asset_img gate_dist.png gating distribution %}  \n\n使用了normalization的模型在training loss和token drop rate上都有更好的表现，如下图所示。  \n\n{% asset_img normaization.png gating logit normalization %}  \n\n而统计gating layer输出的分布中的Max1/Max2和Max2/Max3比值也同样说明了各个expert被更有效地区分开了。  \n\n在千亿Skywork-MoE模型的训练中，使用了 $\\lambda=1$。  \n\n## adaptive auxiliary loss coefficients  \n\n一般来说，MoE模型在训练中都会加入一个auxiliary loss，帮助平衡专家的选择分布，提升训练效率，也增强专家的多样性。对于有M个MoE层的模型，最终loss如下式所示。  \n\n$$\\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{ce}}+\\sum_{l=1}^M\\alpha\\mathcal{L}_{\\mathrm{aux}}^{(l)}$$  \n\n每个MoE层都有对应的auxiliary loss。  \n\nSkywork-MoE认为每层的auxiliary loss的系数 $\\alpha$ 不一定要相同，并且随着训练进行，在gating的平衡已经比较好的时候，可以放宽auxiliary loss的限制强度，避免影响模型的最终效果。  \n\n基于这两个想法，Skywork-MoE提出adaptive auxiliary loss coefficients。  \n\n每个MoE层的auxiliary loss有自己的系数，而这个系数和当前这个MoE层的token drop rate联系了起来。大的token drop rate表示gating的分配不平衡，因此要加强auxiliary loss的约束，反之则可以减小约束。  \n\n对于第l个MoE层，在第i个step的时候，auxiliary loss的系数计算如下  \n\n$$\\begin{array}{rcl}\\hat\\alpha_{i+1}^{(l)}&=&f(d_i^{(l)})\\\\\\alpha_{i+1}^{(l)}&=&\\beta\\alpha_i^{(l)}+(1-\\beta)\\hat\\alpha_{i+1}^{(l)}\\end{array}$$  \n\n其中d表示token drop rate，f是一个单调递增函数。$\\alpha$ 会随着训练，通过moving average更新。$\\beta$ 是moving average的权重，是一个超参。  \n\n实际实现中，f设计成：  \n\n$$f(d)=\\left\\{\\begin{array}{ll}\\xi d&\\text{if }d\\leq\\alpha_{\\text{max}}/\\xi\\\\\\alpha_{\\text{max}}&\\text{if }d>\\alpha_{\\text{max}}/\\xi\\end{array}\\right.$$  \n\n$\\xi$ 表示auxiliary loss coefficient对token drop rate的敏感程度。  \n\n最终训练中，各个超参的设置为：  \n- $\\xi=1/5$  \n- $\\alpha_{\\max}=0.01$  \n- $\\beta=0.99$  \n\n# 其他尝试  \n\n报告中还给出了训练中一些其他尝试，虽然没有直接效果，但是也有参考意义。  \n\n## 学习率  \n\nMoE模型由于路由策略的存在，每个专家平均接受到的输入token数比global batch size要小。  \n\n假设共有n个专家，激活专家数为k，那么平均每个专家接受到的输入只有模型输入的k/n。  \n\n而有效batch size的减小意味着更容易引入noise，对此一般的应对方案就是减小learning rate，可以进行linear scaling（$k/n$），或者square root scaling（$\\sqrt{k/n}$）。  \n\n那么减小learning rate是否能提升效果呢？Skywork-MoE用一个1.8B参数，共32个专家，激活专家数为2的模型，按square root scaling，进行了以下3个实验  \n\n{% asset_img lr_exp.png lr实验 %}  \n\n所有模型在训了300B数据之后，lr会降到peak lr的10%，然后会再继续训10B，在这个过程里lr逐渐降为0。  \n\n训练的loss如下图  \n\n{% asset_img lr_result.png lr实验 %}  \n\n虽然在300B的训练量下，减小lr有一点收益，但是随着最后10B的训练，三个模型都收敛到同样的loss。这说明前面的loss差异并不是不可弥补的，更可能只是因为在300B时三个模型的lr decay到不同的绝对值而已。  \n\n这也说明根据专家数量减少MoE模型的训练学习率并没有太大必要。  \n\n## 多样化初始化  \n\n前面提到，用一个dense模型进行初始化，会导致各个专家相似度过高，从而损害MoE模型的效果。那么我们自然想到用多样化的几个dense模型进行MoE的初始化，效果是不是会更好。  \n\nSkywork-MoE对此进行了实验。把原始dense模型分别用不同的100B数据进行训练，从而获得多个dense模型，并用这些多样化的dense模型初始化MoE模型。  \n\n具体来说，基于原始dense模型 $M_{\\mathrm{base}}$，用了中文、英文、代码三个不同的100B数据集进行训练，获得 $M_{\\mathrm{cn}},M_{\\mathrm{en}},M_{\\mathrm{code}}$ 三个dense模型。之后把 $M_{\\mathrm{cn}}$ 复制3份，$M_{\\mathrm{en}}$ 复制3份，$M_{\\mathrm{code}}$ 复制1份，$M_{\\mathrm{base}}$ 复制1份，共同初始化一个有8个专家的MoE模型。  \n\n多样化和无多样化的初始化方法，训练loss对比如下  \n\n{% asset_img diff_dense.png 初始化实验 %}  \n\n可以看到多样化的初始化方法确实有一点收益，不过随着训练进行，差异在逐渐减小。  \n\n经过90B数据的训练之后，二者的loss只有不到0.01的差距。相较于dense模型的多次继续预训练成本，这个收益并不明显，因此Skywork-MoE最终没有采用多样化的初始化方法。  \n\n# 效果  \n\n146B参数的Skywork-MoE是从Skywork-13B初始化而来的。  \n\n训练数据使用了SkyPile中的一部分数据，再加上一批合成数据。  \n\n中文、英文、代码数据的比例为7:2:1。  \n\nSkywork-MoE在和一些主流模型，在一些benchmark上的对比如下  \n\n{% asset_img perf.png 效果 %}  \n\n基本上达到了同归模型比较好的效果。  \n\n# 小结  \n\nSkywork-MoE开源了一个效果不错的MoE模型，同时对于初始化策略的探索也颇有借鉴意义。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】Skywork-MoE: A Deep Dive into Training Techniques for\nMixture-of-Experts Language Models https://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf  \n","slug":"cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型","published":1,"updated":"2024-06-05T12:41:50.292Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9x000u314k0a0ca50k","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>之前我们对比较热门的十个MoE工作进行了整理：<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a>。</p>\n<p>最近昆仑万维开源了Skywork-MoE，一个总参数量为146B，激活参数量为22B的MoE模型。</p>\n<p>Skywork-MoE技术报告中针对几个实操会遇到的问题做了一些实验，还是挺有借鉴意义的。</p>\n<h1 id=\"skywork-moe模型\">Skywork-MoE模型</h1>\n<p>分析之前，先看下Skywork-MoE的模型设计：<br>\n- Llama-like architecture<br>\n- RoPE<br>\n- RMSNorm<br>\n- SwiGLU activation function</p>\n<p>其他参数如下表</p>\n<img src=\"/1d5bcd45/structure.png\" class title=\"模型结构\">\n<p>Skywork-MoE共有146B参数，16个专家，激活参数量为22B。</p>\n<p>训练集群用到了192个NVIDIAHGX-A800节点，共1536个A800-80G显卡。</p>\n<p>训练框架是基于Megatron搭建的，data\nparallelism开了ZeRO-1优化，训练速度能达到690token/GPU/second，GPU利用率是38%。</p>\n<h1 id=\"训练路线选择\">训练路线选择</h1>\n<p>在当前的情况下，要训练一个MoE模型有两条路线可以选择：<br>\n-\nupcycling：用一个dense模型做MoE模型的初始化，进行一定的继续预训练。这样的好处是MoE模型能在一个比较好的初始化点开始训练，直觉上这样的模型应该收敛得相对比较快，成本也比较低。存在的问题是dense模型的选择可能存在一些权衡取舍，且从dense进行初始化可能对最终效果存在负面影响。<br>\n- from\nscratch：直接随机初始化一个MoE模型，从零开始训练。这样成本相比upcycling就比较高，但是效果可能比upcycling更好。</p>\n<p>当然还有一种方法是，先从零训一个dense模型，再从这个dense模型训练一个MoE模型。但是后面的实验告诉我们，如果这个dense模型纯粹是为最终的MoE模型服务的话，那这种方法是费力不讨好的。</p>\n<p>要决定是upcycling还是from\nscratch，需要看现有的dense模型的水平，以及MoE模型的训练预算。首先如果预算根本支持不了MoE模型这个规模的训练，那我们当然只能选择upcycling。只有当预算充足，我们才有机会选择from\nscratch这条路。而如果没有可用的dense模型，那就只能选择from scratch。</p>\n<p>前面我们从直觉上认为from\nscratch效果会更好，下面就从实验上来验证这个想法。</p>\n<p>首先，在300B\ntoken的数据上训练一个0.3B的dense模型，并分别取100B和300B时的checkpoint作为后续实验的起始点。这两个checkpoint起个名字叫\"checkpoint-100B\"和\"checkpoint-300B\"。</p>\n<p>然后在相同结构下，把dense模型扩成有8个专家的MoE模型，并使用3种不同的初始化策略：from-scratch\n/ checkpoint-100B / checkpoint-300B。</p>\n<p>假设我们现在有两种MoE模型的训练预算，100B和300B（token）。</p>\n<p>对于100B训练预算，对比以下几个模型</p>\n<img src=\"/1d5bcd45/100B.png\" class title=\"100B\">\n<p>同样地，对于300B预算的情况，训练了init_scratch-decay_300b和init_100b-decay_300b。另外还训练了一个init_300b-3xLR，相比init_300b-const提升了3倍的学习率，用于验证学习率的影响。</p>\n<p>各个模型的训练结果如下图所示</p>\n<img src=\"/1d5bcd45/exp_1.png\" class title=\"实验\">\n<p>左图：在100B的训练预算下，from\nscratch已经可以和从dense初始化的MoE模型loss持平，甚至比init_300b-const好。报告认为init_300b-const效果不好有一部分原因是学习率太小了。</p>\n<p>中图：在300B的训练预算下，from\nscratch模型已经超越所有其他模型。另外学习率最小的模型表现最差。</p>\n<p>右图：把中图几个模型的expert similarity画出来，发现expert\nsimilarity越低的模型，表现越好，并且对于upcycling的模型，expert\nsimilarity在训练过程中越来越低，对应着模型效果越来越好。而from\nscratch的模型的expert\nsimilarity基本上一直保持为0，这也说明从dense模型初始化会使得专家多样性比较弱，从而使得模型收敛到suboptimal的点。</p>\n<p>据此，报告给出路线选择的经验法则。假设 <span class=\"math inline\">\\(C_{\\mathrm{dense}}\\)</span>\n是dense模型的训练成本，<span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\)</span>\n是MoE模型的训练预算，那么：<br>\n- 如果 <span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\ll\nC_{\\mathrm{dense}}\\)</span>，选择upcycling，upcycling能更好利用上dense模型已投入的成本。<br>\n- 如果 <span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\geq2C_{\\mathrm{dense}}\\)</span>，选择from\nscratch，能获得更好的效果。</p>\n<p>另外，学习率的影响很大，这个要仔细设置。</p>\n<h1 id=\"模型设计\">模型设计</h1>\n<p>模型设计上，Skywork-MoE提出了两个主要的改进：gating logit\nnormalization和adaptive auxiliary loss coefficients。</p>\n<h2 id=\"gating-logit-normalization\">gating logit normalization</h2>\n<p>研究人员在训练过程中发现有一个现象，那就是有时gating\nlayer会输出熵很高的分布，也就是分配给各个专家的概率接近平均分布。这样的结果就是MoE层的输出基本上相当于是各个专家的平均值，而不是一个weighted\naverage。</p>\n<p>而出现这种现象说明gating\nlayer没有很好地区分各个专家，无法把相应的输入分配给最合适的专家。</p>\n<p>针对这个问题，Skywork-MoE给出的方法就是在gating\nlayer的softmax之前引入一个normalization step，如下式</p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;z=Wx+b\\\\&amp;\\tilde{z}=\\lambda\\cdot\\frac{z-\\mu}{\\sigma}\\\\&amp;g=\\operatorname{softmax}(\\tilde{z})\\end{aligned}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\lambda\\)</span> 是一个超参。</p>\n<p>这样归一化之后我们就得到一个均值为0，而方差受 <span class=\"math inline\">\\(\\lambda\\)</span> 控制的向量。大的 <span class=\"math inline\">\\(\\lambda\\)</span>\n值会使得softmax之后的分布更显著，更不均匀。这就相当于给softmax加上一个放大器，把原本不显著的差异进行放大。</p>\n<p>为了验证这个设计的有效性，Skywork-MoE在2.5B参数16个专家的MoE模型上，分别使用和不使用gating\nlogit normalization进行了训练。</p>\n<p>两个模型的gating分布差异如下图所示，normalization确实可以增大各个专家分配到的概率的差异。</p>\n<img src=\"/1d5bcd45/gate_dist.png\" class title=\"gating distribution\">\n<p>使用了normalization的模型在training loss和token drop\nrate上都有更好的表现，如下图所示。</p>\n<img src=\"/1d5bcd45/normaization.png\" class title=\"gating logit normalization\">\n<p>而统计gating\nlayer输出的分布中的Max1/Max2和Max2/Max3比值也同样说明了各个expert被更有效地区分开了。</p>\n<p>在千亿Skywork-MoE模型的训练中，使用了 <span class=\"math inline\">\\(\\lambda=1\\)</span>。</p>\n<h2 id=\"adaptive-auxiliary-loss-coefficients\">adaptive auxiliary loss\ncoefficients</h2>\n<p>一般来说，MoE模型在训练中都会加入一个auxiliary\nloss，帮助平衡专家的选择分布，提升训练效率，也增强专家的多样性。对于有M个MoE层的模型，最终loss如下式所示。</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{ce}}+\\sum_{l=1}^M\\alpha\\mathcal{L}_{\\mathrm{aux}}^{(l)}\\]</span></p>\n<p>每个MoE层都有对应的auxiliary loss。</p>\n<p>Skywork-MoE认为每层的auxiliary loss的系数 <span class=\"math inline\">\\(\\alpha\\)</span>\n不一定要相同，并且随着训练进行，在gating的平衡已经比较好的时候，可以放宽auxiliary\nloss的限制强度，避免影响模型的最终效果。</p>\n<p>基于这两个想法，Skywork-MoE提出adaptive auxiliary loss\ncoefficients。</p>\n<p>每个MoE层的auxiliary\nloss有自己的系数，而这个系数和当前这个MoE层的token drop\nrate联系了起来。大的token drop\nrate表示gating的分配不平衡，因此要加强auxiliary\nloss的约束，反之则可以减小约束。</p>\n<p>对于第l个MoE层，在第i个step的时候，auxiliary loss的系数计算如下</p>\n<p><span class=\"math display\">\\[\\begin{array}{rcl}\\hat\\alpha_{i+1}^{(l)}&amp;=&amp;f(d_i^{(l)})\\\\\\alpha_{i+1}^{(l)}&amp;=&amp;\\beta\\alpha_i^{(l)}+(1-\\beta)\\hat\\alpha_{i+1}^{(l)}\\end{array}\\]</span></p>\n<p>其中d表示token drop rate，f是一个单调递增函数。<span class=\"math inline\">\\(\\alpha\\)</span> 会随着训练，通过moving\naverage更新。<span class=\"math inline\">\\(\\beta\\)</span> 是moving\naverage的权重，是一个超参。</p>\n<p>实际实现中，f设计成：</p>\n<p><span class=\"math display\">\\[f(d)=\\left\\{\\begin{array}{ll}\\xi\nd&amp;\\text{if\n}d\\leq\\alpha_{\\text{max}}/\\xi\\\\\\alpha_{\\text{max}}&amp;\\text{if\n}d&gt;\\alpha_{\\text{max}}/\\xi\\end{array}\\right.\\]</span></p>\n<p><span class=\"math inline\">\\(\\xi\\)</span> 表示auxiliary loss\ncoefficient对token drop rate的敏感程度。</p>\n<p>最终训练中，各个超参的设置为：<br>\n- <span class=\"math inline\">\\(\\xi=1/5\\)</span><br>\n- <span class=\"math inline\">\\(\\alpha_{\\max}=0.01\\)</span><br>\n- <span class=\"math inline\">\\(\\beta=0.99\\)</span></p>\n<h1 id=\"其他尝试\">其他尝试</h1>\n<p>报告中还给出了训练中一些其他尝试，虽然没有直接效果，但是也有参考意义。</p>\n<h2 id=\"学习率\">学习率</h2>\n<p>MoE模型由于路由策略的存在，每个专家平均接受到的输入token数比global\nbatch size要小。</p>\n<p>假设共有n个专家，激活专家数为k，那么平均每个专家接受到的输入只有模型输入的k/n。</p>\n<p>而有效batch\nsize的减小意味着更容易引入noise，对此一般的应对方案就是减小learning\nrate，可以进行linear scaling（<span class=\"math inline\">\\(k/n\\)</span>），或者square root scaling（<span class=\"math inline\">\\(\\sqrt{k/n}\\)</span>）。</p>\n<p>那么减小learning\nrate是否能提升效果呢？Skywork-MoE用一个1.8B参数，共32个专家，激活专家数为2的模型，按square\nroot scaling，进行了以下3个实验</p>\n<img src=\"/1d5bcd45/lr_exp.png\" class title=\"lr实验\">\n<p>所有模型在训了300B数据之后，lr会降到peak\nlr的10%，然后会再继续训10B，在这个过程里lr逐渐降为0。</p>\n<p>训练的loss如下图</p>\n<img src=\"/1d5bcd45/lr_result.png\" class title=\"lr实验\">\n<p>虽然在300B的训练量下，减小lr有一点收益，但是随着最后10B的训练，三个模型都收敛到同样的loss。这说明前面的loss差异并不是不可弥补的，更可能只是因为在300B时三个模型的lr\ndecay到不同的绝对值而已。</p>\n<p>这也说明根据专家数量减少MoE模型的训练学习率并没有太大必要。</p>\n<h2 id=\"多样化初始化\">多样化初始化</h2>\n<p>前面提到，用一个dense模型进行初始化，会导致各个专家相似度过高，从而损害MoE模型的效果。那么我们自然想到用多样化的几个dense模型进行MoE的初始化，效果是不是会更好。</p>\n<p>Skywork-MoE对此进行了实验。把原始dense模型分别用不同的100B数据进行训练，从而获得多个dense模型，并用这些多样化的dense模型初始化MoE模型。</p>\n<p>具体来说，基于原始dense模型 <span class=\"math inline\">\\(M_{\\mathrm{base}}\\)</span>，用了中文、英文、代码三个不同的100B数据集进行训练，获得\n<span class=\"math inline\">\\(M_{\\mathrm{cn}},M_{\\mathrm{en}},M_{\\mathrm{code}}\\)</span>\n三个dense模型。之后把 <span class=\"math inline\">\\(M_{\\mathrm{cn}}\\)</span> 复制3份，<span class=\"math inline\">\\(M_{\\mathrm{en}}\\)</span> 复制3份，<span class=\"math inline\">\\(M_{\\mathrm{code}}\\)</span> 复制1份，<span class=\"math inline\">\\(M_{\\mathrm{base}}\\)</span>\n复制1份，共同初始化一个有8个专家的MoE模型。</p>\n<p>多样化和无多样化的初始化方法，训练loss对比如下</p>\n<img src=\"/1d5bcd45/diff_dense.png\" class title=\"初始化实验\">\n<p>可以看到多样化的初始化方法确实有一点收益，不过随着训练进行，差异在逐渐减小。</p>\n<p>经过90B数据的训练之后，二者的loss只有不到0.01的差距。相较于dense模型的多次继续预训练成本，这个收益并不明显，因此Skywork-MoE最终没有采用多样化的初始化方法。</p>\n<h1 id=\"效果\">效果</h1>\n<p>146B参数的Skywork-MoE是从Skywork-13B初始化而来的。</p>\n<p>训练数据使用了SkyPile中的一部分数据，再加上一批合成数据。</p>\n<p>中文、英文、代码数据的比例为7:2:1。</p>\n<p>Skywork-MoE在和一些主流模型，在一些benchmark上的对比如下</p>\n<img src=\"/1d5bcd45/perf.png\" class title=\"效果\">\n<p>基本上达到了同归模型比较好的效果。</p>\n<h1 id=\"小结\">小结</h1>\n<p>Skywork-MoE开源了一个效果不错的MoE模型，同时对于初始化策略的探索也颇有借鉴意义。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Skywork-MoE: A Deep Dive into Training Techniques for\nMixture-of-Experts Language Models\nhttps://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf</p>\n","length":5954,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>之前我们对比较热门的十个MoE工作进行了整理：<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a>。</p>\n<p>最近昆仑万维开源了Skywork-MoE，一个总参数量为146B，激活参数量为22B的MoE模型。</p>\n<p>Skywork-MoE技术报告中针对几个实操会遇到的问题做了一些实验，还是挺有借鉴意义的。</p>\n<h1 id=\"skywork-moe模型\">Skywork-MoE模型</h1>\n<p>分析之前，先看下Skywork-MoE的模型设计：<br>\n- Llama-like architecture<br>\n- RoPE<br>\n- RMSNorm<br>\n- SwiGLU activation function</p>\n<p>其他参数如下表</p>\n<img src=\"/1d5bcd45/structure.png\" class title=\"模型结构\">\n<p>Skywork-MoE共有146B参数，16个专家，激活参数量为22B。</p>\n<p>训练集群用到了192个NVIDIAHGX-A800节点，共1536个A800-80G显卡。</p>\n<p>训练框架是基于Megatron搭建的，data\nparallelism开了ZeRO-1优化，训练速度能达到690token/GPU/second，GPU利用率是38%。</p>\n<h1 id=\"训练路线选择\">训练路线选择</h1>\n<p>在当前的情况下，要训练一个MoE模型有两条路线可以选择：<br>\n-\nupcycling：用一个dense模型做MoE模型的初始化，进行一定的继续预训练。这样的好处是MoE模型能在一个比较好的初始化点开始训练，直觉上这样的模型应该收敛得相对比较快，成本也比较低。存在的问题是dense模型的选择可能存在一些权衡取舍，且从dense进行初始化可能对最终效果存在负面影响。<br>\n- from\nscratch：直接随机初始化一个MoE模型，从零开始训练。这样成本相比upcycling就比较高，但是效果可能比upcycling更好。</p>\n<p>当然还有一种方法是，先从零训一个dense模型，再从这个dense模型训练一个MoE模型。但是后面的实验告诉我们，如果这个dense模型纯粹是为最终的MoE模型服务的话，那这种方法是费力不讨好的。</p>\n<p>要决定是upcycling还是from\nscratch，需要看现有的dense模型的水平，以及MoE模型的训练预算。首先如果预算根本支持不了MoE模型这个规模的训练，那我们当然只能选择upcycling。只有当预算充足，我们才有机会选择from\nscratch这条路。而如果没有可用的dense模型，那就只能选择from scratch。</p>\n<p>前面我们从直觉上认为from\nscratch效果会更好，下面就从实验上来验证这个想法。</p>\n<p>首先，在300B\ntoken的数据上训练一个0.3B的dense模型，并分别取100B和300B时的checkpoint作为后续实验的起始点。这两个checkpoint起个名字叫\"checkpoint-100B\"和\"checkpoint-300B\"。</p>\n<p>然后在相同结构下，把dense模型扩成有8个专家的MoE模型，并使用3种不同的初始化策略：from-scratch\n/ checkpoint-100B / checkpoint-300B。</p>\n<p>假设我们现在有两种MoE模型的训练预算，100B和300B（token）。</p>\n<p>对于100B训练预算，对比以下几个模型</p>\n<img src=\"/1d5bcd45/100B.png\" class title=\"100B\">\n<p>同样地，对于300B预算的情况，训练了init_scratch-decay_300b和init_100b-decay_300b。另外还训练了一个init_300b-3xLR，相比init_300b-const提升了3倍的学习率，用于验证学习率的影响。</p>\n<p>各个模型的训练结果如下图所示</p>\n<img src=\"/1d5bcd45/exp_1.png\" class title=\"实验\">\n<p>左图：在100B的训练预算下，from\nscratch已经可以和从dense初始化的MoE模型loss持平，甚至比init_300b-const好。报告认为init_300b-const效果不好有一部分原因是学习率太小了。</p>\n<p>中图：在300B的训练预算下，from\nscratch模型已经超越所有其他模型。另外学习率最小的模型表现最差。</p>\n<p>右图：把中图几个模型的expert similarity画出来，发现expert\nsimilarity越低的模型，表现越好，并且对于upcycling的模型，expert\nsimilarity在训练过程中越来越低，对应着模型效果越来越好。而from\nscratch的模型的expert\nsimilarity基本上一直保持为0，这也说明从dense模型初始化会使得专家多样性比较弱，从而使得模型收敛到suboptimal的点。</p>\n<p>据此，报告给出路线选择的经验法则。假设 <span class=\"math inline\">\\(C_{\\mathrm{dense}}\\)</span>\n是dense模型的训练成本，<span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\)</span>\n是MoE模型的训练预算，那么：<br>\n- 如果 <span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\ll\nC_{\\mathrm{dense}}\\)</span>，选择upcycling，upcycling能更好利用上dense模型已投入的成本。<br>\n- 如果 <span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\geq2C_{\\mathrm{dense}}\\)</span>，选择from\nscratch，能获得更好的效果。</p>\n<p>另外，学习率的影响很大，这个要仔细设置。</p>\n<h1 id=\"模型设计\">模型设计</h1>\n<p>模型设计上，Skywork-MoE提出了两个主要的改进：gating logit\nnormalization和adaptive auxiliary loss coefficients。</p>\n<h2 id=\"gating-logit-normalization\">gating logit normalization</h2>\n<p>研究人员在训练过程中发现有一个现象，那就是有时gating\nlayer会输出熵很高的分布，也就是分配给各个专家的概率接近平均分布。这样的结果就是MoE层的输出基本上相当于是各个专家的平均值，而不是一个weighted\naverage。</p>\n<p>而出现这种现象说明gating\nlayer没有很好地区分各个专家，无法把相应的输入分配给最合适的专家。</p>\n<p>针对这个问题，Skywork-MoE给出的方法就是在gating\nlayer的softmax之前引入一个normalization step，如下式</p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;z=Wx+b\\\\&amp;\\tilde{z}=\\lambda\\cdot\\frac{z-\\mu}{\\sigma}\\\\&amp;g=\\operatorname{softmax}(\\tilde{z})\\end{aligned}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\lambda\\)</span> 是一个超参。</p>\n<p>这样归一化之后我们就得到一个均值为0，而方差受 <span class=\"math inline\">\\(\\lambda\\)</span> 控制的向量。大的 <span class=\"math inline\">\\(\\lambda\\)</span>\n值会使得softmax之后的分布更显著，更不均匀。这就相当于给softmax加上一个放大器，把原本不显著的差异进行放大。</p>\n<p>为了验证这个设计的有效性，Skywork-MoE在2.5B参数16个专家的MoE模型上，分别使用和不使用gating\nlogit normalization进行了训练。</p>\n<p>两个模型的gating分布差异如下图所示，normalization确实可以增大各个专家分配到的概率的差异。</p>\n<img src=\"/1d5bcd45/gate_dist.png\" class title=\"gating distribution\">\n<p>使用了normalization的模型在training loss和token drop\nrate上都有更好的表现，如下图所示。</p>\n<img src=\"/1d5bcd45/normaization.png\" class title=\"gating logit normalization\">\n<p>而统计gating\nlayer输出的分布中的Max1/Max2和Max2/Max3比值也同样说明了各个expert被更有效地区分开了。</p>\n<p>在千亿Skywork-MoE模型的训练中，使用了 <span class=\"math inline\">\\(\\lambda=1\\)</span>。</p>\n<h2 id=\"adaptive-auxiliary-loss-coefficients\">adaptive auxiliary loss\ncoefficients</h2>\n<p>一般来说，MoE模型在训练中都会加入一个auxiliary\nloss，帮助平衡专家的选择分布，提升训练效率，也增强专家的多样性。对于有M个MoE层的模型，最终loss如下式所示。</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{ce}}+\\sum_{l=1}^M\\alpha\\mathcal{L}_{\\mathrm{aux}}^{(l)}\\]</span></p>\n<p>每个MoE层都有对应的auxiliary loss。</p>\n<p>Skywork-MoE认为每层的auxiliary loss的系数 <span class=\"math inline\">\\(\\alpha\\)</span>\n不一定要相同，并且随着训练进行，在gating的平衡已经比较好的时候，可以放宽auxiliary\nloss的限制强度，避免影响模型的最终效果。</p>\n<p>基于这两个想法，Skywork-MoE提出adaptive auxiliary loss\ncoefficients。</p>\n<p>每个MoE层的auxiliary\nloss有自己的系数，而这个系数和当前这个MoE层的token drop\nrate联系了起来。大的token drop\nrate表示gating的分配不平衡，因此要加强auxiliary\nloss的约束，反之则可以减小约束。</p>\n<p>对于第l个MoE层，在第i个step的时候，auxiliary loss的系数计算如下</p>\n<p><span class=\"math display\">\\[\\begin{array}{rcl}\\hat\\alpha_{i+1}^{(l)}&amp;=&amp;f(d_i^{(l)})\\\\\\alpha_{i+1}^{(l)}&amp;=&amp;\\beta\\alpha_i^{(l)}+(1-\\beta)\\hat\\alpha_{i+1}^{(l)}\\end{array}\\]</span></p>\n<p>其中d表示token drop rate，f是一个单调递增函数。<span class=\"math inline\">\\(\\alpha\\)</span> 会随着训练，通过moving\naverage更新。<span class=\"math inline\">\\(\\beta\\)</span> 是moving\naverage的权重，是一个超参。</p>\n<p>实际实现中，f设计成：</p>\n<p><span class=\"math display\">\\[f(d)=\\left\\{\\begin{array}{ll}\\xi\nd&amp;\\text{if\n}d\\leq\\alpha_{\\text{max}}/\\xi\\\\\\alpha_{\\text{max}}&amp;\\text{if\n}d&gt;\\alpha_{\\text{max}}/\\xi\\end{array}\\right.\\]</span></p>\n<p><span class=\"math inline\">\\(\\xi\\)</span> 表示auxiliary loss\ncoefficient对token drop rate的敏感程度。</p>\n<p>最终训练中，各个超参的设置为：<br>\n- <span class=\"math inline\">\\(\\xi=1/5\\)</span><br>\n- <span class=\"math inline\">\\(\\alpha_{\\max}=0.01\\)</span><br>\n- <span class=\"math inline\">\\(\\beta=0.99\\)</span></p>\n<h1 id=\"其他尝试\">其他尝试</h1>\n<p>报告中还给出了训练中一些其他尝试，虽然没有直接效果，但是也有参考意义。</p>\n<h2 id=\"学习率\">学习率</h2>\n<p>MoE模型由于路由策略的存在，每个专家平均接受到的输入token数比global\nbatch size要小。</p>\n<p>假设共有n个专家，激活专家数为k，那么平均每个专家接受到的输入只有模型输入的k/n。</p>\n<p>而有效batch\nsize的减小意味着更容易引入noise，对此一般的应对方案就是减小learning\nrate，可以进行linear scaling（<span class=\"math inline\">\\(k/n\\)</span>），或者square root scaling（<span class=\"math inline\">\\(\\sqrt{k/n}\\)</span>）。</p>\n<p>那么减小learning\nrate是否能提升效果呢？Skywork-MoE用一个1.8B参数，共32个专家，激活专家数为2的模型，按square\nroot scaling，进行了以下3个实验</p>\n<img src=\"/1d5bcd45/lr_exp.png\" class title=\"lr实验\">\n<p>所有模型在训了300B数据之后，lr会降到peak\nlr的10%，然后会再继续训10B，在这个过程里lr逐渐降为0。</p>\n<p>训练的loss如下图</p>\n<img src=\"/1d5bcd45/lr_result.png\" class title=\"lr实验\">\n<p>虽然在300B的训练量下，减小lr有一点收益，但是随着最后10B的训练，三个模型都收敛到同样的loss。这说明前面的loss差异并不是不可弥补的，更可能只是因为在300B时三个模型的lr\ndecay到不同的绝对值而已。</p>\n<p>这也说明根据专家数量减少MoE模型的训练学习率并没有太大必要。</p>\n<h2 id=\"多样化初始化\">多样化初始化</h2>\n<p>前面提到，用一个dense模型进行初始化，会导致各个专家相似度过高，从而损害MoE模型的效果。那么我们自然想到用多样化的几个dense模型进行MoE的初始化，效果是不是会更好。</p>\n<p>Skywork-MoE对此进行了实验。把原始dense模型分别用不同的100B数据进行训练，从而获得多个dense模型，并用这些多样化的dense模型初始化MoE模型。</p>\n<p>具体来说，基于原始dense模型 <span class=\"math inline\">\\(M_{\\mathrm{base}}\\)</span>，用了中文、英文、代码三个不同的100B数据集进行训练，获得\n<span class=\"math inline\">\\(M_{\\mathrm{cn}},M_{\\mathrm{en}},M_{\\mathrm{code}}\\)</span>\n三个dense模型。之后把 <span class=\"math inline\">\\(M_{\\mathrm{cn}}\\)</span> 复制3份，<span class=\"math inline\">\\(M_{\\mathrm{en}}\\)</span> 复制3份，<span class=\"math inline\">\\(M_{\\mathrm{code}}\\)</span> 复制1份，<span class=\"math inline\">\\(M_{\\mathrm{base}}\\)</span>\n复制1份，共同初始化一个有8个专家的MoE模型。</p>\n<p>多样化和无多样化的初始化方法，训练loss对比如下</p>\n<img src=\"/1d5bcd45/diff_dense.png\" class title=\"初始化实验\">\n<p>可以看到多样化的初始化方法确实有一点收益，不过随着训练进行，差异在逐渐减小。</p>\n<p>经过90B数据的训练之后，二者的loss只有不到0.01的差距。相较于dense模型的多次继续预训练成本，这个收益并不明显，因此Skywork-MoE最终没有采用多样化的初始化方法。</p>\n<h1 id=\"效果\">效果</h1>\n<p>146B参数的Skywork-MoE是从Skywork-13B初始化而来的。</p>\n<p>训练数据使用了SkyPile中的一部分数据，再加上一批合成数据。</p>\n<p>中文、英文、代码数据的比例为7:2:1。</p>\n<p>Skywork-MoE在和一些主流模型，在一些benchmark上的对比如下</p>\n<img src=\"/1d5bcd45/perf.png\" class title=\"效果\">\n<p>基本上达到了同归模型比较好的效果。</p>\n<h1 id=\"小结\">小结</h1>\n<p>Skywork-MoE开源了一个效果不错的MoE模型，同时对于初始化策略的探索也颇有借鉴意义。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">大模型偏好对齐-simPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Skywork-MoE: A Deep Dive into Training Techniques for\nMixture-of-Experts Language Models\nhttps://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf</p>\n"},{"title":"大模型偏好对齐-ODPO","abbrlink":"da871ebe","date":"2024-05-30T07:23:05.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n前面对DPO的思路做了整理：[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)。  \n\nDPO把RLHF的两阶段训练，变成了一阶段训练，降低了训练成本。而ODPO（DPO with an offset）在DPO的基础上做了一点改进，在几个下游任务的实验中，获得了比DPO更好的效果。  \n\n# 背景  \n\n直接使用指令微调，是让模型学会处理下游任务的一个快速有效的方法。  \n\n但是指令微调的优化目标是maximize the response log-likelihood，这和“生成人类所偏好的高质量内容”的目标之间存在gap，不完全对齐。  \n\n这个misalignment部分是因为maximum likelihood的目标无法区分数据里“大错”（比如幻觉）和“小错”（比如标点符号不恰当）。  \n\n> Training with the maximum likelihood objective makes the model assign nonzero probability mass to all responses in SFT dataset, even those of lower quality.  \n\n因此有RLHF的方法来解决这个问题。RL通过人类偏好数据训练一个reward模型，并用reward模型来指导策略模型。  \n\n而reward的modeling有两种，pointwise reward和pairwise preference。  \n\npointwise reward一般用于reward有比较确定定义且简单的场景，比如情感分类，我们可以定义positive的情感的reward为1，negative的reward为0。类似的还有toxicity等。这些类别一般也有很多现成的打分模型/classifier可以使用。  \n\npairwise preference一般用于比较复杂的任务，比如文本摘要和对话生成。这类任务难以直接基于单个答案来打分，而需要通过对比才能知道哪个更好。  \n\n但RLHF成本比较高，因此DPO对训练过程进行了简化。  \n\n# Bradley–Terry model的局限  \n\nDPO的损失如下  \n\n$$\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})& =-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}$$  \n\n其中  \n\n$$\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}$$  \n\n是estimated reward。  \n\n这个DPO损失的形式背后用到了Bradley–Terry model对偏好进行建模。而Bradley–Terry model只给出了一个response比另一个response好的概率，而没有告诉我们好的程度。  \n\n而实际上我们很多偏好对比数据都提供了具体的分数，而不仅仅是排序信息。有这些具体分数我们就可以知道两条response之间是差一点点，还是差很多。  \n\n那么把这个差距的信息引入到偏好的建模里，应该能带来收益，这也是ODPO的思路，而两个response之间的差距就是offset。  \n\n# DPO with an Offset  \n\n给 $\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)$ 分别加上Gumbel noise，即得到  \n\n$$\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)$$  \n\n$$\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)$$  \n\n论文中证明了  \n\n$$p\\big(\\tilde{r}_w-\\tilde{r}_l>\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)$$  \n\n基于此，ODPO的损失函数表达成  \n\n$$\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]$$  \n\n这相当于要求preferred response的estimated reward要比dispreferred response的estimated reward大，且要大offset值这么多。  \n\n当offset=0的时候，ODPO的损失等价于DPO的损失。  \n\nODPO的这个做法和softmax margin loss/marginal loss有些相似，都是在原来loss的基础上，加上一个margin，加大对靠得比较近的数据对的penalization的力度。  \n\nODPO里，offset是两个response之间的actual reward的increasing scaling function。  \n\n$$\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)$$  \n\n其中 $\\alpha$ 是超参。  \n\n{% asset_img odpo_intro.png intro %}  \n\n# 实验  \n\n论文在几个下游任务上做了实验。  \n\n## sentiment control  \n\n首先是sentiment control的任务，即要求模型输出positive的response。  \n\n先用GPT2-Large在IMDB dataset做了finetune，获得SFT模型。论文用一个现成的sentiment classifier作为reward的打分模型，给response分别打分，分数如下计算  \n\n$$r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) = 1-p(\\text{negative}\\mid\\cdot)$$  \n\n$$r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) = 1+p(\\text{positive}\\mid\\cdot)$$  \n\n有了reward打分数据之后，还要构造偏好数据对。这里把同一个prompt下生成的所有reward分数不同的response进行排列组合，获得偏好数据对。  \n\n对于DPO，有这些偏好数据对就够了。而ODPO还需要一个offset，按如下方式计算：  \n\n$$\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)$$  \n\n实验里把 $\\alpha$ 设为1。  \n\n实验中使用两个不同的random seed，从SFT模型里进行采样，从而得到了2份不同的偏好数据。  \n\n而 $\\beta$ 使用了14个不同的取值 $\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}$ 进行实验。  \n\n论文在2份数据集下分别使用不同的数据量进行训练（5000，7500,10000），这样DPO和ODPO分别有2×3×14=84个实验。  \n\n每个实验计算模型生成结果的sentiment打分，以及和SFT模型的KL divergence。结果如下图  \n\n{% asset_img sentiment_control.png sentiment control %}  \n\n我们希望模型在sentiment的打分上越高越好，同时不要和SFT模型有太大的差距，因此越靠近左上角的点越符合我们的要求。从结果上看，ODPO比DPO更好一些。  \n\n## toxicity control  \n\ntoxicity control任务和sentiment control类似，要求模型的response的毒性尽量低。  \n\n这次使用GPT-neo-2.7b模型，$\\beta$ 的取值范围为 $\\{0.05,0.1,0.2,0.3,0.4,0.5\\}$，使用从REALTOXICITYPROMPTS数据集里抽样的10000个毒性评分大于0.3的prompt。  \n\n结果如下  \n\n{% asset_img toxicity_control.png toxicity control %}  \n\n在数据量较少的情况下（8000 & 9000），ODPO效果更明显好。  \n\n## summarization  \n\n摘要任务使用REDDIT TL;DR数据集，使用的模型是GPTJ-6B。  \n\nDPO和ODPO训练后的评分：抽了100条测试prompt，用不同的temperature生成结果，并用GPT-4进行评分对比。结果如下  \n\n{% asset_img summarization.png summarization %}  \n\nDPO和ODPO都比SFT好，并且在temperature比较低的设置下，DPO和ODPO都比human-written的结果好。  \n\n## 消融实验：scaling function  \n\n前面实验的offset都是用reward差值的log值，这里使用其他两种计算方式进行对比  \n\n$$\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log r(\\boldsymbol{y}_l)$$  \n\n$$\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}$$  \n\n使用5000对sentiment control的数据，$\\beta \\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}$。  \n\n对比结果如下  \n\n{% asset_img scaling_function.png scaling function %}  \n\n使用log scaling的ODPO在KL divergence更小的时候（0.4）可以达到0.8的reward，而没有使用log scaling的模型需要再更大的KL divergence下才能达到通用的reward。  \n\n## 消融实验：α  \n\n同样使用7500对sentiment control的数据，$\\beta=0.5$，改变$\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}$。  \n\n{% asset_img alpha.png alpha %}  \n\n发现更高的 $\\alpha$ 会使得模型更多偏离SFT模型，并带来更高的reward值。  \n\n# 小结  \n\nODPO在DPO的基础上加入了offset，在实现上并不复杂，而且能带来一些收益。  \n\n略有瑕疵的是ODPO的实验覆盖面并不太全，也没有使用LLAMA等更强大的模型进行实验。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】Direct Preference Optimization with an Offset https://arxiv.org/pdf/2402.10571  ","source":"_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO.md","raw":"---\ntitle: 大模型偏好对齐-ODPO\nabbrlink: da871ebe\ndate: 2024-05-30 15:23:05\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 强化学习\n  - 微调\n  - SFT\n  - 偏好对齐\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n前面对DPO的思路做了整理：[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)。  \n\nDPO把RLHF的两阶段训练，变成了一阶段训练，降低了训练成本。而ODPO（DPO with an offset）在DPO的基础上做了一点改进，在几个下游任务的实验中，获得了比DPO更好的效果。  \n\n# 背景  \n\n直接使用指令微调，是让模型学会处理下游任务的一个快速有效的方法。  \n\n但是指令微调的优化目标是maximize the response log-likelihood，这和“生成人类所偏好的高质量内容”的目标之间存在gap，不完全对齐。  \n\n这个misalignment部分是因为maximum likelihood的目标无法区分数据里“大错”（比如幻觉）和“小错”（比如标点符号不恰当）。  \n\n> Training with the maximum likelihood objective makes the model assign nonzero probability mass to all responses in SFT dataset, even those of lower quality.  \n\n因此有RLHF的方法来解决这个问题。RL通过人类偏好数据训练一个reward模型，并用reward模型来指导策略模型。  \n\n而reward的modeling有两种，pointwise reward和pairwise preference。  \n\npointwise reward一般用于reward有比较确定定义且简单的场景，比如情感分类，我们可以定义positive的情感的reward为1，negative的reward为0。类似的还有toxicity等。这些类别一般也有很多现成的打分模型/classifier可以使用。  \n\npairwise preference一般用于比较复杂的任务，比如文本摘要和对话生成。这类任务难以直接基于单个答案来打分，而需要通过对比才能知道哪个更好。  \n\n但RLHF成本比较高，因此DPO对训练过程进行了简化。  \n\n# Bradley–Terry model的局限  \n\nDPO的损失如下  \n\n$$\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})& =-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}$$  \n\n其中  \n\n$$\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}$$  \n\n是estimated reward。  \n\n这个DPO损失的形式背后用到了Bradley–Terry model对偏好进行建模。而Bradley–Terry model只给出了一个response比另一个response好的概率，而没有告诉我们好的程度。  \n\n而实际上我们很多偏好对比数据都提供了具体的分数，而不仅仅是排序信息。有这些具体分数我们就可以知道两条response之间是差一点点，还是差很多。  \n\n那么把这个差距的信息引入到偏好的建模里，应该能带来收益，这也是ODPO的思路，而两个response之间的差距就是offset。  \n\n# DPO with an Offset  \n\n给 $\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)$ 分别加上Gumbel noise，即得到  \n\n$$\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)$$  \n\n$$\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)$$  \n\n论文中证明了  \n\n$$p\\big(\\tilde{r}_w-\\tilde{r}_l>\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)$$  \n\n基于此，ODPO的损失函数表达成  \n\n$$\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]$$  \n\n这相当于要求preferred response的estimated reward要比dispreferred response的estimated reward大，且要大offset值这么多。  \n\n当offset=0的时候，ODPO的损失等价于DPO的损失。  \n\nODPO的这个做法和softmax margin loss/marginal loss有些相似，都是在原来loss的基础上，加上一个margin，加大对靠得比较近的数据对的penalization的力度。  \n\nODPO里，offset是两个response之间的actual reward的increasing scaling function。  \n\n$$\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)$$  \n\n其中 $\\alpha$ 是超参。  \n\n{% asset_img odpo_intro.png intro %}  \n\n# 实验  \n\n论文在几个下游任务上做了实验。  \n\n## sentiment control  \n\n首先是sentiment control的任务，即要求模型输出positive的response。  \n\n先用GPT2-Large在IMDB dataset做了finetune，获得SFT模型。论文用一个现成的sentiment classifier作为reward的打分模型，给response分别打分，分数如下计算  \n\n$$r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) = 1-p(\\text{negative}\\mid\\cdot)$$  \n\n$$r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) = 1+p(\\text{positive}\\mid\\cdot)$$  \n\n有了reward打分数据之后，还要构造偏好数据对。这里把同一个prompt下生成的所有reward分数不同的response进行排列组合，获得偏好数据对。  \n\n对于DPO，有这些偏好数据对就够了。而ODPO还需要一个offset，按如下方式计算：  \n\n$$\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)$$  \n\n实验里把 $\\alpha$ 设为1。  \n\n实验中使用两个不同的random seed，从SFT模型里进行采样，从而得到了2份不同的偏好数据。  \n\n而 $\\beta$ 使用了14个不同的取值 $\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}$ 进行实验。  \n\n论文在2份数据集下分别使用不同的数据量进行训练（5000，7500,10000），这样DPO和ODPO分别有2×3×14=84个实验。  \n\n每个实验计算模型生成结果的sentiment打分，以及和SFT模型的KL divergence。结果如下图  \n\n{% asset_img sentiment_control.png sentiment control %}  \n\n我们希望模型在sentiment的打分上越高越好，同时不要和SFT模型有太大的差距，因此越靠近左上角的点越符合我们的要求。从结果上看，ODPO比DPO更好一些。  \n\n## toxicity control  \n\ntoxicity control任务和sentiment control类似，要求模型的response的毒性尽量低。  \n\n这次使用GPT-neo-2.7b模型，$\\beta$ 的取值范围为 $\\{0.05,0.1,0.2,0.3,0.4,0.5\\}$，使用从REALTOXICITYPROMPTS数据集里抽样的10000个毒性评分大于0.3的prompt。  \n\n结果如下  \n\n{% asset_img toxicity_control.png toxicity control %}  \n\n在数据量较少的情况下（8000 & 9000），ODPO效果更明显好。  \n\n## summarization  \n\n摘要任务使用REDDIT TL;DR数据集，使用的模型是GPTJ-6B。  \n\nDPO和ODPO训练后的评分：抽了100条测试prompt，用不同的temperature生成结果，并用GPT-4进行评分对比。结果如下  \n\n{% asset_img summarization.png summarization %}  \n\nDPO和ODPO都比SFT好，并且在temperature比较低的设置下，DPO和ODPO都比human-written的结果好。  \n\n## 消融实验：scaling function  \n\n前面实验的offset都是用reward差值的log值，这里使用其他两种计算方式进行对比  \n\n$$\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log r(\\boldsymbol{y}_l)$$  \n\n$$\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}$$  \n\n使用5000对sentiment control的数据，$\\beta \\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}$。  \n\n对比结果如下  \n\n{% asset_img scaling_function.png scaling function %}  \n\n使用log scaling的ODPO在KL divergence更小的时候（0.4）可以达到0.8的reward，而没有使用log scaling的模型需要再更大的KL divergence下才能达到通用的reward。  \n\n## 消融实验：α  \n\n同样使用7500对sentiment control的数据，$\\beta=0.5$，改变$\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}$。  \n\n{% asset_img alpha.png alpha %}  \n\n发现更高的 $\\alpha$ 会使得模型更多偏离SFT模型，并带来更高的reward值。  \n\n# 小结  \n\nODPO在DPO的基础上加入了offset，在实现上并不复杂，而且能带来一些收益。  \n\n略有瑕疵的是ODPO的实验覆盖面并不太全，也没有使用LLAMA等更强大的模型进行实验。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】Direct Preference Optimization with an Offset https://arxiv.org/pdf/2402.10571  ","slug":"cs/nlp/2024/05/大模型偏好对齐-ODPO","published":1,"updated":"2024-05-31T12:26:56.345Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9x000x314kbqercw0q","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>前面对DPO的思路做了整理：<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a>。</p>\n<p>DPO把RLHF的两阶段训练，变成了一阶段训练，降低了训练成本。而ODPO（DPO\nwith an\noffset）在DPO的基础上做了一点改进，在几个下游任务的实验中，获得了比DPO更好的效果。</p>\n<h1 id=\"背景\">背景</h1>\n<p>直接使用指令微调，是让模型学会处理下游任务的一个快速有效的方法。</p>\n<p>但是指令微调的优化目标是maximize the response\nlog-likelihood，这和“生成人类所偏好的高质量内容”的目标之间存在gap，不完全对齐。</p>\n<p>这个misalignment部分是因为maximum\nlikelihood的目标无法区分数据里“大错”（比如幻觉）和“小错”（比如标点符号不恰当）。</p>\n<blockquote>\n<p>Training with the maximum likelihood objective makes the model assign\nnonzero probability mass to all responses in SFT dataset, even those of\nlower quality.</p>\n</blockquote>\n<p>因此有RLHF的方法来解决这个问题。RL通过人类偏好数据训练一个reward模型，并用reward模型来指导策略模型。</p>\n<p>而reward的modeling有两种，pointwise reward和pairwise preference。</p>\n<p>pointwise\nreward一般用于reward有比较确定定义且简单的场景，比如情感分类，我们可以定义positive的情感的reward为1，negative的reward为0。类似的还有toxicity等。这些类别一般也有很多现成的打分模型/classifier可以使用。</p>\n<p>pairwise\npreference一般用于比较复杂的任务，比如文本摘要和对话生成。这类任务难以直接基于单个答案来打分，而需要通过对比才能知道哪个更好。</p>\n<p>但RLHF成本比较高，因此DPO对训练过程进行了简化。</p>\n<h1 id=\"bradleyterry-model的局限\">Bradley–Terry model的局限</h1>\n<p>DPO的损失如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})&amp;\n=-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&amp;=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}\\]</span></p>\n<p>其中</p>\n<p><span class=\"math display\">\\[\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}\\]</span></p>\n<p>是estimated reward。</p>\n<p>这个DPO损失的形式背后用到了Bradley–Terry\nmodel对偏好进行建模。而Bradley–Terry\nmodel只给出了一个response比另一个response好的概率，而没有告诉我们好的程度。</p>\n<p>而实际上我们很多偏好对比数据都提供了具体的分数，而不仅仅是排序信息。有这些具体分数我们就可以知道两条response之间是差一点点，还是差很多。</p>\n<p>那么把这个差距的信息引入到偏好的建模里，应该能带来收益，这也是ODPO的思路，而两个response之间的差距就是offset。</p>\n<h1 id=\"dpo-with-an-offset\">DPO with an Offset</h1>\n<p>给 <span class=\"math inline\">\\(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\)</span>\n分别加上Gumbel noise，即得到</p>\n<p><span class=\"math display\">\\[\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)\\]</span></p>\n<p><span class=\"math display\">\\[\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)\\]</span></p>\n<p>论文中证明了</p>\n<p><span class=\"math display\">\\[p\\big(\\tilde{r}_w-\\tilde{r}_l&gt;\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)\\]</span></p>\n<p>基于此，ODPO的损失函数表达成</p>\n<p><span class=\"math display\">\\[\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]\\]</span></p>\n<p>这相当于要求preferred response的estimated reward要比dispreferred\nresponse的estimated reward大，且要大offset值这么多。</p>\n<p>当offset=0的时候，ODPO的损失等价于DPO的损失。</p>\n<p>ODPO的这个做法和softmax margin loss/marginal\nloss有些相似，都是在原来loss的基础上，加上一个margin，加大对靠得比较近的数据对的penalization的力度。</p>\n<p>ODPO里，offset是两个response之间的actual reward的increasing scaling\nfunction。</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\alpha\\)</span> 是超参。</p>\n<img src=\"/da871ebe/odpo_intro.png\" class title=\"intro\">\n<h1 id=\"实验\">实验</h1>\n<p>论文在几个下游任务上做了实验。</p>\n<h2 id=\"sentiment-control\">sentiment control</h2>\n<p>首先是sentiment control的任务，即要求模型输出positive的response。</p>\n<p>先用GPT2-Large在IMDB\ndataset做了finetune，获得SFT模型。论文用一个现成的sentiment\nclassifier作为reward的打分模型，给response分别打分，分数如下计算</p>\n<p><span class=\"math display\">\\[r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) =\n1-p(\\text{negative}\\mid\\cdot)\\]</span></p>\n<p><span class=\"math display\">\\[r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) =\n1+p(\\text{positive}\\mid\\cdot)\\]</span></p>\n<p>有了reward打分数据之后，还要构造偏好数据对。这里把同一个prompt下生成的所有reward分数不同的response进行排列组合，获得偏好数据对。</p>\n<p>对于DPO，有这些偏好数据对就够了。而ODPO还需要一个offset，按如下方式计算：</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)\\]</span></p>\n<p>实验里把 <span class=\"math inline\">\\(\\alpha\\)</span> 设为1。</p>\n<p>实验中使用两个不同的random\nseed，从SFT模型里进行采样，从而得到了2份不同的偏好数据。</p>\n<p>而 <span class=\"math inline\">\\(\\beta\\)</span> 使用了14个不同的取值\n<span class=\"math inline\">\\(\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}\\)</span>\n进行实验。</p>\n<p>论文在2份数据集下分别使用不同的数据量进行训练（5000，7500,10000），这样DPO和ODPO分别有2×3×14=84个实验。</p>\n<p>每个实验计算模型生成结果的sentiment打分，以及和SFT模型的KL\ndivergence。结果如下图</p>\n<img src=\"/da871ebe/sentiment_control.png\" class title=\"sentiment control\">\n<p>我们希望模型在sentiment的打分上越高越好，同时不要和SFT模型有太大的差距，因此越靠近左上角的点越符合我们的要求。从结果上看，ODPO比DPO更好一些。</p>\n<h2 id=\"toxicity-control\">toxicity control</h2>\n<p>toxicity control任务和sentiment\ncontrol类似，要求模型的response的毒性尽量低。</p>\n<p>这次使用GPT-neo-2.7b模型，<span class=\"math inline\">\\(\\beta\\)</span>\n的取值范围为 <span class=\"math inline\">\\(\\{0.05,0.1,0.2,0.3,0.4,0.5\\}\\)</span>，使用从REALTOXICITYPROMPTS数据集里抽样的10000个毒性评分大于0.3的prompt。</p>\n<p>结果如下</p>\n<img src=\"/da871ebe/toxicity_control.png\" class title=\"toxicity control\">\n<p>在数据量较少的情况下（8000 &amp; 9000），ODPO效果更明显好。</p>\n<h2 id=\"summarization\">summarization</h2>\n<p>摘要任务使用REDDIT TL;DR数据集，使用的模型是GPTJ-6B。</p>\n<p>DPO和ODPO训练后的评分：抽了100条测试prompt，用不同的temperature生成结果，并用GPT-4进行评分对比。结果如下</p>\n<img src=\"/da871ebe/summarization.png\" class title=\"summarization\">\n<p>DPO和ODPO都比SFT好，并且在temperature比较低的设置下，DPO和ODPO都比human-written的结果好。</p>\n<h2 id=\"消融实验scaling-function\">消融实验：scaling function</h2>\n<p>前面实验的offset都是用reward差值的log值，这里使用其他两种计算方式进行对比</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log\nr(\\boldsymbol{y}_l)\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}\\]</span></p>\n<p>使用5000对sentiment control的数据，<span class=\"math inline\">\\(\\beta\n\\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}\\)</span>。</p>\n<p>对比结果如下</p>\n<img src=\"/da871ebe/scaling_function.png\" class title=\"scaling function\">\n<p>使用log scaling的ODPO在KL\ndivergence更小的时候（0.4）可以达到0.8的reward，而没有使用log\nscaling的模型需要再更大的KL divergence下才能达到通用的reward。</p>\n<h2 id=\"消融实验α\">消融实验：α</h2>\n<p>同样使用7500对sentiment control的数据，<span class=\"math inline\">\\(\\beta=0.5\\)</span>，改变<span class=\"math inline\">\\(\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}\\)</span>。</p>\n<img src=\"/da871ebe/alpha.png\" class title=\"alpha\">\n<p>发现更高的 <span class=\"math inline\">\\(\\alpha\\)</span>\n会使得模型更多偏离SFT模型，并带来更高的reward值。</p>\n<h1 id=\"小结\">小结</h1>\n<p>ODPO在DPO的基础上加入了offset，在实现上并不复杂，而且能带来一些收益。</p>\n<p>略有瑕疵的是ODPO的实验覆盖面并不太全，也没有使用LLAMA等更强大的模型进行实验。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Direct Preference Optimization with an Offset\nhttps://arxiv.org/pdf/2402.10571</p>\n","length":5653,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>前面对DPO的思路做了整理：<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a>。</p>\n<p>DPO把RLHF的两阶段训练，变成了一阶段训练，降低了训练成本。而ODPO（DPO\nwith an\noffset）在DPO的基础上做了一点改进，在几个下游任务的实验中，获得了比DPO更好的效果。</p>\n<h1 id=\"背景\">背景</h1>\n<p>直接使用指令微调，是让模型学会处理下游任务的一个快速有效的方法。</p>\n<p>但是指令微调的优化目标是maximize the response\nlog-likelihood，这和“生成人类所偏好的高质量内容”的目标之间存在gap，不完全对齐。</p>\n<p>这个misalignment部分是因为maximum\nlikelihood的目标无法区分数据里“大错”（比如幻觉）和“小错”（比如标点符号不恰当）。</p>\n<blockquote>\n<p>Training with the maximum likelihood objective makes the model assign\nnonzero probability mass to all responses in SFT dataset, even those of\nlower quality.</p>\n</blockquote>\n<p>因此有RLHF的方法来解决这个问题。RL通过人类偏好数据训练一个reward模型，并用reward模型来指导策略模型。</p>\n<p>而reward的modeling有两种，pointwise reward和pairwise preference。</p>\n<p>pointwise\nreward一般用于reward有比较确定定义且简单的场景，比如情感分类，我们可以定义positive的情感的reward为1，negative的reward为0。类似的还有toxicity等。这些类别一般也有很多现成的打分模型/classifier可以使用。</p>\n<p>pairwise\npreference一般用于比较复杂的任务，比如文本摘要和对话生成。这类任务难以直接基于单个答案来打分，而需要通过对比才能知道哪个更好。</p>\n<p>但RLHF成本比较高，因此DPO对训练过程进行了简化。</p>\n<h1 id=\"bradleyterry-model的局限\">Bradley–Terry model的局限</h1>\n<p>DPO的损失如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})&amp;\n=-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&amp;=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}\\]</span></p>\n<p>其中</p>\n<p><span class=\"math display\">\\[\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}\\]</span></p>\n<p>是estimated reward。</p>\n<p>这个DPO损失的形式背后用到了Bradley–Terry\nmodel对偏好进行建模。而Bradley–Terry\nmodel只给出了一个response比另一个response好的概率，而没有告诉我们好的程度。</p>\n<p>而实际上我们很多偏好对比数据都提供了具体的分数，而不仅仅是排序信息。有这些具体分数我们就可以知道两条response之间是差一点点，还是差很多。</p>\n<p>那么把这个差距的信息引入到偏好的建模里，应该能带来收益，这也是ODPO的思路，而两个response之间的差距就是offset。</p>\n<h1 id=\"dpo-with-an-offset\">DPO with an Offset</h1>\n<p>给 <span class=\"math inline\">\\(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\)</span>\n分别加上Gumbel noise，即得到</p>\n<p><span class=\"math display\">\\[\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)\\]</span></p>\n<p><span class=\"math display\">\\[\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)\\]</span></p>\n<p>论文中证明了</p>\n<p><span class=\"math display\">\\[p\\big(\\tilde{r}_w-\\tilde{r}_l&gt;\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)\\]</span></p>\n<p>基于此，ODPO的损失函数表达成</p>\n<p><span class=\"math display\">\\[\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]\\]</span></p>\n<p>这相当于要求preferred response的estimated reward要比dispreferred\nresponse的estimated reward大，且要大offset值这么多。</p>\n<p>当offset=0的时候，ODPO的损失等价于DPO的损失。</p>\n<p>ODPO的这个做法和softmax margin loss/marginal\nloss有些相似，都是在原来loss的基础上，加上一个margin，加大对靠得比较近的数据对的penalization的力度。</p>\n<p>ODPO里，offset是两个response之间的actual reward的increasing scaling\nfunction。</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\alpha\\)</span> 是超参。</p>\n<img src=\"/da871ebe/odpo_intro.png\" class title=\"intro\">\n<h1 id=\"实验\">实验</h1>\n<p>论文在几个下游任务上做了实验。</p>\n<h2 id=\"sentiment-control\">sentiment control</h2>\n<p>首先是sentiment control的任务，即要求模型输出positive的response。</p>\n<p>先用GPT2-Large在IMDB\ndataset做了finetune，获得SFT模型。论文用一个现成的sentiment\nclassifier作为reward的打分模型，给response分别打分，分数如下计算</p>\n<p><span class=\"math display\">\\[r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) =\n1-p(\\text{negative}\\mid\\cdot)\\]</span></p>\n<p><span class=\"math display\">\\[r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) =\n1+p(\\text{positive}\\mid\\cdot)\\]</span></p>\n<p>有了reward打分数据之后，还要构造偏好数据对。这里把同一个prompt下生成的所有reward分数不同的response进行排列组合，获得偏好数据对。</p>\n<p>对于DPO，有这些偏好数据对就够了。而ODPO还需要一个offset，按如下方式计算：</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)\\]</span></p>\n<p>实验里把 <span class=\"math inline\">\\(\\alpha\\)</span> 设为1。</p>\n<p>实验中使用两个不同的random\nseed，从SFT模型里进行采样，从而得到了2份不同的偏好数据。</p>\n<p>而 <span class=\"math inline\">\\(\\beta\\)</span> 使用了14个不同的取值\n<span class=\"math inline\">\\(\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}\\)</span>\n进行实验。</p>\n<p>论文在2份数据集下分别使用不同的数据量进行训练（5000，7500,10000），这样DPO和ODPO分别有2×3×14=84个实验。</p>\n<p>每个实验计算模型生成结果的sentiment打分，以及和SFT模型的KL\ndivergence。结果如下图</p>\n<img src=\"/da871ebe/sentiment_control.png\" class title=\"sentiment control\">\n<p>我们希望模型在sentiment的打分上越高越好，同时不要和SFT模型有太大的差距，因此越靠近左上角的点越符合我们的要求。从结果上看，ODPO比DPO更好一些。</p>\n<h2 id=\"toxicity-control\">toxicity control</h2>\n<p>toxicity control任务和sentiment\ncontrol类似，要求模型的response的毒性尽量低。</p>\n<p>这次使用GPT-neo-2.7b模型，<span class=\"math inline\">\\(\\beta\\)</span>\n的取值范围为 <span class=\"math inline\">\\(\\{0.05,0.1,0.2,0.3,0.4,0.5\\}\\)</span>，使用从REALTOXICITYPROMPTS数据集里抽样的10000个毒性评分大于0.3的prompt。</p>\n<p>结果如下</p>\n<img src=\"/da871ebe/toxicity_control.png\" class title=\"toxicity control\">\n<p>在数据量较少的情况下（8000 &amp; 9000），ODPO效果更明显好。</p>\n<h2 id=\"summarization\">summarization</h2>\n<p>摘要任务使用REDDIT TL;DR数据集，使用的模型是GPTJ-6B。</p>\n<p>DPO和ODPO训练后的评分：抽了100条测试prompt，用不同的temperature生成结果，并用GPT-4进行评分对比。结果如下</p>\n<img src=\"/da871ebe/summarization.png\" class title=\"summarization\">\n<p>DPO和ODPO都比SFT好，并且在temperature比较低的设置下，DPO和ODPO都比human-written的结果好。</p>\n<h2 id=\"消融实验scaling-function\">消融实验：scaling function</h2>\n<p>前面实验的offset都是用reward差值的log值，这里使用其他两种计算方式进行对比</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log\nr(\\boldsymbol{y}_l)\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}\\]</span></p>\n<p>使用5000对sentiment control的数据，<span class=\"math inline\">\\(\\beta\n\\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}\\)</span>。</p>\n<p>对比结果如下</p>\n<img src=\"/da871ebe/scaling_function.png\" class title=\"scaling function\">\n<p>使用log scaling的ODPO在KL\ndivergence更小的时候（0.4）可以达到0.8的reward，而没有使用log\nscaling的模型需要再更大的KL divergence下才能达到通用的reward。</p>\n<h2 id=\"消融实验α\">消融实验：α</h2>\n<p>同样使用7500对sentiment control的数据，<span class=\"math inline\">\\(\\beta=0.5\\)</span>，改变<span class=\"math inline\">\\(\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}\\)</span>。</p>\n<img src=\"/da871ebe/alpha.png\" class title=\"alpha\">\n<p>发现更高的 <span class=\"math inline\">\\(\\alpha\\)</span>\n会使得模型更多偏离SFT模型，并带来更高的reward值。</p>\n<h1 id=\"小结\">小结</h1>\n<p>ODPO在DPO的基础上加入了offset，在实现上并不复杂，而且能带来一些收益。</p>\n<p>略有瑕疵的是ODPO的实验覆盖面并不太全，也没有使用LLAMA等更强大的模型进行实验。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Direct Preference Optimization with an Offset\nhttps://arxiv.org/pdf/2402.10571</p>\n"},{"title":"大模型偏好对齐-simPO","abbrlink":"280fa97a","date":"2024-05-31T14:09:23.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n前面我们对DPO和ODPO的思路做了整理：[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)，[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)。  \n\n最近新出的simPO受到了很多关注。相比DPO，simPO不需要reference model，并且有更好的效果。simPO的另一个好处是，能够保持生成结果在较短长度下的质量。  \n\n{% asset_img intro.png simPO %}  \n\n# DPO的局限  \n\n回顾一下DPO。DPO的reward function有一个closed-form expression  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n基于此，通过Bradley-Terry model进行建模，得到损失函数  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\n理论上，DPO的优化目标和RLHF是一致的，但是DPO有两个缺陷：  \n- 仍然需要一个reference model，这样依然有比较大的内存和计算开销  \n- 训练过程中优化的reward和推理时的生成指标存在差异，也就是训练和推理的目标不完全对齐  \n\n第二点怎么理解呢？模型在自回归生成response时，理论上是寻找最大化所有token平均log likelihood的组合，即  \n\n$$\\begin{aligned}p_\\theta(y\\mid x)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\n当然实际上这个组合空间太大了，没法直接遍历寻找，因此会使用一些解码策略来寻找局部最优解，比如greedy decoding、beam search或者top-k sampling等，不过我们还是可以按这个公式近似计算。另外这个公式还是可用在多个response/多选题的排序上的。  \n\n可以看到推理时的这个目标和DPO的reward差了个referenc model。那么在DPO里，满足 $r(x,y_w)>r(x,y_l)$ 的偏好数据并不一定意味着 $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$。  \n\n论文做了一个统计，对于DPO，满足 $r(x,y_w)>r(x,y_l)$ 和 $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$ 两个结果对齐的比例大概只有50%，如下图所示  \n\n{% asset_img contingency_table.png contingency table %}  \n\n这就是训练和推理目标没有完全对齐。  \n\n而simPO则可以完全对齐  \n\n{% asset_img simpo_contingency.png simPO contingency table %}  \n\n# simPO  \n\n## 损失函数  \n\n从上面这个分析，我们自然就想到要把训练的目标往推理目标上靠拢对齐。那么最直接的做法，就是把reward从  \n\n$$\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}$$  \n\n（这里省略了配分函数Z）\n\n变成  \n\n$$\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\n注意这里有个长度归一化项，这个很重要，没有这一项的话，模型会倾向于生成长度更长但是低质量的内容。  \n\n除了修改reward的计算，simPO和IPO、ODPO一样，引入了一个reward margin，这是一个固定的超参，要求winning response和losing response的reward差值要大于reward margin  \n\n$$p(y_w\\succ y_l\\mid x)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)$$  \n\n按已有的经验，增大这个margin有助于提高模型泛化能力，但是太大的margin也会导致模型的退化。  \n\n至此我们得到了simPO的损失函数  \n\n$$\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]$$  \n\n## simPO梯度更新的直观理解  \n\nDPO和simPO的梯度如下  \n\n{% asset_img gradient.png 梯度 %}  \n\nDPO和simPO的梯度有两个主要区别：  \n- 梯度权重：simPO的梯度权重没有包含reference model，这样当policy model给dispreferred response更高的reward的时候，权重就会变大，加强对这个错误case的修正力度。  \n- simPO的梯度更新带有length-normalized；而如《Disentangling length from quality in direct preference optimization》所发现，DPO里更长的token会有更大的梯度值从而主导了梯度更新的过程，这导致训练出来的模型倾向于生成更长的模型。  \n\n# 实验  \n\n## 设置  \n\n论文使用了Llama3-8B和Mistral-7B的base和instruct模型进行实验。  \n\n对于base模型，就先在UltraChat-200k数据集上训练一个对应的SFT模型，之后在 UltraFeedback数据集上进行preference optimization。  \n\n对于instruct模型，参照《Iterative DPO alignment》的做法，先用这些SFT模型生成preference数据集。具体来说，使用UltraFeedback的prompt，用temperature=0.8的配置，从SFT模型生成5个response，并用PairRM（《LLM-Blender: Ensembling large language models with pairwise ranking and generative fusion》）对这5个response进行打分，选择最高分作为preferred response，最低分的座位dispreferred response。  \n\n这样就得到了四组实验组合：Llama3-Base, Llama3-Instruct, Mistral-Base和Mistral-Instruct。  \n\n此外，论文发现超参对preference optimization的影响很大，因此对不同的方法进行了超参搜索，范围如下  \n\n{% asset_img hyperparameters.png 超参搜索 %}  \n\n{% asset_img simpo_hyperparameters.png 超参搜索 %}  \n\n此外对batch size、解码温度等参数也进行搜索。  \n\n所用的数据集如下  \n\n{% asset_img benchmark.png benchmark %}  \n\n## 对比  \n\n在各个数据集上，不同的优化方法结果对比如下  \n\n{% asset_img main_results.png 对比结果 %}  \n\n其中LC表示length-controlled，即在限制长度条件下的win rate。  \n\n有几个发现：  \n- 在MT-Bench上，各个方法的差异不大，那些微小的波动可能更多来自于随机性。究其原因可能是因为这个数据集的量比较少，且评价的方案也比较单一，这个发现和《From live data to high-quality benchmarks: The Arena-Hard pipeline》的发现是一致的。  \n- instruct模型的表现比base要好，这可能是因为这些精心微调过甚至强化学习过的模型本身质量更高。  \n- 在AlpacaEval 2和Arena-Hard上，simPO在raw win rate和length-controlled win rate相比其他方案都有明显优势。  \n\n## 消融实验  \n\nsimPO两个主要的部分就是length normalization和margin。分别去掉这两个部分之后的结果如下表  \n\n{% asset_img ablation.png 消融实验 %}  \n\n结果上看，length normalization的影响很大，margin也有一定的影响。  \n\n下面具体分析一下。  \n\n首先是关于长度归一化。从上表的结果上看，对于simPO，使用长度归一化会让模型生成更短且质量更高的结果。  \n\n对比其他训练方法，simPO在长度控制下的win rate有明显优势，这说明simPO实现了对生成长度的最小利用，即不通过长篇大论来提高得分。  \n\n而通用来说，生成结果的长度和质量之间并没有什么强联系。如下表所示，各个训练方法的生成长度和wr并没有什么明显规律，这表明，生成结果的长度并不是衡量生成质量的一个可靠指标。  \n\n{% asset_img ln.png 长度归一化 %}  \n\n此外，长度归一化会增大偏好数据对之间的reward差。这个很好理解，在有长度归一化的损失函数下，想要达到相同的reward差，模型需要给出y倍的数值才能比margin大。  \n\n论文把在不同的长度差异下的reward差画出来，如下图所示  \n\n{% asset_img ln_effect.png 长度归一化 %}  \n\n可以发现带有长度归一化的simPO无论数据的长度差如何，都能给出positive reward margin，而没有带长度归一化的模型在winning response的长度更短的情况下，会给出negative reward difference，这表明模型对这些样本的学习效果很差。  \n\n而从上图b和c子图可以看出，移除长度归一化会使得reward和response length呈现强烈的正相关关系，而这显然不是我们想要的。  \n\n接下来看下reward margin的影响。  \n\n把reward accuracy定义为policy model对winning response的reward高于losing response的比例。那么如下图所示，随着margin的增大，reward accuracy也在提升  \n\n{% asset_img reward_accuracy.png reward accuracy %}  \n\n另外实验还发现，增大reward margin，会使得reward difference和winning response的平均对数似然的分布变得扁平，且winning response的平均对数似然会减小，如下图所示  \n\n{% asset_img margin_dist.png 影响分布 %}  \n\n这说明太大的margin设置对模型会有负面影响，因此需要寻找一个中间值使得模型效果最好。  \n\n## DPO和simPO的对比  \n\n1. 虽然DPO的reward表达式里没有显式涵盖长度归一化的信息，但是由于使用了reference model进行对比，在一定程度上可以对抗length bias。如下图所示，DPO在一定程度上可以打破长度和reward之间的正相关关系，但是没有simPO的效果那么好  \n\n{% asset_img dpo_correlation.png correlation %}  \n\n2. simPO比DPO有更高的reward accuracy，这表明simPO的reward设计有更强的泛化能力，可以提供更高质量的生成能力  \n\n{% asset_img reward_accuracy_compare.png reward accuracy对比 %}  \n\n# 小结  \n\nsimPO对损失函数做了一些改变，对齐了训练和推理的目标，使得policy model能够在提升效果的同时，不过分影响生成结果的长度。并且simPO不再需要reference model，这也使得训练的空间成本更加节省。  \n\n论文在LLAMA和Mistral两个热门的模型上进行了比较多的实验，比较有说服力。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】SimPO: Simple Preference Optimization with a Reference-Free Reward https://arxiv.org/abs/2405.14734  \n","source":"_posts/cs/nlp/2024/05/大模型偏好对齐-simPO.md","raw":"---\ntitle: 大模型偏好对齐-simPO\nabbrlink: 280fa97a\ndate: 2024-05-31 22:09:23\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 强化学习\n  - 微调\n  - SFT\n  - 偏好对齐\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***  \n\n前面我们对DPO和ODPO的思路做了整理：[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)，[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)。  \n\n最近新出的simPO受到了很多关注。相比DPO，simPO不需要reference model，并且有更好的效果。simPO的另一个好处是，能够保持生成结果在较短长度下的质量。  \n\n{% asset_img intro.png simPO %}  \n\n# DPO的局限  \n\n回顾一下DPO。DPO的reward function有一个closed-form expression  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n基于此，通过Bradley-Terry model进行建模，得到损失函数  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\n理论上，DPO的优化目标和RLHF是一致的，但是DPO有两个缺陷：  \n- 仍然需要一个reference model，这样依然有比较大的内存和计算开销  \n- 训练过程中优化的reward和推理时的生成指标存在差异，也就是训练和推理的目标不完全对齐  \n\n第二点怎么理解呢？模型在自回归生成response时，理论上是寻找最大化所有token平均log likelihood的组合，即  \n\n$$\\begin{aligned}p_\\theta(y\\mid x)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\n当然实际上这个组合空间太大了，没法直接遍历寻找，因此会使用一些解码策略来寻找局部最优解，比如greedy decoding、beam search或者top-k sampling等，不过我们还是可以按这个公式近似计算。另外这个公式还是可用在多个response/多选题的排序上的。  \n\n可以看到推理时的这个目标和DPO的reward差了个referenc model。那么在DPO里，满足 $r(x,y_w)>r(x,y_l)$ 的偏好数据并不一定意味着 $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$。  \n\n论文做了一个统计，对于DPO，满足 $r(x,y_w)>r(x,y_l)$ 和 $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$ 两个结果对齐的比例大概只有50%，如下图所示  \n\n{% asset_img contingency_table.png contingency table %}  \n\n这就是训练和推理目标没有完全对齐。  \n\n而simPO则可以完全对齐  \n\n{% asset_img simpo_contingency.png simPO contingency table %}  \n\n# simPO  \n\n## 损失函数  \n\n从上面这个分析，我们自然就想到要把训练的目标往推理目标上靠拢对齐。那么最直接的做法，就是把reward从  \n\n$$\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}$$  \n\n（这里省略了配分函数Z）\n\n变成  \n\n$$\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\n注意这里有个长度归一化项，这个很重要，没有这一项的话，模型会倾向于生成长度更长但是低质量的内容。  \n\n除了修改reward的计算，simPO和IPO、ODPO一样，引入了一个reward margin，这是一个固定的超参，要求winning response和losing response的reward差值要大于reward margin  \n\n$$p(y_w\\succ y_l\\mid x)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)$$  \n\n按已有的经验，增大这个margin有助于提高模型泛化能力，但是太大的margin也会导致模型的退化。  \n\n至此我们得到了simPO的损失函数  \n\n$$\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]$$  \n\n## simPO梯度更新的直观理解  \n\nDPO和simPO的梯度如下  \n\n{% asset_img gradient.png 梯度 %}  \n\nDPO和simPO的梯度有两个主要区别：  \n- 梯度权重：simPO的梯度权重没有包含reference model，这样当policy model给dispreferred response更高的reward的时候，权重就会变大，加强对这个错误case的修正力度。  \n- simPO的梯度更新带有length-normalized；而如《Disentangling length from quality in direct preference optimization》所发现，DPO里更长的token会有更大的梯度值从而主导了梯度更新的过程，这导致训练出来的模型倾向于生成更长的模型。  \n\n# 实验  \n\n## 设置  \n\n论文使用了Llama3-8B和Mistral-7B的base和instruct模型进行实验。  \n\n对于base模型，就先在UltraChat-200k数据集上训练一个对应的SFT模型，之后在 UltraFeedback数据集上进行preference optimization。  \n\n对于instruct模型，参照《Iterative DPO alignment》的做法，先用这些SFT模型生成preference数据集。具体来说，使用UltraFeedback的prompt，用temperature=0.8的配置，从SFT模型生成5个response，并用PairRM（《LLM-Blender: Ensembling large language models with pairwise ranking and generative fusion》）对这5个response进行打分，选择最高分作为preferred response，最低分的座位dispreferred response。  \n\n这样就得到了四组实验组合：Llama3-Base, Llama3-Instruct, Mistral-Base和Mistral-Instruct。  \n\n此外，论文发现超参对preference optimization的影响很大，因此对不同的方法进行了超参搜索，范围如下  \n\n{% asset_img hyperparameters.png 超参搜索 %}  \n\n{% asset_img simpo_hyperparameters.png 超参搜索 %}  \n\n此外对batch size、解码温度等参数也进行搜索。  \n\n所用的数据集如下  \n\n{% asset_img benchmark.png benchmark %}  \n\n## 对比  \n\n在各个数据集上，不同的优化方法结果对比如下  \n\n{% asset_img main_results.png 对比结果 %}  \n\n其中LC表示length-controlled，即在限制长度条件下的win rate。  \n\n有几个发现：  \n- 在MT-Bench上，各个方法的差异不大，那些微小的波动可能更多来自于随机性。究其原因可能是因为这个数据集的量比较少，且评价的方案也比较单一，这个发现和《From live data to high-quality benchmarks: The Arena-Hard pipeline》的发现是一致的。  \n- instruct模型的表现比base要好，这可能是因为这些精心微调过甚至强化学习过的模型本身质量更高。  \n- 在AlpacaEval 2和Arena-Hard上，simPO在raw win rate和length-controlled win rate相比其他方案都有明显优势。  \n\n## 消融实验  \n\nsimPO两个主要的部分就是length normalization和margin。分别去掉这两个部分之后的结果如下表  \n\n{% asset_img ablation.png 消融实验 %}  \n\n结果上看，length normalization的影响很大，margin也有一定的影响。  \n\n下面具体分析一下。  \n\n首先是关于长度归一化。从上表的结果上看，对于simPO，使用长度归一化会让模型生成更短且质量更高的结果。  \n\n对比其他训练方法，simPO在长度控制下的win rate有明显优势，这说明simPO实现了对生成长度的最小利用，即不通过长篇大论来提高得分。  \n\n而通用来说，生成结果的长度和质量之间并没有什么强联系。如下表所示，各个训练方法的生成长度和wr并没有什么明显规律，这表明，生成结果的长度并不是衡量生成质量的一个可靠指标。  \n\n{% asset_img ln.png 长度归一化 %}  \n\n此外，长度归一化会增大偏好数据对之间的reward差。这个很好理解，在有长度归一化的损失函数下，想要达到相同的reward差，模型需要给出y倍的数值才能比margin大。  \n\n论文把在不同的长度差异下的reward差画出来，如下图所示  \n\n{% asset_img ln_effect.png 长度归一化 %}  \n\n可以发现带有长度归一化的simPO无论数据的长度差如何，都能给出positive reward margin，而没有带长度归一化的模型在winning response的长度更短的情况下，会给出negative reward difference，这表明模型对这些样本的学习效果很差。  \n\n而从上图b和c子图可以看出，移除长度归一化会使得reward和response length呈现强烈的正相关关系，而这显然不是我们想要的。  \n\n接下来看下reward margin的影响。  \n\n把reward accuracy定义为policy model对winning response的reward高于losing response的比例。那么如下图所示，随着margin的增大，reward accuracy也在提升  \n\n{% asset_img reward_accuracy.png reward accuracy %}  \n\n另外实验还发现，增大reward margin，会使得reward difference和winning response的平均对数似然的分布变得扁平，且winning response的平均对数似然会减小，如下图所示  \n\n{% asset_img margin_dist.png 影响分布 %}  \n\n这说明太大的margin设置对模型会有负面影响，因此需要寻找一个中间值使得模型效果最好。  \n\n## DPO和simPO的对比  \n\n1. 虽然DPO的reward表达式里没有显式涵盖长度归一化的信息，但是由于使用了reference model进行对比，在一定程度上可以对抗length bias。如下图所示，DPO在一定程度上可以打破长度和reward之间的正相关关系，但是没有simPO的效果那么好  \n\n{% asset_img dpo_correlation.png correlation %}  \n\n2. simPO比DPO有更高的reward accuracy，这表明simPO的reward设计有更强的泛化能力，可以提供更高质量的生成能力  \n\n{% asset_img reward_accuracy_compare.png reward accuracy对比 %}  \n\n# 小结  \n\nsimPO对损失函数做了一些改变，对齐了训练和推理的目标，使得policy model能够在提升效果的同时，不过分影响生成结果的长度。并且simPO不再需要reference model，这也使得训练的空间成本更加节省。  \n\n论文在LLAMA和Mistral两个热门的模型上进行了比较多的实验，比较有说服力。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】  \n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  \n[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  \n[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n[大模型算法题(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n【1】SimPO: Simple Preference Optimization with a Reference-Free Reward https://arxiv.org/abs/2405.14734  \n","slug":"cs/nlp/2024/05/大模型偏好对齐-simPO","published":1,"updated":"2024-06-02T04:02:19.947Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9y0010314kdv0ae4sd","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>前面我们对DPO和ODPO的思路做了整理：<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a>，<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a>。</p>\n<p>最近新出的simPO受到了很多关注。相比DPO，simPO不需要reference\nmodel，并且有更好的效果。simPO的另一个好处是，能够保持生成结果在较短长度下的质量。</p>\n<img src=\"/280fa97a/intro.png\" class title=\"simPO\">\n<h1 id=\"dpo的局限\">DPO的局限</h1>\n<p>回顾一下DPO。DPO的reward function有一个closed-form expression</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>基于此，通过Bradley-Terry model进行建模，得到损失函数</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>理论上，DPO的优化目标和RLHF是一致的，但是DPO有两个缺陷：<br>\n- 仍然需要一个reference model，这样依然有比较大的内存和计算开销<br>\n-\n训练过程中优化的reward和推理时的生成指标存在差异，也就是训练和推理的目标不完全对齐</p>\n<p>第二点怎么理解呢？模型在自回归生成response时，理论上是寻找最大化所有token平均log\nlikelihood的组合，即</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p>当然实际上这个组合空间太大了，没法直接遍历寻找，因此会使用一些解码策略来寻找局部最优解，比如greedy\ndecoding、beam search或者top-k\nsampling等，不过我们还是可以按这个公式近似计算。另外这个公式还是可用在多个response/多选题的排序上的。</p>\n<p>可以看到推理时的这个目标和DPO的reward差了个referenc\nmodel。那么在DPO里，满足 <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span>\n的偏好数据并不一定意味着 <span class=\"math inline\">\\(p_\\theta(y_w\\mid\nx)&gt;p_\\theta(y_l\\mid x)\\)</span>。</p>\n<p>论文做了一个统计，对于DPO，满足 <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span> 和 <span class=\"math inline\">\\(p_\\theta(y_w\\mid x)&gt;p_\\theta(y_l\\mid\nx)\\)</span> 两个结果对齐的比例大概只有50%，如下图所示</p>\n<img src=\"/280fa97a/contingency_table.png\" class title=\"contingency table\">\n<p>这就是训练和推理目标没有完全对齐。</p>\n<p>而simPO则可以完全对齐</p>\n<img src=\"/280fa97a/simpo_contingency.png\" class title=\"simPO contingency table\">\n<h1 id=\"simpo\">simPO</h1>\n<h2 id=\"损失函数\">损失函数</h2>\n<p>从上面这个分析，我们自然就想到要把训练的目标往推理目标上靠拢对齐。那么最直接的做法，就是把reward从</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}\\]</span></p>\n<p>（这里省略了配分函数Z）</p>\n<p>变成</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p>注意这里有个长度归一化项，这个很重要，没有这一项的话，模型会倾向于生成长度更长但是低质量的内容。</p>\n<p>除了修改reward的计算，simPO和IPO、ODPO一样，引入了一个reward\nmargin，这是一个固定的超参，要求winning response和losing\nresponse的reward差值要大于reward margin</p>\n<p><span class=\"math display\">\\[p(y_w\\succ y_l\\mid\nx)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)\\]</span></p>\n<p>按已有的经验，增大这个margin有助于提高模型泛化能力，但是太大的margin也会导致模型的退化。</p>\n<p>至此我们得到了simPO的损失函数</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]\\]</span></p>\n<h2 id=\"simpo梯度更新的直观理解\">simPO梯度更新的直观理解</h2>\n<p>DPO和simPO的梯度如下</p>\n<img src=\"/280fa97a/gradient.png\" class title=\"梯度\">\n<p>DPO和simPO的梯度有两个主要区别：<br>\n- 梯度权重：simPO的梯度权重没有包含reference model，这样当policy\nmodel给dispreferred\nresponse更高的reward的时候，权重就会变大，加强对这个错误case的修正力度。<br>\n- simPO的梯度更新带有length-normalized；而如《Disentangling length from\nquality in direct preference\noptimization》所发现，DPO里更长的token会有更大的梯度值从而主导了梯度更新的过程，这导致训练出来的模型倾向于生成更长的模型。</p>\n<h1 id=\"实验\">实验</h1>\n<h2 id=\"设置\">设置</h2>\n<p>论文使用了Llama3-8B和Mistral-7B的base和instruct模型进行实验。</p>\n<p>对于base模型，就先在UltraChat-200k数据集上训练一个对应的SFT模型，之后在\nUltraFeedback数据集上进行preference optimization。</p>\n<p>对于instruct模型，参照《Iterative DPO\nalignment》的做法，先用这些SFT模型生成preference数据集。具体来说，使用UltraFeedback的prompt，用temperature=0.8的配置，从SFT模型生成5个response，并用PairRM（《LLM-Blender:\nEnsembling large language models with pairwise ranking and generative\nfusion》）对这5个response进行打分，选择最高分作为preferred\nresponse，最低分的座位dispreferred response。</p>\n<p>这样就得到了四组实验组合：Llama3-Base, Llama3-Instruct,\nMistral-Base和Mistral-Instruct。</p>\n<p>此外，论文发现超参对preference\noptimization的影响很大，因此对不同的方法进行了超参搜索，范围如下</p>\n<img src=\"/280fa97a/hyperparameters.png\" class title=\"超参搜索\">\n<img src=\"/280fa97a/simpo_hyperparameters.png\" class title=\"超参搜索\">\n<p>此外对batch size、解码温度等参数也进行搜索。</p>\n<p>所用的数据集如下</p>\n<img src=\"/280fa97a/benchmark.png\" class title=\"benchmark\">\n<h2 id=\"对比\">对比</h2>\n<p>在各个数据集上，不同的优化方法结果对比如下</p>\n<img src=\"/280fa97a/main_results.png\" class title=\"对比结果\">\n<p>其中LC表示length-controlled，即在限制长度条件下的win rate。</p>\n<p>有几个发现：<br>\n-\n在MT-Bench上，各个方法的差异不大，那些微小的波动可能更多来自于随机性。究其原因可能是因为这个数据集的量比较少，且评价的方案也比较单一，这个发现和《From\nlive data to high-quality benchmarks: The Arena-Hard\npipeline》的发现是一致的。<br>\n-\ninstruct模型的表现比base要好，这可能是因为这些精心微调过甚至强化学习过的模型本身质量更高。<br>\n- 在AlpacaEval 2和Arena-Hard上，simPO在raw win rate和length-controlled\nwin rate相比其他方案都有明显优势。</p>\n<h2 id=\"消融实验\">消融实验</h2>\n<p>simPO两个主要的部分就是length\nnormalization和margin。分别去掉这两个部分之后的结果如下表</p>\n<img src=\"/280fa97a/ablation.png\" class title=\"消融实验\">\n<p>结果上看，length normalization的影响很大，margin也有一定的影响。</p>\n<p>下面具体分析一下。</p>\n<p>首先是关于长度归一化。从上表的结果上看，对于simPO，使用长度归一化会让模型生成更短且质量更高的结果。</p>\n<p>对比其他训练方法，simPO在长度控制下的win\nrate有明显优势，这说明simPO实现了对生成长度的最小利用，即不通过长篇大论来提高得分。</p>\n<p>而通用来说，生成结果的长度和质量之间并没有什么强联系。如下表所示，各个训练方法的生成长度和wr并没有什么明显规律，这表明，生成结果的长度并不是衡量生成质量的一个可靠指标。</p>\n<img src=\"/280fa97a/ln.png\" class title=\"长度归一化\">\n<p>此外，长度归一化会增大偏好数据对之间的reward差。这个很好理解，在有长度归一化的损失函数下，想要达到相同的reward差，模型需要给出y倍的数值才能比margin大。</p>\n<p>论文把在不同的长度差异下的reward差画出来，如下图所示</p>\n<img src=\"/280fa97a/ln_effect.png\" class title=\"长度归一化\">\n<p>可以发现带有长度归一化的simPO无论数据的长度差如何，都能给出positive\nreward margin，而没有带长度归一化的模型在winning\nresponse的长度更短的情况下，会给出negative reward\ndifference，这表明模型对这些样本的学习效果很差。</p>\n<p>而从上图b和c子图可以看出，移除长度归一化会使得reward和response\nlength呈现强烈的正相关关系，而这显然不是我们想要的。</p>\n<p>接下来看下reward margin的影响。</p>\n<p>把reward accuracy定义为policy model对winning\nresponse的reward高于losing\nresponse的比例。那么如下图所示，随着margin的增大，reward\naccuracy也在提升</p>\n<img src=\"/280fa97a/reward_accuracy.png\" class title=\"reward accuracy\">\n<p>另外实验还发现，增大reward margin，会使得reward difference和winning\nresponse的平均对数似然的分布变得扁平，且winning\nresponse的平均对数似然会减小，如下图所示</p>\n<img src=\"/280fa97a/margin_dist.png\" class title=\"影响分布\">\n<p>这说明太大的margin设置对模型会有负面影响，因此需要寻找一个中间值使得模型效果最好。</p>\n<h2 id=\"dpo和simpo的对比\">DPO和simPO的对比</h2>\n<ol type=\"1\">\n<li>虽然DPO的reward表达式里没有显式涵盖长度归一化的信息，但是由于使用了reference\nmodel进行对比，在一定程度上可以对抗length\nbias。如下图所示，DPO在一定程度上可以打破长度和reward之间的正相关关系，但是没有simPO的效果那么好</li>\n</ol>\n<img src=\"/280fa97a/dpo_correlation.png\" class title=\"correlation\">\n<ol start=\"2\" type=\"1\">\n<li>simPO比DPO有更高的reward\naccuracy，这表明simPO的reward设计有更强的泛化能力，可以提供更高质量的生成能力</li>\n</ol>\n<img src=\"/280fa97a/reward_accuracy_compare.png\" class title=\"reward accuracy对比\">\n<h1 id=\"小结\">小结</h1>\n<p>simPO对损失函数做了一些改变，对齐了训练和推理的目标，使得policy\nmodel能够在提升效果的同时，不过分影响生成结果的长度。并且simPO不再需要reference\nmodel，这也使得训练的空间成本更加节省。</p>\n<p>论文在LLAMA和Mistral两个热门的模型上进行了比较多的实验，比较有说服力。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】SimPO: Simple Preference Optimization with a Reference-Free\nReward https://arxiv.org/abs/2405.14734</p>\n","length":5196,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>前面我们对DPO和ODPO的思路做了整理：<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a>，<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a>。</p>\n<p>最近新出的simPO受到了很多关注。相比DPO，simPO不需要reference\nmodel，并且有更好的效果。simPO的另一个好处是，能够保持生成结果在较短长度下的质量。</p>\n<img src=\"/280fa97a/intro.png\" class title=\"simPO\">\n<h1 id=\"dpo的局限\">DPO的局限</h1>\n<p>回顾一下DPO。DPO的reward function有一个closed-form expression</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>基于此，通过Bradley-Terry model进行建模，得到损失函数</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>理论上，DPO的优化目标和RLHF是一致的，但是DPO有两个缺陷：<br>\n- 仍然需要一个reference model，这样依然有比较大的内存和计算开销<br>\n-\n训练过程中优化的reward和推理时的生成指标存在差异，也就是训练和推理的目标不完全对齐</p>\n<p>第二点怎么理解呢？模型在自回归生成response时，理论上是寻找最大化所有token平均log\nlikelihood的组合，即</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p>当然实际上这个组合空间太大了，没法直接遍历寻找，因此会使用一些解码策略来寻找局部最优解，比如greedy\ndecoding、beam search或者top-k\nsampling等，不过我们还是可以按这个公式近似计算。另外这个公式还是可用在多个response/多选题的排序上的。</p>\n<p>可以看到推理时的这个目标和DPO的reward差了个referenc\nmodel。那么在DPO里，满足 <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span>\n的偏好数据并不一定意味着 <span class=\"math inline\">\\(p_\\theta(y_w\\mid\nx)&gt;p_\\theta(y_l\\mid x)\\)</span>。</p>\n<p>论文做了一个统计，对于DPO，满足 <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span> 和 <span class=\"math inline\">\\(p_\\theta(y_w\\mid x)&gt;p_\\theta(y_l\\mid\nx)\\)</span> 两个结果对齐的比例大概只有50%，如下图所示</p>\n<img src=\"/280fa97a/contingency_table.png\" class title=\"contingency table\">\n<p>这就是训练和推理目标没有完全对齐。</p>\n<p>而simPO则可以完全对齐</p>\n<img src=\"/280fa97a/simpo_contingency.png\" class title=\"simPO contingency table\">\n<h1 id=\"simpo\">simPO</h1>\n<h2 id=\"损失函数\">损失函数</h2>\n<p>从上面这个分析，我们自然就想到要把训练的目标往推理目标上靠拢对齐。那么最直接的做法，就是把reward从</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}\\]</span></p>\n<p>（这里省略了配分函数Z）</p>\n<p>变成</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p>注意这里有个长度归一化项，这个很重要，没有这一项的话，模型会倾向于生成长度更长但是低质量的内容。</p>\n<p>除了修改reward的计算，simPO和IPO、ODPO一样，引入了一个reward\nmargin，这是一个固定的超参，要求winning response和losing\nresponse的reward差值要大于reward margin</p>\n<p><span class=\"math display\">\\[p(y_w\\succ y_l\\mid\nx)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)\\]</span></p>\n<p>按已有的经验，增大这个margin有助于提高模型泛化能力，但是太大的margin也会导致模型的退化。</p>\n<p>至此我们得到了simPO的损失函数</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]\\]</span></p>\n<h2 id=\"simpo梯度更新的直观理解\">simPO梯度更新的直观理解</h2>\n<p>DPO和simPO的梯度如下</p>\n<img src=\"/280fa97a/gradient.png\" class title=\"梯度\">\n<p>DPO和simPO的梯度有两个主要区别：<br>\n- 梯度权重：simPO的梯度权重没有包含reference model，这样当policy\nmodel给dispreferred\nresponse更高的reward的时候，权重就会变大，加强对这个错误case的修正力度。<br>\n- simPO的梯度更新带有length-normalized；而如《Disentangling length from\nquality in direct preference\noptimization》所发现，DPO里更长的token会有更大的梯度值从而主导了梯度更新的过程，这导致训练出来的模型倾向于生成更长的模型。</p>\n<h1 id=\"实验\">实验</h1>\n<h2 id=\"设置\">设置</h2>\n<p>论文使用了Llama3-8B和Mistral-7B的base和instruct模型进行实验。</p>\n<p>对于base模型，就先在UltraChat-200k数据集上训练一个对应的SFT模型，之后在\nUltraFeedback数据集上进行preference optimization。</p>\n<p>对于instruct模型，参照《Iterative DPO\nalignment》的做法，先用这些SFT模型生成preference数据集。具体来说，使用UltraFeedback的prompt，用temperature=0.8的配置，从SFT模型生成5个response，并用PairRM（《LLM-Blender:\nEnsembling large language models with pairwise ranking and generative\nfusion》）对这5个response进行打分，选择最高分作为preferred\nresponse，最低分的座位dispreferred response。</p>\n<p>这样就得到了四组实验组合：Llama3-Base, Llama3-Instruct,\nMistral-Base和Mistral-Instruct。</p>\n<p>此外，论文发现超参对preference\noptimization的影响很大，因此对不同的方法进行了超参搜索，范围如下</p>\n<img src=\"/280fa97a/hyperparameters.png\" class title=\"超参搜索\">\n<img src=\"/280fa97a/simpo_hyperparameters.png\" class title=\"超参搜索\">\n<p>此外对batch size、解码温度等参数也进行搜索。</p>\n<p>所用的数据集如下</p>\n<img src=\"/280fa97a/benchmark.png\" class title=\"benchmark\">\n<h2 id=\"对比\">对比</h2>\n<p>在各个数据集上，不同的优化方法结果对比如下</p>\n<img src=\"/280fa97a/main_results.png\" class title=\"对比结果\">\n<p>其中LC表示length-controlled，即在限制长度条件下的win rate。</p>\n<p>有几个发现：<br>\n-\n在MT-Bench上，各个方法的差异不大，那些微小的波动可能更多来自于随机性。究其原因可能是因为这个数据集的量比较少，且评价的方案也比较单一，这个发现和《From\nlive data to high-quality benchmarks: The Arena-Hard\npipeline》的发现是一致的。<br>\n-\ninstruct模型的表现比base要好，这可能是因为这些精心微调过甚至强化学习过的模型本身质量更高。<br>\n- 在AlpacaEval 2和Arena-Hard上，simPO在raw win rate和length-controlled\nwin rate相比其他方案都有明显优势。</p>\n<h2 id=\"消融实验\">消融实验</h2>\n<p>simPO两个主要的部分就是length\nnormalization和margin。分别去掉这两个部分之后的结果如下表</p>\n<img src=\"/280fa97a/ablation.png\" class title=\"消融实验\">\n<p>结果上看，length normalization的影响很大，margin也有一定的影响。</p>\n<p>下面具体分析一下。</p>\n<p>首先是关于长度归一化。从上表的结果上看，对于simPO，使用长度归一化会让模型生成更短且质量更高的结果。</p>\n<p>对比其他训练方法，simPO在长度控制下的win\nrate有明显优势，这说明simPO实现了对生成长度的最小利用，即不通过长篇大论来提高得分。</p>\n<p>而通用来说，生成结果的长度和质量之间并没有什么强联系。如下表所示，各个训练方法的生成长度和wr并没有什么明显规律，这表明，生成结果的长度并不是衡量生成质量的一个可靠指标。</p>\n<img src=\"/280fa97a/ln.png\" class title=\"长度归一化\">\n<p>此外，长度归一化会增大偏好数据对之间的reward差。这个很好理解，在有长度归一化的损失函数下，想要达到相同的reward差，模型需要给出y倍的数值才能比margin大。</p>\n<p>论文把在不同的长度差异下的reward差画出来，如下图所示</p>\n<img src=\"/280fa97a/ln_effect.png\" class title=\"长度归一化\">\n<p>可以发现带有长度归一化的simPO无论数据的长度差如何，都能给出positive\nreward margin，而没有带长度归一化的模型在winning\nresponse的长度更短的情况下，会给出negative reward\ndifference，这表明模型对这些样本的学习效果很差。</p>\n<p>而从上图b和c子图可以看出，移除长度归一化会使得reward和response\nlength呈现强烈的正相关关系，而这显然不是我们想要的。</p>\n<p>接下来看下reward margin的影响。</p>\n<p>把reward accuracy定义为policy model对winning\nresponse的reward高于losing\nresponse的比例。那么如下图所示，随着margin的增大，reward\naccuracy也在提升</p>\n<img src=\"/280fa97a/reward_accuracy.png\" class title=\"reward accuracy\">\n<p>另外实验还发现，增大reward margin，会使得reward difference和winning\nresponse的平均对数似然的分布变得扁平，且winning\nresponse的平均对数似然会减小，如下图所示</p>\n<img src=\"/280fa97a/margin_dist.png\" class title=\"影响分布\">\n<p>这说明太大的margin设置对模型会有负面影响，因此需要寻找一个中间值使得模型效果最好。</p>\n<h2 id=\"dpo和simpo的对比\">DPO和simPO的对比</h2>\n<ol type=\"1\">\n<li>虽然DPO的reward表达式里没有显式涵盖长度归一化的信息，但是由于使用了reference\nmodel进行对比，在一定程度上可以对抗length\nbias。如下图所示，DPO在一定程度上可以打破长度和reward之间的正相关关系，但是没有simPO的效果那么好</li>\n</ol>\n<img src=\"/280fa97a/dpo_correlation.png\" class title=\"correlation\">\n<ol start=\"2\" type=\"1\">\n<li>simPO比DPO有更高的reward\naccuracy，这表明simPO的reward设计有更强的泛化能力，可以提供更高质量的生成能力</li>\n</ol>\n<img src=\"/280fa97a/reward_accuracy_compare.png\" class title=\"reward accuracy对比\">\n<h1 id=\"小结\">小结</h1>\n<p>simPO对损失函数做了一些改变，对齐了训练和推理的目标，使得policy\nmodel能够在提升效果的同时，不过分影响生成结果的长度。并且simPO不再需要reference\nmodel，这也使得训练的空间成本更加节省。</p>\n<p>论文在LLAMA和Mistral两个热门的模型上进行了比较多的实验，比较有说服力。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">大模型推理加速-投机解码</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">大模型偏好对齐-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">大模型偏好对齐-ODPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">大模型算法题(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】SimPO: Simple Preference Optimization with a Reference-Free\nReward https://arxiv.org/abs/2405.14734</p>\n"},{"title":"大模型推理加速-投机解码","abbrlink":"f5c015c","date":"2024-05-13T08:47:13.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n大语言模型虽然效果很好，但是推理时，朴素的自回归解码策略需要逐个串行解码，耗时较长，这给用户的耐心带来了很大挑战。如今各家大模型提供商基本都有对外提供大模型的体验平台，而模型的推理效率自然也成了一个重要的竞争点。  \n\nspeculative decoding，译作投机解码，就是推理加速的一个比较巧妙的方案。本篇将介绍投机解码的基础思路。  \n\n# 背景  \n\n2022年11月，Google在《Fast Inference from Transformers via Speculative Decoding》里提出投机解码的策略；DeepMind稍晚一点，在2023年初的《Accelerating Large Language Model Decoding with Speculative Sampling》也提出了一样的解码策略。（以这两家的关系，很可能私底下就沟通过这个idea了）Google的论文相比DeepMind的，做了更多的实验和分析，更为详尽一些。  \n\n在speculative decoding之前，研究人员已经在模型推理加速这个方向做了不少工作：  \n- 模型蒸馏：以Hinton的《Distilling the Knowledge in a Neural Network》为代表，以及后面衍生出的各种蒸馏方法（参考《Knowledge Distillation: A Survey》），可以把规模更大的、性能更强的模型的能力，部分迁移到规模较小的模型上，在效果上相比直接训练小模型有一定的提升。transformer上蒸馏相关的经典工作有《TinyBERT: Distilling BERT for Natural Language Understanding》和《DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter》等。  \n- 模型量化：如《Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations》、《LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale》、《Zeroquant: Efficient and affordable post-training quantization for large-scale transformers》等，把模型参数量化到int8、int4以及更低的精度，在减少空间需求的同时，最大化地保持模型的推理效果。  \n- 高效模型结构设计：如使用稀疏层的《Sparse is Enough in Scaling Transformers》，减少KV缓存需求的MQA《Fast Transformer Decoding: One Write-Head is All You Need》、GQA《《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》》以及最近DeepSeek-V2中的MLA等，还有通过进化算法进行高效架构搜索的工作《Primer: Searching for Efficient Transformers for Language Modeling》。  \n\n以上这些做法对不同的输入一视同仁，采用一个全局来看有收益的方案来统一处理，达到推理加速的目的。  \n\n相对地，也有一些其他的方案，认为不是每一步推理都适合一样处理：某些推理step需要大模型，而另一些step只需要高效的小模型，从而根据输入，动态地决定模型参与计算的参数，相关工作有：  \n- 《Dynamic Neural Networks: A Survey》  \n- 《Adaptive Attention Span in Transformers》  \n- 《Consistent Accelerated Inference via Confident Adaptive Transformers》  \n- 《Why should we add early exits to neural networks?》  \n- 《Controlling Computation versus Quality for Neural Sequence Models》  \n- 《The Right Tool for the Job: Matching Model and Instance Complexities》  \n- 《Depth-Adaptive Transformer》  \n- 等  \n\nMoE也属于动态激活的方案之一。  \n\n而《Training compute-optimal large language models》的scaling law则指出模型规模没有原先预想的影响那么大，可以通过增加训练数据等方法让小模型逼近大模型的效果。  \n\n以上这些方案虽然可以在一定程度上提升推理效率，但是要么需要重新训练模型，要么对模型的效果有损害。  \n\n也有一些方案在解码的方法上进行优化，比如《Blockwise Parallel Decoding for Deep Autoregressive Models》和《Lossless Acceleration for Seq2seq Generation with Aggressive Decoding》。  \n\nspeculative decoding也是一个在解码策略上进行优化的方法。投机解码可以在不用训练原模型的基础上，提升2x-3x的推理速度，并且保证结果和原模型完全一致，没有任何效果损失。  \n\n# speculative decoding算法  \n\n回想一下，自回归语言模型在训练的时候，在每一个位置，会根据当前及前面所有的token，预测下一个token。由于强制学习的特性，所有token可以一起训练。在某种特别的情况下，模型对当前的输入拟合得特别好，就有可能出现每个token的预测，都完美命中下一个输入token的情况。举个例子：  \n\n```\n位置：一  二  三  四\n输入：我  爱  中  国\n输出：爱  中  国  EOS\n```\n\n而在推理的时候，这种依赖前面所有token的特性，使得自回归模型只能一个一个串行地解码：  \n\n```\nstep1：输入“我”，输出“爱”；\nstep2：输入“我爱”，输出“中”；\nstep3：输入“我爱中”，输出“国”；\nstep4：输入“我爱中国”，输出“EOS”；\n```\n\n现在，假设我们有一个神奇海螺，你只要输入“我”，就会输出“爱 中 国 EOS”四个token作为草稿，我们就可以拿着这四个draft token一起放到原来的模型，跑一下各个位置的输出，进行验证，跟训练时的前向推理一样：  \n\n```\n位置：一  二  三  四\n输入：我  爱  中  国\n输出：爱  中  国  EOS\n```\n\n然后就会发现模型的输出和神奇海螺给出的草稿完全一致，那就相当于我们只进行了一次模型推理，就解码了四个token，并且和原模型的效果完全一致。并且一般情况下，模型对一个位置进行预测和对四个位置进行预测的耗时基本没有太大的差异，也就是说在这个例子下，模型解码速度提升到了将近4倍。  \n\n当然，神奇海螺不会总是能够给出和模型一模一样的结果，除非它就是模型本身。因此，在上面这个例子中，输入“我”之后，神奇海螺有可能给出的是“爱 中 华 EOS”这四个draft token。这种情况下，我们把这些token一起输入到模型进行验证  \n\n```\n位置：一  二  三  四\n输入：我  爱  中  华\n输出：爱  中  国  EOS\n```\n\n会发现神奇海螺给出的“爱”和“中”命中了模型的结果，但是“华”没对上。不过这种情况下，跑一次模型推理也能解码出两个token，推理效率依然有提升。  \n\n部分情况下，神奇海螺给出的结果也可能完全跑偏，比如给它输入“我”，它有可能输出“叫 小 明”，这就和原模型一个都没对上。但是只要统计上，神奇海螺给出的草稿平均命中token数 > 0，我们就有机会获得推理加速。  \n\n使用神奇海螺的这个思路其实就是speculative decoding的主要思路，而你肯定也已经猜到了，神奇海螺其实就是一个规模比较小的模型，论文中把它称为approximation model或者draft model，而我们想要加速的原模型则叫target model。  \n\n论文给出的一个例子如下  \n\n{% asset_img fi_example.png 例子 %}  \n\n绿色的就是approximation model给出并命中target model验证结果的token，红色的是错误的token，蓝色则是修正后的token。  \n\n在这个例子中，target模型只推理了9次，就解码出了38个token，推理速度获得了较大提升。  \n\n看完了例子，现在对投机解码算法给出正式的描述。  \n\n$M_p$ 是target model， $M_q$ 是approximation model，prefix是当前的输入。  \n\n首先 $M_q$ 给出 $\\gamma$ 个draft token，然后 $M_p$ 并行地对这 $\\gamma$ 个draft token进行验证，根据验证结果，按顺序把通过验证的token加入到当前序列中；如果出现被 $M_p$ 拒绝的token，这些token则按规则重新抽样。  \n\nGoogle论文给出的投机解码算法描述如下图。  \n\n{% asset_img fi_sd_algo.png 投机解码算法 %}  \n\n（DeepMind版本的算法描述在下面）  \n\n这里注意，投机解码单次运行能解码的token数量，除了这 $n$ 个被接受的draft token，还有 $M_p$ 对这些草稿进行验证时顺便推理出来的一个额外token，因此最终可以得到 $n+1$ 个token。因此如果approximation model每次给出 $\\gamma$ 个draft token，理论上最多可以获得 $\\gamma+1$ 新解码token，而最少也能有1个（来自target模型）。  \n\n投机解码的原理大致就是这样，思路还是很巧妙的，但是要实际应用还有几个问题需要解决，比如：  \n- 关于投机采样speculative sampling：target model怎么对approximation model给出的token进行验证？在一个draft token被拒绝之后，怎么重新采样？  \n- 怎么选择 $\\gamma$ 才合理？  \n- 怎么选择approximation model，用什么指标表征approximation model的质量？  \n\n另外，DeepMind论文的给出投机解码算法如下，可以对照Google的算法，方便理解。（DeepMind所用的符号有所不同，本篇采用Google论文的符号描述。）  \n\n{% asset_img acce_alog.png DeepMind投机解码算法 %}  \n\n里面的 $(.)_+$ 操作表示 $(f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}$ 。  \n\n# speculative sampling的正确性  \n\n我们希望投机解码的最终结果，和target model自回归解码的结果一致，即完全无损，因此需要对投机采样做一些设计和分析。  \n\n首先，当前在transformer的解码上已经有很多策略，包括但不限于argmax、top-k采样、使用温度等。而大部分操作都是在logits上进行操作，这相当于改变了模型的输出分布。而在最终分布上的采样操作，都是相同的。因此我们可以只在朴素的标准采样上进行分析，而结果可以推广到其他的解码策略上。  \n\n假设 $p(x)$ 是target model $M_p$ 在当前输入下的分布， $q(x)$ 是approximation model $M_q$ 在当前输入下的分布。  \n\n投机解码的做法是，先采样 $x\\sim q(x)$，如果 $q(x)\\leq p(x)$，就保留 $x$，否则就以 $1-\\frac{p(x)}{q(x)}$ 的概率拒绝 $x$，并在分布 $p'(x)=norm(max(0,p(x)-q(x)))$ 对被拒绝的 $x$ 重新采样，并结束当前的投机解码。  \n\n其中 $norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$  。\n\n看起来并不复杂。一个问题是，为什么这样从 $q(x)$ 采样之后，我们得到的结果符合分布 $p(x)$？即按这样的概率进行拒绝之后，结果和target model自己解码一样？  \n\n从公式上来说，approximation model的抽样有 $\\tilde{x}\\sim q$。假设 $X$ 是最终结果，我们的目标就是证明 $\\mathbb{P}(X=x)=p(x)$。  \n\n而要使得 $X=x$，只有 $\\tilde{x}=x$ 且 $\\tilde{x}$ 被接受，或者在 $\\tilde{x}$ 被拒绝之后重新采样到 $\\tilde{x}=x$ 两种情况，即有  \n\n$$\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{ accepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})$$  \n\n对于第一项，有  \n\n$$\n\\begin{aligned}\n&\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{ ассерґе}d|\\tilde{x}=x)\\\\=&q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n$$  \n\n而第二项里  \n\n$$\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{ accepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ ассерґе}d) \\\\\n=1-\\sum_{x'}\\min(q(x'),p(x')) \\\\\n=\\sum_{x'}\\max(0,p(x')-q(x')) \\\\\n\\end{gathered}$$  \n\n上式第三行到第四行的解释：第三行相当于计算1减区域b的面积，而区域a+区域b的面积和为1，因此第三行相当于区域a的面积，即 $\\sum_{x'}\\max(0,p(x')-q(x'))$。  \n\n{% asset_img formula.png 图解 %}  \n\n从采样规则，有  \n\n$$\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$$  \n\n因此  \n\n$$\\mathbb{P}(\\tilde{x}\\text{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\max(0,p(x)-q(x))$$  \n\n最终有  \n\n$$\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)$$  \n\n因此按照前面设计的规则进行采样，就能保证结果和target model自己解码出来的一样。  \n\n# approximation model的评估  \n\napproximation model的一个采样 $x\\sim q(x)$ 被target model接受的概率为 $\\beta$，我们把这个概率叫acceptance rate接受率。  \n\n那么其期望值 $E(\\beta)$ 就是approximation model对target model拟合质量一个很好的评估指标。  \n\n$E(\\beta)$ 越大，每个token被接受的概率越大，那么每次投机解码能获得的输出token越多。  \n\n我们令 $\\alpha=E(\\beta)$，并且为简化起见，假设 $\\beta$ 的分布是i.i.d.的，那么跑一次投机解码能够获得的token数量是一个capped geometric variable，其期望值如下式  \n\n$$E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$$  \n\n不同 $\\gamma$ 下的图像如下  \n\n{% asset_img fi_expected_token_num.png 解码数量期望值 %}  \n\n而 $\\alpha$ 是可以推算的。  \n\n首先定义一个 $M_p$ 和 $M_q$ 之间的divergence $D_{LK}$  \n\n$$\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}$$  \n\n其中 $M(x)=\\frac{p(x)+q(x)}2$。  \n\n而  \n\n$$\n\\begin{aligned}\n&\\sum_x|p(x)-M(x)|\\\\=&\\sum_x\\frac{|p-q|}{2}\\\\=&1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n$$  \n\n因此有  \n\n$$D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))$$  \n\n$D_{LK}(p,q)$越小，则 $M_p$ 和 $M_q$ 越相近。如果 $D_{LK}(p,q)=0$，说明 $p=q$；如果 $D_{LK}(p,q)=1$，说明 $p$ 和 $q$ 两个分布完全没有交叉的部分。  \n\n根据 $\\beta$ 的定义，有  \n\n$$\n\\begin{aligned}\n\\beta=&E_{x\\sim q(x)}\\begin{cases}1&q(x)\\leq p(x)\\\\\\frac{p(x)}{q(x)}&q(x)>p(x)\\end{cases}\\\\\n=&E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&\\sum_x\\min(p(x),q(x))\\\\\n=&1-D_{LK}(p,q)\n\\end{aligned}\n$$  \n\n最终得到  \n\n$$\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))$$\n\n实验中，不同approximation model和target model之间测得的 $\\alpha$ 值如下表所示  \n\n{% asset_img fi_alpha.png 各种模型的alpha %}  \n\n# 耗时优化的分析  \n\n定义cost coefficient $c$，表示 $M_q$ 单次推理 和 $M_p$ 单次推理的比值。  \n\n和仅与模型相关的 $\\alpha$ 不同，$c$ 的具体值会受到硬件、推理框架等影响。在论文的实验中 $c$ 的值大部分小于0.05。  \n\n假设 $M_p$ 每次推理所需的时间为 $T$，则一次投机解码所需的时间为 $Tc\\gamma+T$。  \n\n根据前面的推算，投机解码每次能获得的token数为 $E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ 个，因此每个token所需的时间为 $\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T$。综上，使用投机解码在推理时间上的improvement factor为  \n\n$$\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$$  \n\n只要 $\\alpha>c$，就一定存在能提升解码效率的 $\\gamma$，并且improvement factor至少为 $\\frac{1+\\alpha}{1+c}$（$\\gamma=1$时）。  \n\n# 计算成本的分析  \n\n$M_p$ 同时对 $\\gamma+1$ 个token进行验证。如果一个token被接受了，那么推理效率就获得了提升；如果token被拒绝了，那么相关的计算就没有实际收益，就会有计算的“浪费”。  \n\n假设 $\\hat{c}$ 是 $M_q$ 和 $M_p$ 计算一个token的arithmetic operations的比例，$\\hat{T}$ 是 $M_p$ 解码一个token所需的arithmetic operations。  \n\n那么一次投机解码的计算量就是 $\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)$，这个计算量除以投机解码每次获得的token数 $\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ 就得到平均每个token的计算量为 $\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$。  \n\n$\\alpha$ 越大，$\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$ 这个比值越小，平均计算成本越低。  \n\n另外，使用投机解码减少了KV cache和显存的读写。\n\n# $\\gamma$ 的选择  \n\n给定 $\\alpha$ 和 $c$，最佳的 $\\gamma$ 应该最大化walltime improvement factor $\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$。  \n\n下图给出不同 $\\alpha$ 和 $c$ 下，最佳的 $\\gamma$ 值  \n\n{% asset_img fi_choose_gamma.png gamma的选择 %}  \n\n推理速度和总计算量之间有tradeoff，即增大 $\\gamma$ 会提升推理速度，同时也会带来更多的计算成本，如下所示  \n\n{% asset_img fi_speed_and_op_table.png 计算量和加速 %}  \n\n{% asset_img fi_speed_and_op.png 计算量和加速 %}  \n\n{% asset_img fi_walltime.png walltime %}  \n\n实际上，$\\beta$ 并不是固定的常数，因此实际上我们可以通过在投机解码的过程中预测 $\\beta$ 值来选择 $\\gamma$，这是未来的一个改进方向。  \n\n# approximation model的选择  \n\n论文的实验中，一部分使用现成的模型作为approximation model。这种情况下，让approximation model的参数规模比target model小两个数量级是比较好的选择，能够平衡推理加速和计算量。  \n\n有趣的是，即使使用很简单的模型，比如n-gram模型作为approximation model，也能获得不错的 $\\alpha$ 值。  \n\n另外，在一些特殊的任务，比如摘要任务，由于生成结果往往会从输入的原文里摘取内容，因此使用一个会从输入里copy token的approximation model可能会得到较高的 $\\alpha$ 值。  \n\napproximation model的另一个选择是如《Blockwise parallel decoding for deep autoregressive models》使用的非自回归模型。  \n\n# 实验  \n\n论文在翻译任务和摘要任务上测试了投机解码的效果。使用了T5的较小规模模型作为approximation model，来加速T5-XXL的推理，效果如下表，最高能达到3倍+的推理加速。    \n\n{% asset_img fi_t5_result.png T5系列加速效果 %}  \n\n此外，论文对更多样的模型组合测试了 $\\alpha$ 值，如下表所示  \n\n{% asset_img fi_alpha.png 各种模型的alpha %}  \n\n可以观察到，比target model小几个数量级的approximation model倾向于产生介于0.5和0.9之间的 $\\alpha$ 值。还注意到，对于所有模型，用于采样的分布越尖（即T比较小，如argmax）， $\\alpha$ 值越高。  \n\n# 小结  \n\n- 投机解码可以在完全无损的情况下，把推理速度提升2~3倍  \n- 即使使用最简单的n-gram模型，也能在投机解码的策略下获得推理速度提升  \n- 正常来说，使用比target model小两个数量级的approximation model就有较好的效果  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n\n【1】Fast Inference from Transformers via Speculative Decoding https://arxiv.org/abs/2211.17192  \n【2】Accelerating Large Language Model Decoding with Speculative Sampling https://arxiv.org/abs/2302.01318  \n","source":"_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础.md","raw":"---\ntitle: 大模型推理加速-投机解码\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 推理加速\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: f5c015c\ndate: 2024-05-13 16:47:13\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n大语言模型虽然效果很好，但是推理时，朴素的自回归解码策略需要逐个串行解码，耗时较长，这给用户的耐心带来了很大挑战。如今各家大模型提供商基本都有对外提供大模型的体验平台，而模型的推理效率自然也成了一个重要的竞争点。  \n\nspeculative decoding，译作投机解码，就是推理加速的一个比较巧妙的方案。本篇将介绍投机解码的基础思路。  \n\n# 背景  \n\n2022年11月，Google在《Fast Inference from Transformers via Speculative Decoding》里提出投机解码的策略；DeepMind稍晚一点，在2023年初的《Accelerating Large Language Model Decoding with Speculative Sampling》也提出了一样的解码策略。（以这两家的关系，很可能私底下就沟通过这个idea了）Google的论文相比DeepMind的，做了更多的实验和分析，更为详尽一些。  \n\n在speculative decoding之前，研究人员已经在模型推理加速这个方向做了不少工作：  \n- 模型蒸馏：以Hinton的《Distilling the Knowledge in a Neural Network》为代表，以及后面衍生出的各种蒸馏方法（参考《Knowledge Distillation: A Survey》），可以把规模更大的、性能更强的模型的能力，部分迁移到规模较小的模型上，在效果上相比直接训练小模型有一定的提升。transformer上蒸馏相关的经典工作有《TinyBERT: Distilling BERT for Natural Language Understanding》和《DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter》等。  \n- 模型量化：如《Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations》、《LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale》、《Zeroquant: Efficient and affordable post-training quantization for large-scale transformers》等，把模型参数量化到int8、int4以及更低的精度，在减少空间需求的同时，最大化地保持模型的推理效果。  \n- 高效模型结构设计：如使用稀疏层的《Sparse is Enough in Scaling Transformers》，减少KV缓存需求的MQA《Fast Transformer Decoding: One Write-Head is All You Need》、GQA《《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》》以及最近DeepSeek-V2中的MLA等，还有通过进化算法进行高效架构搜索的工作《Primer: Searching for Efficient Transformers for Language Modeling》。  \n\n以上这些做法对不同的输入一视同仁，采用一个全局来看有收益的方案来统一处理，达到推理加速的目的。  \n\n相对地，也有一些其他的方案，认为不是每一步推理都适合一样处理：某些推理step需要大模型，而另一些step只需要高效的小模型，从而根据输入，动态地决定模型参与计算的参数，相关工作有：  \n- 《Dynamic Neural Networks: A Survey》  \n- 《Adaptive Attention Span in Transformers》  \n- 《Consistent Accelerated Inference via Confident Adaptive Transformers》  \n- 《Why should we add early exits to neural networks?》  \n- 《Controlling Computation versus Quality for Neural Sequence Models》  \n- 《The Right Tool for the Job: Matching Model and Instance Complexities》  \n- 《Depth-Adaptive Transformer》  \n- 等  \n\nMoE也属于动态激活的方案之一。  \n\n而《Training compute-optimal large language models》的scaling law则指出模型规模没有原先预想的影响那么大，可以通过增加训练数据等方法让小模型逼近大模型的效果。  \n\n以上这些方案虽然可以在一定程度上提升推理效率，但是要么需要重新训练模型，要么对模型的效果有损害。  \n\n也有一些方案在解码的方法上进行优化，比如《Blockwise Parallel Decoding for Deep Autoregressive Models》和《Lossless Acceleration for Seq2seq Generation with Aggressive Decoding》。  \n\nspeculative decoding也是一个在解码策略上进行优化的方法。投机解码可以在不用训练原模型的基础上，提升2x-3x的推理速度，并且保证结果和原模型完全一致，没有任何效果损失。  \n\n# speculative decoding算法  \n\n回想一下，自回归语言模型在训练的时候，在每一个位置，会根据当前及前面所有的token，预测下一个token。由于强制学习的特性，所有token可以一起训练。在某种特别的情况下，模型对当前的输入拟合得特别好，就有可能出现每个token的预测，都完美命中下一个输入token的情况。举个例子：  \n\n```\n位置：一  二  三  四\n输入：我  爱  中  国\n输出：爱  中  国  EOS\n```\n\n而在推理的时候，这种依赖前面所有token的特性，使得自回归模型只能一个一个串行地解码：  \n\n```\nstep1：输入“我”，输出“爱”；\nstep2：输入“我爱”，输出“中”；\nstep3：输入“我爱中”，输出“国”；\nstep4：输入“我爱中国”，输出“EOS”；\n```\n\n现在，假设我们有一个神奇海螺，你只要输入“我”，就会输出“爱 中 国 EOS”四个token作为草稿，我们就可以拿着这四个draft token一起放到原来的模型，跑一下各个位置的输出，进行验证，跟训练时的前向推理一样：  \n\n```\n位置：一  二  三  四\n输入：我  爱  中  国\n输出：爱  中  国  EOS\n```\n\n然后就会发现模型的输出和神奇海螺给出的草稿完全一致，那就相当于我们只进行了一次模型推理，就解码了四个token，并且和原模型的效果完全一致。并且一般情况下，模型对一个位置进行预测和对四个位置进行预测的耗时基本没有太大的差异，也就是说在这个例子下，模型解码速度提升到了将近4倍。  \n\n当然，神奇海螺不会总是能够给出和模型一模一样的结果，除非它就是模型本身。因此，在上面这个例子中，输入“我”之后，神奇海螺有可能给出的是“爱 中 华 EOS”这四个draft token。这种情况下，我们把这些token一起输入到模型进行验证  \n\n```\n位置：一  二  三  四\n输入：我  爱  中  华\n输出：爱  中  国  EOS\n```\n\n会发现神奇海螺给出的“爱”和“中”命中了模型的结果，但是“华”没对上。不过这种情况下，跑一次模型推理也能解码出两个token，推理效率依然有提升。  \n\n部分情况下，神奇海螺给出的结果也可能完全跑偏，比如给它输入“我”，它有可能输出“叫 小 明”，这就和原模型一个都没对上。但是只要统计上，神奇海螺给出的草稿平均命中token数 > 0，我们就有机会获得推理加速。  \n\n使用神奇海螺的这个思路其实就是speculative decoding的主要思路，而你肯定也已经猜到了，神奇海螺其实就是一个规模比较小的模型，论文中把它称为approximation model或者draft model，而我们想要加速的原模型则叫target model。  \n\n论文给出的一个例子如下  \n\n{% asset_img fi_example.png 例子 %}  \n\n绿色的就是approximation model给出并命中target model验证结果的token，红色的是错误的token，蓝色则是修正后的token。  \n\n在这个例子中，target模型只推理了9次，就解码出了38个token，推理速度获得了较大提升。  \n\n看完了例子，现在对投机解码算法给出正式的描述。  \n\n$M_p$ 是target model， $M_q$ 是approximation model，prefix是当前的输入。  \n\n首先 $M_q$ 给出 $\\gamma$ 个draft token，然后 $M_p$ 并行地对这 $\\gamma$ 个draft token进行验证，根据验证结果，按顺序把通过验证的token加入到当前序列中；如果出现被 $M_p$ 拒绝的token，这些token则按规则重新抽样。  \n\nGoogle论文给出的投机解码算法描述如下图。  \n\n{% asset_img fi_sd_algo.png 投机解码算法 %}  \n\n（DeepMind版本的算法描述在下面）  \n\n这里注意，投机解码单次运行能解码的token数量，除了这 $n$ 个被接受的draft token，还有 $M_p$ 对这些草稿进行验证时顺便推理出来的一个额外token，因此最终可以得到 $n+1$ 个token。因此如果approximation model每次给出 $\\gamma$ 个draft token，理论上最多可以获得 $\\gamma+1$ 新解码token，而最少也能有1个（来自target模型）。  \n\n投机解码的原理大致就是这样，思路还是很巧妙的，但是要实际应用还有几个问题需要解决，比如：  \n- 关于投机采样speculative sampling：target model怎么对approximation model给出的token进行验证？在一个draft token被拒绝之后，怎么重新采样？  \n- 怎么选择 $\\gamma$ 才合理？  \n- 怎么选择approximation model，用什么指标表征approximation model的质量？  \n\n另外，DeepMind论文的给出投机解码算法如下，可以对照Google的算法，方便理解。（DeepMind所用的符号有所不同，本篇采用Google论文的符号描述。）  \n\n{% asset_img acce_alog.png DeepMind投机解码算法 %}  \n\n里面的 $(.)_+$ 操作表示 $(f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}$ 。  \n\n# speculative sampling的正确性  \n\n我们希望投机解码的最终结果，和target model自回归解码的结果一致，即完全无损，因此需要对投机采样做一些设计和分析。  \n\n首先，当前在transformer的解码上已经有很多策略，包括但不限于argmax、top-k采样、使用温度等。而大部分操作都是在logits上进行操作，这相当于改变了模型的输出分布。而在最终分布上的采样操作，都是相同的。因此我们可以只在朴素的标准采样上进行分析，而结果可以推广到其他的解码策略上。  \n\n假设 $p(x)$ 是target model $M_p$ 在当前输入下的分布， $q(x)$ 是approximation model $M_q$ 在当前输入下的分布。  \n\n投机解码的做法是，先采样 $x\\sim q(x)$，如果 $q(x)\\leq p(x)$，就保留 $x$，否则就以 $1-\\frac{p(x)}{q(x)}$ 的概率拒绝 $x$，并在分布 $p'(x)=norm(max(0,p(x)-q(x)))$ 对被拒绝的 $x$ 重新采样，并结束当前的投机解码。  \n\n其中 $norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$  。\n\n看起来并不复杂。一个问题是，为什么这样从 $q(x)$ 采样之后，我们得到的结果符合分布 $p(x)$？即按这样的概率进行拒绝之后，结果和target model自己解码一样？  \n\n从公式上来说，approximation model的抽样有 $\\tilde{x}\\sim q$。假设 $X$ 是最终结果，我们的目标就是证明 $\\mathbb{P}(X=x)=p(x)$。  \n\n而要使得 $X=x$，只有 $\\tilde{x}=x$ 且 $\\tilde{x}$ 被接受，或者在 $\\tilde{x}$ 被拒绝之后重新采样到 $\\tilde{x}=x$ 两种情况，即有  \n\n$$\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{ accepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})$$  \n\n对于第一项，有  \n\n$$\n\\begin{aligned}\n&\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{ ассерґе}d|\\tilde{x}=x)\\\\=&q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n$$  \n\n而第二项里  \n\n$$\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{ accepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ ассерґе}d) \\\\\n=1-\\sum_{x'}\\min(q(x'),p(x')) \\\\\n=\\sum_{x'}\\max(0,p(x')-q(x')) \\\\\n\\end{gathered}$$  \n\n上式第三行到第四行的解释：第三行相当于计算1减区域b的面积，而区域a+区域b的面积和为1，因此第三行相当于区域a的面积，即 $\\sum_{x'}\\max(0,p(x')-q(x'))$。  \n\n{% asset_img formula.png 图解 %}  \n\n从采样规则，有  \n\n$$\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$$  \n\n因此  \n\n$$\\mathbb{P}(\\tilde{x}\\text{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\max(0,p(x)-q(x))$$  \n\n最终有  \n\n$$\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)$$  \n\n因此按照前面设计的规则进行采样，就能保证结果和target model自己解码出来的一样。  \n\n# approximation model的评估  \n\napproximation model的一个采样 $x\\sim q(x)$ 被target model接受的概率为 $\\beta$，我们把这个概率叫acceptance rate接受率。  \n\n那么其期望值 $E(\\beta)$ 就是approximation model对target model拟合质量一个很好的评估指标。  \n\n$E(\\beta)$ 越大，每个token被接受的概率越大，那么每次投机解码能获得的输出token越多。  \n\n我们令 $\\alpha=E(\\beta)$，并且为简化起见，假设 $\\beta$ 的分布是i.i.d.的，那么跑一次投机解码能够获得的token数量是一个capped geometric variable，其期望值如下式  \n\n$$E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$$  \n\n不同 $\\gamma$ 下的图像如下  \n\n{% asset_img fi_expected_token_num.png 解码数量期望值 %}  \n\n而 $\\alpha$ 是可以推算的。  \n\n首先定义一个 $M_p$ 和 $M_q$ 之间的divergence $D_{LK}$  \n\n$$\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}$$  \n\n其中 $M(x)=\\frac{p(x)+q(x)}2$。  \n\n而  \n\n$$\n\\begin{aligned}\n&\\sum_x|p(x)-M(x)|\\\\=&\\sum_x\\frac{|p-q|}{2}\\\\=&1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n$$  \n\n因此有  \n\n$$D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))$$  \n\n$D_{LK}(p,q)$越小，则 $M_p$ 和 $M_q$ 越相近。如果 $D_{LK}(p,q)=0$，说明 $p=q$；如果 $D_{LK}(p,q)=1$，说明 $p$ 和 $q$ 两个分布完全没有交叉的部分。  \n\n根据 $\\beta$ 的定义，有  \n\n$$\n\\begin{aligned}\n\\beta=&E_{x\\sim q(x)}\\begin{cases}1&q(x)\\leq p(x)\\\\\\frac{p(x)}{q(x)}&q(x)>p(x)\\end{cases}\\\\\n=&E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&\\sum_x\\min(p(x),q(x))\\\\\n=&1-D_{LK}(p,q)\n\\end{aligned}\n$$  \n\n最终得到  \n\n$$\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))$$\n\n实验中，不同approximation model和target model之间测得的 $\\alpha$ 值如下表所示  \n\n{% asset_img fi_alpha.png 各种模型的alpha %}  \n\n# 耗时优化的分析  \n\n定义cost coefficient $c$，表示 $M_q$ 单次推理 和 $M_p$ 单次推理的比值。  \n\n和仅与模型相关的 $\\alpha$ 不同，$c$ 的具体值会受到硬件、推理框架等影响。在论文的实验中 $c$ 的值大部分小于0.05。  \n\n假设 $M_p$ 每次推理所需的时间为 $T$，则一次投机解码所需的时间为 $Tc\\gamma+T$。  \n\n根据前面的推算，投机解码每次能获得的token数为 $E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ 个，因此每个token所需的时间为 $\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T$。综上，使用投机解码在推理时间上的improvement factor为  \n\n$$\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$$  \n\n只要 $\\alpha>c$，就一定存在能提升解码效率的 $\\gamma$，并且improvement factor至少为 $\\frac{1+\\alpha}{1+c}$（$\\gamma=1$时）。  \n\n# 计算成本的分析  \n\n$M_p$ 同时对 $\\gamma+1$ 个token进行验证。如果一个token被接受了，那么推理效率就获得了提升；如果token被拒绝了，那么相关的计算就没有实际收益，就会有计算的“浪费”。  \n\n假设 $\\hat{c}$ 是 $M_q$ 和 $M_p$ 计算一个token的arithmetic operations的比例，$\\hat{T}$ 是 $M_p$ 解码一个token所需的arithmetic operations。  \n\n那么一次投机解码的计算量就是 $\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)$，这个计算量除以投机解码每次获得的token数 $\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ 就得到平均每个token的计算量为 $\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$。  \n\n$\\alpha$ 越大，$\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$ 这个比值越小，平均计算成本越低。  \n\n另外，使用投机解码减少了KV cache和显存的读写。\n\n# $\\gamma$ 的选择  \n\n给定 $\\alpha$ 和 $c$，最佳的 $\\gamma$ 应该最大化walltime improvement factor $\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$。  \n\n下图给出不同 $\\alpha$ 和 $c$ 下，最佳的 $\\gamma$ 值  \n\n{% asset_img fi_choose_gamma.png gamma的选择 %}  \n\n推理速度和总计算量之间有tradeoff，即增大 $\\gamma$ 会提升推理速度，同时也会带来更多的计算成本，如下所示  \n\n{% asset_img fi_speed_and_op_table.png 计算量和加速 %}  \n\n{% asset_img fi_speed_and_op.png 计算量和加速 %}  \n\n{% asset_img fi_walltime.png walltime %}  \n\n实际上，$\\beta$ 并不是固定的常数，因此实际上我们可以通过在投机解码的过程中预测 $\\beta$ 值来选择 $\\gamma$，这是未来的一个改进方向。  \n\n# approximation model的选择  \n\n论文的实验中，一部分使用现成的模型作为approximation model。这种情况下，让approximation model的参数规模比target model小两个数量级是比较好的选择，能够平衡推理加速和计算量。  \n\n有趣的是，即使使用很简单的模型，比如n-gram模型作为approximation model，也能获得不错的 $\\alpha$ 值。  \n\n另外，在一些特殊的任务，比如摘要任务，由于生成结果往往会从输入的原文里摘取内容，因此使用一个会从输入里copy token的approximation model可能会得到较高的 $\\alpha$ 值。  \n\napproximation model的另一个选择是如《Blockwise parallel decoding for deep autoregressive models》使用的非自回归模型。  \n\n# 实验  \n\n论文在翻译任务和摘要任务上测试了投机解码的效果。使用了T5的较小规模模型作为approximation model，来加速T5-XXL的推理，效果如下表，最高能达到3倍+的推理加速。    \n\n{% asset_img fi_t5_result.png T5系列加速效果 %}  \n\n此外，论文对更多样的模型组合测试了 $\\alpha$ 值，如下表所示  \n\n{% asset_img fi_alpha.png 各种模型的alpha %}  \n\n可以观察到，比target model小几个数量级的approximation model倾向于产生介于0.5和0.9之间的 $\\alpha$ 值。还注意到，对于所有模型，用于采样的分布越尖（即T比较小，如argmax）， $\\alpha$ 值越高。  \n\n# 小结  \n\n- 投机解码可以在完全无损的情况下，把推理速度提升2~3倍  \n- 即使使用最简单的n-gram模型，也能在投机解码的策略下获得推理速度提升  \n- 正常来说，使用比target model小两个数量级的approximation model就有较好的效果  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n\n【1】Fast Inference from Transformers via Speculative Decoding https://arxiv.org/abs/2211.17192  \n【2】Accelerating Large Language Model Decoding with Speculative Sampling https://arxiv.org/abs/2302.01318  \n","slug":"cs/nlp/2024/05/大模型推理加速-投机解码基础","published":1,"updated":"2024-05-25T03:38:10.437Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9y0013314k8borgtqh","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>大语言模型虽然效果很好，但是推理时，朴素的自回归解码策略需要逐个串行解码，耗时较长，这给用户的耐心带来了很大挑战。如今各家大模型提供商基本都有对外提供大模型的体验平台，而模型的推理效率自然也成了一个重要的竞争点。</p>\n<p>speculative\ndecoding，译作投机解码，就是推理加速的一个比较巧妙的方案。本篇将介绍投机解码的基础思路。</p>\n<h1 id=\"背景\">背景</h1>\n<p>2022年11月，Google在《Fast Inference from Transformers via\nSpeculative\nDecoding》里提出投机解码的策略；DeepMind稍晚一点，在2023年初的《Accelerating\nLarge Language Model Decoding with Speculative\nSampling》也提出了一样的解码策略。（以这两家的关系，很可能私底下就沟通过这个idea了）Google的论文相比DeepMind的，做了更多的实验和分析，更为详尽一些。</p>\n<p>在speculative\ndecoding之前，研究人员已经在模型推理加速这个方向做了不少工作：<br>\n- 模型蒸馏：以Hinton的《Distilling the Knowledge in a Neural\nNetwork》为代表，以及后面衍生出的各种蒸馏方法（参考《Knowledge\nDistillation: A\nSurvey》），可以把规模更大的、性能更强的模型的能力，部分迁移到规模较小的模型上，在效果上相比直接训练小模型有一定的提升。transformer上蒸馏相关的经典工作有《TinyBERT:\nDistilling BERT for Natural Language Understanding》和《DistilBERT, a\ndistilled version of BERT: smaller, faster, cheaper and\nlighter》等。<br>\n- 模型量化：如《Quantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations》、《LLM.int8(): 8-bit Matrix\nMultiplication for Transformers at Scale》、《Zeroquant: Efficient and\naffordable post-training quantization for large-scale\ntransformers》等，把模型参数量化到int8、int4以及更低的精度，在减少空间需求的同时，最大化地保持模型的推理效果。<br>\n- 高效模型结构设计：如使用稀疏层的《Sparse is Enough in Scaling\nTransformers》，减少KV缓存需求的MQA《Fast Transformer Decoding: One\nWrite-Head is All You Need》、GQA《《GQA: Training Generalized\nMulti-Query Transformer Models from Multi-Head\nCheckpoints》》以及最近DeepSeek-V2中的MLA等，还有通过进化算法进行高效架构搜索的工作《Primer:\nSearching for Efficient Transformers for Language Modeling》。</p>\n<p>以上这些做法对不同的输入一视同仁，采用一个全局来看有收益的方案来统一处理，达到推理加速的目的。</p>\n<p>相对地，也有一些其他的方案，认为不是每一步推理都适合一样处理：某些推理step需要大模型，而另一些step只需要高效的小模型，从而根据输入，动态地决定模型参与计算的参数，相关工作有：<br>\n- 《Dynamic Neural Networks: A Survey》<br>\n- 《Adaptive Attention Span in Transformers》<br>\n- 《Consistent Accelerated Inference via Confident Adaptive\nTransformers》<br>\n- 《Why should we add early exits to neural networks?》<br>\n- 《Controlling Computation versus Quality for Neural Sequence\nModels》<br>\n- 《The Right Tool for the Job: Matching Model and Instance\nComplexities》<br>\n- 《Depth-Adaptive Transformer》<br>\n- 等</p>\n<p>MoE也属于动态激活的方案之一。</p>\n<p>而《Training compute-optimal large language models》的scaling\nlaw则指出模型规模没有原先预想的影响那么大，可以通过增加训练数据等方法让小模型逼近大模型的效果。</p>\n<p>以上这些方案虽然可以在一定程度上提升推理效率，但是要么需要重新训练模型，要么对模型的效果有损害。</p>\n<p>也有一些方案在解码的方法上进行优化，比如《Blockwise Parallel Decoding\nfor Deep Autoregressive Models》和《Lossless Acceleration for Seq2seq\nGeneration with Aggressive Decoding》。</p>\n<p>speculative\ndecoding也是一个在解码策略上进行优化的方法。投机解码可以在不用训练原模型的基础上，提升2x-3x的推理速度，并且保证结果和原模型完全一致，没有任何效果损失。</p>\n<h1 id=\"speculative-decoding算法\">speculative decoding算法</h1>\n<p>回想一下，自回归语言模型在训练的时候，在每一个位置，会根据当前及前面所有的token，预测下一个token。由于强制学习的特性，所有token可以一起训练。在某种特别的情况下，模型对当前的输入拟合得特别好，就有可能出现每个token的预测，都完美命中下一个输入token的情况。举个例子：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">位置：一  二  三  四</span><br><span class=\"line\">输入：我  爱  中  国</span><br><span class=\"line\">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>\n<p>而在推理的时候，这种依赖前面所有token的特性，使得自回归模型只能一个一个串行地解码：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step1：输入“我”，输出“爱”；</span><br><span class=\"line\">step2：输入“我爱”，输出“中”；</span><br><span class=\"line\">step3：输入“我爱中”，输出“国”；</span><br><span class=\"line\">step4：输入“我爱中国”，输出“EOS”；</span><br></pre></td></tr></table></figure>\n<p>现在，假设我们有一个神奇海螺，你只要输入“我”，就会输出“爱 中 国\nEOS”四个token作为草稿，我们就可以拿着这四个draft\ntoken一起放到原来的模型，跑一下各个位置的输出，进行验证，跟训练时的前向推理一样：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">位置：一  二  三  四</span><br><span class=\"line\">输入：我  爱  中  国</span><br><span class=\"line\">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>\n<p>然后就会发现模型的输出和神奇海螺给出的草稿完全一致，那就相当于我们只进行了一次模型推理，就解码了四个token，并且和原模型的效果完全一致。并且一般情况下，模型对一个位置进行预测和对四个位置进行预测的耗时基本没有太大的差异，也就是说在这个例子下，模型解码速度提升到了将近4倍。</p>\n<p>当然，神奇海螺不会总是能够给出和模型一模一样的结果，除非它就是模型本身。因此，在上面这个例子中，输入“我”之后，神奇海螺有可能给出的是“爱\n中 华 EOS”这四个draft\ntoken。这种情况下，我们把这些token一起输入到模型进行验证</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">位置：一  二  三  四</span><br><span class=\"line\">输入：我  爱  中  华</span><br><span class=\"line\">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>\n<p>会发现神奇海螺给出的“爱”和“中”命中了模型的结果，但是“华”没对上。不过这种情况下，跑一次模型推理也能解码出两个token，推理效率依然有提升。</p>\n<p>部分情况下，神奇海螺给出的结果也可能完全跑偏，比如给它输入“我”，它有可能输出“叫\n小\n明”，这就和原模型一个都没对上。但是只要统计上，神奇海螺给出的草稿平均命中token数\n&gt; 0，我们就有机会获得推理加速。</p>\n<p>使用神奇海螺的这个思路其实就是speculative\ndecoding的主要思路，而你肯定也已经猜到了，神奇海螺其实就是一个规模比较小的模型，论文中把它称为approximation\nmodel或者draft model，而我们想要加速的原模型则叫target model。</p>\n<p>论文给出的一个例子如下</p>\n<img src=\"/f5c015c/fi_example.png\" class title=\"例子\">\n<p>绿色的就是approximation model给出并命中target\nmodel验证结果的token，红色的是错误的token，蓝色则是修正后的token。</p>\n<p>在这个例子中，target模型只推理了9次，就解码出了38个token，推理速度获得了较大提升。</p>\n<p>看完了例子，现在对投机解码算法给出正式的描述。</p>\n<p><span class=\"math inline\">\\(M_p\\)</span> 是target model， <span class=\"math inline\">\\(M_q\\)</span> 是approximation\nmodel，prefix是当前的输入。</p>\n<p>首先 <span class=\"math inline\">\\(M_q\\)</span> 给出 <span class=\"math inline\">\\(\\gamma\\)</span> 个draft token，然后 <span class=\"math inline\">\\(M_p\\)</span> 并行地对这 <span class=\"math inline\">\\(\\gamma\\)</span> 个draft\ntoken进行验证，根据验证结果，按顺序把通过验证的token加入到当前序列中；如果出现被\n<span class=\"math inline\">\\(M_p\\)</span>\n拒绝的token，这些token则按规则重新抽样。</p>\n<p>Google论文给出的投机解码算法描述如下图。</p>\n<img src=\"/f5c015c/fi_sd_algo.png\" class title=\"投机解码算法\">\n<p>（DeepMind版本的算法描述在下面）</p>\n<p>这里注意，投机解码单次运行能解码的token数量，除了这 <span class=\"math inline\">\\(n\\)</span> 个被接受的draft token，还有 <span class=\"math inline\">\\(M_p\\)</span>\n对这些草稿进行验证时顺便推理出来的一个额外token，因此最终可以得到 <span class=\"math inline\">\\(n+1\\)</span> 个token。因此如果approximation\nmodel每次给出 <span class=\"math inline\">\\(\\gamma\\)</span> 个draft\ntoken，理论上最多可以获得 <span class=\"math inline\">\\(\\gamma+1\\)</span>\n新解码token，而最少也能有1个（来自target模型）。</p>\n<p>投机解码的原理大致就是这样，思路还是很巧妙的，但是要实际应用还有几个问题需要解决，比如：<br>\n- 关于投机采样speculative sampling：target model怎么对approximation\nmodel给出的token进行验证？在一个draft\ntoken被拒绝之后，怎么重新采样？<br>\n- 怎么选择 <span class=\"math inline\">\\(\\gamma\\)</span> 才合理？<br>\n- 怎么选择approximation model，用什么指标表征approximation\nmodel的质量？</p>\n<p>另外，DeepMind论文的给出投机解码算法如下，可以对照Google的算法，方便理解。（DeepMind所用的符号有所不同，本篇采用Google论文的符号描述。）</p>\n<img src=\"/f5c015c/acce_alog.png\" class title=\"DeepMind投机解码算法\">\n<p>里面的 <span class=\"math inline\">\\((.)_+\\)</span> 操作表示 <span class=\"math inline\">\\((f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}\\)</span>\n。</p>\n<h1 id=\"speculative-sampling的正确性\">speculative sampling的正确性</h1>\n<p>我们希望投机解码的最终结果，和target\nmodel自回归解码的结果一致，即完全无损，因此需要对投机采样做一些设计和分析。</p>\n<p>首先，当前在transformer的解码上已经有很多策略，包括但不限于argmax、top-k采样、使用温度等。而大部分操作都是在logits上进行操作，这相当于改变了模型的输出分布。而在最终分布上的采样操作，都是相同的。因此我们可以只在朴素的标准采样上进行分析，而结果可以推广到其他的解码策略上。</p>\n<p>假设 <span class=\"math inline\">\\(p(x)\\)</span> 是target model <span class=\"math inline\">\\(M_p\\)</span> 在当前输入下的分布， <span class=\"math inline\">\\(q(x)\\)</span> 是approximation model <span class=\"math inline\">\\(M_q\\)</span> 在当前输入下的分布。</p>\n<p>投机解码的做法是，先采样 <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span>，如果 <span class=\"math inline\">\\(q(x)\\leq\np(x)\\)</span>，就保留 <span class=\"math inline\">\\(x\\)</span>，否则就以\n<span class=\"math inline\">\\(1-\\frac{p(x)}{q(x)}\\)</span> 的概率拒绝\n<span class=\"math inline\">\\(x\\)</span>，并在分布 <span class=\"math inline\">\\(p&#39;(x)=norm(max(0,p(x)-q(x)))\\)</span>\n对被拒绝的 <span class=\"math inline\">\\(x\\)</span>\n重新采样，并结束当前的投机解码。</p>\n<p>其中 <span class=\"math inline\">\\(norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\)</span>\n。</p>\n<p>看起来并不复杂。一个问题是，为什么这样从 <span class=\"math inline\">\\(q(x)\\)</span> 采样之后，我们得到的结果符合分布\n<span class=\"math inline\">\\(p(x)\\)</span>？即按这样的概率进行拒绝之后，结果和target\nmodel自己解码一样？</p>\n<p>从公式上来说，approximation model的抽样有 <span class=\"math inline\">\\(\\tilde{x}\\sim q\\)</span>。假设 <span class=\"math inline\">\\(X\\)</span> 是最终结果，我们的目标就是证明 <span class=\"math inline\">\\(\\mathbb{P}(X=x)=p(x)\\)</span>。</p>\n<p>而要使得 <span class=\"math inline\">\\(X=x\\)</span>，只有 <span class=\"math inline\">\\(\\tilde{x}=x\\)</span> 且 <span class=\"math inline\">\\(\\tilde{x}\\)</span> 被接受，或者在 <span class=\"math inline\">\\(\\tilde{x}\\)</span> 被拒绝之后重新采样到 <span class=\"math inline\">\\(\\tilde{x}=x\\)</span> 两种情况，即有</p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{\naccepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})\\]</span></p>\n<p>对于第一项，有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{\nассерґе}d|\\tilde{x}=x)\\\\=&amp;q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&amp;\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n\\]</span></p>\n<p>而第二项里</p>\n<p><span class=\"math display\">\\[\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{\naccepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ ассерґе}d)\n\\\\\n=1-\\sum_{x&#39;}\\min(q(x&#39;),p(x&#39;)) \\\\\n=\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;)) \\\\\n\\end{gathered}\\]</span></p>\n<p>上式第三行到第四行的解释：第三行相当于计算1减区域b的面积，而区域a+区域b的面积和为1，因此第三行相当于区域a的面积，即\n<span class=\"math inline\">\\(\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;))\\)</span>。</p>\n<img src=\"/f5c015c/formula.png\" class title=\"图解\">\n<p>从采样规则，有</p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\]</span></p>\n<p>因此</p>\n<p><span class=\"math display\">\\[\\mathbb{P}(\\tilde{x}\\text{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\max(0,p(x)-q(x))\\]</span></p>\n<p>最终有</p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)\\]</span></p>\n<p>因此按照前面设计的规则进行采样，就能保证结果和target\nmodel自己解码出来的一样。</p>\n<h1 id=\"approximation-model的评估\">approximation model的评估</h1>\n<p>approximation model的一个采样 <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span> 被target model接受的概率为 <span class=\"math inline\">\\(\\beta\\)</span>，我们把这个概率叫acceptance\nrate接受率。</p>\n<p>那么其期望值 <span class=\"math inline\">\\(E(\\beta)\\)</span>\n就是approximation model对target model拟合质量一个很好的评估指标。</p>\n<p><span class=\"math inline\">\\(E(\\beta)\\)</span>\n越大，每个token被接受的概率越大，那么每次投机解码能获得的输出token越多。</p>\n<p>我们令 <span class=\"math inline\">\\(\\alpha=E(\\beta)\\)</span>，并且为简化起见，假设\n<span class=\"math inline\">\\(\\beta\\)</span>\n的分布是i.i.d.的，那么跑一次投机解码能够获得的token数量是一个capped\ngeometric variable，其期望值如下式</p>\n<p><span class=\"math display\">\\[E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\]</span></p>\n<p>不同 <span class=\"math inline\">\\(\\gamma\\)</span> 下的图像如下</p>\n<img src=\"/f5c015c/fi_expected_token_num.png\" class title=\"解码数量期望值\">\n<p>而 <span class=\"math inline\">\\(\\alpha\\)</span> 是可以推算的。</p>\n<p>首先定义一个 <span class=\"math inline\">\\(M_p\\)</span> 和 <span class=\"math inline\">\\(M_q\\)</span> 之间的divergence <span class=\"math inline\">\\(D_{LK}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(M(x)=\\frac{p(x)+q(x)}2\\)</span>。</p>\n<p>而</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\sum_x|p(x)-M(x)|\\\\=&amp;\\sum_x\\frac{|p-q|}{2}\\\\=&amp;1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&amp;1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n\\]</span></p>\n<p>因此有</p>\n<p><span class=\"math display\">\\[D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))\\]</span></p>\n<p><span class=\"math inline\">\\(D_{LK}(p,q)\\)</span>越小，则 <span class=\"math inline\">\\(M_p\\)</span> 和 <span class=\"math inline\">\\(M_q\\)</span> 越相近。如果 <span class=\"math inline\">\\(D_{LK}(p,q)=0\\)</span>，说明 <span class=\"math inline\">\\(p=q\\)</span>；如果 <span class=\"math inline\">\\(D_{LK}(p,q)=1\\)</span>，说明 <span class=\"math inline\">\\(p\\)</span> 和 <span class=\"math inline\">\\(q\\)</span> 两个分布完全没有交叉的部分。</p>\n<p>根据 <span class=\"math inline\">\\(\\beta\\)</span> 的定义，有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n\\beta=&amp;E_{x\\sim q(x)}\\begin{cases}1&amp;q(x)\\leq\np(x)\\\\\\frac{p(x)}{q(x)}&amp;q(x)&gt;p(x)\\end{cases}\\\\\n=&amp;E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&amp;\\sum_x\\min(p(x),q(x))\\\\\n=&amp;1-D_{LK}(p,q)\n\\end{aligned}\n\\]</span></p>\n<p>最终得到</p>\n<p><span class=\"math display\">\\[\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))\\]</span></p>\n<p>实验中，不同approximation model和target model之间测得的 <span class=\"math inline\">\\(\\alpha\\)</span> 值如下表所示</p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"各种模型的alpha\">\n<h1 id=\"耗时优化的分析\">耗时优化的分析</h1>\n<p>定义cost coefficient <span class=\"math inline\">\\(c\\)</span>，表示\n<span class=\"math inline\">\\(M_q\\)</span> 单次推理 和 <span class=\"math inline\">\\(M_p\\)</span> 单次推理的比值。</p>\n<p>和仅与模型相关的 <span class=\"math inline\">\\(\\alpha\\)</span>\n不同，<span class=\"math inline\">\\(c\\)</span>\n的具体值会受到硬件、推理框架等影响。在论文的实验中 <span class=\"math inline\">\\(c\\)</span> 的值大部分小于0.05。</p>\n<p>假设 <span class=\"math inline\">\\(M_p\\)</span> 每次推理所需的时间为\n<span class=\"math inline\">\\(T\\)</span>，则一次投机解码所需的时间为 <span class=\"math inline\">\\(Tc\\gamma+T\\)</span>。</p>\n<p>根据前面的推算，投机解码每次能获得的token数为 <span class=\"math inline\">\\(E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\n个，因此每个token所需的时间为 <span class=\"math inline\">\\(\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T\\)</span>。综上，使用投机解码在推理时间上的improvement\nfactor为</p>\n<p><span class=\"math display\">\\[\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\]</span></p>\n<p>只要 <span class=\"math inline\">\\(\\alpha&gt;c\\)</span>，就一定存在能提升解码效率的\n<span class=\"math inline\">\\(\\gamma\\)</span>，并且improvement\nfactor至少为 <span class=\"math inline\">\\(\\frac{1+\\alpha}{1+c}\\)</span>（<span class=\"math inline\">\\(\\gamma=1\\)</span>时）。</p>\n<h1 id=\"计算成本的分析\">计算成本的分析</h1>\n<p><span class=\"math inline\">\\(M_p\\)</span> 同时对 <span class=\"math inline\">\\(\\gamma+1\\)</span>\n个token进行验证。如果一个token被接受了，那么推理效率就获得了提升；如果token被拒绝了，那么相关的计算就没有实际收益，就会有计算的“浪费”。</p>\n<p>假设 <span class=\"math inline\">\\(\\hat{c}\\)</span> 是 <span class=\"math inline\">\\(M_q\\)</span> 和 <span class=\"math inline\">\\(M_p\\)</span> 计算一个token的arithmetic\noperations的比例，<span class=\"math inline\">\\(\\hat{T}\\)</span> 是 <span class=\"math inline\">\\(M_p\\)</span> 解码一个token所需的arithmetic\noperations。</p>\n<p>那么一次投机解码的计算量就是 <span class=\"math inline\">\\(\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)\\)</span>，这个计算量除以投机解码每次获得的token数\n<span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\n就得到平均每个token的计算量为 <span class=\"math inline\">\\(\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span>。</p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span> 越大，<span class=\"math inline\">\\(\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span>\n这个比值越小，平均计算成本越低。</p>\n<p>另外，使用投机解码减少了KV cache和显存的读写。</p>\n<h1 id=\"gamma-的选择\"><span class=\"math inline\">\\(\\gamma\\)</span>\n的选择</h1>\n<p>给定 <span class=\"math inline\">\\(\\alpha\\)</span> 和 <span class=\"math inline\">\\(c\\)</span>，最佳的 <span class=\"math inline\">\\(\\gamma\\)</span> 应该最大化walltime improvement\nfactor <span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\)</span>。</p>\n<p>下图给出不同 <span class=\"math inline\">\\(\\alpha\\)</span> 和 <span class=\"math inline\">\\(c\\)</span> 下，最佳的 <span class=\"math inline\">\\(\\gamma\\)</span> 值</p>\n<img src=\"/f5c015c/fi_choose_gamma.png\" class title=\"gamma的选择\">\n<p>推理速度和总计算量之间有tradeoff，即增大 <span class=\"math inline\">\\(\\gamma\\)</span>\n会提升推理速度，同时也会带来更多的计算成本，如下所示</p>\n<img src=\"/f5c015c/fi_speed_and_op_table.png\" class title=\"计算量和加速\">\n<img src=\"/f5c015c/fi_speed_and_op.png\" class title=\"计算量和加速\">\n<img src=\"/f5c015c/fi_walltime.png\" class title=\"walltime\">\n<p>实际上，<span class=\"math inline\">\\(\\beta\\)</span>\n并不是固定的常数，因此实际上我们可以通过在投机解码的过程中预测 <span class=\"math inline\">\\(\\beta\\)</span> 值来选择 <span class=\"math inline\">\\(\\gamma\\)</span>，这是未来的一个改进方向。</p>\n<h1 id=\"approximation-model的选择\">approximation model的选择</h1>\n<p>论文的实验中，一部分使用现成的模型作为approximation\nmodel。这种情况下，让approximation model的参数规模比target\nmodel小两个数量级是比较好的选择，能够平衡推理加速和计算量。</p>\n<p>有趣的是，即使使用很简单的模型，比如n-gram模型作为approximation\nmodel，也能获得不错的 <span class=\"math inline\">\\(\\alpha\\)</span>\n值。</p>\n<p>另外，在一些特殊的任务，比如摘要任务，由于生成结果往往会从输入的原文里摘取内容，因此使用一个会从输入里copy\ntoken的approximation model可能会得到较高的 <span class=\"math inline\">\\(\\alpha\\)</span> 值。</p>\n<p>approximation model的另一个选择是如《Blockwise parallel decoding for\ndeep autoregressive models》使用的非自回归模型。</p>\n<h1 id=\"实验\">实验</h1>\n<p>论文在翻译任务和摘要任务上测试了投机解码的效果。使用了T5的较小规模模型作为approximation\nmodel，来加速T5-XXL的推理，效果如下表，最高能达到3倍+的推理加速。</p>\n<img src=\"/f5c015c/fi_t5_result.png\" class title=\"T5系列加速效果\">\n<p>此外，论文对更多样的模型组合测试了 <span class=\"math inline\">\\(\\alpha\\)</span> 值，如下表所示</p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"各种模型的alpha\">\n<p>可以观察到，比target model小几个数量级的approximation\nmodel倾向于产生介于0.5和0.9之间的 <span class=\"math inline\">\\(\\alpha\\)</span>\n值。还注意到，对于所有模型，用于采样的分布越尖（即T比较小，如argmax），\n<span class=\"math inline\">\\(\\alpha\\)</span> 值越高。</p>\n<h1 id=\"小结\">小结</h1>\n<ul>\n<li>投机解码可以在完全无损的情况下，把推理速度提升2~3倍<br>\n</li>\n<li>即使使用最简单的n-gram模型，也能在投机解码的策略下获得推理速度提升<br>\n</li>\n<li>正常来说，使用比target model小两个数量级的approximation\nmodel就有较好的效果</li>\n</ul>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Fast Inference from Transformers via Speculative Decoding\nhttps://arxiv.org/abs/2211.17192<br>\n【2】Accelerating Large Language Model Decoding with Speculative\nSampling https://arxiv.org/abs/2302.01318</p>\n","length":10078,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>大语言模型虽然效果很好，但是推理时，朴素的自回归解码策略需要逐个串行解码，耗时较长，这给用户的耐心带来了很大挑战。如今各家大模型提供商基本都有对外提供大模型的体验平台，而模型的推理效率自然也成了一个重要的竞争点。</p>\n<p>speculative\ndecoding，译作投机解码，就是推理加速的一个比较巧妙的方案。本篇将介绍投机解码的基础思路。</p>\n<h1 id=\"背景\">背景</h1>\n<p>2022年11月，Google在《Fast Inference from Transformers via\nSpeculative\nDecoding》里提出投机解码的策略；DeepMind稍晚一点，在2023年初的《Accelerating\nLarge Language Model Decoding with Speculative\nSampling》也提出了一样的解码策略。（以这两家的关系，很可能私底下就沟通过这个idea了）Google的论文相比DeepMind的，做了更多的实验和分析，更为详尽一些。</p>\n<p>在speculative\ndecoding之前，研究人员已经在模型推理加速这个方向做了不少工作：<br>\n- 模型蒸馏：以Hinton的《Distilling the Knowledge in a Neural\nNetwork》为代表，以及后面衍生出的各种蒸馏方法（参考《Knowledge\nDistillation: A\nSurvey》），可以把规模更大的、性能更强的模型的能力，部分迁移到规模较小的模型上，在效果上相比直接训练小模型有一定的提升。transformer上蒸馏相关的经典工作有《TinyBERT:\nDistilling BERT for Natural Language Understanding》和《DistilBERT, a\ndistilled version of BERT: smaller, faster, cheaper and\nlighter》等。<br>\n- 模型量化：如《Quantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations》、《LLM.int8(): 8-bit Matrix\nMultiplication for Transformers at Scale》、《Zeroquant: Efficient and\naffordable post-training quantization for large-scale\ntransformers》等，把模型参数量化到int8、int4以及更低的精度，在减少空间需求的同时，最大化地保持模型的推理效果。<br>\n- 高效模型结构设计：如使用稀疏层的《Sparse is Enough in Scaling\nTransformers》，减少KV缓存需求的MQA《Fast Transformer Decoding: One\nWrite-Head is All You Need》、GQA《《GQA: Training Generalized\nMulti-Query Transformer Models from Multi-Head\nCheckpoints》》以及最近DeepSeek-V2中的MLA等，还有通过进化算法进行高效架构搜索的工作《Primer:\nSearching for Efficient Transformers for Language Modeling》。</p>\n<p>以上这些做法对不同的输入一视同仁，采用一个全局来看有收益的方案来统一处理，达到推理加速的目的。</p>\n<p>相对地，也有一些其他的方案，认为不是每一步推理都适合一样处理：某些推理step需要大模型，而另一些step只需要高效的小模型，从而根据输入，动态地决定模型参与计算的参数，相关工作有：<br>\n- 《Dynamic Neural Networks: A Survey》<br>\n- 《Adaptive Attention Span in Transformers》<br>\n- 《Consistent Accelerated Inference via Confident Adaptive\nTransformers》<br>\n- 《Why should we add early exits to neural networks?》<br>\n- 《Controlling Computation versus Quality for Neural Sequence\nModels》<br>\n- 《The Right Tool for the Job: Matching Model and Instance\nComplexities》<br>\n- 《Depth-Adaptive Transformer》<br>\n- 等</p>\n<p>MoE也属于动态激活的方案之一。</p>\n<p>而《Training compute-optimal large language models》的scaling\nlaw则指出模型规模没有原先预想的影响那么大，可以通过增加训练数据等方法让小模型逼近大模型的效果。</p>\n<p>以上这些方案虽然可以在一定程度上提升推理效率，但是要么需要重新训练模型，要么对模型的效果有损害。</p>\n<p>也有一些方案在解码的方法上进行优化，比如《Blockwise Parallel Decoding\nfor Deep Autoregressive Models》和《Lossless Acceleration for Seq2seq\nGeneration with Aggressive Decoding》。</p>\n<p>speculative\ndecoding也是一个在解码策略上进行优化的方法。投机解码可以在不用训练原模型的基础上，提升2x-3x的推理速度，并且保证结果和原模型完全一致，没有任何效果损失。</p>\n<h1 id=\"speculative-decoding算法\">speculative decoding算法</h1>\n<p>回想一下，自回归语言模型在训练的时候，在每一个位置，会根据当前及前面所有的token，预测下一个token。由于强制学习的特性，所有token可以一起训练。在某种特别的情况下，模型对当前的输入拟合得特别好，就有可能出现每个token的预测，都完美命中下一个输入token的情况。举个例子：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">位置：一  二  三  四</span><br><span class=\"line\">输入：我  爱  中  国</span><br><span class=\"line\">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>\n<p>而在推理的时候，这种依赖前面所有token的特性，使得自回归模型只能一个一个串行地解码：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step1：输入“我”，输出“爱”；</span><br><span class=\"line\">step2：输入“我爱”，输出“中”；</span><br><span class=\"line\">step3：输入“我爱中”，输出“国”；</span><br><span class=\"line\">step4：输入“我爱中国”，输出“EOS”；</span><br></pre></td></tr></table></figure>\n<p>现在，假设我们有一个神奇海螺，你只要输入“我”，就会输出“爱 中 国\nEOS”四个token作为草稿，我们就可以拿着这四个draft\ntoken一起放到原来的模型，跑一下各个位置的输出，进行验证，跟训练时的前向推理一样：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">位置：一  二  三  四</span><br><span class=\"line\">输入：我  爱  中  国</span><br><span class=\"line\">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>\n<p>然后就会发现模型的输出和神奇海螺给出的草稿完全一致，那就相当于我们只进行了一次模型推理，就解码了四个token，并且和原模型的效果完全一致。并且一般情况下，模型对一个位置进行预测和对四个位置进行预测的耗时基本没有太大的差异，也就是说在这个例子下，模型解码速度提升到了将近4倍。</p>\n<p>当然，神奇海螺不会总是能够给出和模型一模一样的结果，除非它就是模型本身。因此，在上面这个例子中，输入“我”之后，神奇海螺有可能给出的是“爱\n中 华 EOS”这四个draft\ntoken。这种情况下，我们把这些token一起输入到模型进行验证</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">位置：一  二  三  四</span><br><span class=\"line\">输入：我  爱  中  华</span><br><span class=\"line\">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>\n<p>会发现神奇海螺给出的“爱”和“中”命中了模型的结果，但是“华”没对上。不过这种情况下，跑一次模型推理也能解码出两个token，推理效率依然有提升。</p>\n<p>部分情况下，神奇海螺给出的结果也可能完全跑偏，比如给它输入“我”，它有可能输出“叫\n小\n明”，这就和原模型一个都没对上。但是只要统计上，神奇海螺给出的草稿平均命中token数\n&gt; 0，我们就有机会获得推理加速。</p>\n<p>使用神奇海螺的这个思路其实就是speculative\ndecoding的主要思路，而你肯定也已经猜到了，神奇海螺其实就是一个规模比较小的模型，论文中把它称为approximation\nmodel或者draft model，而我们想要加速的原模型则叫target model。</p>\n<p>论文给出的一个例子如下</p>\n<img src=\"/f5c015c/fi_example.png\" class title=\"例子\">\n<p>绿色的就是approximation model给出并命中target\nmodel验证结果的token，红色的是错误的token，蓝色则是修正后的token。</p>\n<p>在这个例子中，target模型只推理了9次，就解码出了38个token，推理速度获得了较大提升。</p>\n<p>看完了例子，现在对投机解码算法给出正式的描述。</p>\n<p><span class=\"math inline\">\\(M_p\\)</span> 是target model， <span class=\"math inline\">\\(M_q\\)</span> 是approximation\nmodel，prefix是当前的输入。</p>\n<p>首先 <span class=\"math inline\">\\(M_q\\)</span> 给出 <span class=\"math inline\">\\(\\gamma\\)</span> 个draft token，然后 <span class=\"math inline\">\\(M_p\\)</span> 并行地对这 <span class=\"math inline\">\\(\\gamma\\)</span> 个draft\ntoken进行验证，根据验证结果，按顺序把通过验证的token加入到当前序列中；如果出现被\n<span class=\"math inline\">\\(M_p\\)</span>\n拒绝的token，这些token则按规则重新抽样。</p>\n<p>Google论文给出的投机解码算法描述如下图。</p>\n<img src=\"/f5c015c/fi_sd_algo.png\" class title=\"投机解码算法\">\n<p>（DeepMind版本的算法描述在下面）</p>\n<p>这里注意，投机解码单次运行能解码的token数量，除了这 <span class=\"math inline\">\\(n\\)</span> 个被接受的draft token，还有 <span class=\"math inline\">\\(M_p\\)</span>\n对这些草稿进行验证时顺便推理出来的一个额外token，因此最终可以得到 <span class=\"math inline\">\\(n+1\\)</span> 个token。因此如果approximation\nmodel每次给出 <span class=\"math inline\">\\(\\gamma\\)</span> 个draft\ntoken，理论上最多可以获得 <span class=\"math inline\">\\(\\gamma+1\\)</span>\n新解码token，而最少也能有1个（来自target模型）。</p>\n<p>投机解码的原理大致就是这样，思路还是很巧妙的，但是要实际应用还有几个问题需要解决，比如：<br>\n- 关于投机采样speculative sampling：target model怎么对approximation\nmodel给出的token进行验证？在一个draft\ntoken被拒绝之后，怎么重新采样？<br>\n- 怎么选择 <span class=\"math inline\">\\(\\gamma\\)</span> 才合理？<br>\n- 怎么选择approximation model，用什么指标表征approximation\nmodel的质量？</p>\n<p>另外，DeepMind论文的给出投机解码算法如下，可以对照Google的算法，方便理解。（DeepMind所用的符号有所不同，本篇采用Google论文的符号描述。）</p>\n<img src=\"/f5c015c/acce_alog.png\" class title=\"DeepMind投机解码算法\">\n<p>里面的 <span class=\"math inline\">\\((.)_+\\)</span> 操作表示 <span class=\"math inline\">\\((f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}\\)</span>\n。</p>\n<h1 id=\"speculative-sampling的正确性\">speculative sampling的正确性</h1>\n<p>我们希望投机解码的最终结果，和target\nmodel自回归解码的结果一致，即完全无损，因此需要对投机采样做一些设计和分析。</p>\n<p>首先，当前在transformer的解码上已经有很多策略，包括但不限于argmax、top-k采样、使用温度等。而大部分操作都是在logits上进行操作，这相当于改变了模型的输出分布。而在最终分布上的采样操作，都是相同的。因此我们可以只在朴素的标准采样上进行分析，而结果可以推广到其他的解码策略上。</p>\n<p>假设 <span class=\"math inline\">\\(p(x)\\)</span> 是target model <span class=\"math inline\">\\(M_p\\)</span> 在当前输入下的分布， <span class=\"math inline\">\\(q(x)\\)</span> 是approximation model <span class=\"math inline\">\\(M_q\\)</span> 在当前输入下的分布。</p>\n<p>投机解码的做法是，先采样 <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span>，如果 <span class=\"math inline\">\\(q(x)\\leq\np(x)\\)</span>，就保留 <span class=\"math inline\">\\(x\\)</span>，否则就以\n<span class=\"math inline\">\\(1-\\frac{p(x)}{q(x)}\\)</span> 的概率拒绝\n<span class=\"math inline\">\\(x\\)</span>，并在分布 <span class=\"math inline\">\\(p&#39;(x)=norm(max(0,p(x)-q(x)))\\)</span>\n对被拒绝的 <span class=\"math inline\">\\(x\\)</span>\n重新采样，并结束当前的投机解码。</p>\n<p>其中 <span class=\"math inline\">\\(norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\)</span>\n。</p>\n<p>看起来并不复杂。一个问题是，为什么这样从 <span class=\"math inline\">\\(q(x)\\)</span> 采样之后，我们得到的结果符合分布\n<span class=\"math inline\">\\(p(x)\\)</span>？即按这样的概率进行拒绝之后，结果和target\nmodel自己解码一样？</p>\n<p>从公式上来说，approximation model的抽样有 <span class=\"math inline\">\\(\\tilde{x}\\sim q\\)</span>。假设 <span class=\"math inline\">\\(X\\)</span> 是最终结果，我们的目标就是证明 <span class=\"math inline\">\\(\\mathbb{P}(X=x)=p(x)\\)</span>。</p>\n<p>而要使得 <span class=\"math inline\">\\(X=x\\)</span>，只有 <span class=\"math inline\">\\(\\tilde{x}=x\\)</span> 且 <span class=\"math inline\">\\(\\tilde{x}\\)</span> 被接受，或者在 <span class=\"math inline\">\\(\\tilde{x}\\)</span> 被拒绝之后重新采样到 <span class=\"math inline\">\\(\\tilde{x}=x\\)</span> 两种情况，即有</p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{\naccepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})\\]</span></p>\n<p>对于第一项，有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{\nассерґе}d|\\tilde{x}=x)\\\\=&amp;q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&amp;\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n\\]</span></p>\n<p>而第二项里</p>\n<p><span class=\"math display\">\\[\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{\naccepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ ассерґе}d)\n\\\\\n=1-\\sum_{x&#39;}\\min(q(x&#39;),p(x&#39;)) \\\\\n=\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;)) \\\\\n\\end{gathered}\\]</span></p>\n<p>上式第三行到第四行的解释：第三行相当于计算1减区域b的面积，而区域a+区域b的面积和为1，因此第三行相当于区域a的面积，即\n<span class=\"math inline\">\\(\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;))\\)</span>。</p>\n<img src=\"/f5c015c/formula.png\" class title=\"图解\">\n<p>从采样规则，有</p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\]</span></p>\n<p>因此</p>\n<p><span class=\"math display\">\\[\\mathbb{P}(\\tilde{x}\\text{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\max(0,p(x)-q(x))\\]</span></p>\n<p>最终有</p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)\\]</span></p>\n<p>因此按照前面设计的规则进行采样，就能保证结果和target\nmodel自己解码出来的一样。</p>\n<h1 id=\"approximation-model的评估\">approximation model的评估</h1>\n<p>approximation model的一个采样 <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span> 被target model接受的概率为 <span class=\"math inline\">\\(\\beta\\)</span>，我们把这个概率叫acceptance\nrate接受率。</p>\n<p>那么其期望值 <span class=\"math inline\">\\(E(\\beta)\\)</span>\n就是approximation model对target model拟合质量一个很好的评估指标。</p>\n<p><span class=\"math inline\">\\(E(\\beta)\\)</span>\n越大，每个token被接受的概率越大，那么每次投机解码能获得的输出token越多。</p>\n<p>我们令 <span class=\"math inline\">\\(\\alpha=E(\\beta)\\)</span>，并且为简化起见，假设\n<span class=\"math inline\">\\(\\beta\\)</span>\n的分布是i.i.d.的，那么跑一次投机解码能够获得的token数量是一个capped\ngeometric variable，其期望值如下式</p>\n<p><span class=\"math display\">\\[E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\]</span></p>\n<p>不同 <span class=\"math inline\">\\(\\gamma\\)</span> 下的图像如下</p>\n<img src=\"/f5c015c/fi_expected_token_num.png\" class title=\"解码数量期望值\">\n<p>而 <span class=\"math inline\">\\(\\alpha\\)</span> 是可以推算的。</p>\n<p>首先定义一个 <span class=\"math inline\">\\(M_p\\)</span> 和 <span class=\"math inline\">\\(M_q\\)</span> 之间的divergence <span class=\"math inline\">\\(D_{LK}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(M(x)=\\frac{p(x)+q(x)}2\\)</span>。</p>\n<p>而</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\sum_x|p(x)-M(x)|\\\\=&amp;\\sum_x\\frac{|p-q|}{2}\\\\=&amp;1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&amp;1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n\\]</span></p>\n<p>因此有</p>\n<p><span class=\"math display\">\\[D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))\\]</span></p>\n<p><span class=\"math inline\">\\(D_{LK}(p,q)\\)</span>越小，则 <span class=\"math inline\">\\(M_p\\)</span> 和 <span class=\"math inline\">\\(M_q\\)</span> 越相近。如果 <span class=\"math inline\">\\(D_{LK}(p,q)=0\\)</span>，说明 <span class=\"math inline\">\\(p=q\\)</span>；如果 <span class=\"math inline\">\\(D_{LK}(p,q)=1\\)</span>，说明 <span class=\"math inline\">\\(p\\)</span> 和 <span class=\"math inline\">\\(q\\)</span> 两个分布完全没有交叉的部分。</p>\n<p>根据 <span class=\"math inline\">\\(\\beta\\)</span> 的定义，有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n\\beta=&amp;E_{x\\sim q(x)}\\begin{cases}1&amp;q(x)\\leq\np(x)\\\\\\frac{p(x)}{q(x)}&amp;q(x)&gt;p(x)\\end{cases}\\\\\n=&amp;E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&amp;\\sum_x\\min(p(x),q(x))\\\\\n=&amp;1-D_{LK}(p,q)\n\\end{aligned}\n\\]</span></p>\n<p>最终得到</p>\n<p><span class=\"math display\">\\[\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))\\]</span></p>\n<p>实验中，不同approximation model和target model之间测得的 <span class=\"math inline\">\\(\\alpha\\)</span> 值如下表所示</p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"各种模型的alpha\">\n<h1 id=\"耗时优化的分析\">耗时优化的分析</h1>\n<p>定义cost coefficient <span class=\"math inline\">\\(c\\)</span>，表示\n<span class=\"math inline\">\\(M_q\\)</span> 单次推理 和 <span class=\"math inline\">\\(M_p\\)</span> 单次推理的比值。</p>\n<p>和仅与模型相关的 <span class=\"math inline\">\\(\\alpha\\)</span>\n不同，<span class=\"math inline\">\\(c\\)</span>\n的具体值会受到硬件、推理框架等影响。在论文的实验中 <span class=\"math inline\">\\(c\\)</span> 的值大部分小于0.05。</p>\n<p>假设 <span class=\"math inline\">\\(M_p\\)</span> 每次推理所需的时间为\n<span class=\"math inline\">\\(T\\)</span>，则一次投机解码所需的时间为 <span class=\"math inline\">\\(Tc\\gamma+T\\)</span>。</p>\n<p>根据前面的推算，投机解码每次能获得的token数为 <span class=\"math inline\">\\(E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\n个，因此每个token所需的时间为 <span class=\"math inline\">\\(\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T\\)</span>。综上，使用投机解码在推理时间上的improvement\nfactor为</p>\n<p><span class=\"math display\">\\[\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\]</span></p>\n<p>只要 <span class=\"math inline\">\\(\\alpha&gt;c\\)</span>，就一定存在能提升解码效率的\n<span class=\"math inline\">\\(\\gamma\\)</span>，并且improvement\nfactor至少为 <span class=\"math inline\">\\(\\frac{1+\\alpha}{1+c}\\)</span>（<span class=\"math inline\">\\(\\gamma=1\\)</span>时）。</p>\n<h1 id=\"计算成本的分析\">计算成本的分析</h1>\n<p><span class=\"math inline\">\\(M_p\\)</span> 同时对 <span class=\"math inline\">\\(\\gamma+1\\)</span>\n个token进行验证。如果一个token被接受了，那么推理效率就获得了提升；如果token被拒绝了，那么相关的计算就没有实际收益，就会有计算的“浪费”。</p>\n<p>假设 <span class=\"math inline\">\\(\\hat{c}\\)</span> 是 <span class=\"math inline\">\\(M_q\\)</span> 和 <span class=\"math inline\">\\(M_p\\)</span> 计算一个token的arithmetic\noperations的比例，<span class=\"math inline\">\\(\\hat{T}\\)</span> 是 <span class=\"math inline\">\\(M_p\\)</span> 解码一个token所需的arithmetic\noperations。</p>\n<p>那么一次投机解码的计算量就是 <span class=\"math inline\">\\(\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)\\)</span>，这个计算量除以投机解码每次获得的token数\n<span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\n就得到平均每个token的计算量为 <span class=\"math inline\">\\(\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span>。</p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span> 越大，<span class=\"math inline\">\\(\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span>\n这个比值越小，平均计算成本越低。</p>\n<p>另外，使用投机解码减少了KV cache和显存的读写。</p>\n<h1 id=\"gamma-的选择\"><span class=\"math inline\">\\(\\gamma\\)</span>\n的选择</h1>\n<p>给定 <span class=\"math inline\">\\(\\alpha\\)</span> 和 <span class=\"math inline\">\\(c\\)</span>，最佳的 <span class=\"math inline\">\\(\\gamma\\)</span> 应该最大化walltime improvement\nfactor <span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\)</span>。</p>\n<p>下图给出不同 <span class=\"math inline\">\\(\\alpha\\)</span> 和 <span class=\"math inline\">\\(c\\)</span> 下，最佳的 <span class=\"math inline\">\\(\\gamma\\)</span> 值</p>\n<img src=\"/f5c015c/fi_choose_gamma.png\" class title=\"gamma的选择\">\n<p>推理速度和总计算量之间有tradeoff，即增大 <span class=\"math inline\">\\(\\gamma\\)</span>\n会提升推理速度，同时也会带来更多的计算成本，如下所示</p>\n<img src=\"/f5c015c/fi_speed_and_op_table.png\" class title=\"计算量和加速\">\n<img src=\"/f5c015c/fi_speed_and_op.png\" class title=\"计算量和加速\">\n<img src=\"/f5c015c/fi_walltime.png\" class title=\"walltime\">\n<p>实际上，<span class=\"math inline\">\\(\\beta\\)</span>\n并不是固定的常数，因此实际上我们可以通过在投机解码的过程中预测 <span class=\"math inline\">\\(\\beta\\)</span> 值来选择 <span class=\"math inline\">\\(\\gamma\\)</span>，这是未来的一个改进方向。</p>\n<h1 id=\"approximation-model的选择\">approximation model的选择</h1>\n<p>论文的实验中，一部分使用现成的模型作为approximation\nmodel。这种情况下，让approximation model的参数规模比target\nmodel小两个数量级是比较好的选择，能够平衡推理加速和计算量。</p>\n<p>有趣的是，即使使用很简单的模型，比如n-gram模型作为approximation\nmodel，也能获得不错的 <span class=\"math inline\">\\(\\alpha\\)</span>\n值。</p>\n<p>另外，在一些特殊的任务，比如摘要任务，由于生成结果往往会从输入的原文里摘取内容，因此使用一个会从输入里copy\ntoken的approximation model可能会得到较高的 <span class=\"math inline\">\\(\\alpha\\)</span> 值。</p>\n<p>approximation model的另一个选择是如《Blockwise parallel decoding for\ndeep autoregressive models》使用的非自回归模型。</p>\n<h1 id=\"实验\">实验</h1>\n<p>论文在翻译任务和摘要任务上测试了投机解码的效果。使用了T5的较小规模模型作为approximation\nmodel，来加速T5-XXL的推理，效果如下表，最高能达到3倍+的推理加速。</p>\n<img src=\"/f5c015c/fi_t5_result.png\" class title=\"T5系列加速效果\">\n<p>此外，论文对更多样的模型组合测试了 <span class=\"math inline\">\\(\\alpha\\)</span> 值，如下表所示</p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"各种模型的alpha\">\n<p>可以观察到，比target model小几个数量级的approximation\nmodel倾向于产生介于0.5和0.9之间的 <span class=\"math inline\">\\(\\alpha\\)</span>\n值。还注意到，对于所有模型，用于采样的分布越尖（即T比较小，如argmax），\n<span class=\"math inline\">\\(\\alpha\\)</span> 值越高。</p>\n<h1 id=\"小结\">小结</h1>\n<ul>\n<li>投机解码可以在完全无损的情况下，把推理速度提升2~3倍<br>\n</li>\n<li>即使使用最简单的n-gram模型，也能在投机解码的策略下获得推理速度提升<br>\n</li>\n<li>正常来说，使用比target model小两个数量级的approximation\nmodel就有较好的效果</li>\n</ul>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Fast Inference from Transformers via Speculative Decoding\nhttps://arxiv.org/abs/2211.17192<br>\n【2】Accelerating Large Language Model Decoding with Speculative\nSampling https://arxiv.org/abs/2302.01318</p>\n"},{"title":"大模型推理窗口-从有限到无限大","abbrlink":"45ee1a6d","date":"2024-05-06T08:22:38.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n步入2024年Q2，大模型在RAG、文档对话、大模型Agent能力等方向的发展持续升温。在平时的日常生活和工作中，大模型工具提供的文档总结、文本润色、代码生成等能力已经是提高效率的必备帮手，甚至在一些复杂或者不熟悉的场景上，大模型也已经能提供一些比较专业的帮助。  \n\n在这些方向上，大模型(超)长上下文的能力都是基础。目前不少模型已经在128k+的长度上展示出比较强的能力，这些模型有的收集了超长训练数据，有的优化位置编码，有的则暴力训练，以提升模型的长上下文能力。  \n\n另外一些工作则另辟蹊径，直接让模型具备无限大窗口的能力。本篇将梳理几个理论上支持无限大上下文长度的工作。  \n\n# StreamingLLM  \n\n论文：Efficient Streaming Language Models with Attention Sinks  \n\n时间：2023年9月  \n\n长度：测试4M，理论∞  \n\n阶段：无需训练，直接应用在推理阶段  \n\n这是一篇由MIT，CMU，NVIDIA，META联合发布的论文。论文提出了一个在流式输出场景下支持无限大上下文长度的方法StreamingLLM，并且不需要进行任何形式的训练或微调 -- 在有限长度上训练，在无限长度上推理。论文用LLAMA2, MPT, Falcon和Pythia作为基线模型进行验证，在4M的上下文长度，StreamingLLM仍然能够保持较低的PPL。  \n\n需要注意的是，StreamingLLM主要关注在超长上下文的情况下，让模型能够生成低PPL的内容，但是在内容的准确性上并没有保证。  \n\n## 背景  \n\n大模型流式应用，比如现在大部分对话机器人，在部署推理的时候，在长上下文的情况下通常会面临两个问题：  \n- 缓存不足：流式应用下，为了提升响应速度，KV缓存目前是不可缺少的。而随着对话交互的进行，KV缓存所需的空间也以线性速度在增长。鉴于目前模型的规模动辄百亿千亿的参数量，即使较强的GPU H100、A100拥有40G/80G的显存，推理时也捉襟见肘。在显存不足的情况下，我们就不得不抛弃部分旧的KV缓存值，这也导致了模型效果的下降。  \n- 训练长度：主流模型大部分使用旋转位置编码RoPE或其变体，而RoPE的外推能力一般，因此当推理输入的上下文长度远超训练的长度时，模型效果也会迅速下降。  \n\n现有一些工作针对其中的部分问题进行优化：  \n- 类似Longformer那样的window attention通过只保存最近的KV值来缓解缓存不足的问题，同时保持一定的窗口扩展能力。但是论文观察到，一旦上下文长度超过缓存大小，window attention的机制会把最初的部分token移出缓存，这时模型的效果依然会迅速下滑。  \n- sliding window with recomputation（https://github.com/mit-han-lab/streaming-llm/issues/51），可以节省KV所需的空间，但是recomputation计算量和长度是平方关系。这个方案也是论文基线方案里效果最接近可用的。  \n- 针对位置编码的工作，比如线性插值、NTK插值、YaRN，已经被证明是有效的，但是优化的窗口依然是有限的，大概能在原来的训练长度的基础上，提升一倍或者几倍的推理窗口。论文还提到长窗口的一个实践工作，一个关注在NSFW内容(手动滑稽)的模型：https://kaiokendev.github.io/til#extending-context-to-8k  \n- 针对推理效率，也有一些工作：《Efficiently scaling transformer inference，SmoothQuant: Accurate and efficient post-training quantization for large language models》、《Dynamic context pruning for efficient and interpretable autoregressive transformers》、《Spatten: Efficient sparse attention architecture with cascade token and head pruning》、《H2o: Heavyhitter oracle for efficient generative inference of large language models》，但是在支持的最大长度依然有限制。  \n- 无损的FlashAttention已经被广泛采用，无论是训练还是推理都很有帮助。  \n- 其他一些有损的扩展方案，比如Big Bird、Linformer等。  \n\nStreamingLLM这篇论文发现在长上下文的情况下，存在attention sink的现象，这也是window attention在上下文超过缓存大小之后效果迅速变差的原因。通过利用attention sink，让模型在超长上下文的情况还可以保持较低的PPL。并且和基线里唯一效果比较好的sliding window with recomputation方案相比，StreamingLLM在速度上有22+倍的提升。  \n\nStreamingLLM和其他方案在长上下文上的PPL对比如下图所示，StreamingLLM在各个模型上都能保持稳定较低的PPL。  \n\n{% asset_img streamingllm_model_ppl.png PPL对比 %}  \n\n## attention sink  \n\n从window attention在上下文超过缓存大小之后的失效，论文发现自回归LLM存在的一个有趣现象：对于输入文本最靠前的少量几个token，无论它们在语义上与语言建模任务的相关性如何，大量的注意力分数都会分配给他们，如下图所示  \n\n{% asset_img stremingllm_attention_sink.png attention sink %}  \n\n模型的前两层还能保持attention score更多分配给当前token附近位置的特性，而在其他层，靠前的几个token都会接受到大量的注意力。  \n\n论文里把这些token称为attention sink。尽管这些token在语义上很可能并没有什么重要性，但它们却聚集了大量的注意力分数。  \n\n出现这个现象的原因就是softmax操作。softmax要求所有上下文token的注意力分数加起来等于1，因此，即使当前token跟前面的其他token都没有匹配的需要，模型仍然需要将多余的注意力值分配到前面的某些token，以使得总和为1。  \n\n那么文本最开头的几个初始token就会承担“接收多余的、不需要的注意力”的任务。为什么是初始token来承担这个任务，最简单的原因就是，对于自回归语言建模，初始token对所有后续token都是可见的，这使得它们更容易被训练成attention sink。  \n\n上面这个解释还只是猜想，于是论文做了一个实验来验证这个猜想：把初始的4个token都换成没有重要实际语义的换行符号\\n，结果发现模型依然会把大量的注意力分配给这些token，这就说明attention sink这个现象和内容语义无关，而只和这些token所在的位置相关。  \n\n这个现象在一些quant的工作里也有发现，比如《SmoothQuant: Accurate and efficient post-training quantization for large language models》和《Quantizable transformers: Removing outliers by helping attention heads do nothing》等。  \n\n## 推理到无限长  \n\n基于以上的发现，论文提出了StreamingLLM。StreamingLLM利用了attention sink具有高注意力值的事实，认为保留它们可以保持注意力分数的分布接近正常。  \n\n因此，StreamingLLM的方案就是，在window attention的基础上，增加一个策略，保留了attention sink token的KV值，这些token的KV值会和滑动窗口内的token一起构成完整的KV cache，用于进行新token的推理计算，StreamingLLM和其他方案的attention计算如下图所示。  \n\n{% asset_img streamingllm_compare.png StreamingLLM对比 %}  \n\n这里有一个问题，要保留多少个初始token才足够。论文在几个不同的模型上做了实验，结果如下表所示。  \n\n{% asset_img stremingllm_init_token_num.png attention sink number %}  \n\n实验发现，使用4个初始token就可以基本把PPL降下来，而只使用一个或者两个初始token则仍有很大损失。  \n\n最终方案是，在window attention的基础上，StreamingLLM把KV cache分成两部分：  \n- 包含4个初始token的attention sink  \n- 正常滚动的KV cache  \n\nStreamingLLM的KV缓存如下所示  \n\n{% asset_img stremingllm_kv_cache.png cache %}  \n\n一个重要的点是，StreamingLLM在实际应用上，位置编码中的相对位置，不再直接使用原文中的distance，而是改成使用这些token在cache中的distance。对于像RoPE这样的位置编码，需要缓存引入旋转变换之前token的KV值，解码时再把这些token在缓存内的相对位置加上。这样做使得模型不用处理大于预训练窗口大小的位置编码，而保证了效果。（关于这一点，在下面一篇论文《LM-Infinite》有相关实验）  \n\n当前的大模型基本都没有针对attention sink的现象而做针对性的设计，论文提出可以在训练的时候增加一个特殊的token作为attention sink token使用，方便模型把多余的注意力值放在这个特殊token上。  \n\n如果不增加一个特殊的token，那另外一个方法就是使用softmax的变体，softmax-off-by-one，替换attention中的softmax。  \n\nsoftmax-off-by-one的公式如下所示  \n\n$$\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}$$  \n\nsoftmax-off-by-one在分母增加了一个常数1，这样就不要求所有attention score的和为1。这相当于有一个K和V全都是0的虚拟token，固定和所有其他token的注意力得分为1，这个虚拟token在一定程度上也起到了attention sink的作用。  \n\n为了验证增加特殊token作为attention sink，以及使用softmax-off-by-one的方案效果，论文在相同设置下训练了3个160M参数的模型，效果如下  \n\n{% asset_img stremingllm_exp.png 实验 %}  \n\n从结果上看，即使使用softmax-off-by-one（zero sink），模型还是会依赖前几个token作为attention sink。  \n\n最后，论文在LLAMA2, MPT, Falcon和Pythia模型验证StreamingLLM在4M长度上的效果。MPT使用的位置编码是ALIBI，其他模型则是RoPE。效果如下图\n\n{% asset_img stremingllm_perf_4m.png 效果 %}  \n\n在4M的长度上，StreamingLLM仍然能保持PPL较为稳定。  \n\n理论上StreamingLLM可以把窗口推到无限长，但是由于本质上还是window attention，因此超过cache大小的部分还是会被丢弃。因此StreamingLLM虽然能保持PPL较低，但是对于需要高精度阅读理解和推理的任务，StreamingLLM还是有局限。  \n\n# LM-Infinite  \n\n论文：LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models  \n\n时间：2023年8月  \n\n长度：在2k或者4k长度上训练，在200M长度上推理保持较低PPL  \n\n阶段：无需训练，直接应用在推理阶段  \n\nLM-Infinite可以和现有的LLM结合使用，在Passkey Retrieval和Qasper任务上有比较好的效果，且和原始模型相比，在解码速度上有2.7倍提升，显存上节省7.5倍。  \n\n## 三个挑战  \n\nLM-Infinite认为，Transformer LLM无法有效地泛化到长上下文的情况，主要是因为会面临3个挑战。  \n\n- 挑战1：challenges in handling unseen distances among tokens  \n\n对于使用相对位置编码的模型，两个token的位置对注意力值的影响取决于两个token之间的距离。当推理的长度越来越长，就会出现在训练时没有见过的相对距离，此时attention logits就会倾向于爆炸增长到无穷大，以区分在训练时从未见过的距离。论文对此给出了数学证明。  \n\n而从实践上来看，把LLAMA2所有注意力头在ArXiv数据集上8k以内的attention logits抽取出来，其均值和方差如下图所示。  \n\n{% asset_img lm_infinite_attention_logits_explode.png logits爆炸 %}  \n\nLLAMA2的预训练窗口大小为4k，可以看到4k之后，attention logits的均值和方差开始有明显的上升。  \n\n那么为了缓解这个问题，一个自然的想法是将token之间的相对距离值限制在模型预训练期间看到过的最大值，即设置一个距离上限。这样可以解决logits爆炸，但是会导致下面这个问题。  \n\n- 挑战2：attending to unseen numbers of tokens  \n\n对于较长的上下文，位置靠后的token需要在更大的上下文长度范围内，分配它的注意力权重。论文提出，如果注意力logits有界，那么随着上下文长度变长，注意力熵(attention entropy)将增长至无穷大。这里原文也给出了数学证明。  \n\n直观来说，在注意力权重分配为平均分配的时候，注意力熵最大。而注意力熵增大，表示当前的token不能确定应该将主要注意力放在上文的哪些token，即无法集中注意力。  \n\n同样地，我们把模型在8k内的注意力熵画出来，结果如下图所示，随着长度增大，注意力熵也逐渐增大。  \n\n{% asset_img lm_infinite_attention_entropy.png 注意力熵爆炸 %}  \n\n这表明我们应该限制attention context size，以确保注意力熵保持在预训练见过的范围内。  \n\n如果简单地使用一个window attention，使得每个token只能关注到一定距离内的token，在一定程度上可以一定程度上handle挑战1和挑战2。这样的思路和XPos、Longformer类似。  \n\n但是使用window attention，又会引入新的问题。  \n\n- 挑战3：starting tokens occupy a distinct feature space  \n\n即使没有显式的绝对位置编码，位置最靠前的几个初始token的注意力输出和其他位置相比，也会占据一个独特的表示空间。  \n\n这一结论来自于《The impact of positional encoding on length generalization in transformers》中的定理1，该定理证明了即使没有使用位置编码，单向注意力使得单个注意力层中的token输出也可以隐式地编码绝对位置，而初始token的信号是最强的，且很容易和其他token区分开来。  \n\n这个观察和StreamingLLM中指出的attention sink很相似，总之就是最靠前的几个初始token有很大的影响，因此使用window attention把初始token忽略会带来很大的效果损失。  \n\n论文把LLAMA2中各层的hidden state output做了PCA，降到2维之后画出来，如下图所示  \n\n{% asset_img lm_infinite_starting_tokens.png 初始token %}  \n\n蓝色的点表示初始token，红色的点表示靠后的token。上图显示了初始token的特征在特征空间中占据了和其他token很不同的区域。这就解释了为什么简单地将注意力窗口限制在最近的token上会效果不好：由于注意力本质上是加权平均，丢弃初始的几个token影响了注意力输出到达它们应占据的空间。  \n\n因此，我们需要保留初始token。  \n\n## 解决方案  \n\n基于上面的这些观察和发现，论文提出LM-Infinite，给LLM做zero-shot的长度泛化。LM-Infinite设计了两个主要机制，Λ-shaped attention mask和Distance ceiling。  \n\n- Λ-shaped attention mask  \n\nΛ-shaped attention mask在概念上和Longformer、LongNet、Big Bird的思路类似。  \n\nΛ-shaped attention mask在window attention的基础上，保持了初始token对后续所有token可见。具体来说，每个token能关注到两部分：（1）$n_{\\mathrm{starting}}$ 个初始token（2）$L_{\\mathrm{pretrain}}$ 个最近的token。其中$n_{\\mathrm{starting}}$ 为超参，$L_{\\mathrm{pretrain}}$ 是预训练时的最大窗口大小。除了这两个部分的token之外，其他token都会被直接忽略。  \n\n论文对 $n_{\\mathrm{starting}}$ 的数量进行了实验，发现 $n_{\\mathrm{starting}}\\in[5,100]$ 时的效果比较好，如下图所示。  \n\n{% asset_img lm_infinite_starting_tokens_num.png 初始token数量实验 %}  \n\n注意这里不需要对模型进行任何训练和微调，只需要修改推理的方式即可。  \n\nΛ-shaped attention mask可以解决挑战2和挑战3。  \n\n- Distance ceiling  \n\nLM-Infinite把最大距离限制在 $L_{\\mathrm{pretrain}}$，这样做基本只影响初始token的位置编码。  \n\n具体来说，对于相对位置编码，假设原来的attention logit是 $w(\\mathbf{q},\\mathbf{k},d)$，其中 $d$ 是两个token之间的距离；那么Distance ceiling就是把attention logit的计算变成：  \n\n$$\\text{attention logits}=w(\\mathbf{q},\\mathbf{k},d')$$  \n\n$$d'=\\min(d,L_\\text{pretrain})$$  \n\nDistance ceiling限制了最大距离，可以解决挑战1。  \n\n- Optionally attending to top-k tokens in the middle  \n\n除了Λ-shaped attention mask和Distance ceiling以外，论文还设计了一个可选的机制，用于把“除了当前窗口内的最近token和初始token”以外的中间部分token也加入到attention计算中，这部分token本来是会被直接丢弃的。  \n\n具体来说，会从原本被抛弃的中间token里，选择 $k$ 个attention logits最大的token加入到attention 计算中来。这 $k$ 个token的距离都会被统一设置为 $d=\\frac12L_\\text{pre-train}$。  \n\n对 $k$ 的取值，论文在Passkey Retrieval的validation set上做了实验，结果如下  \n\n{% asset_img lm_infinite_middle_k.png k的数量 %}  \n\n最终取了 $k=5$，并且只在模型>5的层中加入。  \n\n加入middle token主要用于优化需要精准获取中间信息的任务，对于没有这个需要的任务，加入过多的middle token不仅无益，甚至有害，原因如前面挑战2所分析的，会增大注意力熵。  \n\n最终，LM-Infinite的整体方案如下图所示  \n\n{% asset_img lm_infinite_design.png 设计 %}  \n\n## 实验  \n\n论文在LLaMA-7B、LLaMA2-7B、MPT-7B和GPT-J-6B这几个模型验证LM-Infinite的效果。除了MPT-7B使用Alibi之外，其他模型都是用RoPE。  \n\n- Language Modeling  \n\n首先是语言建模的评测，使用的数据集是ArXiv和OpenWebText2。  \n\n选择的几个基线模型，以及这些模型使用LM-Infinite的推理策略之后，在0-12k长度上的PPL如下图所示  \n\n{% asset_img lm_infinite_ppl_figure.png PPL %}  \n\nLLAMA2在略长于10K的长度上就输出了NaN，因此其曲线较短。所有原始模型在约32K的时候就出现OOM。  \n\n当测试长度超过训练长度时，基线模型的PPL会迅速爆炸。而使用LM-Infinite之后，所有模型都可以泛化到比训练长度长得多，而仍然保持较低PPL。\n\n为了验证LM-Infinite在更大长度上的效果，用LM-Infinite + Llama2，在由ArXiv数据重复抽样拼接构成的200M token长度上的评估PPL。结果如下图所示，在200M长度下，依然可以保持稳定较低的PPL。  \n\n{% asset_img lm_infinite_ppl_200m.png PPL 200M %}  \n\n- Passkey Retrieval and Qapser  \n\nPasskey Retrieval和Qapser就是需要对整个上下文有精确理解的任务，因此对于这两个任务，加入了top-5个middle token到attention计算中。  \n\n结果如下表。原始模型在长度超出训练长度之后，几乎无法输出有效内容，因此Passkey Retrieval上全得0分。而LM-Infinite相比简单的长度裁剪方案，在两个任务上都有更好效果。  \n\n{% asset_img lm_infinite_downstream.png 下游任务 %}  \n\n- Ablation study  \n\n对于LM-Infinite的两个主要设计，Λ-shaped attention mask和Distance ceiling，论文做了消融实验，如下图所示。  \n\n{% asset_img lm_infinite_ablation.png 消融实验 %}  \n\n只使用Λ-shaped attention mask或者distance ceiling单一策略的情况下，PPL仍然会出现较明显的上升，说明这两个组件都是必须的。  \n\n# Transformer-XL  \n\n讲下一篇Infini-Transformer之前，先简单回顾下Transformer-XL的思路。  \n\nTransformer-XL是2019年6月，由CMU和Google Brain发布的一个工作，目的是赋予在固定长度上训练的模型，在更大长度上的推理能力。  \n\n一般来说，transformer的训练和推理如下图所示，训练的时候模型在固定的长度上训练，如果输入文本超过了模型的训练窗口大小，那就把输入切分成多个segment，模型在各个segment分别独立地进行训练；而推理的时候，如果文本超过了模型的推理窗口大小，模型可以使用滑动窗口的方式，逐步进行推理。  \n\n{% asset_img xl_vanilla_sw.png vanilla transformer %}  \n\n在窗口滑动的过程中，模型始终只能看到窗口内的内容，对于超出的部分则无法把信息纳入计算中，这显然对有长距离依赖的任务有损害。  \n\n为了让模型看得更远，Transformer-XL把attention计算修改成下面的样子：把输入切分成多个segment，每个的长度为L，对于两个相邻的segment $[\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]$ 和 $\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]$，计算如下  \n\n$$\\begin{aligned}&\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}$$  \n\n其中SG是stop gradient的操作，这样在训练的时候能保持仍然只有固定窗口大小的参数进行梯度计算和更新，而不会扩大显存需求。$\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}$ 表示在序列长度维度进行拼接。h、q、k、v的上标 $n$ 和 $n-1$ 是模型的层序号。  \n\n对于当前segment的每一层attention计算，Transformer-XL把上一个segment的信息加入到当前的K和V中再进行注意力计算，这样相当于每层相比上一层，都能往前多关注到一个segment长度的信息，模型的层数越多，模型最终能关注的长度越长，最后一个segment的最终输出依赖第一个segment的第一层输出，如下图所示。  \n\n{% asset_img xl_attention.png Transformer-XL %}  \n\n# Infini-Transformer  \n\n论文：Leave No Context Behind:Efficient Infinite Context Transformers with Infini-attention  \n\n时间：2024年4月  \n\n长度：实验500k/1M，理论∞  \n\n阶段：继续预训练/微调  \n\nGoogle提出的一种叫Infini-attention的注意力计算方式，可以使用有限的显存计算无限长的上下文。  \n\n## 背景\n\n记忆能力是智能的基石，因为它使得针对特定上下文的高效计算成为可能。  \n\n但是transformer的注意力计算在时间和空间上都跟输入长度是二次的关系，这就使得在长上下文的场景下很难高效、准确地处理信息。  \n\n使用记忆模块可以在大大减少计算量和显存需求的前提下，依然保持长上下文完整信息对模型的可见性。理论上所有输入信息都可以保存在记忆模块中，因此模型具备处理“有长距离依赖，且需要对输入信息进行准确检索”的任务。从这点上看，和StreamingLLM相比，记忆模块的效果上限更高。  \n\n针对高效的记忆，目前也有一些相关工作，比如《Metalearned neural memory》/《Enhancing the transformer with explicit relational encoding for math problem solving》所使用的compressive memory，可以用固定大小的记忆模块来编码context，但是现有的记忆模块在效果和设计的simplicity上仍然存在tradeoff，比较难平衡。  \n\n## Infini-attention  \n\n基于此，Infini-Transformer主要的改进点就在记忆模块的设计上。Infini-attention的结构设计如下图所示，每个Transformer block内包括了常规的mask local attention，和储存长期记忆的long term linear attention。  \n\n{% asset_img infini_attention_structure.png infini-attention结构 %}  \n\nInfini-attention机制复用了标准注意力计算中的所有QKV值，构建了固定大小的注意力模块，用于长期记忆。在处理后续序列时，根据当前的Q从记忆模块中检索相关内容。Infini-attention将长期记忆检索到的值与局部注意力上下文通过concat进行聚合，共同计算最终的输出。  \n\n这种对Transformer注意力层的微小但关键的修改，使得现有的LLM能够通过继续预训练和微调自然地扩展到无限长的上下文。  \n\nInfini-attention和Transformer-XL的对比如下图所示  \n\n{% asset_img infini_attention_process.png infini-attention计算方式 %}  \n\n和Transformer-XL类似，Infini-Transformer也把输入分成多个segment来处理。不同的是，Transformer-XL每层只能比前一层多看到一个segment的信息，因此需要通过堆叠模型的层数来提升模型处理窗口的大小；而由于层数是有限的，最终窗口的大小还是有限的。  \n\nInfini-Transformer则是每层都有对之前所有segment的长期记忆，因此不受模型层数的影响，可以扩展到无限大的窗口。  \n\ncompressive memory的具体设计，出于simplicity和computational efficiency的考虑，论文参考《Learning associative inference using fast weight memory》，把memory设计成一个associative matrix。  \n\n对于记忆模块，最重要的就是记忆的更新和检索。  \n\n- 记忆检索  \n\n先看下记忆的检索。假设每个segment的长度为 $N$，我们使用当前的 $Q\\in\\mathbf{R}^{N\\times d_{key}}$ 从memory $M_{s-1}\\in\\mathbf{R}^{d_{key}\\times d_{value}}$ 中检索，计算如下  \n\n$$A_{\\text{mеm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}$$  \n\n其中$\\sigma$是激活函数，使用的是element-wise ELU + 1。  \n\n$z_{s-1}\\in\\mathbf{R}^{d_{key}}$是normalization term。这里normalization term参考《Transformers are rnns: Fast autoregressive transformers with linear attention》的做法，使用所有K值的求和值。  \n\n- 记忆更新  \n\n完成检索之后，就要更新记忆。需要更新的有normalization term和memory两项，更新方式如下。  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^TV$$  \n\n$$z_s\\leftarrow z_{s-1}+\\sum_{t=1}^N\\sigma(K_t)$$  \n\nmemory的更新，参考《Metalearned neural memory》和《Learning associative inference using fast weight memory》所使用的delta rule，进一步优化成如下形式  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})$$  \n\n- 长期记忆和local attention结合  \n\n回到当前segment的注意力计算中，需要把长期记忆检索项 $A_{mem}$ 和local attention state $A_{dot}$ 结合  \n\n$$A=sigmoid(\\beta)\\odot A_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}$$  \n\n其中 $\\beta$ 是一个可学习的标量参数。  \n\n在训练中， $\\beta$ 所学习到的值如下，0则表示这个头只关注当前segment，1则表示值关注长期记忆的内容，介于0和1之间的则是两种的混合，可以看到大部分的注意力头会使用二者的混合结果  \n\n{% asset_img infini_attention_gating.png gating分布 %}  \n\n整体策略上，和其他包含segment-level memory的模型在方案设计上的比较如下  \n\n{% asset_img infini_attention_compare.png 方案比较 %}  \n\n## 实验\n\n实验上，每个segment的长度 $N$ 设为2048，模型的训练窗口大小为32768，即总共有16个segment。  \n\n- 语言建模能力\n\nInfini-Transformer和Transformer-XL/Memorzing Transformer/RMT在PG-19和Arxiv数据集上的language modeling能力对比如下  \n\n{% asset_img infini_attention_language_modeling.png 语言建模能力对比 %}  \n\nInfini-Transformer的效果显著更好，而且所需的记忆空间只有Memorizing Transformers的不到1%。  \n\n原文指出，把训练的数据长度提升到100k，Infini-Transformer的PPL还能进一步降低。  \n\n- 下游任务\n\n用1B的模型进行轻量级的继续预训练，训练配置如下：  \n- batch size = 64\n- step = 30k\n- 训练数据长度 > 4k\n- segment length = 2k\n\n之后在1M长度的passkey retrieval任务上评测，zero-shot和fine-tune模型在各个长度的效果如下表  \n\n{% asset_img infini_attention_passkey.png passkey效果 %}  \n\nInfini-Transformer（linear + delta）在1M长度的passkey retrieval任务上做到完全正确。  \n\n另外在8B模型上，用8k长度训练30k步，在500k的BookSum任务上评测，结果如下  \n\n{% asset_img infini_attention_booksum.png booksum效果 %}  \n\nInfini-Transformer（linear + delta）相比其他模型有一定优势。  \n\n# 小结  \n\n1. Streaming和LM-Infinite思路有些相似，都观察到attention sink对模型效果的影响，并设计了相关的注意力计算机制保留初始token以稳定PPL，而LM-Infinite在这个基础上使用distance ceiling，使得模型在推理时不会使用超出训练所用的token相对距离。虽然这两个工作使得模型可以在M级别的长度仍然保持较低PPL，但是实际大量中间token被丢弃，导致对需要在上下文精准检索的任务效果有损害。  \n2. Infini-Transformer使用记忆模块，把输入进行分段，并把靠前的内容通过固定大小的记忆矩阵进行压缩。理论上记忆模块可以提供所有上文的信息，能够应用在更精细和困难的任务。记忆模块的检索和更新的设计是方案的核心，需要考虑复杂度和效果的平衡。  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n【1】Efficient Streaming Language Models with Attention Sinks https://arxiv.org/abs/2309.17453  \n【2】Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://arxiv.org/abs/1901.02860  \n【3】Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention https://arxiv.org/abs/2404.07143  \n【4】LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models https://arxiv.org/abs/2308.16137  ","source":"_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大.md","raw":"---\ntitle: 大模型推理窗口-从有限到无限大\nabbrlink: 45ee1a6d\ndate: 2024-05-06 16:22:38\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 无限大\n  - 长上下文\n  - 预训练\n  - 微调\n  - attention\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n步入2024年Q2，大模型在RAG、文档对话、大模型Agent能力等方向的发展持续升温。在平时的日常生活和工作中，大模型工具提供的文档总结、文本润色、代码生成等能力已经是提高效率的必备帮手，甚至在一些复杂或者不熟悉的场景上，大模型也已经能提供一些比较专业的帮助。  \n\n在这些方向上，大模型(超)长上下文的能力都是基础。目前不少模型已经在128k+的长度上展示出比较强的能力，这些模型有的收集了超长训练数据，有的优化位置编码，有的则暴力训练，以提升模型的长上下文能力。  \n\n另外一些工作则另辟蹊径，直接让模型具备无限大窗口的能力。本篇将梳理几个理论上支持无限大上下文长度的工作。  \n\n# StreamingLLM  \n\n论文：Efficient Streaming Language Models with Attention Sinks  \n\n时间：2023年9月  \n\n长度：测试4M，理论∞  \n\n阶段：无需训练，直接应用在推理阶段  \n\n这是一篇由MIT，CMU，NVIDIA，META联合发布的论文。论文提出了一个在流式输出场景下支持无限大上下文长度的方法StreamingLLM，并且不需要进行任何形式的训练或微调 -- 在有限长度上训练，在无限长度上推理。论文用LLAMA2, MPT, Falcon和Pythia作为基线模型进行验证，在4M的上下文长度，StreamingLLM仍然能够保持较低的PPL。  \n\n需要注意的是，StreamingLLM主要关注在超长上下文的情况下，让模型能够生成低PPL的内容，但是在内容的准确性上并没有保证。  \n\n## 背景  \n\n大模型流式应用，比如现在大部分对话机器人，在部署推理的时候，在长上下文的情况下通常会面临两个问题：  \n- 缓存不足：流式应用下，为了提升响应速度，KV缓存目前是不可缺少的。而随着对话交互的进行，KV缓存所需的空间也以线性速度在增长。鉴于目前模型的规模动辄百亿千亿的参数量，即使较强的GPU H100、A100拥有40G/80G的显存，推理时也捉襟见肘。在显存不足的情况下，我们就不得不抛弃部分旧的KV缓存值，这也导致了模型效果的下降。  \n- 训练长度：主流模型大部分使用旋转位置编码RoPE或其变体，而RoPE的外推能力一般，因此当推理输入的上下文长度远超训练的长度时，模型效果也会迅速下降。  \n\n现有一些工作针对其中的部分问题进行优化：  \n- 类似Longformer那样的window attention通过只保存最近的KV值来缓解缓存不足的问题，同时保持一定的窗口扩展能力。但是论文观察到，一旦上下文长度超过缓存大小，window attention的机制会把最初的部分token移出缓存，这时模型的效果依然会迅速下滑。  \n- sliding window with recomputation（https://github.com/mit-han-lab/streaming-llm/issues/51），可以节省KV所需的空间，但是recomputation计算量和长度是平方关系。这个方案也是论文基线方案里效果最接近可用的。  \n- 针对位置编码的工作，比如线性插值、NTK插值、YaRN，已经被证明是有效的，但是优化的窗口依然是有限的，大概能在原来的训练长度的基础上，提升一倍或者几倍的推理窗口。论文还提到长窗口的一个实践工作，一个关注在NSFW内容(手动滑稽)的模型：https://kaiokendev.github.io/til#extending-context-to-8k  \n- 针对推理效率，也有一些工作：《Efficiently scaling transformer inference，SmoothQuant: Accurate and efficient post-training quantization for large language models》、《Dynamic context pruning for efficient and interpretable autoregressive transformers》、《Spatten: Efficient sparse attention architecture with cascade token and head pruning》、《H2o: Heavyhitter oracle for efficient generative inference of large language models》，但是在支持的最大长度依然有限制。  \n- 无损的FlashAttention已经被广泛采用，无论是训练还是推理都很有帮助。  \n- 其他一些有损的扩展方案，比如Big Bird、Linformer等。  \n\nStreamingLLM这篇论文发现在长上下文的情况下，存在attention sink的现象，这也是window attention在上下文超过缓存大小之后效果迅速变差的原因。通过利用attention sink，让模型在超长上下文的情况还可以保持较低的PPL。并且和基线里唯一效果比较好的sliding window with recomputation方案相比，StreamingLLM在速度上有22+倍的提升。  \n\nStreamingLLM和其他方案在长上下文上的PPL对比如下图所示，StreamingLLM在各个模型上都能保持稳定较低的PPL。  \n\n{% asset_img streamingllm_model_ppl.png PPL对比 %}  \n\n## attention sink  \n\n从window attention在上下文超过缓存大小之后的失效，论文发现自回归LLM存在的一个有趣现象：对于输入文本最靠前的少量几个token，无论它们在语义上与语言建模任务的相关性如何，大量的注意力分数都会分配给他们，如下图所示  \n\n{% asset_img stremingllm_attention_sink.png attention sink %}  \n\n模型的前两层还能保持attention score更多分配给当前token附近位置的特性，而在其他层，靠前的几个token都会接受到大量的注意力。  \n\n论文里把这些token称为attention sink。尽管这些token在语义上很可能并没有什么重要性，但它们却聚集了大量的注意力分数。  \n\n出现这个现象的原因就是softmax操作。softmax要求所有上下文token的注意力分数加起来等于1，因此，即使当前token跟前面的其他token都没有匹配的需要，模型仍然需要将多余的注意力值分配到前面的某些token，以使得总和为1。  \n\n那么文本最开头的几个初始token就会承担“接收多余的、不需要的注意力”的任务。为什么是初始token来承担这个任务，最简单的原因就是，对于自回归语言建模，初始token对所有后续token都是可见的，这使得它们更容易被训练成attention sink。  \n\n上面这个解释还只是猜想，于是论文做了一个实验来验证这个猜想：把初始的4个token都换成没有重要实际语义的换行符号\\n，结果发现模型依然会把大量的注意力分配给这些token，这就说明attention sink这个现象和内容语义无关，而只和这些token所在的位置相关。  \n\n这个现象在一些quant的工作里也有发现，比如《SmoothQuant: Accurate and efficient post-training quantization for large language models》和《Quantizable transformers: Removing outliers by helping attention heads do nothing》等。  \n\n## 推理到无限长  \n\n基于以上的发现，论文提出了StreamingLLM。StreamingLLM利用了attention sink具有高注意力值的事实，认为保留它们可以保持注意力分数的分布接近正常。  \n\n因此，StreamingLLM的方案就是，在window attention的基础上，增加一个策略，保留了attention sink token的KV值，这些token的KV值会和滑动窗口内的token一起构成完整的KV cache，用于进行新token的推理计算，StreamingLLM和其他方案的attention计算如下图所示。  \n\n{% asset_img streamingllm_compare.png StreamingLLM对比 %}  \n\n这里有一个问题，要保留多少个初始token才足够。论文在几个不同的模型上做了实验，结果如下表所示。  \n\n{% asset_img stremingllm_init_token_num.png attention sink number %}  \n\n实验发现，使用4个初始token就可以基本把PPL降下来，而只使用一个或者两个初始token则仍有很大损失。  \n\n最终方案是，在window attention的基础上，StreamingLLM把KV cache分成两部分：  \n- 包含4个初始token的attention sink  \n- 正常滚动的KV cache  \n\nStreamingLLM的KV缓存如下所示  \n\n{% asset_img stremingllm_kv_cache.png cache %}  \n\n一个重要的点是，StreamingLLM在实际应用上，位置编码中的相对位置，不再直接使用原文中的distance，而是改成使用这些token在cache中的distance。对于像RoPE这样的位置编码，需要缓存引入旋转变换之前token的KV值，解码时再把这些token在缓存内的相对位置加上。这样做使得模型不用处理大于预训练窗口大小的位置编码，而保证了效果。（关于这一点，在下面一篇论文《LM-Infinite》有相关实验）  \n\n当前的大模型基本都没有针对attention sink的现象而做针对性的设计，论文提出可以在训练的时候增加一个特殊的token作为attention sink token使用，方便模型把多余的注意力值放在这个特殊token上。  \n\n如果不增加一个特殊的token，那另外一个方法就是使用softmax的变体，softmax-off-by-one，替换attention中的softmax。  \n\nsoftmax-off-by-one的公式如下所示  \n\n$$\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}$$  \n\nsoftmax-off-by-one在分母增加了一个常数1，这样就不要求所有attention score的和为1。这相当于有一个K和V全都是0的虚拟token，固定和所有其他token的注意力得分为1，这个虚拟token在一定程度上也起到了attention sink的作用。  \n\n为了验证增加特殊token作为attention sink，以及使用softmax-off-by-one的方案效果，论文在相同设置下训练了3个160M参数的模型，效果如下  \n\n{% asset_img stremingllm_exp.png 实验 %}  \n\n从结果上看，即使使用softmax-off-by-one（zero sink），模型还是会依赖前几个token作为attention sink。  \n\n最后，论文在LLAMA2, MPT, Falcon和Pythia模型验证StreamingLLM在4M长度上的效果。MPT使用的位置编码是ALIBI，其他模型则是RoPE。效果如下图\n\n{% asset_img stremingllm_perf_4m.png 效果 %}  \n\n在4M的长度上，StreamingLLM仍然能保持PPL较为稳定。  \n\n理论上StreamingLLM可以把窗口推到无限长，但是由于本质上还是window attention，因此超过cache大小的部分还是会被丢弃。因此StreamingLLM虽然能保持PPL较低，但是对于需要高精度阅读理解和推理的任务，StreamingLLM还是有局限。  \n\n# LM-Infinite  \n\n论文：LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models  \n\n时间：2023年8月  \n\n长度：在2k或者4k长度上训练，在200M长度上推理保持较低PPL  \n\n阶段：无需训练，直接应用在推理阶段  \n\nLM-Infinite可以和现有的LLM结合使用，在Passkey Retrieval和Qasper任务上有比较好的效果，且和原始模型相比，在解码速度上有2.7倍提升，显存上节省7.5倍。  \n\n## 三个挑战  \n\nLM-Infinite认为，Transformer LLM无法有效地泛化到长上下文的情况，主要是因为会面临3个挑战。  \n\n- 挑战1：challenges in handling unseen distances among tokens  \n\n对于使用相对位置编码的模型，两个token的位置对注意力值的影响取决于两个token之间的距离。当推理的长度越来越长，就会出现在训练时没有见过的相对距离，此时attention logits就会倾向于爆炸增长到无穷大，以区分在训练时从未见过的距离。论文对此给出了数学证明。  \n\n而从实践上来看，把LLAMA2所有注意力头在ArXiv数据集上8k以内的attention logits抽取出来，其均值和方差如下图所示。  \n\n{% asset_img lm_infinite_attention_logits_explode.png logits爆炸 %}  \n\nLLAMA2的预训练窗口大小为4k，可以看到4k之后，attention logits的均值和方差开始有明显的上升。  \n\n那么为了缓解这个问题，一个自然的想法是将token之间的相对距离值限制在模型预训练期间看到过的最大值，即设置一个距离上限。这样可以解决logits爆炸，但是会导致下面这个问题。  \n\n- 挑战2：attending to unseen numbers of tokens  \n\n对于较长的上下文，位置靠后的token需要在更大的上下文长度范围内，分配它的注意力权重。论文提出，如果注意力logits有界，那么随着上下文长度变长，注意力熵(attention entropy)将增长至无穷大。这里原文也给出了数学证明。  \n\n直观来说，在注意力权重分配为平均分配的时候，注意力熵最大。而注意力熵增大，表示当前的token不能确定应该将主要注意力放在上文的哪些token，即无法集中注意力。  \n\n同样地，我们把模型在8k内的注意力熵画出来，结果如下图所示，随着长度增大，注意力熵也逐渐增大。  \n\n{% asset_img lm_infinite_attention_entropy.png 注意力熵爆炸 %}  \n\n这表明我们应该限制attention context size，以确保注意力熵保持在预训练见过的范围内。  \n\n如果简单地使用一个window attention，使得每个token只能关注到一定距离内的token，在一定程度上可以一定程度上handle挑战1和挑战2。这样的思路和XPos、Longformer类似。  \n\n但是使用window attention，又会引入新的问题。  \n\n- 挑战3：starting tokens occupy a distinct feature space  \n\n即使没有显式的绝对位置编码，位置最靠前的几个初始token的注意力输出和其他位置相比，也会占据一个独特的表示空间。  \n\n这一结论来自于《The impact of positional encoding on length generalization in transformers》中的定理1，该定理证明了即使没有使用位置编码，单向注意力使得单个注意力层中的token输出也可以隐式地编码绝对位置，而初始token的信号是最强的，且很容易和其他token区分开来。  \n\n这个观察和StreamingLLM中指出的attention sink很相似，总之就是最靠前的几个初始token有很大的影响，因此使用window attention把初始token忽略会带来很大的效果损失。  \n\n论文把LLAMA2中各层的hidden state output做了PCA，降到2维之后画出来，如下图所示  \n\n{% asset_img lm_infinite_starting_tokens.png 初始token %}  \n\n蓝色的点表示初始token，红色的点表示靠后的token。上图显示了初始token的特征在特征空间中占据了和其他token很不同的区域。这就解释了为什么简单地将注意力窗口限制在最近的token上会效果不好：由于注意力本质上是加权平均，丢弃初始的几个token影响了注意力输出到达它们应占据的空间。  \n\n因此，我们需要保留初始token。  \n\n## 解决方案  \n\n基于上面的这些观察和发现，论文提出LM-Infinite，给LLM做zero-shot的长度泛化。LM-Infinite设计了两个主要机制，Λ-shaped attention mask和Distance ceiling。  \n\n- Λ-shaped attention mask  \n\nΛ-shaped attention mask在概念上和Longformer、LongNet、Big Bird的思路类似。  \n\nΛ-shaped attention mask在window attention的基础上，保持了初始token对后续所有token可见。具体来说，每个token能关注到两部分：（1）$n_{\\mathrm{starting}}$ 个初始token（2）$L_{\\mathrm{pretrain}}$ 个最近的token。其中$n_{\\mathrm{starting}}$ 为超参，$L_{\\mathrm{pretrain}}$ 是预训练时的最大窗口大小。除了这两个部分的token之外，其他token都会被直接忽略。  \n\n论文对 $n_{\\mathrm{starting}}$ 的数量进行了实验，发现 $n_{\\mathrm{starting}}\\in[5,100]$ 时的效果比较好，如下图所示。  \n\n{% asset_img lm_infinite_starting_tokens_num.png 初始token数量实验 %}  \n\n注意这里不需要对模型进行任何训练和微调，只需要修改推理的方式即可。  \n\nΛ-shaped attention mask可以解决挑战2和挑战3。  \n\n- Distance ceiling  \n\nLM-Infinite把最大距离限制在 $L_{\\mathrm{pretrain}}$，这样做基本只影响初始token的位置编码。  \n\n具体来说，对于相对位置编码，假设原来的attention logit是 $w(\\mathbf{q},\\mathbf{k},d)$，其中 $d$ 是两个token之间的距离；那么Distance ceiling就是把attention logit的计算变成：  \n\n$$\\text{attention logits}=w(\\mathbf{q},\\mathbf{k},d')$$  \n\n$$d'=\\min(d,L_\\text{pretrain})$$  \n\nDistance ceiling限制了最大距离，可以解决挑战1。  \n\n- Optionally attending to top-k tokens in the middle  \n\n除了Λ-shaped attention mask和Distance ceiling以外，论文还设计了一个可选的机制，用于把“除了当前窗口内的最近token和初始token”以外的中间部分token也加入到attention计算中，这部分token本来是会被直接丢弃的。  \n\n具体来说，会从原本被抛弃的中间token里，选择 $k$ 个attention logits最大的token加入到attention 计算中来。这 $k$ 个token的距离都会被统一设置为 $d=\\frac12L_\\text{pre-train}$。  \n\n对 $k$ 的取值，论文在Passkey Retrieval的validation set上做了实验，结果如下  \n\n{% asset_img lm_infinite_middle_k.png k的数量 %}  \n\n最终取了 $k=5$，并且只在模型>5的层中加入。  \n\n加入middle token主要用于优化需要精准获取中间信息的任务，对于没有这个需要的任务，加入过多的middle token不仅无益，甚至有害，原因如前面挑战2所分析的，会增大注意力熵。  \n\n最终，LM-Infinite的整体方案如下图所示  \n\n{% asset_img lm_infinite_design.png 设计 %}  \n\n## 实验  \n\n论文在LLaMA-7B、LLaMA2-7B、MPT-7B和GPT-J-6B这几个模型验证LM-Infinite的效果。除了MPT-7B使用Alibi之外，其他模型都是用RoPE。  \n\n- Language Modeling  \n\n首先是语言建模的评测，使用的数据集是ArXiv和OpenWebText2。  \n\n选择的几个基线模型，以及这些模型使用LM-Infinite的推理策略之后，在0-12k长度上的PPL如下图所示  \n\n{% asset_img lm_infinite_ppl_figure.png PPL %}  \n\nLLAMA2在略长于10K的长度上就输出了NaN，因此其曲线较短。所有原始模型在约32K的时候就出现OOM。  \n\n当测试长度超过训练长度时，基线模型的PPL会迅速爆炸。而使用LM-Infinite之后，所有模型都可以泛化到比训练长度长得多，而仍然保持较低PPL。\n\n为了验证LM-Infinite在更大长度上的效果，用LM-Infinite + Llama2，在由ArXiv数据重复抽样拼接构成的200M token长度上的评估PPL。结果如下图所示，在200M长度下，依然可以保持稳定较低的PPL。  \n\n{% asset_img lm_infinite_ppl_200m.png PPL 200M %}  \n\n- Passkey Retrieval and Qapser  \n\nPasskey Retrieval和Qapser就是需要对整个上下文有精确理解的任务，因此对于这两个任务，加入了top-5个middle token到attention计算中。  \n\n结果如下表。原始模型在长度超出训练长度之后，几乎无法输出有效内容，因此Passkey Retrieval上全得0分。而LM-Infinite相比简单的长度裁剪方案，在两个任务上都有更好效果。  \n\n{% asset_img lm_infinite_downstream.png 下游任务 %}  \n\n- Ablation study  \n\n对于LM-Infinite的两个主要设计，Λ-shaped attention mask和Distance ceiling，论文做了消融实验，如下图所示。  \n\n{% asset_img lm_infinite_ablation.png 消融实验 %}  \n\n只使用Λ-shaped attention mask或者distance ceiling单一策略的情况下，PPL仍然会出现较明显的上升，说明这两个组件都是必须的。  \n\n# Transformer-XL  \n\n讲下一篇Infini-Transformer之前，先简单回顾下Transformer-XL的思路。  \n\nTransformer-XL是2019年6月，由CMU和Google Brain发布的一个工作，目的是赋予在固定长度上训练的模型，在更大长度上的推理能力。  \n\n一般来说，transformer的训练和推理如下图所示，训练的时候模型在固定的长度上训练，如果输入文本超过了模型的训练窗口大小，那就把输入切分成多个segment，模型在各个segment分别独立地进行训练；而推理的时候，如果文本超过了模型的推理窗口大小，模型可以使用滑动窗口的方式，逐步进行推理。  \n\n{% asset_img xl_vanilla_sw.png vanilla transformer %}  \n\n在窗口滑动的过程中，模型始终只能看到窗口内的内容，对于超出的部分则无法把信息纳入计算中，这显然对有长距离依赖的任务有损害。  \n\n为了让模型看得更远，Transformer-XL把attention计算修改成下面的样子：把输入切分成多个segment，每个的长度为L，对于两个相邻的segment $[\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]$ 和 $\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]$，计算如下  \n\n$$\\begin{aligned}&\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}$$  \n\n其中SG是stop gradient的操作，这样在训练的时候能保持仍然只有固定窗口大小的参数进行梯度计算和更新，而不会扩大显存需求。$\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}$ 表示在序列长度维度进行拼接。h、q、k、v的上标 $n$ 和 $n-1$ 是模型的层序号。  \n\n对于当前segment的每一层attention计算，Transformer-XL把上一个segment的信息加入到当前的K和V中再进行注意力计算，这样相当于每层相比上一层，都能往前多关注到一个segment长度的信息，模型的层数越多，模型最终能关注的长度越长，最后一个segment的最终输出依赖第一个segment的第一层输出，如下图所示。  \n\n{% asset_img xl_attention.png Transformer-XL %}  \n\n# Infini-Transformer  \n\n论文：Leave No Context Behind:Efficient Infinite Context Transformers with Infini-attention  \n\n时间：2024年4月  \n\n长度：实验500k/1M，理论∞  \n\n阶段：继续预训练/微调  \n\nGoogle提出的一种叫Infini-attention的注意力计算方式，可以使用有限的显存计算无限长的上下文。  \n\n## 背景\n\n记忆能力是智能的基石，因为它使得针对特定上下文的高效计算成为可能。  \n\n但是transformer的注意力计算在时间和空间上都跟输入长度是二次的关系，这就使得在长上下文的场景下很难高效、准确地处理信息。  \n\n使用记忆模块可以在大大减少计算量和显存需求的前提下，依然保持长上下文完整信息对模型的可见性。理论上所有输入信息都可以保存在记忆模块中，因此模型具备处理“有长距离依赖，且需要对输入信息进行准确检索”的任务。从这点上看，和StreamingLLM相比，记忆模块的效果上限更高。  \n\n针对高效的记忆，目前也有一些相关工作，比如《Metalearned neural memory》/《Enhancing the transformer with explicit relational encoding for math problem solving》所使用的compressive memory，可以用固定大小的记忆模块来编码context，但是现有的记忆模块在效果和设计的simplicity上仍然存在tradeoff，比较难平衡。  \n\n## Infini-attention  \n\n基于此，Infini-Transformer主要的改进点就在记忆模块的设计上。Infini-attention的结构设计如下图所示，每个Transformer block内包括了常规的mask local attention，和储存长期记忆的long term linear attention。  \n\n{% asset_img infini_attention_structure.png infini-attention结构 %}  \n\nInfini-attention机制复用了标准注意力计算中的所有QKV值，构建了固定大小的注意力模块，用于长期记忆。在处理后续序列时，根据当前的Q从记忆模块中检索相关内容。Infini-attention将长期记忆检索到的值与局部注意力上下文通过concat进行聚合，共同计算最终的输出。  \n\n这种对Transformer注意力层的微小但关键的修改，使得现有的LLM能够通过继续预训练和微调自然地扩展到无限长的上下文。  \n\nInfini-attention和Transformer-XL的对比如下图所示  \n\n{% asset_img infini_attention_process.png infini-attention计算方式 %}  \n\n和Transformer-XL类似，Infini-Transformer也把输入分成多个segment来处理。不同的是，Transformer-XL每层只能比前一层多看到一个segment的信息，因此需要通过堆叠模型的层数来提升模型处理窗口的大小；而由于层数是有限的，最终窗口的大小还是有限的。  \n\nInfini-Transformer则是每层都有对之前所有segment的长期记忆，因此不受模型层数的影响，可以扩展到无限大的窗口。  \n\ncompressive memory的具体设计，出于simplicity和computational efficiency的考虑，论文参考《Learning associative inference using fast weight memory》，把memory设计成一个associative matrix。  \n\n对于记忆模块，最重要的就是记忆的更新和检索。  \n\n- 记忆检索  \n\n先看下记忆的检索。假设每个segment的长度为 $N$，我们使用当前的 $Q\\in\\mathbf{R}^{N\\times d_{key}}$ 从memory $M_{s-1}\\in\\mathbf{R}^{d_{key}\\times d_{value}}$ 中检索，计算如下  \n\n$$A_{\\text{mеm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}$$  \n\n其中$\\sigma$是激活函数，使用的是element-wise ELU + 1。  \n\n$z_{s-1}\\in\\mathbf{R}^{d_{key}}$是normalization term。这里normalization term参考《Transformers are rnns: Fast autoregressive transformers with linear attention》的做法，使用所有K值的求和值。  \n\n- 记忆更新  \n\n完成检索之后，就要更新记忆。需要更新的有normalization term和memory两项，更新方式如下。  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^TV$$  \n\n$$z_s\\leftarrow z_{s-1}+\\sum_{t=1}^N\\sigma(K_t)$$  \n\nmemory的更新，参考《Metalearned neural memory》和《Learning associative inference using fast weight memory》所使用的delta rule，进一步优化成如下形式  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})$$  \n\n- 长期记忆和local attention结合  \n\n回到当前segment的注意力计算中，需要把长期记忆检索项 $A_{mem}$ 和local attention state $A_{dot}$ 结合  \n\n$$A=sigmoid(\\beta)\\odot A_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}$$  \n\n其中 $\\beta$ 是一个可学习的标量参数。  \n\n在训练中， $\\beta$ 所学习到的值如下，0则表示这个头只关注当前segment，1则表示值关注长期记忆的内容，介于0和1之间的则是两种的混合，可以看到大部分的注意力头会使用二者的混合结果  \n\n{% asset_img infini_attention_gating.png gating分布 %}  \n\n整体策略上，和其他包含segment-level memory的模型在方案设计上的比较如下  \n\n{% asset_img infini_attention_compare.png 方案比较 %}  \n\n## 实验\n\n实验上，每个segment的长度 $N$ 设为2048，模型的训练窗口大小为32768，即总共有16个segment。  \n\n- 语言建模能力\n\nInfini-Transformer和Transformer-XL/Memorzing Transformer/RMT在PG-19和Arxiv数据集上的language modeling能力对比如下  \n\n{% asset_img infini_attention_language_modeling.png 语言建模能力对比 %}  \n\nInfini-Transformer的效果显著更好，而且所需的记忆空间只有Memorizing Transformers的不到1%。  \n\n原文指出，把训练的数据长度提升到100k，Infini-Transformer的PPL还能进一步降低。  \n\n- 下游任务\n\n用1B的模型进行轻量级的继续预训练，训练配置如下：  \n- batch size = 64\n- step = 30k\n- 训练数据长度 > 4k\n- segment length = 2k\n\n之后在1M长度的passkey retrieval任务上评测，zero-shot和fine-tune模型在各个长度的效果如下表  \n\n{% asset_img infini_attention_passkey.png passkey效果 %}  \n\nInfini-Transformer（linear + delta）在1M长度的passkey retrieval任务上做到完全正确。  \n\n另外在8B模型上，用8k长度训练30k步，在500k的BookSum任务上评测，结果如下  \n\n{% asset_img infini_attention_booksum.png booksum效果 %}  \n\nInfini-Transformer（linear + delta）相比其他模型有一定优势。  \n\n# 小结  \n\n1. Streaming和LM-Infinite思路有些相似，都观察到attention sink对模型效果的影响，并设计了相关的注意力计算机制保留初始token以稳定PPL，而LM-Infinite在这个基础上使用distance ceiling，使得模型在推理时不会使用超出训练所用的token相对距离。虽然这两个工作使得模型可以在M级别的长度仍然保持较低PPL，但是实际大量中间token被丢弃，导致对需要在上下文精准检索的任务效果有损害。  \n2. Infini-Transformer使用记忆模块，把输入进行分段，并把靠前的内容通过固定大小的记忆矩阵进行压缩。理论上记忆模块可以提供所有上文的信息，能够应用在更精细和困难的任务。记忆模块的检索和更新的设计是方案的核心，需要考虑复杂度和效果的平衡。  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n【1】Efficient Streaming Language Models with Attention Sinks https://arxiv.org/abs/2309.17453  \n【2】Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://arxiv.org/abs/1901.02860  \n【3】Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention https://arxiv.org/abs/2404.07143  \n【4】LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models https://arxiv.org/abs/2308.16137  ","slug":"cs/nlp/2024/05/大模型推理窗口-从有限到无限大","published":1,"updated":"2024-05-13T09:06:18.036Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9y0016314kg0dp28gv","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>步入2024年Q2，大模型在RAG、文档对话、大模型Agent能力等方向的发展持续升温。在平时的日常生活和工作中，大模型工具提供的文档总结、文本润色、代码生成等能力已经是提高效率的必备帮手，甚至在一些复杂或者不熟悉的场景上，大模型也已经能提供一些比较专业的帮助。</p>\n<p>在这些方向上，大模型(超)长上下文的能力都是基础。目前不少模型已经在128k+的长度上展示出比较强的能力，这些模型有的收集了超长训练数据，有的优化位置编码，有的则暴力训练，以提升模型的长上下文能力。</p>\n<p>另外一些工作则另辟蹊径，直接让模型具备无限大窗口的能力。本篇将梳理几个理论上支持无限大上下文长度的工作。</p>\n<h1 id=\"streamingllm\">StreamingLLM</h1>\n<p>论文：Efficient Streaming Language Models with Attention Sinks</p>\n<p>时间：2023年9月</p>\n<p>长度：测试4M，理论∞</p>\n<p>阶段：无需训练，直接应用在推理阶段</p>\n<p>这是一篇由MIT，CMU，NVIDIA，META联合发布的论文。论文提出了一个在流式输出场景下支持无限大上下文长度的方法StreamingLLM，并且不需要进行任何形式的训练或微调\n-- 在有限长度上训练，在无限长度上推理。论文用LLAMA2, MPT,\nFalcon和Pythia作为基线模型进行验证，在4M的上下文长度，StreamingLLM仍然能够保持较低的PPL。</p>\n<p>需要注意的是，StreamingLLM主要关注在超长上下文的情况下，让模型能够生成低PPL的内容，但是在内容的准确性上并没有保证。</p>\n<h2 id=\"背景\">背景</h2>\n<p>大模型流式应用，比如现在大部分对话机器人，在部署推理的时候，在长上下文的情况下通常会面临两个问题：<br>\n-\n缓存不足：流式应用下，为了提升响应速度，KV缓存目前是不可缺少的。而随着对话交互的进行，KV缓存所需的空间也以线性速度在增长。鉴于目前模型的规模动辄百亿千亿的参数量，即使较强的GPU\nH100、A100拥有40G/80G的显存，推理时也捉襟见肘。在显存不足的情况下，我们就不得不抛弃部分旧的KV缓存值，这也导致了模型效果的下降。<br>\n-\n训练长度：主流模型大部分使用旋转位置编码RoPE或其变体，而RoPE的外推能力一般，因此当推理输入的上下文长度远超训练的长度时，模型效果也会迅速下降。</p>\n<p>现有一些工作针对其中的部分问题进行优化：<br>\n- 类似Longformer那样的window\nattention通过只保存最近的KV值来缓解缓存不足的问题，同时保持一定的窗口扩展能力。但是论文观察到，一旦上下文长度超过缓存大小，window\nattention的机制会把最初的部分token移出缓存，这时模型的效果依然会迅速下滑。<br>\n- sliding window with\nrecomputation（https://github.com/mit-han-lab/streaming-llm/issues/51），可以节省KV所需的空间，但是recomputation计算量和长度是平方关系。这个方案也是论文基线方案里效果最接近可用的。<br>\n-\n针对位置编码的工作，比如线性插值、NTK插值、YaRN，已经被证明是有效的，但是优化的窗口依然是有限的，大概能在原来的训练长度的基础上，提升一倍或者几倍的推理窗口。论文还提到长窗口的一个实践工作，一个关注在NSFW内容(手动滑稽)的模型：https://kaiokendev.github.io/til#extending-context-to-8k<br>\n- 针对推理效率，也有一些工作：《Efficiently scaling transformer\ninference，SmoothQuant: Accurate and efficient post-training\nquantization for large language models》、《Dynamic context pruning for\nefficient and interpretable autoregressive transformers》、《Spatten:\nEfficient sparse attention architecture with cascade token and head\npruning》、《H2o: Heavyhitter oracle for efficient generative inference\nof large language models》，但是在支持的最大长度依然有限制。<br>\n-\n无损的FlashAttention已经被广泛采用，无论是训练还是推理都很有帮助。<br>\n- 其他一些有损的扩展方案，比如Big Bird、Linformer等。</p>\n<p>StreamingLLM这篇论文发现在长上下文的情况下，存在attention\nsink的现象，这也是window\nattention在上下文超过缓存大小之后效果迅速变差的原因。通过利用attention\nsink，让模型在超长上下文的情况还可以保持较低的PPL。并且和基线里唯一效果比较好的sliding\nwindow with\nrecomputation方案相比，StreamingLLM在速度上有22+倍的提升。</p>\n<p>StreamingLLM和其他方案在长上下文上的PPL对比如下图所示，StreamingLLM在各个模型上都能保持稳定较低的PPL。</p>\n<img src=\"/45ee1a6d/streamingllm_model_ppl.png\" class title=\"PPL对比\">\n<h2 id=\"attention-sink\">attention sink</h2>\n<p>从window\nattention在上下文超过缓存大小之后的失效，论文发现自回归LLM存在的一个有趣现象：对于输入文本最靠前的少量几个token，无论它们在语义上与语言建模任务的相关性如何，大量的注意力分数都会分配给他们，如下图所示</p>\n<img src=\"/45ee1a6d/stremingllm_attention_sink.png\" class title=\"attention sink\">\n<p>模型的前两层还能保持attention\nscore更多分配给当前token附近位置的特性，而在其他层，靠前的几个token都会接受到大量的注意力。</p>\n<p>论文里把这些token称为attention\nsink。尽管这些token在语义上很可能并没有什么重要性，但它们却聚集了大量的注意力分数。</p>\n<p>出现这个现象的原因就是softmax操作。softmax要求所有上下文token的注意力分数加起来等于1，因此，即使当前token跟前面的其他token都没有匹配的需要，模型仍然需要将多余的注意力值分配到前面的某些token，以使得总和为1。</p>\n<p>那么文本最开头的几个初始token就会承担“接收多余的、不需要的注意力”的任务。为什么是初始token来承担这个任务，最简单的原因就是，对于自回归语言建模，初始token对所有后续token都是可见的，这使得它们更容易被训练成attention\nsink。</p>\n<p>上面这个解释还只是猜想，于是论文做了一个实验来验证这个猜想：把初始的4个token都换成没有重要实际语义的换行符号，结果发现模型依然会把大量的注意力分配给这些token，这就说明attention\nsink这个现象和内容语义无关，而只和这些token所在的位置相关。</p>\n<p>这个现象在一些quant的工作里也有发现，比如《SmoothQuant: Accurate and\nefficient post-training quantization for large language\nmodels》和《Quantizable transformers: Removing outliers by helping\nattention heads do nothing》等。</p>\n<h2 id=\"推理到无限长\">推理到无限长</h2>\n<p>基于以上的发现，论文提出了StreamingLLM。StreamingLLM利用了attention\nsink具有高注意力值的事实，认为保留它们可以保持注意力分数的分布接近正常。</p>\n<p>因此，StreamingLLM的方案就是，在window\nattention的基础上，增加一个策略，保留了attention sink\ntoken的KV值，这些token的KV值会和滑动窗口内的token一起构成完整的KV\ncache，用于进行新token的推理计算，StreamingLLM和其他方案的attention计算如下图所示。</p>\n<img src=\"/45ee1a6d/streamingllm_compare.png\" class title=\"StreamingLLM对比\">\n<p>这里有一个问题，要保留多少个初始token才足够。论文在几个不同的模型上做了实验，结果如下表所示。</p>\n<img src=\"/45ee1a6d/stremingllm_init_token_num.png\" class title=\"attention sink number\">\n<p>实验发现，使用4个初始token就可以基本把PPL降下来，而只使用一个或者两个初始token则仍有很大损失。</p>\n<p>最终方案是，在window attention的基础上，StreamingLLM把KV\ncache分成两部分：<br>\n- 包含4个初始token的attention sink<br>\n- 正常滚动的KV cache</p>\n<p>StreamingLLM的KV缓存如下所示</p>\n<img src=\"/45ee1a6d/stremingllm_kv_cache.png\" class title=\"cache\">\n<p>一个重要的点是，StreamingLLM在实际应用上，位置编码中的相对位置，不再直接使用原文中的distance，而是改成使用这些token在cache中的distance。对于像RoPE这样的位置编码，需要缓存引入旋转变换之前token的KV值，解码时再把这些token在缓存内的相对位置加上。这样做使得模型不用处理大于预训练窗口大小的位置编码，而保证了效果。（关于这一点，在下面一篇论文《LM-Infinite》有相关实验）</p>\n<p>当前的大模型基本都没有针对attention\nsink的现象而做针对性的设计，论文提出可以在训练的时候增加一个特殊的token作为attention\nsink token使用，方便模型把多余的注意力值放在这个特殊token上。</p>\n<p>如果不增加一个特殊的token，那另外一个方法就是使用softmax的变体，softmax-off-by-one，替换attention中的softmax。</p>\n<p>softmax-off-by-one的公式如下所示</p>\n<p><span class=\"math display\">\\[\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}\\]</span></p>\n<p>softmax-off-by-one在分母增加了一个常数1，这样就不要求所有attention\nscore的和为1。这相当于有一个K和V全都是0的虚拟token，固定和所有其他token的注意力得分为1，这个虚拟token在一定程度上也起到了attention\nsink的作用。</p>\n<p>为了验证增加特殊token作为attention\nsink，以及使用softmax-off-by-one的方案效果，论文在相同设置下训练了3个160M参数的模型，效果如下</p>\n<img src=\"/45ee1a6d/stremingllm_exp.png\" class title=\"实验\">\n<p>从结果上看，即使使用softmax-off-by-one（zero\nsink），模型还是会依赖前几个token作为attention sink。</p>\n<p>最后，论文在LLAMA2, MPT,\nFalcon和Pythia模型验证StreamingLLM在4M长度上的效果。MPT使用的位置编码是ALIBI，其他模型则是RoPE。效果如下图</p>\n<img src=\"/45ee1a6d/stremingllm_perf_4m.png\" class title=\"效果\">\n<p>在4M的长度上，StreamingLLM仍然能保持PPL较为稳定。</p>\n<p>理论上StreamingLLM可以把窗口推到无限长，但是由于本质上还是window\nattention，因此超过cache大小的部分还是会被丢弃。因此StreamingLLM虽然能保持PPL较低，但是对于需要高精度阅读理解和推理的任务，StreamingLLM还是有局限。</p>\n<h1 id=\"lm-infinite\">LM-Infinite</h1>\n<p>论文：LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models</p>\n<p>时间：2023年8月</p>\n<p>长度：在2k或者4k长度上训练，在200M长度上推理保持较低PPL</p>\n<p>阶段：无需训练，直接应用在推理阶段</p>\n<p>LM-Infinite可以和现有的LLM结合使用，在Passkey\nRetrieval和Qasper任务上有比较好的效果，且和原始模型相比，在解码速度上有2.7倍提升，显存上节省7.5倍。</p>\n<h2 id=\"三个挑战\">三个挑战</h2>\n<p>LM-Infinite认为，Transformer\nLLM无法有效地泛化到长上下文的情况，主要是因为会面临3个挑战。</p>\n<ul>\n<li>挑战1：challenges in handling unseen distances among tokens</li>\n</ul>\n<p>对于使用相对位置编码的模型，两个token的位置对注意力值的影响取决于两个token之间的距离。当推理的长度越来越长，就会出现在训练时没有见过的相对距离，此时attention\nlogits就会倾向于爆炸增长到无穷大，以区分在训练时从未见过的距离。论文对此给出了数学证明。</p>\n<p>而从实践上来看，把LLAMA2所有注意力头在ArXiv数据集上8k以内的attention\nlogits抽取出来，其均值和方差如下图所示。</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_logits_explode.png\" class title=\"logits爆炸\">\n<p>LLAMA2的预训练窗口大小为4k，可以看到4k之后，attention\nlogits的均值和方差开始有明显的上升。</p>\n<p>那么为了缓解这个问题，一个自然的想法是将token之间的相对距离值限制在模型预训练期间看到过的最大值，即设置一个距离上限。这样可以解决logits爆炸，但是会导致下面这个问题。</p>\n<ul>\n<li>挑战2：attending to unseen numbers of tokens</li>\n</ul>\n<p>对于较长的上下文，位置靠后的token需要在更大的上下文长度范围内，分配它的注意力权重。论文提出，如果注意力logits有界，那么随着上下文长度变长，注意力熵(attention\nentropy)将增长至无穷大。这里原文也给出了数学证明。</p>\n<p>直观来说，在注意力权重分配为平均分配的时候，注意力熵最大。而注意力熵增大，表示当前的token不能确定应该将主要注意力放在上文的哪些token，即无法集中注意力。</p>\n<p>同样地，我们把模型在8k内的注意力熵画出来，结果如下图所示，随着长度增大，注意力熵也逐渐增大。</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_entropy.png\" class title=\"注意力熵爆炸\">\n<p>这表明我们应该限制attention context\nsize，以确保注意力熵保持在预训练见过的范围内。</p>\n<p>如果简单地使用一个window\nattention，使得每个token只能关注到一定距离内的token，在一定程度上可以一定程度上handle挑战1和挑战2。这样的思路和XPos、Longformer类似。</p>\n<p>但是使用window attention，又会引入新的问题。</p>\n<ul>\n<li>挑战3：starting tokens occupy a distinct feature space</li>\n</ul>\n<p>即使没有显式的绝对位置编码，位置最靠前的几个初始token的注意力输出和其他位置相比，也会占据一个独特的表示空间。</p>\n<p>这一结论来自于《The impact of positional encoding on length\ngeneralization in\ntransformers》中的定理1，该定理证明了即使没有使用位置编码，单向注意力使得单个注意力层中的token输出也可以隐式地编码绝对位置，而初始token的信号是最强的，且很容易和其他token区分开来。</p>\n<p>这个观察和StreamingLLM中指出的attention\nsink很相似，总之就是最靠前的几个初始token有很大的影响，因此使用window\nattention把初始token忽略会带来很大的效果损失。</p>\n<p>论文把LLAMA2中各层的hidden state\noutput做了PCA，降到2维之后画出来，如下图所示</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens.png\" class title=\"初始token\">\n<p>蓝色的点表示初始token，红色的点表示靠后的token。上图显示了初始token的特征在特征空间中占据了和其他token很不同的区域。这就解释了为什么简单地将注意力窗口限制在最近的token上会效果不好：由于注意力本质上是加权平均，丢弃初始的几个token影响了注意力输出到达它们应占据的空间。</p>\n<p>因此，我们需要保留初始token。</p>\n<h2 id=\"解决方案\">解决方案</h2>\n<p>基于上面的这些观察和发现，论文提出LM-Infinite，给LLM做zero-shot的长度泛化。LM-Infinite设计了两个主要机制，Λ-shaped\nattention mask和Distance ceiling。</p>\n<ul>\n<li>Λ-shaped attention mask</li>\n</ul>\n<p>Λ-shaped attention mask在概念上和Longformer、LongNet、Big\nBird的思路类似。</p>\n<p>Λ-shaped attention mask在window\nattention的基础上，保持了初始token对后续所有token可见。具体来说，每个token能关注到两部分：（1）<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\n个初始token（2）<span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\n个最近的token。其中<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span> 为超参，<span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\n是预训练时的最大窗口大小。除了这两个部分的token之外，其他token都会被直接忽略。</p>\n<p>论文对 <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\n的数量进行了实验，发现 <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\in[5,100]\\)</span>\n时的效果比较好，如下图所示。</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens_num.png\" class title=\"初始token数量实验\">\n<p>注意这里不需要对模型进行任何训练和微调，只需要修改推理的方式即可。</p>\n<p>Λ-shaped attention mask可以解决挑战2和挑战3。</p>\n<ul>\n<li>Distance ceiling</li>\n</ul>\n<p>LM-Infinite把最大距离限制在 <span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>，这样做基本只影响初始token的位置编码。</p>\n<p>具体来说，对于相对位置编码，假设原来的attention logit是 <span class=\"math inline\">\\(w(\\mathbf{q},\\mathbf{k},d)\\)</span>，其中 <span class=\"math inline\">\\(d\\)</span> 是两个token之间的距离；那么Distance\nceiling就是把attention logit的计算变成：</p>\n<p><span class=\"math display\">\\[\\text{attention\nlogits}=w(\\mathbf{q},\\mathbf{k},d&#39;)\\]</span></p>\n<p><span class=\"math display\">\\[d&#39;=\\min(d,L_\\text{pretrain})\\]</span></p>\n<p>Distance ceiling限制了最大距离，可以解决挑战1。</p>\n<ul>\n<li>Optionally attending to top-k tokens in the middle</li>\n</ul>\n<p>除了Λ-shaped attention mask和Distance\nceiling以外，论文还设计了一个可选的机制，用于把“除了当前窗口内的最近token和初始token”以外的中间部分token也加入到attention计算中，这部分token本来是会被直接丢弃的。</p>\n<p>具体来说，会从原本被抛弃的中间token里，选择 <span class=\"math inline\">\\(k\\)</span> 个attention\nlogits最大的token加入到attention 计算中来。这 <span class=\"math inline\">\\(k\\)</span> 个token的距离都会被统一设置为 <span class=\"math inline\">\\(d=\\frac12L_\\text{pre-train}\\)</span>。</p>\n<p>对 <span class=\"math inline\">\\(k\\)</span> 的取值，论文在Passkey\nRetrieval的validation set上做了实验，结果如下</p>\n<img src=\"/45ee1a6d/lm_infinite_middle_k.png\" class title=\"k的数量\">\n<p>最终取了 <span class=\"math inline\">\\(k=5\\)</span>，并且只在模型&gt;5的层中加入。</p>\n<p>加入middle\ntoken主要用于优化需要精准获取中间信息的任务，对于没有这个需要的任务，加入过多的middle\ntoken不仅无益，甚至有害，原因如前面挑战2所分析的，会增大注意力熵。</p>\n<p>最终，LM-Infinite的整体方案如下图所示</p>\n<img src=\"/45ee1a6d/lm_infinite_design.png\" class title=\"设计\">\n<h2 id=\"实验\">实验</h2>\n<p>论文在LLaMA-7B、LLaMA2-7B、MPT-7B和GPT-J-6B这几个模型验证LM-Infinite的效果。除了MPT-7B使用Alibi之外，其他模型都是用RoPE。</p>\n<ul>\n<li>Language Modeling</li>\n</ul>\n<p>首先是语言建模的评测，使用的数据集是ArXiv和OpenWebText2。</p>\n<p>选择的几个基线模型，以及这些模型使用LM-Infinite的推理策略之后，在0-12k长度上的PPL如下图所示</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_figure.png\" class title=\"PPL\">\n<p>LLAMA2在略长于10K的长度上就输出了NaN，因此其曲线较短。所有原始模型在约32K的时候就出现OOM。</p>\n<p>当测试长度超过训练长度时，基线模型的PPL会迅速爆炸。而使用LM-Infinite之后，所有模型都可以泛化到比训练长度长得多，而仍然保持较低PPL。</p>\n<p>为了验证LM-Infinite在更大长度上的效果，用LM-Infinite +\nLlama2，在由ArXiv数据重复抽样拼接构成的200M\ntoken长度上的评估PPL。结果如下图所示，在200M长度下，依然可以保持稳定较低的PPL。</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_200m.png\" class title=\"PPL 200M\">\n<ul>\n<li>Passkey Retrieval and Qapser</li>\n</ul>\n<p>Passkey\nRetrieval和Qapser就是需要对整个上下文有精确理解的任务，因此对于这两个任务，加入了top-5个middle\ntoken到attention计算中。</p>\n<p>结果如下表。原始模型在长度超出训练长度之后，几乎无法输出有效内容，因此Passkey\nRetrieval上全得0分。而LM-Infinite相比简单的长度裁剪方案，在两个任务上都有更好效果。</p>\n<img src=\"/45ee1a6d/lm_infinite_downstream.png\" class title=\"下游任务\">\n<ul>\n<li>Ablation study</li>\n</ul>\n<p>对于LM-Infinite的两个主要设计，Λ-shaped attention mask和Distance\nceiling，论文做了消融实验，如下图所示。</p>\n<img src=\"/45ee1a6d/lm_infinite_ablation.png\" class title=\"消融实验\">\n<p>只使用Λ-shaped attention mask或者distance\nceiling单一策略的情况下，PPL仍然会出现较明显的上升，说明这两个组件都是必须的。</p>\n<h1 id=\"transformer-xl\">Transformer-XL</h1>\n<p>讲下一篇Infini-Transformer之前，先简单回顾下Transformer-XL的思路。</p>\n<p>Transformer-XL是2019年6月，由CMU和Google\nBrain发布的一个工作，目的是赋予在固定长度上训练的模型，在更大长度上的推理能力。</p>\n<p>一般来说，transformer的训练和推理如下图所示，训练的时候模型在固定的长度上训练，如果输入文本超过了模型的训练窗口大小，那就把输入切分成多个segment，模型在各个segment分别独立地进行训练；而推理的时候，如果文本超过了模型的推理窗口大小，模型可以使用滑动窗口的方式，逐步进行推理。</p>\n<img src=\"/45ee1a6d/xl_vanilla_sw.png\" class title=\"vanilla transformer\">\n<p>在窗口滑动的过程中，模型始终只能看到窗口内的内容，对于超出的部分则无法把信息纳入计算中，这显然对有长距离依赖的任务有损害。</p>\n<p>为了让模型看得更远，Transformer-XL把attention计算修改成下面的样子：把输入切分成多个segment，每个的长度为L，对于两个相邻的segment\n<span class=\"math inline\">\\([\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]\\)</span>\n和 <span class=\"math inline\">\\(\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]\\)</span>，计算如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&amp;\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&amp;\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}\\]</span></p>\n<p>其中SG是stop\ngradient的操作，这样在训练的时候能保持仍然只有固定窗口大小的参数进行梯度计算和更新，而不会扩大显存需求。<span class=\"math inline\">\\(\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}\\)</span>\n表示在序列长度维度进行拼接。h、q、k、v的上标 <span class=\"math inline\">\\(n\\)</span> 和 <span class=\"math inline\">\\(n-1\\)</span> 是模型的层序号。</p>\n<p>对于当前segment的每一层attention计算，Transformer-XL把上一个segment的信息加入到当前的K和V中再进行注意力计算，这样相当于每层相比上一层，都能往前多关注到一个segment长度的信息，模型的层数越多，模型最终能关注的长度越长，最后一个segment的最终输出依赖第一个segment的第一层输出，如下图所示。</p>\n<img src=\"/45ee1a6d/xl_attention.png\" class title=\"Transformer-XL\">\n<h1 id=\"infini-transformer\">Infini-Transformer</h1>\n<p>论文：Leave No Context Behind:Efficient Infinite Context Transformers\nwith Infini-attention</p>\n<p>时间：2024年4月</p>\n<p>长度：实验500k/1M，理论∞</p>\n<p>阶段：继续预训练/微调</p>\n<p>Google提出的一种叫Infini-attention的注意力计算方式，可以使用有限的显存计算无限长的上下文。</p>\n<h2 id=\"背景-1\">背景</h2>\n<p>记忆能力是智能的基石，因为它使得针对特定上下文的高效计算成为可能。</p>\n<p>但是transformer的注意力计算在时间和空间上都跟输入长度是二次的关系，这就使得在长上下文的场景下很难高效、准确地处理信息。</p>\n<p>使用记忆模块可以在大大减少计算量和显存需求的前提下，依然保持长上下文完整信息对模型的可见性。理论上所有输入信息都可以保存在记忆模块中，因此模型具备处理“有长距离依赖，且需要对输入信息进行准确检索”的任务。从这点上看，和StreamingLLM相比，记忆模块的效果上限更高。</p>\n<p>针对高效的记忆，目前也有一些相关工作，比如《Metalearned neural\nmemory》/《Enhancing the transformer with explicit relational encoding\nfor math problem solving》所使用的compressive\nmemory，可以用固定大小的记忆模块来编码context，但是现有的记忆模块在效果和设计的simplicity上仍然存在tradeoff，比较难平衡。</p>\n<h2 id=\"infini-attention\">Infini-attention</h2>\n<p>基于此，Infini-Transformer主要的改进点就在记忆模块的设计上。Infini-attention的结构设计如下图所示，每个Transformer\nblock内包括了常规的mask local attention，和储存长期记忆的long term\nlinear attention。</p>\n<img src=\"/45ee1a6d/infini_attention_structure.png\" class title=\"infini-attention结构\">\n<p>Infini-attention机制复用了标准注意力计算中的所有QKV值，构建了固定大小的注意力模块，用于长期记忆。在处理后续序列时，根据当前的Q从记忆模块中检索相关内容。Infini-attention将长期记忆检索到的值与局部注意力上下文通过concat进行聚合，共同计算最终的输出。</p>\n<p>这种对Transformer注意力层的微小但关键的修改，使得现有的LLM能够通过继续预训练和微调自然地扩展到无限长的上下文。</p>\n<p>Infini-attention和Transformer-XL的对比如下图所示</p>\n<img src=\"/45ee1a6d/infini_attention_process.png\" class title=\"infini-attention计算方式\">\n<p>和Transformer-XL类似，Infini-Transformer也把输入分成多个segment来处理。不同的是，Transformer-XL每层只能比前一层多看到一个segment的信息，因此需要通过堆叠模型的层数来提升模型处理窗口的大小；而由于层数是有限的，最终窗口的大小还是有限的。</p>\n<p>Infini-Transformer则是每层都有对之前所有segment的长期记忆，因此不受模型层数的影响，可以扩展到无限大的窗口。</p>\n<p>compressive memory的具体设计，出于simplicity和computational\nefficiency的考虑，论文参考《Learning associative inference using fast\nweight memory》，把memory设计成一个associative matrix。</p>\n<p>对于记忆模块，最重要的就是记忆的更新和检索。</p>\n<ul>\n<li>记忆检索</li>\n</ul>\n<p>先看下记忆的检索。假设每个segment的长度为 <span class=\"math inline\">\\(N\\)</span>，我们使用当前的 <span class=\"math inline\">\\(Q\\in\\mathbf{R}^{N\\times d_{key}}\\)</span> 从memory\n<span class=\"math inline\">\\(M_{s-1}\\in\\mathbf{R}^{d_{key}\\times\nd_{value}}\\)</span> 中检索，计算如下</p>\n<p><span class=\"math display\">\\[A_{\\text{mеm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}\\]</span></p>\n<p>其中<span class=\"math inline\">\\(\\sigma\\)</span>是激活函数，使用的是element-wise\nELU + 1。</p>\n<p><span class=\"math inline\">\\(z_{s-1}\\in\\mathbf{R}^{d_{key}}\\)</span>是normalization\nterm。这里normalization term参考《Transformers are rnns: Fast\nautoregressive transformers with linear\nattention》的做法，使用所有K值的求和值。</p>\n<ul>\n<li>记忆更新</li>\n</ul>\n<p>完成检索之后，就要更新记忆。需要更新的有normalization\nterm和memory两项，更新方式如下。</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^TV\\]</span></p>\n<p><span class=\"math display\">\\[z_s\\leftarrow\nz_{s-1}+\\sum_{t=1}^N\\sigma(K_t)\\]</span></p>\n<p>memory的更新，参考《Metalearned neural memory》和《Learning\nassociative inference using fast weight memory》所使用的delta\nrule，进一步优化成如下形式</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})\\]</span></p>\n<ul>\n<li>长期记忆和local attention结合</li>\n</ul>\n<p>回到当前segment的注意力计算中，需要把长期记忆检索项 <span class=\"math inline\">\\(A_{mem}\\)</span> 和local attention state <span class=\"math inline\">\\(A_{dot}\\)</span> 结合</p>\n<p><span class=\"math display\">\\[A=sigmoid(\\beta)\\odot\nA_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\beta\\)</span>\n是一个可学习的标量参数。</p>\n<p>在训练中， <span class=\"math inline\">\\(\\beta\\)</span>\n所学习到的值如下，0则表示这个头只关注当前segment，1则表示值关注长期记忆的内容，介于0和1之间的则是两种的混合，可以看到大部分的注意力头会使用二者的混合结果</p>\n<img src=\"/45ee1a6d/infini_attention_gating.png\" class title=\"gating分布\">\n<p>整体策略上，和其他包含segment-level\nmemory的模型在方案设计上的比较如下</p>\n<img src=\"/45ee1a6d/infini_attention_compare.png\" class title=\"方案比较\">\n<h2 id=\"实验-1\">实验</h2>\n<p>实验上，每个segment的长度 <span class=\"math inline\">\\(N\\)</span>\n设为2048，模型的训练窗口大小为32768，即总共有16个segment。</p>\n<ul>\n<li>语言建模能力</li>\n</ul>\n<p>Infini-Transformer和Transformer-XL/Memorzing\nTransformer/RMT在PG-19和Arxiv数据集上的language modeling能力对比如下</p>\n<img src=\"/45ee1a6d/infini_attention_language_modeling.png\" class title=\"语言建模能力对比\">\n<p>Infini-Transformer的效果显著更好，而且所需的记忆空间只有Memorizing\nTransformers的不到1%。</p>\n<p>原文指出，把训练的数据长度提升到100k，Infini-Transformer的PPL还能进一步降低。</p>\n<ul>\n<li>下游任务</li>\n</ul>\n<p>用1B的模型进行轻量级的继续预训练，训练配置如下：<br>\n- batch size = 64 - step = 30k - 训练数据长度 &gt; 4k - segment length =\n2k</p>\n<p>之后在1M长度的passkey\nretrieval任务上评测，zero-shot和fine-tune模型在各个长度的效果如下表</p>\n<img src=\"/45ee1a6d/infini_attention_passkey.png\" class title=\"passkey效果\">\n<p>Infini-Transformer（linear + delta）在1M长度的passkey\nretrieval任务上做到完全正确。</p>\n<p>另外在8B模型上，用8k长度训练30k步，在500k的BookSum任务上评测，结果如下</p>\n<img src=\"/45ee1a6d/infini_attention_booksum.png\" class title=\"booksum效果\">\n<p>Infini-Transformer（linear + delta）相比其他模型有一定优势。</p>\n<h1 id=\"小结\">小结</h1>\n<ol type=\"1\">\n<li>Streaming和LM-Infinite思路有些相似，都观察到attention\nsink对模型效果的影响，并设计了相关的注意力计算机制保留初始token以稳定PPL，而LM-Infinite在这个基础上使用distance\nceiling，使得模型在推理时不会使用超出训练所用的token相对距离。虽然这两个工作使得模型可以在M级别的长度仍然保持较低PPL，但是实际大量中间token被丢弃，导致对需要在上下文精准检索的任务效果有损害。<br>\n</li>\n<li>Infini-Transformer使用记忆模块，把输入进行分段，并把靠前的内容通过固定大小的记忆矩阵进行压缩。理论上记忆模块可以提供所有上文的信息，能够应用在更精细和困难的任务。记忆模块的检索和更新的设计是方案的核心，需要考虑复杂度和效果的平衡。</li>\n</ol>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Efficient Streaming Language Models with Attention Sinks\nhttps://arxiv.org/abs/2309.17453<br>\n【2】Transformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext https://arxiv.org/abs/1901.02860<br>\n【3】Leave No Context Behind: Efficient Infinite Context Transformers\nwith Infini-attention https://arxiv.org/abs/2404.07143<br>\n【4】LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models https://arxiv.org/abs/2308.16137</p>\n","length":14036,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>步入2024年Q2，大模型在RAG、文档对话、大模型Agent能力等方向的发展持续升温。在平时的日常生活和工作中，大模型工具提供的文档总结、文本润色、代码生成等能力已经是提高效率的必备帮手，甚至在一些复杂或者不熟悉的场景上，大模型也已经能提供一些比较专业的帮助。</p>\n<p>在这些方向上，大模型(超)长上下文的能力都是基础。目前不少模型已经在128k+的长度上展示出比较强的能力，这些模型有的收集了超长训练数据，有的优化位置编码，有的则暴力训练，以提升模型的长上下文能力。</p>\n<p>另外一些工作则另辟蹊径，直接让模型具备无限大窗口的能力。本篇将梳理几个理论上支持无限大上下文长度的工作。</p>\n<h1 id=\"streamingllm\">StreamingLLM</h1>\n<p>论文：Efficient Streaming Language Models with Attention Sinks</p>\n<p>时间：2023年9月</p>\n<p>长度：测试4M，理论∞</p>\n<p>阶段：无需训练，直接应用在推理阶段</p>\n<p>这是一篇由MIT，CMU，NVIDIA，META联合发布的论文。论文提出了一个在流式输出场景下支持无限大上下文长度的方法StreamingLLM，并且不需要进行任何形式的训练或微调\n-- 在有限长度上训练，在无限长度上推理。论文用LLAMA2, MPT,\nFalcon和Pythia作为基线模型进行验证，在4M的上下文长度，StreamingLLM仍然能够保持较低的PPL。</p>\n<p>需要注意的是，StreamingLLM主要关注在超长上下文的情况下，让模型能够生成低PPL的内容，但是在内容的准确性上并没有保证。</p>\n<h2 id=\"背景\">背景</h2>\n<p>大模型流式应用，比如现在大部分对话机器人，在部署推理的时候，在长上下文的情况下通常会面临两个问题：<br>\n-\n缓存不足：流式应用下，为了提升响应速度，KV缓存目前是不可缺少的。而随着对话交互的进行，KV缓存所需的空间也以线性速度在增长。鉴于目前模型的规模动辄百亿千亿的参数量，即使较强的GPU\nH100、A100拥有40G/80G的显存，推理时也捉襟见肘。在显存不足的情况下，我们就不得不抛弃部分旧的KV缓存值，这也导致了模型效果的下降。<br>\n-\n训练长度：主流模型大部分使用旋转位置编码RoPE或其变体，而RoPE的外推能力一般，因此当推理输入的上下文长度远超训练的长度时，模型效果也会迅速下降。</p>\n<p>现有一些工作针对其中的部分问题进行优化：<br>\n- 类似Longformer那样的window\nattention通过只保存最近的KV值来缓解缓存不足的问题，同时保持一定的窗口扩展能力。但是论文观察到，一旦上下文长度超过缓存大小，window\nattention的机制会把最初的部分token移出缓存，这时模型的效果依然会迅速下滑。<br>\n- sliding window with\nrecomputation（https://github.com/mit-han-lab/streaming-llm/issues/51），可以节省KV所需的空间，但是recomputation计算量和长度是平方关系。这个方案也是论文基线方案里效果最接近可用的。<br>\n-\n针对位置编码的工作，比如线性插值、NTK插值、YaRN，已经被证明是有效的，但是优化的窗口依然是有限的，大概能在原来的训练长度的基础上，提升一倍或者几倍的推理窗口。论文还提到长窗口的一个实践工作，一个关注在NSFW内容(手动滑稽)的模型：https://kaiokendev.github.io/til#extending-context-to-8k<br>\n- 针对推理效率，也有一些工作：《Efficiently scaling transformer\ninference，SmoothQuant: Accurate and efficient post-training\nquantization for large language models》、《Dynamic context pruning for\nefficient and interpretable autoregressive transformers》、《Spatten:\nEfficient sparse attention architecture with cascade token and head\npruning》、《H2o: Heavyhitter oracle for efficient generative inference\nof large language models》，但是在支持的最大长度依然有限制。<br>\n-\n无损的FlashAttention已经被广泛采用，无论是训练还是推理都很有帮助。<br>\n- 其他一些有损的扩展方案，比如Big Bird、Linformer等。</p>\n<p>StreamingLLM这篇论文发现在长上下文的情况下，存在attention\nsink的现象，这也是window\nattention在上下文超过缓存大小之后效果迅速变差的原因。通过利用attention\nsink，让模型在超长上下文的情况还可以保持较低的PPL。并且和基线里唯一效果比较好的sliding\nwindow with\nrecomputation方案相比，StreamingLLM在速度上有22+倍的提升。</p>\n<p>StreamingLLM和其他方案在长上下文上的PPL对比如下图所示，StreamingLLM在各个模型上都能保持稳定较低的PPL。</p>\n<img src=\"/45ee1a6d/streamingllm_model_ppl.png\" class title=\"PPL对比\">\n<h2 id=\"attention-sink\">attention sink</h2>\n<p>从window\nattention在上下文超过缓存大小之后的失效，论文发现自回归LLM存在的一个有趣现象：对于输入文本最靠前的少量几个token，无论它们在语义上与语言建模任务的相关性如何，大量的注意力分数都会分配给他们，如下图所示</p>\n<img src=\"/45ee1a6d/stremingllm_attention_sink.png\" class title=\"attention sink\">\n<p>模型的前两层还能保持attention\nscore更多分配给当前token附近位置的特性，而在其他层，靠前的几个token都会接受到大量的注意力。</p>\n<p>论文里把这些token称为attention\nsink。尽管这些token在语义上很可能并没有什么重要性，但它们却聚集了大量的注意力分数。</p>\n<p>出现这个现象的原因就是softmax操作。softmax要求所有上下文token的注意力分数加起来等于1，因此，即使当前token跟前面的其他token都没有匹配的需要，模型仍然需要将多余的注意力值分配到前面的某些token，以使得总和为1。</p>\n<p>那么文本最开头的几个初始token就会承担“接收多余的、不需要的注意力”的任务。为什么是初始token来承担这个任务，最简单的原因就是，对于自回归语言建模，初始token对所有后续token都是可见的，这使得它们更容易被训练成attention\nsink。</p>\n<p>上面这个解释还只是猜想，于是论文做了一个实验来验证这个猜想：把初始的4个token都换成没有重要实际语义的换行符号，结果发现模型依然会把大量的注意力分配给这些token，这就说明attention\nsink这个现象和内容语义无关，而只和这些token所在的位置相关。</p>\n<p>这个现象在一些quant的工作里也有发现，比如《SmoothQuant: Accurate and\nefficient post-training quantization for large language\nmodels》和《Quantizable transformers: Removing outliers by helping\nattention heads do nothing》等。</p>\n<h2 id=\"推理到无限长\">推理到无限长</h2>\n<p>基于以上的发现，论文提出了StreamingLLM。StreamingLLM利用了attention\nsink具有高注意力值的事实，认为保留它们可以保持注意力分数的分布接近正常。</p>\n<p>因此，StreamingLLM的方案就是，在window\nattention的基础上，增加一个策略，保留了attention sink\ntoken的KV值，这些token的KV值会和滑动窗口内的token一起构成完整的KV\ncache，用于进行新token的推理计算，StreamingLLM和其他方案的attention计算如下图所示。</p>\n<img src=\"/45ee1a6d/streamingllm_compare.png\" class title=\"StreamingLLM对比\">\n<p>这里有一个问题，要保留多少个初始token才足够。论文在几个不同的模型上做了实验，结果如下表所示。</p>\n<img src=\"/45ee1a6d/stremingllm_init_token_num.png\" class title=\"attention sink number\">\n<p>实验发现，使用4个初始token就可以基本把PPL降下来，而只使用一个或者两个初始token则仍有很大损失。</p>\n<p>最终方案是，在window attention的基础上，StreamingLLM把KV\ncache分成两部分：<br>\n- 包含4个初始token的attention sink<br>\n- 正常滚动的KV cache</p>\n<p>StreamingLLM的KV缓存如下所示</p>\n<img src=\"/45ee1a6d/stremingllm_kv_cache.png\" class title=\"cache\">\n<p>一个重要的点是，StreamingLLM在实际应用上，位置编码中的相对位置，不再直接使用原文中的distance，而是改成使用这些token在cache中的distance。对于像RoPE这样的位置编码，需要缓存引入旋转变换之前token的KV值，解码时再把这些token在缓存内的相对位置加上。这样做使得模型不用处理大于预训练窗口大小的位置编码，而保证了效果。（关于这一点，在下面一篇论文《LM-Infinite》有相关实验）</p>\n<p>当前的大模型基本都没有针对attention\nsink的现象而做针对性的设计，论文提出可以在训练的时候增加一个特殊的token作为attention\nsink token使用，方便模型把多余的注意力值放在这个特殊token上。</p>\n<p>如果不增加一个特殊的token，那另外一个方法就是使用softmax的变体，softmax-off-by-one，替换attention中的softmax。</p>\n<p>softmax-off-by-one的公式如下所示</p>\n<p><span class=\"math display\">\\[\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}\\]</span></p>\n<p>softmax-off-by-one在分母增加了一个常数1，这样就不要求所有attention\nscore的和为1。这相当于有一个K和V全都是0的虚拟token，固定和所有其他token的注意力得分为1，这个虚拟token在一定程度上也起到了attention\nsink的作用。</p>\n<p>为了验证增加特殊token作为attention\nsink，以及使用softmax-off-by-one的方案效果，论文在相同设置下训练了3个160M参数的模型，效果如下</p>\n<img src=\"/45ee1a6d/stremingllm_exp.png\" class title=\"实验\">\n<p>从结果上看，即使使用softmax-off-by-one（zero\nsink），模型还是会依赖前几个token作为attention sink。</p>\n<p>最后，论文在LLAMA2, MPT,\nFalcon和Pythia模型验证StreamingLLM在4M长度上的效果。MPT使用的位置编码是ALIBI，其他模型则是RoPE。效果如下图</p>\n<img src=\"/45ee1a6d/stremingllm_perf_4m.png\" class title=\"效果\">\n<p>在4M的长度上，StreamingLLM仍然能保持PPL较为稳定。</p>\n<p>理论上StreamingLLM可以把窗口推到无限长，但是由于本质上还是window\nattention，因此超过cache大小的部分还是会被丢弃。因此StreamingLLM虽然能保持PPL较低，但是对于需要高精度阅读理解和推理的任务，StreamingLLM还是有局限。</p>\n<h1 id=\"lm-infinite\">LM-Infinite</h1>\n<p>论文：LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models</p>\n<p>时间：2023年8月</p>\n<p>长度：在2k或者4k长度上训练，在200M长度上推理保持较低PPL</p>\n<p>阶段：无需训练，直接应用在推理阶段</p>\n<p>LM-Infinite可以和现有的LLM结合使用，在Passkey\nRetrieval和Qasper任务上有比较好的效果，且和原始模型相比，在解码速度上有2.7倍提升，显存上节省7.5倍。</p>\n<h2 id=\"三个挑战\">三个挑战</h2>\n<p>LM-Infinite认为，Transformer\nLLM无法有效地泛化到长上下文的情况，主要是因为会面临3个挑战。</p>\n<ul>\n<li>挑战1：challenges in handling unseen distances among tokens</li>\n</ul>\n<p>对于使用相对位置编码的模型，两个token的位置对注意力值的影响取决于两个token之间的距离。当推理的长度越来越长，就会出现在训练时没有见过的相对距离，此时attention\nlogits就会倾向于爆炸增长到无穷大，以区分在训练时从未见过的距离。论文对此给出了数学证明。</p>\n<p>而从实践上来看，把LLAMA2所有注意力头在ArXiv数据集上8k以内的attention\nlogits抽取出来，其均值和方差如下图所示。</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_logits_explode.png\" class title=\"logits爆炸\">\n<p>LLAMA2的预训练窗口大小为4k，可以看到4k之后，attention\nlogits的均值和方差开始有明显的上升。</p>\n<p>那么为了缓解这个问题，一个自然的想法是将token之间的相对距离值限制在模型预训练期间看到过的最大值，即设置一个距离上限。这样可以解决logits爆炸，但是会导致下面这个问题。</p>\n<ul>\n<li>挑战2：attending to unseen numbers of tokens</li>\n</ul>\n<p>对于较长的上下文，位置靠后的token需要在更大的上下文长度范围内，分配它的注意力权重。论文提出，如果注意力logits有界，那么随着上下文长度变长，注意力熵(attention\nentropy)将增长至无穷大。这里原文也给出了数学证明。</p>\n<p>直观来说，在注意力权重分配为平均分配的时候，注意力熵最大。而注意力熵增大，表示当前的token不能确定应该将主要注意力放在上文的哪些token，即无法集中注意力。</p>\n<p>同样地，我们把模型在8k内的注意力熵画出来，结果如下图所示，随着长度增大，注意力熵也逐渐增大。</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_entropy.png\" class title=\"注意力熵爆炸\">\n<p>这表明我们应该限制attention context\nsize，以确保注意力熵保持在预训练见过的范围内。</p>\n<p>如果简单地使用一个window\nattention，使得每个token只能关注到一定距离内的token，在一定程度上可以一定程度上handle挑战1和挑战2。这样的思路和XPos、Longformer类似。</p>\n<p>但是使用window attention，又会引入新的问题。</p>\n<ul>\n<li>挑战3：starting tokens occupy a distinct feature space</li>\n</ul>\n<p>即使没有显式的绝对位置编码，位置最靠前的几个初始token的注意力输出和其他位置相比，也会占据一个独特的表示空间。</p>\n<p>这一结论来自于《The impact of positional encoding on length\ngeneralization in\ntransformers》中的定理1，该定理证明了即使没有使用位置编码，单向注意力使得单个注意力层中的token输出也可以隐式地编码绝对位置，而初始token的信号是最强的，且很容易和其他token区分开来。</p>\n<p>这个观察和StreamingLLM中指出的attention\nsink很相似，总之就是最靠前的几个初始token有很大的影响，因此使用window\nattention把初始token忽略会带来很大的效果损失。</p>\n<p>论文把LLAMA2中各层的hidden state\noutput做了PCA，降到2维之后画出来，如下图所示</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens.png\" class title=\"初始token\">\n<p>蓝色的点表示初始token，红色的点表示靠后的token。上图显示了初始token的特征在特征空间中占据了和其他token很不同的区域。这就解释了为什么简单地将注意力窗口限制在最近的token上会效果不好：由于注意力本质上是加权平均，丢弃初始的几个token影响了注意力输出到达它们应占据的空间。</p>\n<p>因此，我们需要保留初始token。</p>\n<h2 id=\"解决方案\">解决方案</h2>\n<p>基于上面的这些观察和发现，论文提出LM-Infinite，给LLM做zero-shot的长度泛化。LM-Infinite设计了两个主要机制，Λ-shaped\nattention mask和Distance ceiling。</p>\n<ul>\n<li>Λ-shaped attention mask</li>\n</ul>\n<p>Λ-shaped attention mask在概念上和Longformer、LongNet、Big\nBird的思路类似。</p>\n<p>Λ-shaped attention mask在window\nattention的基础上，保持了初始token对后续所有token可见。具体来说，每个token能关注到两部分：（1）<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\n个初始token（2）<span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\n个最近的token。其中<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span> 为超参，<span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\n是预训练时的最大窗口大小。除了这两个部分的token之外，其他token都会被直接忽略。</p>\n<p>论文对 <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\n的数量进行了实验，发现 <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\in[5,100]\\)</span>\n时的效果比较好，如下图所示。</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens_num.png\" class title=\"初始token数量实验\">\n<p>注意这里不需要对模型进行任何训练和微调，只需要修改推理的方式即可。</p>\n<p>Λ-shaped attention mask可以解决挑战2和挑战3。</p>\n<ul>\n<li>Distance ceiling</li>\n</ul>\n<p>LM-Infinite把最大距离限制在 <span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>，这样做基本只影响初始token的位置编码。</p>\n<p>具体来说，对于相对位置编码，假设原来的attention logit是 <span class=\"math inline\">\\(w(\\mathbf{q},\\mathbf{k},d)\\)</span>，其中 <span class=\"math inline\">\\(d\\)</span> 是两个token之间的距离；那么Distance\nceiling就是把attention logit的计算变成：</p>\n<p><span class=\"math display\">\\[\\text{attention\nlogits}=w(\\mathbf{q},\\mathbf{k},d&#39;)\\]</span></p>\n<p><span class=\"math display\">\\[d&#39;=\\min(d,L_\\text{pretrain})\\]</span></p>\n<p>Distance ceiling限制了最大距离，可以解决挑战1。</p>\n<ul>\n<li>Optionally attending to top-k tokens in the middle</li>\n</ul>\n<p>除了Λ-shaped attention mask和Distance\nceiling以外，论文还设计了一个可选的机制，用于把“除了当前窗口内的最近token和初始token”以外的中间部分token也加入到attention计算中，这部分token本来是会被直接丢弃的。</p>\n<p>具体来说，会从原本被抛弃的中间token里，选择 <span class=\"math inline\">\\(k\\)</span> 个attention\nlogits最大的token加入到attention 计算中来。这 <span class=\"math inline\">\\(k\\)</span> 个token的距离都会被统一设置为 <span class=\"math inline\">\\(d=\\frac12L_\\text{pre-train}\\)</span>。</p>\n<p>对 <span class=\"math inline\">\\(k\\)</span> 的取值，论文在Passkey\nRetrieval的validation set上做了实验，结果如下</p>\n<img src=\"/45ee1a6d/lm_infinite_middle_k.png\" class title=\"k的数量\">\n<p>最终取了 <span class=\"math inline\">\\(k=5\\)</span>，并且只在模型&gt;5的层中加入。</p>\n<p>加入middle\ntoken主要用于优化需要精准获取中间信息的任务，对于没有这个需要的任务，加入过多的middle\ntoken不仅无益，甚至有害，原因如前面挑战2所分析的，会增大注意力熵。</p>\n<p>最终，LM-Infinite的整体方案如下图所示</p>\n<img src=\"/45ee1a6d/lm_infinite_design.png\" class title=\"设计\">\n<h2 id=\"实验\">实验</h2>\n<p>论文在LLaMA-7B、LLaMA2-7B、MPT-7B和GPT-J-6B这几个模型验证LM-Infinite的效果。除了MPT-7B使用Alibi之外，其他模型都是用RoPE。</p>\n<ul>\n<li>Language Modeling</li>\n</ul>\n<p>首先是语言建模的评测，使用的数据集是ArXiv和OpenWebText2。</p>\n<p>选择的几个基线模型，以及这些模型使用LM-Infinite的推理策略之后，在0-12k长度上的PPL如下图所示</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_figure.png\" class title=\"PPL\">\n<p>LLAMA2在略长于10K的长度上就输出了NaN，因此其曲线较短。所有原始模型在约32K的时候就出现OOM。</p>\n<p>当测试长度超过训练长度时，基线模型的PPL会迅速爆炸。而使用LM-Infinite之后，所有模型都可以泛化到比训练长度长得多，而仍然保持较低PPL。</p>\n<p>为了验证LM-Infinite在更大长度上的效果，用LM-Infinite +\nLlama2，在由ArXiv数据重复抽样拼接构成的200M\ntoken长度上的评估PPL。结果如下图所示，在200M长度下，依然可以保持稳定较低的PPL。</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_200m.png\" class title=\"PPL 200M\">\n<ul>\n<li>Passkey Retrieval and Qapser</li>\n</ul>\n<p>Passkey\nRetrieval和Qapser就是需要对整个上下文有精确理解的任务，因此对于这两个任务，加入了top-5个middle\ntoken到attention计算中。</p>\n<p>结果如下表。原始模型在长度超出训练长度之后，几乎无法输出有效内容，因此Passkey\nRetrieval上全得0分。而LM-Infinite相比简单的长度裁剪方案，在两个任务上都有更好效果。</p>\n<img src=\"/45ee1a6d/lm_infinite_downstream.png\" class title=\"下游任务\">\n<ul>\n<li>Ablation study</li>\n</ul>\n<p>对于LM-Infinite的两个主要设计，Λ-shaped attention mask和Distance\nceiling，论文做了消融实验，如下图所示。</p>\n<img src=\"/45ee1a6d/lm_infinite_ablation.png\" class title=\"消融实验\">\n<p>只使用Λ-shaped attention mask或者distance\nceiling单一策略的情况下，PPL仍然会出现较明显的上升，说明这两个组件都是必须的。</p>\n<h1 id=\"transformer-xl\">Transformer-XL</h1>\n<p>讲下一篇Infini-Transformer之前，先简单回顾下Transformer-XL的思路。</p>\n<p>Transformer-XL是2019年6月，由CMU和Google\nBrain发布的一个工作，目的是赋予在固定长度上训练的模型，在更大长度上的推理能力。</p>\n<p>一般来说，transformer的训练和推理如下图所示，训练的时候模型在固定的长度上训练，如果输入文本超过了模型的训练窗口大小，那就把输入切分成多个segment，模型在各个segment分别独立地进行训练；而推理的时候，如果文本超过了模型的推理窗口大小，模型可以使用滑动窗口的方式，逐步进行推理。</p>\n<img src=\"/45ee1a6d/xl_vanilla_sw.png\" class title=\"vanilla transformer\">\n<p>在窗口滑动的过程中，模型始终只能看到窗口内的内容，对于超出的部分则无法把信息纳入计算中，这显然对有长距离依赖的任务有损害。</p>\n<p>为了让模型看得更远，Transformer-XL把attention计算修改成下面的样子：把输入切分成多个segment，每个的长度为L，对于两个相邻的segment\n<span class=\"math inline\">\\([\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]\\)</span>\n和 <span class=\"math inline\">\\(\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]\\)</span>，计算如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&amp;\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&amp;\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}\\]</span></p>\n<p>其中SG是stop\ngradient的操作，这样在训练的时候能保持仍然只有固定窗口大小的参数进行梯度计算和更新，而不会扩大显存需求。<span class=\"math inline\">\\(\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}\\)</span>\n表示在序列长度维度进行拼接。h、q、k、v的上标 <span class=\"math inline\">\\(n\\)</span> 和 <span class=\"math inline\">\\(n-1\\)</span> 是模型的层序号。</p>\n<p>对于当前segment的每一层attention计算，Transformer-XL把上一个segment的信息加入到当前的K和V中再进行注意力计算，这样相当于每层相比上一层，都能往前多关注到一个segment长度的信息，模型的层数越多，模型最终能关注的长度越长，最后一个segment的最终输出依赖第一个segment的第一层输出，如下图所示。</p>\n<img src=\"/45ee1a6d/xl_attention.png\" class title=\"Transformer-XL\">\n<h1 id=\"infini-transformer\">Infini-Transformer</h1>\n<p>论文：Leave No Context Behind:Efficient Infinite Context Transformers\nwith Infini-attention</p>\n<p>时间：2024年4月</p>\n<p>长度：实验500k/1M，理论∞</p>\n<p>阶段：继续预训练/微调</p>\n<p>Google提出的一种叫Infini-attention的注意力计算方式，可以使用有限的显存计算无限长的上下文。</p>\n<h2 id=\"背景-1\">背景</h2>\n<p>记忆能力是智能的基石，因为它使得针对特定上下文的高效计算成为可能。</p>\n<p>但是transformer的注意力计算在时间和空间上都跟输入长度是二次的关系，这就使得在长上下文的场景下很难高效、准确地处理信息。</p>\n<p>使用记忆模块可以在大大减少计算量和显存需求的前提下，依然保持长上下文完整信息对模型的可见性。理论上所有输入信息都可以保存在记忆模块中，因此模型具备处理“有长距离依赖，且需要对输入信息进行准确检索”的任务。从这点上看，和StreamingLLM相比，记忆模块的效果上限更高。</p>\n<p>针对高效的记忆，目前也有一些相关工作，比如《Metalearned neural\nmemory》/《Enhancing the transformer with explicit relational encoding\nfor math problem solving》所使用的compressive\nmemory，可以用固定大小的记忆模块来编码context，但是现有的记忆模块在效果和设计的simplicity上仍然存在tradeoff，比较难平衡。</p>\n<h2 id=\"infini-attention\">Infini-attention</h2>\n<p>基于此，Infini-Transformer主要的改进点就在记忆模块的设计上。Infini-attention的结构设计如下图所示，每个Transformer\nblock内包括了常规的mask local attention，和储存长期记忆的long term\nlinear attention。</p>\n<img src=\"/45ee1a6d/infini_attention_structure.png\" class title=\"infini-attention结构\">\n<p>Infini-attention机制复用了标准注意力计算中的所有QKV值，构建了固定大小的注意力模块，用于长期记忆。在处理后续序列时，根据当前的Q从记忆模块中检索相关内容。Infini-attention将长期记忆检索到的值与局部注意力上下文通过concat进行聚合，共同计算最终的输出。</p>\n<p>这种对Transformer注意力层的微小但关键的修改，使得现有的LLM能够通过继续预训练和微调自然地扩展到无限长的上下文。</p>\n<p>Infini-attention和Transformer-XL的对比如下图所示</p>\n<img src=\"/45ee1a6d/infini_attention_process.png\" class title=\"infini-attention计算方式\">\n<p>和Transformer-XL类似，Infini-Transformer也把输入分成多个segment来处理。不同的是，Transformer-XL每层只能比前一层多看到一个segment的信息，因此需要通过堆叠模型的层数来提升模型处理窗口的大小；而由于层数是有限的，最终窗口的大小还是有限的。</p>\n<p>Infini-Transformer则是每层都有对之前所有segment的长期记忆，因此不受模型层数的影响，可以扩展到无限大的窗口。</p>\n<p>compressive memory的具体设计，出于simplicity和computational\nefficiency的考虑，论文参考《Learning associative inference using fast\nweight memory》，把memory设计成一个associative matrix。</p>\n<p>对于记忆模块，最重要的就是记忆的更新和检索。</p>\n<ul>\n<li>记忆检索</li>\n</ul>\n<p>先看下记忆的检索。假设每个segment的长度为 <span class=\"math inline\">\\(N\\)</span>，我们使用当前的 <span class=\"math inline\">\\(Q\\in\\mathbf{R}^{N\\times d_{key}}\\)</span> 从memory\n<span class=\"math inline\">\\(M_{s-1}\\in\\mathbf{R}^{d_{key}\\times\nd_{value}}\\)</span> 中检索，计算如下</p>\n<p><span class=\"math display\">\\[A_{\\text{mеm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}\\]</span></p>\n<p>其中<span class=\"math inline\">\\(\\sigma\\)</span>是激活函数，使用的是element-wise\nELU + 1。</p>\n<p><span class=\"math inline\">\\(z_{s-1}\\in\\mathbf{R}^{d_{key}}\\)</span>是normalization\nterm。这里normalization term参考《Transformers are rnns: Fast\nautoregressive transformers with linear\nattention》的做法，使用所有K值的求和值。</p>\n<ul>\n<li>记忆更新</li>\n</ul>\n<p>完成检索之后，就要更新记忆。需要更新的有normalization\nterm和memory两项，更新方式如下。</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^TV\\]</span></p>\n<p><span class=\"math display\">\\[z_s\\leftarrow\nz_{s-1}+\\sum_{t=1}^N\\sigma(K_t)\\]</span></p>\n<p>memory的更新，参考《Metalearned neural memory》和《Learning\nassociative inference using fast weight memory》所使用的delta\nrule，进一步优化成如下形式</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})\\]</span></p>\n<ul>\n<li>长期记忆和local attention结合</li>\n</ul>\n<p>回到当前segment的注意力计算中，需要把长期记忆检索项 <span class=\"math inline\">\\(A_{mem}\\)</span> 和local attention state <span class=\"math inline\">\\(A_{dot}\\)</span> 结合</p>\n<p><span class=\"math display\">\\[A=sigmoid(\\beta)\\odot\nA_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\beta\\)</span>\n是一个可学习的标量参数。</p>\n<p>在训练中， <span class=\"math inline\">\\(\\beta\\)</span>\n所学习到的值如下，0则表示这个头只关注当前segment，1则表示值关注长期记忆的内容，介于0和1之间的则是两种的混合，可以看到大部分的注意力头会使用二者的混合结果</p>\n<img src=\"/45ee1a6d/infini_attention_gating.png\" class title=\"gating分布\">\n<p>整体策略上，和其他包含segment-level\nmemory的模型在方案设计上的比较如下</p>\n<img src=\"/45ee1a6d/infini_attention_compare.png\" class title=\"方案比较\">\n<h2 id=\"实验-1\">实验</h2>\n<p>实验上，每个segment的长度 <span class=\"math inline\">\\(N\\)</span>\n设为2048，模型的训练窗口大小为32768，即总共有16个segment。</p>\n<ul>\n<li>语言建模能力</li>\n</ul>\n<p>Infini-Transformer和Transformer-XL/Memorzing\nTransformer/RMT在PG-19和Arxiv数据集上的language modeling能力对比如下</p>\n<img src=\"/45ee1a6d/infini_attention_language_modeling.png\" class title=\"语言建模能力对比\">\n<p>Infini-Transformer的效果显著更好，而且所需的记忆空间只有Memorizing\nTransformers的不到1%。</p>\n<p>原文指出，把训练的数据长度提升到100k，Infini-Transformer的PPL还能进一步降低。</p>\n<ul>\n<li>下游任务</li>\n</ul>\n<p>用1B的模型进行轻量级的继续预训练，训练配置如下：<br>\n- batch size = 64 - step = 30k - 训练数据长度 &gt; 4k - segment length =\n2k</p>\n<p>之后在1M长度的passkey\nretrieval任务上评测，zero-shot和fine-tune模型在各个长度的效果如下表</p>\n<img src=\"/45ee1a6d/infini_attention_passkey.png\" class title=\"passkey效果\">\n<p>Infini-Transformer（linear + delta）在1M长度的passkey\nretrieval任务上做到完全正确。</p>\n<p>另外在8B模型上，用8k长度训练30k步，在500k的BookSum任务上评测，结果如下</p>\n<img src=\"/45ee1a6d/infini_attention_booksum.png\" class title=\"booksum效果\">\n<p>Infini-Transformer（linear + delta）相比其他模型有一定优势。</p>\n<h1 id=\"小结\">小结</h1>\n<ol type=\"1\">\n<li>Streaming和LM-Infinite思路有些相似，都观察到attention\nsink对模型效果的影响，并设计了相关的注意力计算机制保留初始token以稳定PPL，而LM-Infinite在这个基础上使用distance\nceiling，使得模型在推理时不会使用超出训练所用的token相对距离。虽然这两个工作使得模型可以在M级别的长度仍然保持较低PPL，但是实际大量中间token被丢弃，导致对需要在上下文精准检索的任务效果有损害。<br>\n</li>\n<li>Infini-Transformer使用记忆模块，把输入进行分段，并把靠前的内容通过固定大小的记忆矩阵进行压缩。理论上记忆模块可以提供所有上文的信息，能够应用在更精细和困难的任务。记忆模块的检索和更新的设计是方案的核心，需要考虑复杂度和效果的平衡。</li>\n</ol>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Efficient Streaming Language Models with Attention Sinks\nhttps://arxiv.org/abs/2309.17453<br>\n【2】Transformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext https://arxiv.org/abs/1901.02860<br>\n【3】Leave No Context Behind: Efficient Infinite Context Transformers\nwith Infini-attention https://arxiv.org/abs/2404.07143<br>\n【4】LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models https://arxiv.org/abs/2308.16137</p>\n"},{"title":"解锁大模型长上下文能力","abbrlink":"cc852861","date":"2024-05-04T11:05:48.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n步入2024年Q2，大模型在RAG、文档对话、大模型Agent能力等方向的发展持续升温。在平时的日常生活和工作中，大模型工具提供的文档总结、文本润色、代码生成等能力已经是提高效率的必备帮手，甚至在一些复杂或者不熟悉的场景上，大模型也已经能提供一些比较专业的帮助。  \n\n在这些方向上，大模型(超)长上下文的能力都是基础。无论是使用详细的CoT/ToT，还是通过多篇检索文档提供专业知识，抑或是使用相关样例提升回复质量，都需要模型具备处理很长的输入输出信息的能力。这不仅要求模型在较长的位置编码下依然具有良好的语言建模能力，而且还需要模型能够进行长距离的、细致的阅、准确的阅读和理解。  \n\n本篇将梳理几个通过轻量级训练解锁大模型长上下文能力的工作。  \n\n# 支持128k上下文的数据工程  \n\n论文：Data Engineering for Scaling Language Models to 128K Context  \n\n时间：2024年2月  \n\n阶段：预训练  \n\n长度：128k\n\n## 评测指标  \n\n模型的长上下文能力不仅体现在文本较长的时候，模型的PPL依然能保持在较低的水平，还体现在对于长上下文输入，模型依然能够进行准确的阅读理解和推理。  \n\n以往一些工作仅使用validation dataset上的PPL作为评测指标，并不能很好地表征模型的真实长上下文能力。而目前被广泛使用的 Needle in a Haystack，或者叫大海捞针任务，是对长上下文能力的一个比较好的评测。这篇论文主要就以大海捞针任务为标准，对不同的模型和方案进行对比。  \n\n两个PPL几乎相同的模型，在大海捞针任务上的差距可以很大，如下图所示，颜色越绿代表正确率越高  \n\n{% asset_img eng_ppl.png PPL和大海捞针 %}  \n\n目前已有的一些扩展大模型上下文窗口的方法，比如LongLoRA和Mistral所采用的YaRN，虽然理论上来说，能够支持>100k的上下文长度，但是实际上在大海捞针任务的表现却不太好。相关模型在大海捞针任务上的效果对比如下所示，只有GPT-4的效果比较好。  \n\n{% asset_img eng_needle_comp.png 大海捞针任务对比 %}  \n\n## 数据分布  \n\n这篇论文认为，在<=4k窗口长度完成预训练的模型，其实就已经基本具备在128k或者更大的上下文窗口进行推理的能力，只需要进行轻量级的继续预训练（e.g. <5B token），就能够解锁这种能力。  \n\n（而一些其他的工作在这方面则有着相反的观点，比如在32k窗口训练了400B token的《Effective long-context scaling of foundation models》，以及Xverse）  \n\n要做继续预训练，最重要的一点就是要决定使用什么样的数据。  \n\n这篇论文里的实验是基于LLAMA的，因此使用了和LLAMA预训练数据具有相近领域分布的SlimPajama数据集作为基础。  \n\n对于长上下文的继续预训练数据，需要仔细考虑数据长度和领域分布的影响。通常来说，某些领域天然会有更高比例的长文本数据，比如书籍、论文和github，而一些其他领域的长数据就较少，比如新闻。如果直接从整体数据中挑选长数据而忽略领域分布，就可能造成训练数据在领域分布上的偏移。  \n\n论文使用了几种不同的数据处理策略，用于后面的实验对比：  \n- Cut at 4K：把所有的数据按4k长度进行分块，这样不会影响领域分布。这也是很多4k预训练模型所采样的方案，比如LLAMA。  \n- Cut at 128K：截断长度提升到128k，可以保留长文本内部信息的依赖关系。LongLoRA就是这么做的。  \n- Per-source Upsampling：在保持各个领域的比例不变的前提下，对长文本进行上采样，提高长文本的比例。这是这篇论文所推荐的方法，实验效果最好。  \n- Global Upsampling：不管领域，直接对长文本进行上采样。  \n- Upsample Arxiv/ Book/ Github：提高特定领域的数据比例，对长文本进行上采样。  \n\n这些策略基本涵盖了大部分长文本相关工作在数据上的处理策略。  \n\n不同数据处理策略下，SlimPajama数据内各领域的分布如下图所示  \n\n{% asset_img eng_data_dist.png 数据分布 %}  \n\nPer-source Upsampling是效果最好的，也是这篇论文所推荐的数据工程策略。  \n\n## 实验配置  \n\n实验上，用80k的窗口长度训练LLAMA2-7B模型，用64k的窗口训练LLAMA2-13B模型。  \n\n虽然理论上，计算复杂度度和模型训练窗口长度是平方关系，但是实际实现上，由于有FlashAttention等方案，可以把Attention的计算通过设备间通讯，在多个设备间并行起来。而设备间的通讯（包括GPU和CPU，GPU和GPU之间）成本都是constant或者linear，因此实际上80k窗口的的训练耗时只是4k长度的训练的3倍，而不是理论上的400倍。  \n\n当然，实际所需的计算量并没有减少，但是至少时间成本从平方变成了线性。剩下的，只要堆jia卡qian就可以提速。  \n\n{% asset_img add_money.jpg 加钱就行 %}  \n\nPer-source Upsampling和其他工作的数据处理策略的对比如下  \n\n{% asset_img eng_data.png 模型策略 %}  \n\n训练的配置和耗时如下所示  \n\n{% asset_img eng_config.png 训练配置 %}  \n\n实验的其他配置：  \n- lr = 2e-5  \n- RoPE base从1,0000改为500,000  \n- batch size = 4M token  \n\n## 训练量  \n\n前面提到，论文认为只需要轻量级的继续预训练就可以解锁长上下文能力，那么到底需要训练多少token呢？  \n\n论文分别取了训练了100M、300M、500M、1B、5B、10B token的中间checkpoint进行PPL和海底捞针任务评测，结果如下  \n\n{% asset_img eng_tokens.png 训练量 %}  \n\n结论是，在训练了500M token的时候，模型基本解锁了长上下文的能力；在训练了5B token的时候，模型已经收敛，而且继续训练到10B token也没有进一步收益了。  \n\n## 数据策略对比  \n\n使用前面提到的不同数据策略在LLAMA2-7B模型用5B token进行训练，并对比效果。  \n\nLLAMA2的预训练长度为4k，因此对比的时候分成了0-4k和4k-128k两段，分别评测模型经过长文本训练后，在短文本上的效果是否有变差，以及在长文本上是否有提升。  \n\n各个数据策略在不同领域的效果变化如下  \n\n{% asset_img eng_sample.png 采样的影响 %}  \n\n可以得到几个结论：  \n- 在0-4k长度上，除了Per-source Upsampling以外，各种数据策略都会对模型效果有损害  \n- 在一些领域上的提升，并不能很好地迁移到其他领域，比如Book和Github之间就有点跷跷板效应，其中一个效果好了，另一个可能就有损失  \n- 在4k-128k，Per-source Upsampling在各个领域的效果相对较为平衡（绿色的数量最多）  \n\n此外，length upsampling很重要。Per-source Upsampling的策略在领域上可以和源数据保持一致，而提升长文本的比例。  \n\n用同样80k的训练窗口在LLAMA2-7B进行实验，一个使用原数据进行拼接，另一个使用Per-source Upsampling，结果如下。在PPL基本相同的情况下，Per-source Upsampling在大海捞针的效果远超原数据。这说明提高长文本的比例，能极大优化模型远距离建模的能力。  \n\n{% asset_img eng_ppl.png PPL和大海捞针 %}  \n\n## 结论\n\n通过实验，论文提出提升模型长上下文能力的数据工程实践的几个关键点：  \n- 在长窗口上进行轻量级训练，可以提升模型实际的远距离建模能力，而不仅仅是保持PPL较低  \n- 领域之间有竞争关系，最好和原预训练模型所用的分布保持一致  \n- 长度上采样对最终效果有很大影响，要提高各领域内长文本的比例  \n\n# Paraphrasing  \n\n论文：Training With \"Paraphrasing the Original Text\" Improves Long-Context Performance  \n\n时间：2023年12月  \n\n阶段：微调  \n\n长度：在50k长度依然能有较好的效果，如下所示。  \n\n{% asset_img paraphrasing_intro.png paraphrasing %}  \n\n## 检索能力  \n\n对于长上下文的任务，有用的信息通常是稀疏的，一般只有少量的句子或者段落包含了可以用于回答问题的有用信息。可以隐式地将这样长上下文的任务拆分成两个子任务，即相关信息的检索，和根据相关信息回答问题两个任务。  \n\n目前一些支持长上下文的方法，比如位置编码相关的线性插值、NTK插值、YaRN等，虽然使得模型在形式上支持了长上下文的任务，但是在任务的准确性上效果却不佳。  \n\n使用这些优化方案的模型依然会遇到lost in the middle的问题，即模型天然更容易关注到输入文本的开头和结尾部分的信息，而更容易忽略中间部分的信息，注意力迷失在大量无关内容上，而无法集中到少数相关的位置上。而对于长上下文的任务，大量的信息是处于middle的位置的，如果忽略这些信息自然会使得任务效果不好。而效果不好的原因就是模型在长上下文的情况下，retrieval能力偏弱，不能找到有用的信息。  \n\n## 相关工作  \n\n一些工作直接把模型在长窗口下进行训练，比如：  \n- Together的LLaMA-2-7B-32K（https://huggingface.co/datasets/togethercomputer/Long-Data-Collections）；Together开源了Multipassage-QA-from-Natural-Questions和BookSum微调数据集。  \n- LongAlpaca（《LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models》）  \n- Ziya-Reader（《Never Lost in the Middle:Improving Large Language Models via Attention Strengthening Question Answering》）  \n\n直接在长窗口训练有一定的效果，但是依然有几个问题：  \n- 模型推理窗口越来越大，所需的训练数据集长度也要不断更新。  \n- 随着长度增大，训练成本变高。  \n- 构建长上下文数据集的成本比价高，高质量的数据并不容易获得。虽然有一些开源的数据集，但是在实际场景上可能还需要做领域适配，分布调整等工作。  \n\n一个更简单一点的方法是优化prompt的设计，比如CoT。  \n\n在长上下文的场景下，可以通过prompt让模型显式地先找到原文的相关信息再进行回答。比如Claude-2.1就通过在prompt增加“Here is the most relevant sentence in the context”让长文本问答的准确率从27%提升到98%（https://www.anthropic.com/news/claude-2-1-prompting）。  \n\n也可以对输入内容进行重新的编排：  \n- LongLLMLingua（《LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression》）对输入文本进行了压缩。  \n- Attention Sorting（《Attention Sorting Combats Recency Bias In Long Context Language Models》）在decode过程中根据各个文档被分配到的注意力值，对文档进行重新排序。  \n\n## 提高检索能力  \n\n这篇论文提出了一个叫检索相关度（retrieval relevance）的指标，一个token（或者n-gram） $x$ 的检索相关度 $R(x)$ 定义如下。  \n\n$$R(x)=\\frac{n^\\prime}n\\log\\frac N{N^\\prime+1}$$  \n\n这个指标和TF-IDF很像。其中，$n^\\prime$ 表示 $x$ 在gold-chunk中的频率，而 $n$ 是gold-chunk中的总token数；$N$ 表示整个上下文中总chunk数，$N^\\prime$ 是包含x的chunk的数量。  \n\n基于token $x$ 的检索相关度 $R(x)$ ，定义训练样本 $S$ 的检索相关度如下  \n\n$$\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)$$  \n\n其中 $S_a$ 表示 $S$ 的答案部分。  \n\n通过 $\\mathcal{R}(S)$ 这个指标可以反映出一个训练样本对模型提高检索能力的贡献。$\\mathcal{R}(S)$ 越高，这个样本对提高模型检索能力的贡献越大。    \n\n那么一个简单有效提升训练样本检索相关度的做法，就是把gold-chunk放到答案中，即paraphrasing the original text。  \n\n一个paraphrasing和其他答案设计方案对比的例子如下  \n\n{% asset_img paraphrasing_example.png paraphrasing例子 %}  \n\n其中高亮部分的token是高检索相关度的token，明显paraphrasing拥有更高的比例。  \n\n论文使用GPT-4来构建包含paraphrasing的问答对，流程实际如下  \n\n{% asset_img paraphrasing_dataset.png 构建数据集 %}  \n\n这种方式收集了一批单文档问答和多文档问答的数据，再加上一些传统文本摘要数据（摘要不好用这种方式构建，因此直接使用）等，构成一个包含10,825条英文数据，8,454条中文数据，长度在8k和32k之间的数据集。数据集详细的领域分布如下所示  \n\n{% asset_img paraphrasing_dataset_dist.png 数据集分布 %}  \n\n论文构建的数据集和Multi-passage-QA-from-NQ的检索相关性指标对比如下  \n\n{% asset_img paraphrasing_quality.png 数据集检索相关性对比 %}  \n\n使用这个数据集微调的模型，和其他模型在LongBench上的效果对比如下  \n\n{% asset_img paraphrasing_perf.png 效果对比 %}  \n\n另外，在这个数据集上微调之后，模型对于lost in the middle的问题也有一定的缓解，如下所示  \n\n{% asset_img paraphrasing_lost.png 缓解lost in the middle %}  \n\n# PoSE  \n\n论文：PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training  \n\n时间：2023年9月  \n\n阶段：微调  \n\n长度：128k\n\n## 背景  \n\n目前大部分流行的大模型使用旋转位置编码RoPE。在短文本上训练的模型，在长输入上效果不好的原因之一，就是长文本有很多模型没有见过没有训练过的位置编码。  \n\n基于位置编码的长上下文优化，比如线性插值、NTK插值和YaRN等，依然需要进行目标长度的训练才能有比价好的效果。而随着目标长度越来越长（8k，32k，128k...），这样的训练成本也越来越高，逐渐变得不容易进行。  \n\n这篇论文提出**Po**sitional **S**kip-wis**E**，PoSE，通过在短的训练窗口模拟长距离的位置编码，提升模型处理长上下文的能力。模型可以在2k的训练窗口进行训练，而在128k的长度进行推理。相比直接训练128k模型效率更高。  \n\n也有一些工作的思路和这篇文章有相近之处，比如RandPos（《Randomized positional encodings boost length generalization of transformers》），但是RandPos主要用于预训练阶段，并且相邻token之间的位置是不连续的，而PoSE主要用于微调阶段，相邻token之间的位置是连续的。  \n\n## 位置模拟  \n\nPoSE提出两个设计原则：  \n- 模拟所用的位置编码index要覆盖目标长度的范围。如果我们想在128k的窗口进行推理，那就要保证训练的时候，模型从1-128k的位置编码都见过。  \n- 为了不损害原模型的能力，位置编码应该尽量保持原来预训练的结构，即尽量连续，和保持顺序关系。  \n\n假设我们的训练窗口长度为 $L_c$，首先我们随机把它切成 $N$ 个chunk， $c_0,c_1,\\ldots,c_{N-1}$，长度分别为 $l_0,l_1,\\ldots,l_{N-1}$。对于chunk $i$，其中token的位置编码下标如下  \n\n$$\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad st_i=\\sum_{j=0}^{i-1}l_j$$  \n\n然后我们给每个chunk，从uniform distribution $\\mathcal{U}(S)$ 中随机采样一个skipping bias $u_i$，把这个bias加到这个对应chunk的token的位置编码下标中，就有  \n\n$$\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}$$\n\n这里要注意，处理后各个chunk的位置编码下标不能有overlap，所以要求 $u_i\\geq u_{i-1}$。  \n\n直观地说，引入skipping bias使模型能接触到更大范围的位置编码。为了全面覆盖目标上下文窗口，我们为每个训练sample单独采样每个chunk的长度和skipping bias。  \n\n此外，位置编码index在每个chunk内的连续性，与原模型预训练期间所采用的结构非常相似。因此，在这些新的index上进行微调，不会损害模型原有的能力。  \n\n现在，位置编码的下标决定好了，我们还需要决定每个chunk的token使用哪些。  \n\ntoken的采样和位置编码下标的采样类似，具体来说，我们采样$v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})$，那么 $c_i$ 的token如下  \n\n$$c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]$$  \n\n论文对一些采样变体，比如 $v_i=u_i$，$v_i=0$ 等进行了探索，发现基本没有什么影响，因此 $v_i$ 保持原来的采样方案即可。  \n\n在实际训练中，$N$ 设置为2，因为如果太大可能对原模型的能力造成损害。而 $u_0$ 和 $v_0$ 设为了0。  \n\nPoSE方案如下图所示  \n\n{% asset_img pose_method.png PoSE %}  \n\n实验上，使用了LLAMA-7B模型，在2k的窗口上进行了1,000步的训练，batch size为64。使用lr=2e-5，warmup step=10。  \n\nPoSE和其他模型在PPL上的对比如下，基本能达到和Full-length训练相近的水平。  \n\n{% asset_img pose_ppl.png PPL %}  \n\n而在passkey retrieval任务上，也有不错的效果，如下图所示  \n\n{% asset_img pose_passkey.png passkey %}  \n\n相比其他方案，PoSE的一个优势是可以在没有任何成本增加的情况下，支持更长的推理长度。比如可以通过简单修改采样策略的参数，PoSE就可以支持到1M，甚至更大的窗口长度，这是其他方法难以做到的。  \n\n# 小结  \n\n1. 有了FlashAttention等方案之后，在128k这个长度，我们也有能力在合理的成本下，进行继续预训练，使用5B左右的token解锁模型的长上下文能力。  \n2. 预训练中，长文本对模型的远距离建模能力很重要，要提高长文本的比例才有更好的效果。此外，领域的分布也是一个需要关注的点。  \n3. 在长窗口的微调上，精心设计输入输出形式能带来一些收益。\n4. 对于更长的窗口，比如M级别这种几乎无法直接训练/微调的长度，PoSE这种模拟的方案能够在不增加成本的情况下，在效果上达到接近直接训练/微调的表现。  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n【1】Data Engineering for Scaling Language Models to 128K Context https://arxiv.org/abs/2402.10171  \n【2】Training With \"Paraphrasing the Original Text\" Improves Long-Context Performance https://arxiv.org/abs/2312.11193  \n【3】PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training https://arxiv.org/abs/2309.10400  \n","source":"_posts/cs/nlp/2024/05/解锁大模型长上下文能力.md","raw":"---\ntitle: 解锁大模型长上下文能力\nabbrlink: cc852861\ndate: 2024-05-04 19:05:48\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 长上下文\n  - 预训练\n  - 微调\n  - attention\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n步入2024年Q2，大模型在RAG、文档对话、大模型Agent能力等方向的发展持续升温。在平时的日常生活和工作中，大模型工具提供的文档总结、文本润色、代码生成等能力已经是提高效率的必备帮手，甚至在一些复杂或者不熟悉的场景上，大模型也已经能提供一些比较专业的帮助。  \n\n在这些方向上，大模型(超)长上下文的能力都是基础。无论是使用详细的CoT/ToT，还是通过多篇检索文档提供专业知识，抑或是使用相关样例提升回复质量，都需要模型具备处理很长的输入输出信息的能力。这不仅要求模型在较长的位置编码下依然具有良好的语言建模能力，而且还需要模型能够进行长距离的、细致的阅、准确的阅读和理解。  \n\n本篇将梳理几个通过轻量级训练解锁大模型长上下文能力的工作。  \n\n# 支持128k上下文的数据工程  \n\n论文：Data Engineering for Scaling Language Models to 128K Context  \n\n时间：2024年2月  \n\n阶段：预训练  \n\n长度：128k\n\n## 评测指标  \n\n模型的长上下文能力不仅体现在文本较长的时候，模型的PPL依然能保持在较低的水平，还体现在对于长上下文输入，模型依然能够进行准确的阅读理解和推理。  \n\n以往一些工作仅使用validation dataset上的PPL作为评测指标，并不能很好地表征模型的真实长上下文能力。而目前被广泛使用的 Needle in a Haystack，或者叫大海捞针任务，是对长上下文能力的一个比较好的评测。这篇论文主要就以大海捞针任务为标准，对不同的模型和方案进行对比。  \n\n两个PPL几乎相同的模型，在大海捞针任务上的差距可以很大，如下图所示，颜色越绿代表正确率越高  \n\n{% asset_img eng_ppl.png PPL和大海捞针 %}  \n\n目前已有的一些扩展大模型上下文窗口的方法，比如LongLoRA和Mistral所采用的YaRN，虽然理论上来说，能够支持>100k的上下文长度，但是实际上在大海捞针任务的表现却不太好。相关模型在大海捞针任务上的效果对比如下所示，只有GPT-4的效果比较好。  \n\n{% asset_img eng_needle_comp.png 大海捞针任务对比 %}  \n\n## 数据分布  \n\n这篇论文认为，在<=4k窗口长度完成预训练的模型，其实就已经基本具备在128k或者更大的上下文窗口进行推理的能力，只需要进行轻量级的继续预训练（e.g. <5B token），就能够解锁这种能力。  \n\n（而一些其他的工作在这方面则有着相反的观点，比如在32k窗口训练了400B token的《Effective long-context scaling of foundation models》，以及Xverse）  \n\n要做继续预训练，最重要的一点就是要决定使用什么样的数据。  \n\n这篇论文里的实验是基于LLAMA的，因此使用了和LLAMA预训练数据具有相近领域分布的SlimPajama数据集作为基础。  \n\n对于长上下文的继续预训练数据，需要仔细考虑数据长度和领域分布的影响。通常来说，某些领域天然会有更高比例的长文本数据，比如书籍、论文和github，而一些其他领域的长数据就较少，比如新闻。如果直接从整体数据中挑选长数据而忽略领域分布，就可能造成训练数据在领域分布上的偏移。  \n\n论文使用了几种不同的数据处理策略，用于后面的实验对比：  \n- Cut at 4K：把所有的数据按4k长度进行分块，这样不会影响领域分布。这也是很多4k预训练模型所采样的方案，比如LLAMA。  \n- Cut at 128K：截断长度提升到128k，可以保留长文本内部信息的依赖关系。LongLoRA就是这么做的。  \n- Per-source Upsampling：在保持各个领域的比例不变的前提下，对长文本进行上采样，提高长文本的比例。这是这篇论文所推荐的方法，实验效果最好。  \n- Global Upsampling：不管领域，直接对长文本进行上采样。  \n- Upsample Arxiv/ Book/ Github：提高特定领域的数据比例，对长文本进行上采样。  \n\n这些策略基本涵盖了大部分长文本相关工作在数据上的处理策略。  \n\n不同数据处理策略下，SlimPajama数据内各领域的分布如下图所示  \n\n{% asset_img eng_data_dist.png 数据分布 %}  \n\nPer-source Upsampling是效果最好的，也是这篇论文所推荐的数据工程策略。  \n\n## 实验配置  \n\n实验上，用80k的窗口长度训练LLAMA2-7B模型，用64k的窗口训练LLAMA2-13B模型。  \n\n虽然理论上，计算复杂度度和模型训练窗口长度是平方关系，但是实际实现上，由于有FlashAttention等方案，可以把Attention的计算通过设备间通讯，在多个设备间并行起来。而设备间的通讯（包括GPU和CPU，GPU和GPU之间）成本都是constant或者linear，因此实际上80k窗口的的训练耗时只是4k长度的训练的3倍，而不是理论上的400倍。  \n\n当然，实际所需的计算量并没有减少，但是至少时间成本从平方变成了线性。剩下的，只要堆jia卡qian就可以提速。  \n\n{% asset_img add_money.jpg 加钱就行 %}  \n\nPer-source Upsampling和其他工作的数据处理策略的对比如下  \n\n{% asset_img eng_data.png 模型策略 %}  \n\n训练的配置和耗时如下所示  \n\n{% asset_img eng_config.png 训练配置 %}  \n\n实验的其他配置：  \n- lr = 2e-5  \n- RoPE base从1,0000改为500,000  \n- batch size = 4M token  \n\n## 训练量  \n\n前面提到，论文认为只需要轻量级的继续预训练就可以解锁长上下文能力，那么到底需要训练多少token呢？  \n\n论文分别取了训练了100M、300M、500M、1B、5B、10B token的中间checkpoint进行PPL和海底捞针任务评测，结果如下  \n\n{% asset_img eng_tokens.png 训练量 %}  \n\n结论是，在训练了500M token的时候，模型基本解锁了长上下文的能力；在训练了5B token的时候，模型已经收敛，而且继续训练到10B token也没有进一步收益了。  \n\n## 数据策略对比  \n\n使用前面提到的不同数据策略在LLAMA2-7B模型用5B token进行训练，并对比效果。  \n\nLLAMA2的预训练长度为4k，因此对比的时候分成了0-4k和4k-128k两段，分别评测模型经过长文本训练后，在短文本上的效果是否有变差，以及在长文本上是否有提升。  \n\n各个数据策略在不同领域的效果变化如下  \n\n{% asset_img eng_sample.png 采样的影响 %}  \n\n可以得到几个结论：  \n- 在0-4k长度上，除了Per-source Upsampling以外，各种数据策略都会对模型效果有损害  \n- 在一些领域上的提升，并不能很好地迁移到其他领域，比如Book和Github之间就有点跷跷板效应，其中一个效果好了，另一个可能就有损失  \n- 在4k-128k，Per-source Upsampling在各个领域的效果相对较为平衡（绿色的数量最多）  \n\n此外，length upsampling很重要。Per-source Upsampling的策略在领域上可以和源数据保持一致，而提升长文本的比例。  \n\n用同样80k的训练窗口在LLAMA2-7B进行实验，一个使用原数据进行拼接，另一个使用Per-source Upsampling，结果如下。在PPL基本相同的情况下，Per-source Upsampling在大海捞针的效果远超原数据。这说明提高长文本的比例，能极大优化模型远距离建模的能力。  \n\n{% asset_img eng_ppl.png PPL和大海捞针 %}  \n\n## 结论\n\n通过实验，论文提出提升模型长上下文能力的数据工程实践的几个关键点：  \n- 在长窗口上进行轻量级训练，可以提升模型实际的远距离建模能力，而不仅仅是保持PPL较低  \n- 领域之间有竞争关系，最好和原预训练模型所用的分布保持一致  \n- 长度上采样对最终效果有很大影响，要提高各领域内长文本的比例  \n\n# Paraphrasing  \n\n论文：Training With \"Paraphrasing the Original Text\" Improves Long-Context Performance  \n\n时间：2023年12月  \n\n阶段：微调  \n\n长度：在50k长度依然能有较好的效果，如下所示。  \n\n{% asset_img paraphrasing_intro.png paraphrasing %}  \n\n## 检索能力  \n\n对于长上下文的任务，有用的信息通常是稀疏的，一般只有少量的句子或者段落包含了可以用于回答问题的有用信息。可以隐式地将这样长上下文的任务拆分成两个子任务，即相关信息的检索，和根据相关信息回答问题两个任务。  \n\n目前一些支持长上下文的方法，比如位置编码相关的线性插值、NTK插值、YaRN等，虽然使得模型在形式上支持了长上下文的任务，但是在任务的准确性上效果却不佳。  \n\n使用这些优化方案的模型依然会遇到lost in the middle的问题，即模型天然更容易关注到输入文本的开头和结尾部分的信息，而更容易忽略中间部分的信息，注意力迷失在大量无关内容上，而无法集中到少数相关的位置上。而对于长上下文的任务，大量的信息是处于middle的位置的，如果忽略这些信息自然会使得任务效果不好。而效果不好的原因就是模型在长上下文的情况下，retrieval能力偏弱，不能找到有用的信息。  \n\n## 相关工作  \n\n一些工作直接把模型在长窗口下进行训练，比如：  \n- Together的LLaMA-2-7B-32K（https://huggingface.co/datasets/togethercomputer/Long-Data-Collections）；Together开源了Multipassage-QA-from-Natural-Questions和BookSum微调数据集。  \n- LongAlpaca（《LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models》）  \n- Ziya-Reader（《Never Lost in the Middle:Improving Large Language Models via Attention Strengthening Question Answering》）  \n\n直接在长窗口训练有一定的效果，但是依然有几个问题：  \n- 模型推理窗口越来越大，所需的训练数据集长度也要不断更新。  \n- 随着长度增大，训练成本变高。  \n- 构建长上下文数据集的成本比价高，高质量的数据并不容易获得。虽然有一些开源的数据集，但是在实际场景上可能还需要做领域适配，分布调整等工作。  \n\n一个更简单一点的方法是优化prompt的设计，比如CoT。  \n\n在长上下文的场景下，可以通过prompt让模型显式地先找到原文的相关信息再进行回答。比如Claude-2.1就通过在prompt增加“Here is the most relevant sentence in the context”让长文本问答的准确率从27%提升到98%（https://www.anthropic.com/news/claude-2-1-prompting）。  \n\n也可以对输入内容进行重新的编排：  \n- LongLLMLingua（《LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression》）对输入文本进行了压缩。  \n- Attention Sorting（《Attention Sorting Combats Recency Bias In Long Context Language Models》）在decode过程中根据各个文档被分配到的注意力值，对文档进行重新排序。  \n\n## 提高检索能力  \n\n这篇论文提出了一个叫检索相关度（retrieval relevance）的指标，一个token（或者n-gram） $x$ 的检索相关度 $R(x)$ 定义如下。  \n\n$$R(x)=\\frac{n^\\prime}n\\log\\frac N{N^\\prime+1}$$  \n\n这个指标和TF-IDF很像。其中，$n^\\prime$ 表示 $x$ 在gold-chunk中的频率，而 $n$ 是gold-chunk中的总token数；$N$ 表示整个上下文中总chunk数，$N^\\prime$ 是包含x的chunk的数量。  \n\n基于token $x$ 的检索相关度 $R(x)$ ，定义训练样本 $S$ 的检索相关度如下  \n\n$$\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)$$  \n\n其中 $S_a$ 表示 $S$ 的答案部分。  \n\n通过 $\\mathcal{R}(S)$ 这个指标可以反映出一个训练样本对模型提高检索能力的贡献。$\\mathcal{R}(S)$ 越高，这个样本对提高模型检索能力的贡献越大。    \n\n那么一个简单有效提升训练样本检索相关度的做法，就是把gold-chunk放到答案中，即paraphrasing the original text。  \n\n一个paraphrasing和其他答案设计方案对比的例子如下  \n\n{% asset_img paraphrasing_example.png paraphrasing例子 %}  \n\n其中高亮部分的token是高检索相关度的token，明显paraphrasing拥有更高的比例。  \n\n论文使用GPT-4来构建包含paraphrasing的问答对，流程实际如下  \n\n{% asset_img paraphrasing_dataset.png 构建数据集 %}  \n\n这种方式收集了一批单文档问答和多文档问答的数据，再加上一些传统文本摘要数据（摘要不好用这种方式构建，因此直接使用）等，构成一个包含10,825条英文数据，8,454条中文数据，长度在8k和32k之间的数据集。数据集详细的领域分布如下所示  \n\n{% asset_img paraphrasing_dataset_dist.png 数据集分布 %}  \n\n论文构建的数据集和Multi-passage-QA-from-NQ的检索相关性指标对比如下  \n\n{% asset_img paraphrasing_quality.png 数据集检索相关性对比 %}  \n\n使用这个数据集微调的模型，和其他模型在LongBench上的效果对比如下  \n\n{% asset_img paraphrasing_perf.png 效果对比 %}  \n\n另外，在这个数据集上微调之后，模型对于lost in the middle的问题也有一定的缓解，如下所示  \n\n{% asset_img paraphrasing_lost.png 缓解lost in the middle %}  \n\n# PoSE  \n\n论文：PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training  \n\n时间：2023年9月  \n\n阶段：微调  \n\n长度：128k\n\n## 背景  \n\n目前大部分流行的大模型使用旋转位置编码RoPE。在短文本上训练的模型，在长输入上效果不好的原因之一，就是长文本有很多模型没有见过没有训练过的位置编码。  \n\n基于位置编码的长上下文优化，比如线性插值、NTK插值和YaRN等，依然需要进行目标长度的训练才能有比价好的效果。而随着目标长度越来越长（8k，32k，128k...），这样的训练成本也越来越高，逐渐变得不容易进行。  \n\n这篇论文提出**Po**sitional **S**kip-wis**E**，PoSE，通过在短的训练窗口模拟长距离的位置编码，提升模型处理长上下文的能力。模型可以在2k的训练窗口进行训练，而在128k的长度进行推理。相比直接训练128k模型效率更高。  \n\n也有一些工作的思路和这篇文章有相近之处，比如RandPos（《Randomized positional encodings boost length generalization of transformers》），但是RandPos主要用于预训练阶段，并且相邻token之间的位置是不连续的，而PoSE主要用于微调阶段，相邻token之间的位置是连续的。  \n\n## 位置模拟  \n\nPoSE提出两个设计原则：  \n- 模拟所用的位置编码index要覆盖目标长度的范围。如果我们想在128k的窗口进行推理，那就要保证训练的时候，模型从1-128k的位置编码都见过。  \n- 为了不损害原模型的能力，位置编码应该尽量保持原来预训练的结构，即尽量连续，和保持顺序关系。  \n\n假设我们的训练窗口长度为 $L_c$，首先我们随机把它切成 $N$ 个chunk， $c_0,c_1,\\ldots,c_{N-1}$，长度分别为 $l_0,l_1,\\ldots,l_{N-1}$。对于chunk $i$，其中token的位置编码下标如下  \n\n$$\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad st_i=\\sum_{j=0}^{i-1}l_j$$  \n\n然后我们给每个chunk，从uniform distribution $\\mathcal{U}(S)$ 中随机采样一个skipping bias $u_i$，把这个bias加到这个对应chunk的token的位置编码下标中，就有  \n\n$$\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}$$\n\n这里要注意，处理后各个chunk的位置编码下标不能有overlap，所以要求 $u_i\\geq u_{i-1}$。  \n\n直观地说，引入skipping bias使模型能接触到更大范围的位置编码。为了全面覆盖目标上下文窗口，我们为每个训练sample单独采样每个chunk的长度和skipping bias。  \n\n此外，位置编码index在每个chunk内的连续性，与原模型预训练期间所采用的结构非常相似。因此，在这些新的index上进行微调，不会损害模型原有的能力。  \n\n现在，位置编码的下标决定好了，我们还需要决定每个chunk的token使用哪些。  \n\ntoken的采样和位置编码下标的采样类似，具体来说，我们采样$v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})$，那么 $c_i$ 的token如下  \n\n$$c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]$$  \n\n论文对一些采样变体，比如 $v_i=u_i$，$v_i=0$ 等进行了探索，发现基本没有什么影响，因此 $v_i$ 保持原来的采样方案即可。  \n\n在实际训练中，$N$ 设置为2，因为如果太大可能对原模型的能力造成损害。而 $u_0$ 和 $v_0$ 设为了0。  \n\nPoSE方案如下图所示  \n\n{% asset_img pose_method.png PoSE %}  \n\n实验上，使用了LLAMA-7B模型，在2k的窗口上进行了1,000步的训练，batch size为64。使用lr=2e-5，warmup step=10。  \n\nPoSE和其他模型在PPL上的对比如下，基本能达到和Full-length训练相近的水平。  \n\n{% asset_img pose_ppl.png PPL %}  \n\n而在passkey retrieval任务上，也有不错的效果，如下图所示  \n\n{% asset_img pose_passkey.png passkey %}  \n\n相比其他方案，PoSE的一个优势是可以在没有任何成本增加的情况下，支持更长的推理长度。比如可以通过简单修改采样策略的参数，PoSE就可以支持到1M，甚至更大的窗口长度，这是其他方法难以做到的。  \n\n# 小结  \n\n1. 有了FlashAttention等方案之后，在128k这个长度，我们也有能力在合理的成本下，进行继续预训练，使用5B左右的token解锁模型的长上下文能力。  \n2. 预训练中，长文本对模型的远距离建模能力很重要，要提高长文本的比例才有更好的效果。此外，领域的分布也是一个需要关注的点。  \n3. 在长窗口的微调上，精心设计输入输出形式能带来一些收益。\n4. 对于更长的窗口，比如M级别这种几乎无法直接训练/微调的长度，PoSE这种模拟的方案能够在不增加成本的情况下，在效果上达到接近直接训练/微调的表现。  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n【1】Data Engineering for Scaling Language Models to 128K Context https://arxiv.org/abs/2402.10171  \n【2】Training With \"Paraphrasing the Original Text\" Improves Long-Context Performance https://arxiv.org/abs/2312.11193  \n【3】PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training https://arxiv.org/abs/2309.10400  \n","slug":"cs/nlp/2024/05/解锁大模型长上下文能力","published":1,"updated":"2024-05-10T06:50:20.731Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9y0018314k0v8mbilw","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>步入2024年Q2，大模型在RAG、文档对话、大模型Agent能力等方向的发展持续升温。在平时的日常生活和工作中，大模型工具提供的文档总结、文本润色、代码生成等能力已经是提高效率的必备帮手，甚至在一些复杂或者不熟悉的场景上，大模型也已经能提供一些比较专业的帮助。</p>\n<p>在这些方向上，大模型(超)长上下文的能力都是基础。无论是使用详细的CoT/ToT，还是通过多篇检索文档提供专业知识，抑或是使用相关样例提升回复质量，都需要模型具备处理很长的输入输出信息的能力。这不仅要求模型在较长的位置编码下依然具有良好的语言建模能力，而且还需要模型能够进行长距离的、细致的阅、准确的阅读和理解。</p>\n<p>本篇将梳理几个通过轻量级训练解锁大模型长上下文能力的工作。</p>\n<h1 id=\"支持128k上下文的数据工程\">支持128k上下文的数据工程</h1>\n<p>论文：Data Engineering for Scaling Language Models to 128K\nContext</p>\n<p>时间：2024年2月</p>\n<p>阶段：预训练</p>\n<p>长度：128k</p>\n<h2 id=\"评测指标\">评测指标</h2>\n<p>模型的长上下文能力不仅体现在文本较长的时候，模型的PPL依然能保持在较低的水平，还体现在对于长上下文输入，模型依然能够进行准确的阅读理解和推理。</p>\n<p>以往一些工作仅使用validation\ndataset上的PPL作为评测指标，并不能很好地表征模型的真实长上下文能力。而目前被广泛使用的\nNeedle in a\nHaystack，或者叫大海捞针任务，是对长上下文能力的一个比较好的评测。这篇论文主要就以大海捞针任务为标准，对不同的模型和方案进行对比。</p>\n<p>两个PPL几乎相同的模型，在大海捞针任务上的差距可以很大，如下图所示，颜色越绿代表正确率越高</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL和大海捞针\">\n<p>目前已有的一些扩展大模型上下文窗口的方法，比如LongLoRA和Mistral所采用的YaRN，虽然理论上来说，能够支持&gt;100k的上下文长度，但是实际上在大海捞针任务的表现却不太好。相关模型在大海捞针任务上的效果对比如下所示，只有GPT-4的效果比较好。</p>\n<img src=\"/cc852861/eng_needle_comp.png\" class title=\"大海捞针任务对比\">\n<h2 id=\"数据分布\">数据分布</h2>\n<p>这篇论文认为，在&lt;=4k窗口长度完成预训练的模型，其实就已经基本具备在128k或者更大的上下文窗口进行推理的能力，只需要进行轻量级的继续预训练（e.g.\n&lt;5B token），就能够解锁这种能力。</p>\n<p>（而一些其他的工作在这方面则有着相反的观点，比如在32k窗口训练了400B\ntoken的《Effective long-context scaling of foundation\nmodels》，以及Xverse）</p>\n<p>要做继续预训练，最重要的一点就是要决定使用什么样的数据。</p>\n<p>这篇论文里的实验是基于LLAMA的，因此使用了和LLAMA预训练数据具有相近领域分布的SlimPajama数据集作为基础。</p>\n<p>对于长上下文的继续预训练数据，需要仔细考虑数据长度和领域分布的影响。通常来说，某些领域天然会有更高比例的长文本数据，比如书籍、论文和github，而一些其他领域的长数据就较少，比如新闻。如果直接从整体数据中挑选长数据而忽略领域分布，就可能造成训练数据在领域分布上的偏移。</p>\n<p>论文使用了几种不同的数据处理策略，用于后面的实验对比：<br>\n- Cut at\n4K：把所有的数据按4k长度进行分块，这样不会影响领域分布。这也是很多4k预训练模型所采样的方案，比如LLAMA。<br>\n- Cut at\n128K：截断长度提升到128k，可以保留长文本内部信息的依赖关系。LongLoRA就是这么做的。<br>\n- Per-source\nUpsampling：在保持各个领域的比例不变的前提下，对长文本进行上采样，提高长文本的比例。这是这篇论文所推荐的方法，实验效果最好。<br>\n- Global Upsampling：不管领域，直接对长文本进行上采样。<br>\n- Upsample Arxiv/ Book/\nGithub：提高特定领域的数据比例，对长文本进行上采样。</p>\n<p>这些策略基本涵盖了大部分长文本相关工作在数据上的处理策略。</p>\n<p>不同数据处理策略下，SlimPajama数据内各领域的分布如下图所示</p>\n<img src=\"/cc852861/eng_data_dist.png\" class title=\"数据分布\">\n<p>Per-source\nUpsampling是效果最好的，也是这篇论文所推荐的数据工程策略。</p>\n<h2 id=\"实验配置\">实验配置</h2>\n<p>实验上，用80k的窗口长度训练LLAMA2-7B模型，用64k的窗口训练LLAMA2-13B模型。</p>\n<p>虽然理论上，计算复杂度度和模型训练窗口长度是平方关系，但是实际实现上，由于有FlashAttention等方案，可以把Attention的计算通过设备间通讯，在多个设备间并行起来。而设备间的通讯（包括GPU和CPU，GPU和GPU之间）成本都是constant或者linear，因此实际上80k窗口的的训练耗时只是4k长度的训练的3倍，而不是理论上的400倍。</p>\n<p>当然，实际所需的计算量并没有减少，但是至少时间成本从平方变成了线性。剩下的，只要堆jia卡qian就可以提速。</p>\n<img src=\"/cc852861/add_money.jpg\" class title=\"加钱就行\">\n<p>Per-source Upsampling和其他工作的数据处理策略的对比如下</p>\n<img src=\"/cc852861/eng_data.png\" class title=\"模型策略\">\n<p>训练的配置和耗时如下所示</p>\n<img src=\"/cc852861/eng_config.png\" class title=\"训练配置\">\n<p>实验的其他配置：<br>\n- lr = 2e-5<br>\n- RoPE base从1,0000改为500,000<br>\n- batch size = 4M token</p>\n<h2 id=\"训练量\">训练量</h2>\n<p>前面提到，论文认为只需要轻量级的继续预训练就可以解锁长上下文能力，那么到底需要训练多少token呢？</p>\n<p>论文分别取了训练了100M、300M、500M、1B、5B、10B\ntoken的中间checkpoint进行PPL和海底捞针任务评测，结果如下</p>\n<img src=\"/cc852861/eng_tokens.png\" class title=\"训练量\">\n<p>结论是，在训练了500M\ntoken的时候，模型基本解锁了长上下文的能力；在训练了5B\ntoken的时候，模型已经收敛，而且继续训练到10B\ntoken也没有进一步收益了。</p>\n<h2 id=\"数据策略对比\">数据策略对比</h2>\n<p>使用前面提到的不同数据策略在LLAMA2-7B模型用5B\ntoken进行训练，并对比效果。</p>\n<p>LLAMA2的预训练长度为4k，因此对比的时候分成了0-4k和4k-128k两段，分别评测模型经过长文本训练后，在短文本上的效果是否有变差，以及在长文本上是否有提升。</p>\n<p>各个数据策略在不同领域的效果变化如下</p>\n<img src=\"/cc852861/eng_sample.png\" class title=\"采样的影响\">\n<p>可以得到几个结论：<br>\n- 在0-4k长度上，除了Per-source\nUpsampling以外，各种数据策略都会对模型效果有损害<br>\n-\n在一些领域上的提升，并不能很好地迁移到其他领域，比如Book和Github之间就有点跷跷板效应，其中一个效果好了，另一个可能就有损失<br>\n- 在4k-128k，Per-source\nUpsampling在各个领域的效果相对较为平衡（绿色的数量最多）</p>\n<p>此外，length upsampling很重要。Per-source\nUpsampling的策略在领域上可以和源数据保持一致，而提升长文本的比例。</p>\n<p>用同样80k的训练窗口在LLAMA2-7B进行实验，一个使用原数据进行拼接，另一个使用Per-source\nUpsampling，结果如下。在PPL基本相同的情况下，Per-source\nUpsampling在大海捞针的效果远超原数据。这说明提高长文本的比例，能极大优化模型远距离建模的能力。</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL和大海捞针\">\n<h2 id=\"结论\">结论</h2>\n<p>通过实验，论文提出提升模型长上下文能力的数据工程实践的几个关键点：<br>\n-\n在长窗口上进行轻量级训练，可以提升模型实际的远距离建模能力，而不仅仅是保持PPL较低<br>\n- 领域之间有竞争关系，最好和原预训练模型所用的分布保持一致<br>\n- 长度上采样对最终效果有很大影响，要提高各领域内长文本的比例</p>\n<h1 id=\"paraphrasing\">Paraphrasing</h1>\n<p>论文：Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance</p>\n<p>时间：2023年12月</p>\n<p>阶段：微调</p>\n<p>长度：在50k长度依然能有较好的效果，如下所示。</p>\n<img src=\"/cc852861/paraphrasing_intro.png\" class title=\"paraphrasing\">\n<h2 id=\"检索能力\">检索能力</h2>\n<p>对于长上下文的任务，有用的信息通常是稀疏的，一般只有少量的句子或者段落包含了可以用于回答问题的有用信息。可以隐式地将这样长上下文的任务拆分成两个子任务，即相关信息的检索，和根据相关信息回答问题两个任务。</p>\n<p>目前一些支持长上下文的方法，比如位置编码相关的线性插值、NTK插值、YaRN等，虽然使得模型在形式上支持了长上下文的任务，但是在任务的准确性上效果却不佳。</p>\n<p>使用这些优化方案的模型依然会遇到lost in the\nmiddle的问题，即模型天然更容易关注到输入文本的开头和结尾部分的信息，而更容易忽略中间部分的信息，注意力迷失在大量无关内容上，而无法集中到少数相关的位置上。而对于长上下文的任务，大量的信息是处于middle的位置的，如果忽略这些信息自然会使得任务效果不好。而效果不好的原因就是模型在长上下文的情况下，retrieval能力偏弱，不能找到有用的信息。</p>\n<h2 id=\"相关工作\">相关工作</h2>\n<p>一些工作直接把模型在长窗口下进行训练，比如：<br>\n-\nTogether的LLaMA-2-7B-32K（https://huggingface.co/datasets/togethercomputer/Long-Data-Collections）；Together开源了Multipassage-QA-from-Natural-Questions和BookSum微调数据集。<br>\n- LongAlpaca（《LongLoRA: Efficient Fine-tuning of Long-Context Large\nLanguage Models》）<br>\n- Ziya-Reader（《Never Lost in the Middle:Improving Large Language\nModels via Attention Strengthening Question Answering》）</p>\n<p>直接在长窗口训练有一定的效果，但是依然有几个问题：<br>\n- 模型推理窗口越来越大，所需的训练数据集长度也要不断更新。<br>\n- 随着长度增大，训练成本变高。<br>\n-\n构建长上下文数据集的成本比价高，高质量的数据并不容易获得。虽然有一些开源的数据集，但是在实际场景上可能还需要做领域适配，分布调整等工作。</p>\n<p>一个更简单一点的方法是优化prompt的设计，比如CoT。</p>\n<p>在长上下文的场景下，可以通过prompt让模型显式地先找到原文的相关信息再进行回答。比如Claude-2.1就通过在prompt增加“Here\nis the most relevant sentence in the\ncontext”让长文本问答的准确率从27%提升到98%（https://www.anthropic.com/news/claude-2-1-prompting）。</p>\n<p>也可以对输入内容进行重新的编排：<br>\n- LongLLMLingua（《LongLLMLingua: Accelerating and Enhancing LLMs in\nLong Context Scenarios via Prompt\nCompression》）对输入文本进行了压缩。<br>\n- Attention Sorting（《Attention Sorting Combats Recency Bias In Long\nContext Language\nModels》）在decode过程中根据各个文档被分配到的注意力值，对文档进行重新排序。</p>\n<h2 id=\"提高检索能力\">提高检索能力</h2>\n<p>这篇论文提出了一个叫检索相关度（retrieval\nrelevance）的指标，一个token（或者n-gram） <span class=\"math inline\">\\(x\\)</span> 的检索相关度 <span class=\"math inline\">\\(R(x)\\)</span> 定义如下。</p>\n<p><span class=\"math display\">\\[R(x)=\\frac{n^\\prime}n\\log\\frac\nN{N^\\prime+1}\\]</span></p>\n<p>这个指标和TF-IDF很像。其中，<span class=\"math inline\">\\(n^\\prime\\)</span> 表示 <span class=\"math inline\">\\(x\\)</span> 在gold-chunk中的频率，而 <span class=\"math inline\">\\(n\\)</span> 是gold-chunk中的总token数；<span class=\"math inline\">\\(N\\)</span> 表示整个上下文中总chunk数，<span class=\"math inline\">\\(N^\\prime\\)</span> 是包含x的chunk的数量。</p>\n<p>基于token <span class=\"math inline\">\\(x\\)</span> 的检索相关度 <span class=\"math inline\">\\(R(x)\\)</span> ，定义训练样本 <span class=\"math inline\">\\(S\\)</span> 的检索相关度如下</p>\n<p><span class=\"math display\">\\[\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(S_a\\)</span> 表示 <span class=\"math inline\">\\(S\\)</span> 的答案部分。</p>\n<p>通过 <span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n这个指标可以反映出一个训练样本对模型提高检索能力的贡献。<span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n越高，这个样本对提高模型检索能力的贡献越大。</p>\n<p>那么一个简单有效提升训练样本检索相关度的做法，就是把gold-chunk放到答案中，即paraphrasing\nthe original text。</p>\n<p>一个paraphrasing和其他答案设计方案对比的例子如下</p>\n<img src=\"/cc852861/paraphrasing_example.png\" class title=\"paraphrasing例子\">\n<p>其中高亮部分的token是高检索相关度的token，明显paraphrasing拥有更高的比例。</p>\n<p>论文使用GPT-4来构建包含paraphrasing的问答对，流程实际如下</p>\n<img src=\"/cc852861/paraphrasing_dataset.png\" class title=\"构建数据集\">\n<p>这种方式收集了一批单文档问答和多文档问答的数据，再加上一些传统文本摘要数据（摘要不好用这种方式构建，因此直接使用）等，构成一个包含10,825条英文数据，8,454条中文数据，长度在8k和32k之间的数据集。数据集详细的领域分布如下所示</p>\n<img src=\"/cc852861/paraphrasing_dataset_dist.png\" class title=\"数据集分布\">\n<p>论文构建的数据集和Multi-passage-QA-from-NQ的检索相关性指标对比如下</p>\n<img src=\"/cc852861/paraphrasing_quality.png\" class title=\"数据集检索相关性对比\">\n<p>使用这个数据集微调的模型，和其他模型在LongBench上的效果对比如下</p>\n<img src=\"/cc852861/paraphrasing_perf.png\" class title=\"效果对比\">\n<p>另外，在这个数据集上微调之后，模型对于lost in the\nmiddle的问题也有一定的缓解，如下所示</p>\n<img src=\"/cc852861/paraphrasing_lost.png\" class title=\"缓解lost in the middle\">\n<h1 id=\"pose\">PoSE</h1>\n<p>论文：PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training</p>\n<p>时间：2023年9月</p>\n<p>阶段：微调</p>\n<p>长度：128k</p>\n<h2 id=\"背景\">背景</h2>\n<p>目前大部分流行的大模型使用旋转位置编码RoPE。在短文本上训练的模型，在长输入上效果不好的原因之一，就是长文本有很多模型没有见过没有训练过的位置编码。</p>\n<p>基于位置编码的长上下文优化，比如线性插值、NTK插值和YaRN等，依然需要进行目标长度的训练才能有比价好的效果。而随着目标长度越来越长（8k，32k，128k...），这样的训练成本也越来越高，逐渐变得不容易进行。</p>\n<p>这篇论文提出<strong>Po</strong>sitional\n<strong>S</strong>kip-wis<strong>E</strong>，PoSE，通过在短的训练窗口模拟长距离的位置编码，提升模型处理长上下文的能力。模型可以在2k的训练窗口进行训练，而在128k的长度进行推理。相比直接训练128k模型效率更高。</p>\n<p>也有一些工作的思路和这篇文章有相近之处，比如RandPos（《Randomized\npositional encodings boost length generalization of\ntransformers》），但是RandPos主要用于预训练阶段，并且相邻token之间的位置是不连续的，而PoSE主要用于微调阶段，相邻token之间的位置是连续的。</p>\n<h2 id=\"位置模拟\">位置模拟</h2>\n<p>PoSE提出两个设计原则：<br>\n-\n模拟所用的位置编码index要覆盖目标长度的范围。如果我们想在128k的窗口进行推理，那就要保证训练的时候，模型从1-128k的位置编码都见过。<br>\n-\n为了不损害原模型的能力，位置编码应该尽量保持原来预训练的结构，即尽量连续，和保持顺序关系。</p>\n<p>假设我们的训练窗口长度为 <span class=\"math inline\">\\(L_c\\)</span>，首先我们随机把它切成 <span class=\"math inline\">\\(N\\)</span> 个chunk， <span class=\"math inline\">\\(c_0,c_1,\\ldots,c_{N-1}\\)</span>，长度分别为 <span class=\"math inline\">\\(l_0,l_1,\\ldots,l_{N-1}\\)</span>。对于chunk <span class=\"math inline\">\\(i\\)</span>，其中token的位置编码下标如下</p>\n<p><span class=\"math display\">\\[\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad\nst_i=\\sum_{j=0}^{i-1}l_j\\]</span></p>\n<p>然后我们给每个chunk，从uniform distribution <span class=\"math inline\">\\(\\mathcal{U}(S)\\)</span> 中随机采样一个skipping\nbias <span class=\"math inline\">\\(u_i\\)</span>，把这个bias加到这个对应chunk的token的位置编码下标中，就有</p>\n<p><span class=\"math display\">\\[\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}\\]</span></p>\n<p>这里要注意，处理后各个chunk的位置编码下标不能有overlap，所以要求\n<span class=\"math inline\">\\(u_i\\geq u_{i-1}\\)</span>。</p>\n<p>直观地说，引入skipping\nbias使模型能接触到更大范围的位置编码。为了全面覆盖目标上下文窗口，我们为每个训练sample单独采样每个chunk的长度和skipping\nbias。</p>\n<p>此外，位置编码index在每个chunk内的连续性，与原模型预训练期间所采用的结构非常相似。因此，在这些新的index上进行微调，不会损害模型原有的能力。</p>\n<p>现在，位置编码的下标决定好了，我们还需要决定每个chunk的token使用哪些。</p>\n<p>token的采样和位置编码下标的采样类似，具体来说，我们采样<span class=\"math inline\">\\(v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})\\)</span>，那么\n<span class=\"math inline\">\\(c_i\\)</span> 的token如下</p>\n<p><span class=\"math display\">\\[c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]\\]</span></p>\n<p>论文对一些采样变体，比如 <span class=\"math inline\">\\(v_i=u_i\\)</span>，<span class=\"math inline\">\\(v_i=0\\)</span>\n等进行了探索，发现基本没有什么影响，因此 <span class=\"math inline\">\\(v_i\\)</span> 保持原来的采样方案即可。</p>\n<p>在实际训练中，<span class=\"math inline\">\\(N\\)</span>\n设置为2，因为如果太大可能对原模型的能力造成损害。而 <span class=\"math inline\">\\(u_0\\)</span> 和 <span class=\"math inline\">\\(v_0\\)</span> 设为了0。</p>\n<p>PoSE方案如下图所示</p>\n<img src=\"/cc852861/pose_method.png\" class title=\"PoSE\">\n<p>实验上，使用了LLAMA-7B模型，在2k的窗口上进行了1,000步的训练，batch\nsize为64。使用lr=2e-5，warmup step=10。</p>\n<p>PoSE和其他模型在PPL上的对比如下，基本能达到和Full-length训练相近的水平。</p>\n<img src=\"/cc852861/pose_ppl.png\" class title=\"PPL\">\n<p>而在passkey retrieval任务上，也有不错的效果，如下图所示</p>\n<img src=\"/cc852861/pose_passkey.png\" class title=\"passkey\">\n<p>相比其他方案，PoSE的一个优势是可以在没有任何成本增加的情况下，支持更长的推理长度。比如可以通过简单修改采样策略的参数，PoSE就可以支持到1M，甚至更大的窗口长度，这是其他方法难以做到的。</p>\n<h1 id=\"小结\">小结</h1>\n<ol type=\"1\">\n<li>有了FlashAttention等方案之后，在128k这个长度，我们也有能力在合理的成本下，进行继续预训练，使用5B左右的token解锁模型的长上下文能力。<br>\n</li>\n<li>预训练中，长文本对模型的远距离建模能力很重要，要提高长文本的比例才有更好的效果。此外，领域的分布也是一个需要关注的点。<br>\n</li>\n<li>在长窗口的微调上，精心设计输入输出形式能带来一些收益。</li>\n<li>对于更长的窗口，比如M级别这种几乎无法直接训练/微调的长度，PoSE这种模拟的方案能够在不增加成本的情况下，在效果上达到接近直接训练/微调的表现。</li>\n</ol>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Data Engineering for Scaling Language Models to 128K Context\nhttps://arxiv.org/abs/2402.10171<br>\n【2】Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance https://arxiv.org/abs/2312.11193<br>\n【3】PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training https://arxiv.org/abs/2309.10400</p>\n","length":8097,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>步入2024年Q2，大模型在RAG、文档对话、大模型Agent能力等方向的发展持续升温。在平时的日常生活和工作中，大模型工具提供的文档总结、文本润色、代码生成等能力已经是提高效率的必备帮手，甚至在一些复杂或者不熟悉的场景上，大模型也已经能提供一些比较专业的帮助。</p>\n<p>在这些方向上，大模型(超)长上下文的能力都是基础。无论是使用详细的CoT/ToT，还是通过多篇检索文档提供专业知识，抑或是使用相关样例提升回复质量，都需要模型具备处理很长的输入输出信息的能力。这不仅要求模型在较长的位置编码下依然具有良好的语言建模能力，而且还需要模型能够进行长距离的、细致的阅、准确的阅读和理解。</p>\n<p>本篇将梳理几个通过轻量级训练解锁大模型长上下文能力的工作。</p>\n<h1 id=\"支持128k上下文的数据工程\">支持128k上下文的数据工程</h1>\n<p>论文：Data Engineering for Scaling Language Models to 128K\nContext</p>\n<p>时间：2024年2月</p>\n<p>阶段：预训练</p>\n<p>长度：128k</p>\n<h2 id=\"评测指标\">评测指标</h2>\n<p>模型的长上下文能力不仅体现在文本较长的时候，模型的PPL依然能保持在较低的水平，还体现在对于长上下文输入，模型依然能够进行准确的阅读理解和推理。</p>\n<p>以往一些工作仅使用validation\ndataset上的PPL作为评测指标，并不能很好地表征模型的真实长上下文能力。而目前被广泛使用的\nNeedle in a\nHaystack，或者叫大海捞针任务，是对长上下文能力的一个比较好的评测。这篇论文主要就以大海捞针任务为标准，对不同的模型和方案进行对比。</p>\n<p>两个PPL几乎相同的模型，在大海捞针任务上的差距可以很大，如下图所示，颜色越绿代表正确率越高</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL和大海捞针\">\n<p>目前已有的一些扩展大模型上下文窗口的方法，比如LongLoRA和Mistral所采用的YaRN，虽然理论上来说，能够支持&gt;100k的上下文长度，但是实际上在大海捞针任务的表现却不太好。相关模型在大海捞针任务上的效果对比如下所示，只有GPT-4的效果比较好。</p>\n<img src=\"/cc852861/eng_needle_comp.png\" class title=\"大海捞针任务对比\">\n<h2 id=\"数据分布\">数据分布</h2>\n<p>这篇论文认为，在&lt;=4k窗口长度完成预训练的模型，其实就已经基本具备在128k或者更大的上下文窗口进行推理的能力，只需要进行轻量级的继续预训练（e.g.\n&lt;5B token），就能够解锁这种能力。</p>\n<p>（而一些其他的工作在这方面则有着相反的观点，比如在32k窗口训练了400B\ntoken的《Effective long-context scaling of foundation\nmodels》，以及Xverse）</p>\n<p>要做继续预训练，最重要的一点就是要决定使用什么样的数据。</p>\n<p>这篇论文里的实验是基于LLAMA的，因此使用了和LLAMA预训练数据具有相近领域分布的SlimPajama数据集作为基础。</p>\n<p>对于长上下文的继续预训练数据，需要仔细考虑数据长度和领域分布的影响。通常来说，某些领域天然会有更高比例的长文本数据，比如书籍、论文和github，而一些其他领域的长数据就较少，比如新闻。如果直接从整体数据中挑选长数据而忽略领域分布，就可能造成训练数据在领域分布上的偏移。</p>\n<p>论文使用了几种不同的数据处理策略，用于后面的实验对比：<br>\n- Cut at\n4K：把所有的数据按4k长度进行分块，这样不会影响领域分布。这也是很多4k预训练模型所采样的方案，比如LLAMA。<br>\n- Cut at\n128K：截断长度提升到128k，可以保留长文本内部信息的依赖关系。LongLoRA就是这么做的。<br>\n- Per-source\nUpsampling：在保持各个领域的比例不变的前提下，对长文本进行上采样，提高长文本的比例。这是这篇论文所推荐的方法，实验效果最好。<br>\n- Global Upsampling：不管领域，直接对长文本进行上采样。<br>\n- Upsample Arxiv/ Book/\nGithub：提高特定领域的数据比例，对长文本进行上采样。</p>\n<p>这些策略基本涵盖了大部分长文本相关工作在数据上的处理策略。</p>\n<p>不同数据处理策略下，SlimPajama数据内各领域的分布如下图所示</p>\n<img src=\"/cc852861/eng_data_dist.png\" class title=\"数据分布\">\n<p>Per-source\nUpsampling是效果最好的，也是这篇论文所推荐的数据工程策略。</p>\n<h2 id=\"实验配置\">实验配置</h2>\n<p>实验上，用80k的窗口长度训练LLAMA2-7B模型，用64k的窗口训练LLAMA2-13B模型。</p>\n<p>虽然理论上，计算复杂度度和模型训练窗口长度是平方关系，但是实际实现上，由于有FlashAttention等方案，可以把Attention的计算通过设备间通讯，在多个设备间并行起来。而设备间的通讯（包括GPU和CPU，GPU和GPU之间）成本都是constant或者linear，因此实际上80k窗口的的训练耗时只是4k长度的训练的3倍，而不是理论上的400倍。</p>\n<p>当然，实际所需的计算量并没有减少，但是至少时间成本从平方变成了线性。剩下的，只要堆jia卡qian就可以提速。</p>\n<img src=\"/cc852861/add_money.jpg\" class title=\"加钱就行\">\n<p>Per-source Upsampling和其他工作的数据处理策略的对比如下</p>\n<img src=\"/cc852861/eng_data.png\" class title=\"模型策略\">\n<p>训练的配置和耗时如下所示</p>\n<img src=\"/cc852861/eng_config.png\" class title=\"训练配置\">\n<p>实验的其他配置：<br>\n- lr = 2e-5<br>\n- RoPE base从1,0000改为500,000<br>\n- batch size = 4M token</p>\n<h2 id=\"训练量\">训练量</h2>\n<p>前面提到，论文认为只需要轻量级的继续预训练就可以解锁长上下文能力，那么到底需要训练多少token呢？</p>\n<p>论文分别取了训练了100M、300M、500M、1B、5B、10B\ntoken的中间checkpoint进行PPL和海底捞针任务评测，结果如下</p>\n<img src=\"/cc852861/eng_tokens.png\" class title=\"训练量\">\n<p>结论是，在训练了500M\ntoken的时候，模型基本解锁了长上下文的能力；在训练了5B\ntoken的时候，模型已经收敛，而且继续训练到10B\ntoken也没有进一步收益了。</p>\n<h2 id=\"数据策略对比\">数据策略对比</h2>\n<p>使用前面提到的不同数据策略在LLAMA2-7B模型用5B\ntoken进行训练，并对比效果。</p>\n<p>LLAMA2的预训练长度为4k，因此对比的时候分成了0-4k和4k-128k两段，分别评测模型经过长文本训练后，在短文本上的效果是否有变差，以及在长文本上是否有提升。</p>\n<p>各个数据策略在不同领域的效果变化如下</p>\n<img src=\"/cc852861/eng_sample.png\" class title=\"采样的影响\">\n<p>可以得到几个结论：<br>\n- 在0-4k长度上，除了Per-source\nUpsampling以外，各种数据策略都会对模型效果有损害<br>\n-\n在一些领域上的提升，并不能很好地迁移到其他领域，比如Book和Github之间就有点跷跷板效应，其中一个效果好了，另一个可能就有损失<br>\n- 在4k-128k，Per-source\nUpsampling在各个领域的效果相对较为平衡（绿色的数量最多）</p>\n<p>此外，length upsampling很重要。Per-source\nUpsampling的策略在领域上可以和源数据保持一致，而提升长文本的比例。</p>\n<p>用同样80k的训练窗口在LLAMA2-7B进行实验，一个使用原数据进行拼接，另一个使用Per-source\nUpsampling，结果如下。在PPL基本相同的情况下，Per-source\nUpsampling在大海捞针的效果远超原数据。这说明提高长文本的比例，能极大优化模型远距离建模的能力。</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL和大海捞针\">\n<h2 id=\"结论\">结论</h2>\n<p>通过实验，论文提出提升模型长上下文能力的数据工程实践的几个关键点：<br>\n-\n在长窗口上进行轻量级训练，可以提升模型实际的远距离建模能力，而不仅仅是保持PPL较低<br>\n- 领域之间有竞争关系，最好和原预训练模型所用的分布保持一致<br>\n- 长度上采样对最终效果有很大影响，要提高各领域内长文本的比例</p>\n<h1 id=\"paraphrasing\">Paraphrasing</h1>\n<p>论文：Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance</p>\n<p>时间：2023年12月</p>\n<p>阶段：微调</p>\n<p>长度：在50k长度依然能有较好的效果，如下所示。</p>\n<img src=\"/cc852861/paraphrasing_intro.png\" class title=\"paraphrasing\">\n<h2 id=\"检索能力\">检索能力</h2>\n<p>对于长上下文的任务，有用的信息通常是稀疏的，一般只有少量的句子或者段落包含了可以用于回答问题的有用信息。可以隐式地将这样长上下文的任务拆分成两个子任务，即相关信息的检索，和根据相关信息回答问题两个任务。</p>\n<p>目前一些支持长上下文的方法，比如位置编码相关的线性插值、NTK插值、YaRN等，虽然使得模型在形式上支持了长上下文的任务，但是在任务的准确性上效果却不佳。</p>\n<p>使用这些优化方案的模型依然会遇到lost in the\nmiddle的问题，即模型天然更容易关注到输入文本的开头和结尾部分的信息，而更容易忽略中间部分的信息，注意力迷失在大量无关内容上，而无法集中到少数相关的位置上。而对于长上下文的任务，大量的信息是处于middle的位置的，如果忽略这些信息自然会使得任务效果不好。而效果不好的原因就是模型在长上下文的情况下，retrieval能力偏弱，不能找到有用的信息。</p>\n<h2 id=\"相关工作\">相关工作</h2>\n<p>一些工作直接把模型在长窗口下进行训练，比如：<br>\n-\nTogether的LLaMA-2-7B-32K（https://huggingface.co/datasets/togethercomputer/Long-Data-Collections）；Together开源了Multipassage-QA-from-Natural-Questions和BookSum微调数据集。<br>\n- LongAlpaca（《LongLoRA: Efficient Fine-tuning of Long-Context Large\nLanguage Models》）<br>\n- Ziya-Reader（《Never Lost in the Middle:Improving Large Language\nModels via Attention Strengthening Question Answering》）</p>\n<p>直接在长窗口训练有一定的效果，但是依然有几个问题：<br>\n- 模型推理窗口越来越大，所需的训练数据集长度也要不断更新。<br>\n- 随着长度增大，训练成本变高。<br>\n-\n构建长上下文数据集的成本比价高，高质量的数据并不容易获得。虽然有一些开源的数据集，但是在实际场景上可能还需要做领域适配，分布调整等工作。</p>\n<p>一个更简单一点的方法是优化prompt的设计，比如CoT。</p>\n<p>在长上下文的场景下，可以通过prompt让模型显式地先找到原文的相关信息再进行回答。比如Claude-2.1就通过在prompt增加“Here\nis the most relevant sentence in the\ncontext”让长文本问答的准确率从27%提升到98%（https://www.anthropic.com/news/claude-2-1-prompting）。</p>\n<p>也可以对输入内容进行重新的编排：<br>\n- LongLLMLingua（《LongLLMLingua: Accelerating and Enhancing LLMs in\nLong Context Scenarios via Prompt\nCompression》）对输入文本进行了压缩。<br>\n- Attention Sorting（《Attention Sorting Combats Recency Bias In Long\nContext Language\nModels》）在decode过程中根据各个文档被分配到的注意力值，对文档进行重新排序。</p>\n<h2 id=\"提高检索能力\">提高检索能力</h2>\n<p>这篇论文提出了一个叫检索相关度（retrieval\nrelevance）的指标，一个token（或者n-gram） <span class=\"math inline\">\\(x\\)</span> 的检索相关度 <span class=\"math inline\">\\(R(x)\\)</span> 定义如下。</p>\n<p><span class=\"math display\">\\[R(x)=\\frac{n^\\prime}n\\log\\frac\nN{N^\\prime+1}\\]</span></p>\n<p>这个指标和TF-IDF很像。其中，<span class=\"math inline\">\\(n^\\prime\\)</span> 表示 <span class=\"math inline\">\\(x\\)</span> 在gold-chunk中的频率，而 <span class=\"math inline\">\\(n\\)</span> 是gold-chunk中的总token数；<span class=\"math inline\">\\(N\\)</span> 表示整个上下文中总chunk数，<span class=\"math inline\">\\(N^\\prime\\)</span> 是包含x的chunk的数量。</p>\n<p>基于token <span class=\"math inline\">\\(x\\)</span> 的检索相关度 <span class=\"math inline\">\\(R(x)\\)</span> ，定义训练样本 <span class=\"math inline\">\\(S\\)</span> 的检索相关度如下</p>\n<p><span class=\"math display\">\\[\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(S_a\\)</span> 表示 <span class=\"math inline\">\\(S\\)</span> 的答案部分。</p>\n<p>通过 <span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n这个指标可以反映出一个训练样本对模型提高检索能力的贡献。<span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n越高，这个样本对提高模型检索能力的贡献越大。</p>\n<p>那么一个简单有效提升训练样本检索相关度的做法，就是把gold-chunk放到答案中，即paraphrasing\nthe original text。</p>\n<p>一个paraphrasing和其他答案设计方案对比的例子如下</p>\n<img src=\"/cc852861/paraphrasing_example.png\" class title=\"paraphrasing例子\">\n<p>其中高亮部分的token是高检索相关度的token，明显paraphrasing拥有更高的比例。</p>\n<p>论文使用GPT-4来构建包含paraphrasing的问答对，流程实际如下</p>\n<img src=\"/cc852861/paraphrasing_dataset.png\" class title=\"构建数据集\">\n<p>这种方式收集了一批单文档问答和多文档问答的数据，再加上一些传统文本摘要数据（摘要不好用这种方式构建，因此直接使用）等，构成一个包含10,825条英文数据，8,454条中文数据，长度在8k和32k之间的数据集。数据集详细的领域分布如下所示</p>\n<img src=\"/cc852861/paraphrasing_dataset_dist.png\" class title=\"数据集分布\">\n<p>论文构建的数据集和Multi-passage-QA-from-NQ的检索相关性指标对比如下</p>\n<img src=\"/cc852861/paraphrasing_quality.png\" class title=\"数据集检索相关性对比\">\n<p>使用这个数据集微调的模型，和其他模型在LongBench上的效果对比如下</p>\n<img src=\"/cc852861/paraphrasing_perf.png\" class title=\"效果对比\">\n<p>另外，在这个数据集上微调之后，模型对于lost in the\nmiddle的问题也有一定的缓解，如下所示</p>\n<img src=\"/cc852861/paraphrasing_lost.png\" class title=\"缓解lost in the middle\">\n<h1 id=\"pose\">PoSE</h1>\n<p>论文：PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training</p>\n<p>时间：2023年9月</p>\n<p>阶段：微调</p>\n<p>长度：128k</p>\n<h2 id=\"背景\">背景</h2>\n<p>目前大部分流行的大模型使用旋转位置编码RoPE。在短文本上训练的模型，在长输入上效果不好的原因之一，就是长文本有很多模型没有见过没有训练过的位置编码。</p>\n<p>基于位置编码的长上下文优化，比如线性插值、NTK插值和YaRN等，依然需要进行目标长度的训练才能有比价好的效果。而随着目标长度越来越长（8k，32k，128k...），这样的训练成本也越来越高，逐渐变得不容易进行。</p>\n<p>这篇论文提出<strong>Po</strong>sitional\n<strong>S</strong>kip-wis<strong>E</strong>，PoSE，通过在短的训练窗口模拟长距离的位置编码，提升模型处理长上下文的能力。模型可以在2k的训练窗口进行训练，而在128k的长度进行推理。相比直接训练128k模型效率更高。</p>\n<p>也有一些工作的思路和这篇文章有相近之处，比如RandPos（《Randomized\npositional encodings boost length generalization of\ntransformers》），但是RandPos主要用于预训练阶段，并且相邻token之间的位置是不连续的，而PoSE主要用于微调阶段，相邻token之间的位置是连续的。</p>\n<h2 id=\"位置模拟\">位置模拟</h2>\n<p>PoSE提出两个设计原则：<br>\n-\n模拟所用的位置编码index要覆盖目标长度的范围。如果我们想在128k的窗口进行推理，那就要保证训练的时候，模型从1-128k的位置编码都见过。<br>\n-\n为了不损害原模型的能力，位置编码应该尽量保持原来预训练的结构，即尽量连续，和保持顺序关系。</p>\n<p>假设我们的训练窗口长度为 <span class=\"math inline\">\\(L_c\\)</span>，首先我们随机把它切成 <span class=\"math inline\">\\(N\\)</span> 个chunk， <span class=\"math inline\">\\(c_0,c_1,\\ldots,c_{N-1}\\)</span>，长度分别为 <span class=\"math inline\">\\(l_0,l_1,\\ldots,l_{N-1}\\)</span>。对于chunk <span class=\"math inline\">\\(i\\)</span>，其中token的位置编码下标如下</p>\n<p><span class=\"math display\">\\[\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad\nst_i=\\sum_{j=0}^{i-1}l_j\\]</span></p>\n<p>然后我们给每个chunk，从uniform distribution <span class=\"math inline\">\\(\\mathcal{U}(S)\\)</span> 中随机采样一个skipping\nbias <span class=\"math inline\">\\(u_i\\)</span>，把这个bias加到这个对应chunk的token的位置编码下标中，就有</p>\n<p><span class=\"math display\">\\[\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}\\]</span></p>\n<p>这里要注意，处理后各个chunk的位置编码下标不能有overlap，所以要求\n<span class=\"math inline\">\\(u_i\\geq u_{i-1}\\)</span>。</p>\n<p>直观地说，引入skipping\nbias使模型能接触到更大范围的位置编码。为了全面覆盖目标上下文窗口，我们为每个训练sample单独采样每个chunk的长度和skipping\nbias。</p>\n<p>此外，位置编码index在每个chunk内的连续性，与原模型预训练期间所采用的结构非常相似。因此，在这些新的index上进行微调，不会损害模型原有的能力。</p>\n<p>现在，位置编码的下标决定好了，我们还需要决定每个chunk的token使用哪些。</p>\n<p>token的采样和位置编码下标的采样类似，具体来说，我们采样<span class=\"math inline\">\\(v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})\\)</span>，那么\n<span class=\"math inline\">\\(c_i\\)</span> 的token如下</p>\n<p><span class=\"math display\">\\[c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]\\]</span></p>\n<p>论文对一些采样变体，比如 <span class=\"math inline\">\\(v_i=u_i\\)</span>，<span class=\"math inline\">\\(v_i=0\\)</span>\n等进行了探索，发现基本没有什么影响，因此 <span class=\"math inline\">\\(v_i\\)</span> 保持原来的采样方案即可。</p>\n<p>在实际训练中，<span class=\"math inline\">\\(N\\)</span>\n设置为2，因为如果太大可能对原模型的能力造成损害。而 <span class=\"math inline\">\\(u_0\\)</span> 和 <span class=\"math inline\">\\(v_0\\)</span> 设为了0。</p>\n<p>PoSE方案如下图所示</p>\n<img src=\"/cc852861/pose_method.png\" class title=\"PoSE\">\n<p>实验上，使用了LLAMA-7B模型，在2k的窗口上进行了1,000步的训练，batch\nsize为64。使用lr=2e-5，warmup step=10。</p>\n<p>PoSE和其他模型在PPL上的对比如下，基本能达到和Full-length训练相近的水平。</p>\n<img src=\"/cc852861/pose_ppl.png\" class title=\"PPL\">\n<p>而在passkey retrieval任务上，也有不错的效果，如下图所示</p>\n<img src=\"/cc852861/pose_passkey.png\" class title=\"passkey\">\n<p>相比其他方案，PoSE的一个优势是可以在没有任何成本增加的情况下，支持更长的推理长度。比如可以通过简单修改采样策略的参数，PoSE就可以支持到1M，甚至更大的窗口长度，这是其他方法难以做到的。</p>\n<h1 id=\"小结\">小结</h1>\n<ol type=\"1\">\n<li>有了FlashAttention等方案之后，在128k这个长度，我们也有能力在合理的成本下，进行继续预训练，使用5B左右的token解锁模型的长上下文能力。<br>\n</li>\n<li>预训练中，长文本对模型的远距离建模能力很重要，要提高长文本的比例才有更好的效果。此外，领域的分布也是一个需要关注的点。<br>\n</li>\n<li>在长窗口的微调上，精心设计输入输出形式能带来一些收益。</li>\n<li>对于更长的窗口，比如M级别这种几乎无法直接训练/微调的长度，PoSE这种模拟的方案能够在不增加成本的情况下，在效果上达到接近直接训练/微调的表现。</li>\n</ol>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Data Engineering for Scaling Language Models to 128K Context\nhttps://arxiv.org/abs/2402.10171<br>\n【2】Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance https://arxiv.org/abs/2312.11193<br>\n【3】PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training https://arxiv.org/abs/2309.10400</p>\n"},{"title":"大模型算法题(6)","abbrlink":"7c04944d","date":"2024-05-14T11:21:09.000Z","_content":"\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~  \n\n如有错漏，欢迎指正~\n\n***  \n\n# 1.Xavier初始化思路是什么，是怎么做的？  \n\n2010年的《Understanding the difficulty of training deep feedforward neural networks》提出Xavier初始化，目的是为了保持模型中每一层的输出方差大致相等，这样做的原因主要有：  \n\n（1）避免梯度消失或梯度爆炸：在深度神经网络中，如果层与层之间的输出方差相差很大，就会有很大的值出现，那么在反向传播过程中，梯度可能会变得非常小（梯度消失）或者非常大（梯度爆炸）导致梯度更新缓慢，或者训练过程不稳定。  \n\n（2）保持信号的传播：通过保持每一层的输出方差大致相等，可以确保网络中的信号在前向传播和反向传播时不会因为方差的变化而减弱或增强，从而有助于网络更好地学习和传递信息。  \n\n（3）提高训练的稳定性和效率：当每一层的输出方差保持一致时，训练过程会更加稳定，因为权重更新的magnitude更加可控。这样可以提高训练的效率。  \n\n（4）避免过拟合或欠拟合：适当的方差控制有助于网络在训练过程中保持适当的泛化能力。过大的方差可能导致过拟合，而过小的方差可能导致欠拟合。  \n\nXavier初始化方法通过考虑前一层的节点数（fan-in）和后一层的节点数（fan-out）来设置初始权重的分布范围。有三种方案：  \n\n（1）只考虑输入，设置参数的方差为1/fan-in  \n\n（2）只考虑输出，设置参数的方差为1/fan-out  \n\n（3）同时考虑输入和输出，设置参数的方差为2/（fan-in + fan-out）。实际使用中1/fan-in效果就较好。  \n\n# 2.RLHF中，Reward模型和Critic模型的作用分别是什么？  \n\nCritic模型主要负责评估当前策略下的行为，并预测未来的回报。在PPO中，Critic模型的输出用于计算优势函数（advantage function），这是一个衡量实际回报与预期回报之间差异的指标。优势函数是策略梯度方法中的关键组成部分，它帮助actor模型了解哪些行为比预期要好，哪些行为比预期要差。因此，Critic模型的评分直接影响actor模型的优化过程，指导其调整行为以提高整体性能。  \n\nReward模型是基于人类反馈训练的，用于评估和打分模型生成的文本或行为的质量。在RLHF中，Reward模型的评分通常被用作奖励信号，直接反馈给Actor模型。这些奖励信号反映了人类对生成内容的偏好和评价标准，Actor模型会根据这些信号调整其行为，以生成更符合人类期望的输出。\nRLHF中，Actor模型的优化目标是最大化期望回报。这个期望回报可以由Critic模型的预测和Reward模型的评分共同决定。具体来说，Actor模型的优化目标（objective）通常包括（1）策略损失：这部分损失来自于策略模型尝试最大化奖励信号（由Reward模型提供）和/或优势函数（由Critic模型提供）。（2）价值损失：在Actor-Critic架构中，Actor模型也负责优化Critic模型，那么价值损失会尝试最小化Critic模型预测的价值与实际回报之间的差异。（3）其他正则化项：可能还包括一些正则化项。  \n\n因此，Critic模型的评分用于计算优势函数，而Reward模型的评分直接作为奖励信号。这两个模型共同作用于Actor模型，帮助其学习如何生成更符合人类偏好和期望的行为。  \n\n# 3.Kaiming初始化是怎么做的？  \n\nKaiming初始化在Xavier初始化的基础上做了调整。Xavier初始化的推导是基于线性函数的，但实际上今天所有模型都会大量使用非线性函数，比如ReLU，这样就导致了Xavier初始化的失效。\nKaiming初始化针对使用ReLU的模型提出。因为ReLU会抛弃掉小于0的值，对于一个均值为0的输入数据，这就相当于砍掉了一半的值，这样输出的方差和均值就变了。因此把Xavier初始化中使用的方差sqrt(1/N)改为sqrt(2/N)，这样数据不会因为ReLU而变得越来越小。  \n\n# 4.使用Bert中的[CLS]作为输出进行分类和相似度计算，可能会有什么问题？  \n\nself-attention中，每个token都天然倾向于关注自己和附近的token，而对更远的内容分配较少的注意力。在Bert中，[CLS]一般是第一个token，放在最前面。这就导致[CLS]更容易关注到靠近它的输入，也就是文本的靠前部分。如果文本较长，且关键内容出现在靠后的位置，就有可能出现[CLS]中关注不够的情况，而导致效果下降。可以通过强制注意力分配，或者使用所有位置的输出而不单是[CLS]token的方法来缓解这个问题。\n\n\n# 5.pytorch中的register_buffer是什么，有什么用？  \n\nregister_buffer是nn.Module类中的一个方法，它用于注册一个不需要梯度的缓冲区。一般会把模型中需要持续使用或者跟踪，但是不需要通过梯度来更新的参数使用register_buffer注册。使用register_buffer注册的参数可以和其他训练的参数一样，保存在state_dict中，比如旋转位置编码的旋转矩阵，batchnorm中的全局均值和方差，一般都会用register_buffer注册。如果不使用register_buffer注册，普通的常量或者参数保存的时候不会被state_dict跟踪到。\n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  ","source":"_posts/cs/nlp/2024/05/大模型算法题-6.md","raw":"---\ntitle: 大模型算法题(6)\ntags:\n  - NLP\n  - LLM\n  - 算法题\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 7c04944d\ndate: 2024-05-14 19:21:09\n---\n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~  \n\n如有错漏，欢迎指正~\n\n***  \n\n# 1.Xavier初始化思路是什么，是怎么做的？  \n\n2010年的《Understanding the difficulty of training deep feedforward neural networks》提出Xavier初始化，目的是为了保持模型中每一层的输出方差大致相等，这样做的原因主要有：  \n\n（1）避免梯度消失或梯度爆炸：在深度神经网络中，如果层与层之间的输出方差相差很大，就会有很大的值出现，那么在反向传播过程中，梯度可能会变得非常小（梯度消失）或者非常大（梯度爆炸）导致梯度更新缓慢，或者训练过程不稳定。  \n\n（2）保持信号的传播：通过保持每一层的输出方差大致相等，可以确保网络中的信号在前向传播和反向传播时不会因为方差的变化而减弱或增强，从而有助于网络更好地学习和传递信息。  \n\n（3）提高训练的稳定性和效率：当每一层的输出方差保持一致时，训练过程会更加稳定，因为权重更新的magnitude更加可控。这样可以提高训练的效率。  \n\n（4）避免过拟合或欠拟合：适当的方差控制有助于网络在训练过程中保持适当的泛化能力。过大的方差可能导致过拟合，而过小的方差可能导致欠拟合。  \n\nXavier初始化方法通过考虑前一层的节点数（fan-in）和后一层的节点数（fan-out）来设置初始权重的分布范围。有三种方案：  \n\n（1）只考虑输入，设置参数的方差为1/fan-in  \n\n（2）只考虑输出，设置参数的方差为1/fan-out  \n\n（3）同时考虑输入和输出，设置参数的方差为2/（fan-in + fan-out）。实际使用中1/fan-in效果就较好。  \n\n# 2.RLHF中，Reward模型和Critic模型的作用分别是什么？  \n\nCritic模型主要负责评估当前策略下的行为，并预测未来的回报。在PPO中，Critic模型的输出用于计算优势函数（advantage function），这是一个衡量实际回报与预期回报之间差异的指标。优势函数是策略梯度方法中的关键组成部分，它帮助actor模型了解哪些行为比预期要好，哪些行为比预期要差。因此，Critic模型的评分直接影响actor模型的优化过程，指导其调整行为以提高整体性能。  \n\nReward模型是基于人类反馈训练的，用于评估和打分模型生成的文本或行为的质量。在RLHF中，Reward模型的评分通常被用作奖励信号，直接反馈给Actor模型。这些奖励信号反映了人类对生成内容的偏好和评价标准，Actor模型会根据这些信号调整其行为，以生成更符合人类期望的输出。\nRLHF中，Actor模型的优化目标是最大化期望回报。这个期望回报可以由Critic模型的预测和Reward模型的评分共同决定。具体来说，Actor模型的优化目标（objective）通常包括（1）策略损失：这部分损失来自于策略模型尝试最大化奖励信号（由Reward模型提供）和/或优势函数（由Critic模型提供）。（2）价值损失：在Actor-Critic架构中，Actor模型也负责优化Critic模型，那么价值损失会尝试最小化Critic模型预测的价值与实际回报之间的差异。（3）其他正则化项：可能还包括一些正则化项。  \n\n因此，Critic模型的评分用于计算优势函数，而Reward模型的评分直接作为奖励信号。这两个模型共同作用于Actor模型，帮助其学习如何生成更符合人类偏好和期望的行为。  \n\n# 3.Kaiming初始化是怎么做的？  \n\nKaiming初始化在Xavier初始化的基础上做了调整。Xavier初始化的推导是基于线性函数的，但实际上今天所有模型都会大量使用非线性函数，比如ReLU，这样就导致了Xavier初始化的失效。\nKaiming初始化针对使用ReLU的模型提出。因为ReLU会抛弃掉小于0的值，对于一个均值为0的输入数据，这就相当于砍掉了一半的值，这样输出的方差和均值就变了。因此把Xavier初始化中使用的方差sqrt(1/N)改为sqrt(2/N)，这样数据不会因为ReLU而变得越来越小。  \n\n# 4.使用Bert中的[CLS]作为输出进行分类和相似度计算，可能会有什么问题？  \n\nself-attention中，每个token都天然倾向于关注自己和附近的token，而对更远的内容分配较少的注意力。在Bert中，[CLS]一般是第一个token，放在最前面。这就导致[CLS]更容易关注到靠近它的输入，也就是文本的靠前部分。如果文本较长，且关键内容出现在靠后的位置，就有可能出现[CLS]中关注不够的情况，而导致效果下降。可以通过强制注意力分配，或者使用所有位置的输出而不单是[CLS]token的方法来缓解这个问题。\n\n\n# 5.pytorch中的register_buffer是什么，有什么用？  \n\nregister_buffer是nn.Module类中的一个方法，它用于注册一个不需要梯度的缓冲区。一般会把模型中需要持续使用或者跟踪，但是不需要通过梯度来更新的参数使用register_buffer注册。使用register_buffer注册的参数可以和其他训练的参数一样，保存在state_dict中，比如旋转位置编码的旋转矩阵，batchnorm中的全局均值和方差，一般都会用register_buffer注册。如果不使用register_buffer注册，普通的常量或者参数保存的时候不会被state_dict跟踪到。\n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  ","slug":"cs/nlp/2024/05/大模型算法题-6","published":1,"updated":"2024-05-14T11:26:49.973Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9z001a314k66ligkpn","content":"<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~</p>\n<p>如有错漏，欢迎指正~</p>\n<hr>\n<h1 id=\"xavier初始化思路是什么是怎么做的\">1.Xavier初始化思路是什么，是怎么做的？</h1>\n<p>2010年的《Understanding the difficulty of training deep feedforward\nneural\nnetworks》提出Xavier初始化，目的是为了保持模型中每一层的输出方差大致相等，这样做的原因主要有：</p>\n<p>（1）避免梯度消失或梯度爆炸：在深度神经网络中，如果层与层之间的输出方差相差很大，就会有很大的值出现，那么在反向传播过程中，梯度可能会变得非常小（梯度消失）或者非常大（梯度爆炸）导致梯度更新缓慢，或者训练过程不稳定。</p>\n<p>（2）保持信号的传播：通过保持每一层的输出方差大致相等，可以确保网络中的信号在前向传播和反向传播时不会因为方差的变化而减弱或增强，从而有助于网络更好地学习和传递信息。</p>\n<p>（3）提高训练的稳定性和效率：当每一层的输出方差保持一致时，训练过程会更加稳定，因为权重更新的magnitude更加可控。这样可以提高训练的效率。</p>\n<p>（4）避免过拟合或欠拟合：适当的方差控制有助于网络在训练过程中保持适当的泛化能力。过大的方差可能导致过拟合，而过小的方差可能导致欠拟合。</p>\n<p>Xavier初始化方法通过考虑前一层的节点数（fan-in）和后一层的节点数（fan-out）来设置初始权重的分布范围。有三种方案：</p>\n<p>（1）只考虑输入，设置参数的方差为1/fan-in</p>\n<p>（2）只考虑输出，设置参数的方差为1/fan-out</p>\n<p>（3）同时考虑输入和输出，设置参数的方差为2/（fan-in +\nfan-out）。实际使用中1/fan-in效果就较好。</p>\n<h1 id=\"rlhf中reward模型和critic模型的作用分别是什么\">2.RLHF中，Reward模型和Critic模型的作用分别是什么？</h1>\n<p>Critic模型主要负责评估当前策略下的行为，并预测未来的回报。在PPO中，Critic模型的输出用于计算优势函数（advantage\nfunction），这是一个衡量实际回报与预期回报之间差异的指标。优势函数是策略梯度方法中的关键组成部分，它帮助actor模型了解哪些行为比预期要好，哪些行为比预期要差。因此，Critic模型的评分直接影响actor模型的优化过程，指导其调整行为以提高整体性能。</p>\n<p>Reward模型是基于人类反馈训练的，用于评估和打分模型生成的文本或行为的质量。在RLHF中，Reward模型的评分通常被用作奖励信号，直接反馈给Actor模型。这些奖励信号反映了人类对生成内容的偏好和评价标准，Actor模型会根据这些信号调整其行为，以生成更符合人类期望的输出。\nRLHF中，Actor模型的优化目标是最大化期望回报。这个期望回报可以由Critic模型的预测和Reward模型的评分共同决定。具体来说，Actor模型的优化目标（objective）通常包括（1）策略损失：这部分损失来自于策略模型尝试最大化奖励信号（由Reward模型提供）和/或优势函数（由Critic模型提供）。（2）价值损失：在Actor-Critic架构中，Actor模型也负责优化Critic模型，那么价值损失会尝试最小化Critic模型预测的价值与实际回报之间的差异。（3）其他正则化项：可能还包括一些正则化项。</p>\n<p>因此，Critic模型的评分用于计算优势函数，而Reward模型的评分直接作为奖励信号。这两个模型共同作用于Actor模型，帮助其学习如何生成更符合人类偏好和期望的行为。</p>\n<h1 id=\"kaiming初始化是怎么做的\">3.Kaiming初始化是怎么做的？</h1>\n<p>Kaiming初始化在Xavier初始化的基础上做了调整。Xavier初始化的推导是基于线性函数的，但实际上今天所有模型都会大量使用非线性函数，比如ReLU，这样就导致了Xavier初始化的失效。\nKaiming初始化针对使用ReLU的模型提出。因为ReLU会抛弃掉小于0的值，对于一个均值为0的输入数据，这就相当于砍掉了一半的值，这样输出的方差和均值就变了。因此把Xavier初始化中使用的方差sqrt(1/N)改为sqrt(2/N)，这样数据不会因为ReLU而变得越来越小。</p>\n<h1 id=\"使用bert中的cls作为输出进行分类和相似度计算可能会有什么问题\">4.使用Bert中的[CLS]作为输出进行分类和相似度计算，可能会有什么问题？</h1>\n<p>self-attention中，每个token都天然倾向于关注自己和附近的token，而对更远的内容分配较少的注意力。在Bert中，[CLS]一般是第一个token，放在最前面。这就导致[CLS]更容易关注到靠近它的输入，也就是文本的靠前部分。如果文本较长，且关键内容出现在靠后的位置，就有可能出现[CLS]中关注不够的情况，而导致效果下降。可以通过强制注意力分配，或者使用所有位置的输出而不单是[CLS]token的方法来缓解这个问题。</p>\n<h1 id=\"pytorch中的register_buffer是什么有什么用\">5.pytorch中的register_buffer是什么，有什么用？</h1>\n<p>register_buffer是nn.Module类中的一个方法，它用于注册一个不需要梯度的缓冲区。一般会把模型中需要持续使用或者跟踪，但是不需要通过梯度来更新的参数使用register_buffer注册。使用register_buffer注册的参数可以和其他训练的参数一样，保存在state_dict中，比如旋转位置编码的旋转矩阵，batchnorm中的全局均值和方差，一般都会用register_buffer注册。如果不使用register_buffer注册，普通的常量或者参数保存的时候不会被state_dict跟踪到。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n","length":2799,"excerpt":"","more":"<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~</p>\n<p>如有错漏，欢迎指正~</p>\n<hr>\n<h1 id=\"xavier初始化思路是什么是怎么做的\">1.Xavier初始化思路是什么，是怎么做的？</h1>\n<p>2010年的《Understanding the difficulty of training deep feedforward\nneural\nnetworks》提出Xavier初始化，目的是为了保持模型中每一层的输出方差大致相等，这样做的原因主要有：</p>\n<p>（1）避免梯度消失或梯度爆炸：在深度神经网络中，如果层与层之间的输出方差相差很大，就会有很大的值出现，那么在反向传播过程中，梯度可能会变得非常小（梯度消失）或者非常大（梯度爆炸）导致梯度更新缓慢，或者训练过程不稳定。</p>\n<p>（2）保持信号的传播：通过保持每一层的输出方差大致相等，可以确保网络中的信号在前向传播和反向传播时不会因为方差的变化而减弱或增强，从而有助于网络更好地学习和传递信息。</p>\n<p>（3）提高训练的稳定性和效率：当每一层的输出方差保持一致时，训练过程会更加稳定，因为权重更新的magnitude更加可控。这样可以提高训练的效率。</p>\n<p>（4）避免过拟合或欠拟合：适当的方差控制有助于网络在训练过程中保持适当的泛化能力。过大的方差可能导致过拟合，而过小的方差可能导致欠拟合。</p>\n<p>Xavier初始化方法通过考虑前一层的节点数（fan-in）和后一层的节点数（fan-out）来设置初始权重的分布范围。有三种方案：</p>\n<p>（1）只考虑输入，设置参数的方差为1/fan-in</p>\n<p>（2）只考虑输出，设置参数的方差为1/fan-out</p>\n<p>（3）同时考虑输入和输出，设置参数的方差为2/（fan-in +\nfan-out）。实际使用中1/fan-in效果就较好。</p>\n<h1 id=\"rlhf中reward模型和critic模型的作用分别是什么\">2.RLHF中，Reward模型和Critic模型的作用分别是什么？</h1>\n<p>Critic模型主要负责评估当前策略下的行为，并预测未来的回报。在PPO中，Critic模型的输出用于计算优势函数（advantage\nfunction），这是一个衡量实际回报与预期回报之间差异的指标。优势函数是策略梯度方法中的关键组成部分，它帮助actor模型了解哪些行为比预期要好，哪些行为比预期要差。因此，Critic模型的评分直接影响actor模型的优化过程，指导其调整行为以提高整体性能。</p>\n<p>Reward模型是基于人类反馈训练的，用于评估和打分模型生成的文本或行为的质量。在RLHF中，Reward模型的评分通常被用作奖励信号，直接反馈给Actor模型。这些奖励信号反映了人类对生成内容的偏好和评价标准，Actor模型会根据这些信号调整其行为，以生成更符合人类期望的输出。\nRLHF中，Actor模型的优化目标是最大化期望回报。这个期望回报可以由Critic模型的预测和Reward模型的评分共同决定。具体来说，Actor模型的优化目标（objective）通常包括（1）策略损失：这部分损失来自于策略模型尝试最大化奖励信号（由Reward模型提供）和/或优势函数（由Critic模型提供）。（2）价值损失：在Actor-Critic架构中，Actor模型也负责优化Critic模型，那么价值损失会尝试最小化Critic模型预测的价值与实际回报之间的差异。（3）其他正则化项：可能还包括一些正则化项。</p>\n<p>因此，Critic模型的评分用于计算优势函数，而Reward模型的评分直接作为奖励信号。这两个模型共同作用于Actor模型，帮助其学习如何生成更符合人类偏好和期望的行为。</p>\n<h1 id=\"kaiming初始化是怎么做的\">3.Kaiming初始化是怎么做的？</h1>\n<p>Kaiming初始化在Xavier初始化的基础上做了调整。Xavier初始化的推导是基于线性函数的，但实际上今天所有模型都会大量使用非线性函数，比如ReLU，这样就导致了Xavier初始化的失效。\nKaiming初始化针对使用ReLU的模型提出。因为ReLU会抛弃掉小于0的值，对于一个均值为0的输入数据，这就相当于砍掉了一半的值，这样输出的方差和均值就变了。因此把Xavier初始化中使用的方差sqrt(1/N)改为sqrt(2/N)，这样数据不会因为ReLU而变得越来越小。</p>\n<h1 id=\"使用bert中的cls作为输出进行分类和相似度计算可能会有什么问题\">4.使用Bert中的[CLS]作为输出进行分类和相似度计算，可能会有什么问题？</h1>\n<p>self-attention中，每个token都天然倾向于关注自己和附近的token，而对更远的内容分配较少的注意力。在Bert中，[CLS]一般是第一个token，放在最前面。这就导致[CLS]更容易关注到靠近它的输入，也就是文本的靠前部分。如果文本较长，且关键内容出现在靠后的位置，就有可能出现[CLS]中关注不够的情况，而导致效果下降。可以通过强制注意力分配，或者使用所有位置的输出而不单是[CLS]token的方法来缓解这个问题。</p>\n<h1 id=\"pytorch中的register_buffer是什么有什么用\">5.pytorch中的register_buffer是什么，有什么用？</h1>\n<p>register_buffer是nn.Module类中的一个方法，它用于注册一个不需要梯度的缓冲区。一般会把模型中需要持续使用或者跟踪，但是不需要通过梯度来更新的参数使用register_buffer注册。使用register_buffer注册的参数可以和其他训练的参数一样，保存在state_dict中，比如旋转位置编码的旋转矩阵，batchnorm中的全局均值和方差，一般都会用register_buffer注册。如果不使用register_buffer注册，普通的常量或者参数保存的时候不会被state_dict跟踪到。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">大模型推理窗口-从有限到无限大</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n"},{"title":"稀疏注意力计算:sliding window attention","abbrlink":"c61d17e3","date":"2024-03-12T09:26:00.000Z","_content":"\n【本文已在同名微信公众号/知乎/个人博客同步上线】  \n\nLLM的长文本能力现在已经是各个大模型巨头的必争之地。  \n\n我们之前在[《LLM长上下文的问题》](http://www.linsight.cn/c4da56c0.html)简单介绍了目前把大模型理解和生成能力推广到32k+/128k+的主流方法，在[《理解Attention:从起源到MHA,MQA和GQA》](http://www.linsight.cn/3dc22f96.html)一文中也解析了MQA和GQA通过节省KV缓存的方式，支持模型在长上下文情况下推理加速的方案。  \n\n在这讲一下另一种（理论有损）提升注意力计算效率的方法：SWA（sliding window attention）。  \n\n一些效果受到广泛关注的模型，如Qwen系列和Mistral就使用了SWA。  \n\n关于Mistral：  \n\nMistral AI是法国一家AI独角兽公司，2023年5月才成立，但是在2023年9月和12月就分别推出了Mistral 7B和MoE模型Mistral 8x7B并开源。  \n\n2024年2月，微软也投资了它。  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n它在2024年2月发布的Mistral Large，支持多语言 & 32k的上下文长度，在MMLU上也是获得了直逼GPT4的效果  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\n（大家也因此对Mistral寄予了厚望，希望它能成为大模型行业的鲶鱼，激活一下OPENAI和META加速一下开源。）  \n\n# SWA\n\n虽然SWA的思路最早不是Mistral提出的，我们还是先以Mistral 7B为例来看下SWA的具体做法。  \n\n## Mistral 7B\n\n2023年10月，Mistral发布了Mistral 7B的[技术报告](https://arxiv.org/pdf/2310.06825.pdf)。其中开篇就说到，相比Llama，Mistral在结构上做了一些改动，除了GQA，另一个用于支持长文本下高效推理的改动就是SWA。  \n\n来看下Mistral 7B的模型结构参数  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistral使用了kv组数=8的GQA，intermediate size相比Llama2（11008）大一些，其他基本没有太大变化。  \n\n## 计算量和缓存\n\n对于原始的causal attention，其注意力矩阵是一个下三角矩阵，这样每个token都能看到自己和在自己前面的token。  \n\n这样随着输入长度 $s$ 增大，这个下三角矩阵中1的元素数量以 $s^2$ 的速度增长，带来的是计算量和所需的KV Cache以平方的速度增长。  \n\n（我们知道计算量/缓存和长度 $s$ 成平方关系，这里放一些更具体的推算细节，已经熟悉的朋友可以跳过）\n\n（1）计算量\n\n对于两个这样大小的矩阵相乘： $[m,n]\\times[n,p]$ ，输出矩阵大小为 $[m,p]$，共有 $m\\times p$ 个元素，每个元素需要 $n$ 次乘法和 $n$ 次加法，因此一次矩阵乘法有 $2mpn$ 个floating point operations（FLOPs）。  \n\n计算量上，按[《Training Compute-Optimal Large Language Models》](https://arxiv.org/pdf/2203.15556.pdf)的算法来。  \n\n对于一般MHA，输入长度为 $s$ ，层数为 $L$ ，模型hidden size为 $d_{model}$ ，每个头的维度为 $d_{q}$ ， 头的数量为 $n_{q}$（这里假设有 $d_{model} = n_{q}\\times d_{q}$ ），各个operation的FLOPs如下  \n\n<center>\n\n| Operation | FLOPs（MHA） |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Softmax | $n_{q}\\times 3\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax项中，对一个 $[1,s]$ 的向量做softmax，计算量为 $3s$ （一个 $s$ 是算每个元素的exp，一个 $s$ 是求和算分母，一个 $s$ 是算除法），而对 $[s,s]$ 的矩阵做softmax，则计算量为  $3s^2$ ，每个头都要计算一遍，因此再乘以 $n_{q}$ 。\n\n（这里忽略了其他一些operation，比如scaling，dropout等，有兴趣的朋友可以自己推算一下）\n\n顺便算下对于Mistral 7B这样使用了GQA的情况。  \n\n其实只有第一项的KV有变化，其他都没变。假设kv头的数量为 $n_{kv}$，则有\n\n<center>\n\n| Operation | FLOPs（GQA） |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\n从上面的推算可以看到QK logits、Softmax和Reduction三项是和长度 $s$ 成平方关系的，其他则是线性关系。  \n\n（2）缓存\n\nKV Cache需要缓存的参数量为  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n如果使用的是半精度浮点数，那么总共所需的空间就是\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n对于Mistral 7B，在输入长度为16k的情况下，所需的KV_Cache约为2G。  \n\n看来虽然用了GQA，但是在长文本（16k+）的情况下计算量和显存还是颇有压力。\n\n## SWA思路\n\n看来要提升attention计算效率，需要想办法减小上面推算中的 $s$ ，但是怎么在减小 $s$ 的同时，还能保持模型长上下文的理解和生成能力呢？\n\n来看一下，CNN中的感受野  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n如上图，假设模型有3层，每层卷积核大小为 $3\\times 3$ （实际上CNN里卷积操作就是一个sliding window）。  \n\n那对于layer 3，每一个像素能看到layer 2中的一个 $3\\times 3$ 的区域，layer 2中其他较远的像素就看到不了。  \n\n但我们再往前推，layer 2里的每个像素也可以看到layer 1中的一个 $3\\times 3$ 区域，那么layer 2中的 $3\\times 3$ 区域就可以看到layer 1中一个 $5\\times 5$ 的区域，相当于layer 3中一个像素可以<u>**间接**</u>看到一个 $5\\times 5$ 的输入。  \n\n以此类推，如果我们再增加一层layer 4，那么layer 4中一个像素就能获取输入层（layer 1） 一个 $7\\times 7$ 区域的信息。  \n\n虽然每层只能多看周围一格的信息，但是只要我们层数够多，理论上靠近输出端的层想看多远就能看多远。  \n\n值得注意的一点是，我们一般认为模型低层部分提取比较基础的特征，而高层会提取高级的语义特征。  \n\n在CNN里，前几层提取的可能更多是关于简单的边界、颜色、形状等基础特征，而后面的层则提取较复杂的语义特征，比如在分类任务中会是和分类类别相关的花纹、物体大小、风格等特征。  \n\n如果我们把模型设计成，最后一层的一个像素刚好要到第一层才能接收到全局信息（在其它层都只能看到局部），那对于图像边缘的语义特征识别能力可能会受到一些限制。  \n\n具体来说，假设我们做猫和狗的图像分类任务，如果这个时候决定性的特征出现在图像最边缘几个像素里，那这种情况下的错误率会比特征出现在图像中间时要高。  \n\n而对于语言模型，一般情况下，越远距离的信息，对当前位置的重要性越低，因此只要我们的窗口大小不要太过极限小，问题应该都还不大。\n\n看下Mistral的SWA具体是怎么做的  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\n左边是正常的causal attention，每个位置能看到自己和前面的位置，attention mask是个下三角矩阵。  \n\n中间则是SWA的attention mask，这里的窗口大小为3。包括自己在内，每个位置只能往前看3个输入。  \n\n同CNN的感受野一样，随着层数的堆叠，模型理论上能处理的最远距离也逐层线性递增。只是LLM里递增的方向是单向的，只能往前。  \n\nMistral 7B使用了4096的窗口大小，模型层数为32，则最终输出的”感受野“为 $4096\\times 32=131,072$ 达到131k的长度。  \n\n前面我们推算了attention的计算量，其中QK logits、Softmax和Reduction三项是和长度 $s$ 成平方关系。在使用了SWA之后，理论上，这几个operation仅使用4k的计算量，就能获得131k的上下文效果。当输入长度为131k时，除去已经缓存部分的数值，新的输入计算量相差 $32\\times 32=1024$ 倍。  \n\n而缓存和上下文长度 $s$ 成线性关系，当上下文长度为131k时，最大也能节省 $31/32$ 的显存。  \n\n即SWA在上下文长度在4k以下时，和普通causal attention一样；当上下文长度超过4k时，则相对节省资源，长度越大，节省的比例越高。\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\n实际使用中，Mistral通过把SWA实现在FlashAttention和xFormers中，对于16k的上下文长度，获得了2倍的速度提升。  \n\n## 和KV Cache的配合实现\n\n在不使用sliding window的情况下，随着自回归推理的进行，KV Cache是只增不减的。  \n\n而在使用SWA的情况下，超出窗口长度的kv就可以不用再缓存了，因此使用一个轮转替换的策略。  \n\n比如窗口大小 $W=4$ ，则当第5个token需要缓存是，直接替换掉第1个token，这样就可以保持kv缓存有一个最大值（为窗口大小），而不会无限增长。  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\n这样便于我们估计硬件设备所能支持的throughput，也不会因为少量超长的case而造成堵塞，在工程上有利于提高硬件利用率，降低成本。  \n\n## 长Prompt的分块\n\n更近一步，考虑到我们使用RAG或者funciton call的时候，都会使用比较长的，固定的prompt来知道模型的行为。  \n\n比如GPT4就被诱导说出它接收到的长system prompt（当然未必真的就是OPENAI用的）  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: 【{message idx}†{link text}】.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\n除了预先计算好system prompt的kv值，并保存在缓存中方便每次用户输入使用外，如果system prompt很长（比sliding window大），还可以通过对system prompt的kv值进行切分来进一步优化计算。\n\n比如窗口大小 $W=4$，system prompt大小为9时，就可以把system prompt的kv缓存切成 [4,4,1] 三块。  \n\n第一块由于和当前的输入距离超过了一个window的大小，所以是完全看不见的，对应的attention mask全为0，因此可以完全忽略。  \n\n第二块的attention mask则是一个上三角矩阵，当前的输入需要用到这部分信息。  \n\n第三块是一个下三角矩阵（的左边部分），包含了当前的输入在内。  \n\n在推理的时候，我们只需要用到第二块和第三块的内容，这就节省了缓存的操作。  \n\n而且无论prompt有多长，只要我们按窗口大小分块，一定只会用到最后两块。  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\n（实际上现在推理框架基本上都有FlashAttention/PagedAttention等技术加持，能够进一步节省资源，提高效率，这个后续再开一篇讲）\n\nMistral 7B整体的效果上的效果相比Llama是有优势的，部分任务甚至超过了Llama 34B。  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral认为大语言模型压缩知识的能力实际超过我们的认知，7B这个规模的效果还有提升空间。  \n\n# Sparse Attention\n\nSWA实际上是一种sparse attention，而sparse attention也有许多工作做了深入探索。  \n\n这里简单说一小部分，有机会再完整梳理一遍sparse attention的理论和实践。  \n\n## Longformer\n\n前面提到，Mistral并不是第一个使用SWA的。  \n\n2020年，[《Longformer: The Long-Document Transformer》](https://arxiv.org/pdf/2004.05150.pdf)就提出包含SWA在内的一系列sparse attention的做法。  \n\n从文章名字就看到出来，Longformer主要目的也是为了解决长上下文的问题。  \n\n{% asset_img longformer_attention.png longformer %}  \n\n上图中的（b）就是SWA，只是用在Bert中的时候它是双向的。  \n\n在SWA的基础上，还可以进行空洞滑窗（dilated sliding window），在不增加计算量的情况下，提升感受野。这也是从空洞卷积（下图）来的灵感了。    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\n还可以更进一步优化attention。无论是SWA还是dilated sliding window，每个位置都只能看到局部的信息。  \n\n但是实际上有些位置就是对全局信息有很高的需求。  \n\n在Bert中，[CLS] token就常常作为分类token或者相似度向量使用，这种情况下就需要它能获取整个上下文的完整信息。  \n\n而在GPT中，instruction，或者说prompt的部分也对全局信息有更高要求，因为我们希望在整个对话过程中，模型都能遵循我们给出的规则。  \n\n对于这些token，我们让它可以看到其他所有位置，使用完整的global attention，而其他位置则使用sliding window，如（d）中所示。  \n\n## Big Bird\n\n无独有偶，同样在2020年，和Longformer差不多在同一时期，也有另外一个通过sparse attention来优化长文本效果的工作，[《Big Bird: Transformers for Longer Sequences》](https://arxiv.org/abs/2007.14062)。  \n\n其中sliding window和global attention结合的思路和Longformer相似。Big Bird还额外加入了一个random attention的做法。  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n上图中 $r=2$ 即每个位置使用2个随机注意力。\n\n# 小结  \n\nSWA在优化长上下文的计算效率上有明显的收益。而在模型效果上，目前基本没有看到不可接受的损失。对长上下文有需求的业务，值得探索。  \n\n除了SWA，sparse attention还有许多其他探索。目前来看，这些做法都有一定的理论基础，效果也不错。但是阻碍这些方案大规模使用的一个原因就是<big>**工程实现**</big>，比如如何高效计算global + local attention，在flash attention中能够支持random attention，这都是要考虑的内容。\n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n【1】Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n【2】Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n【3】Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n【4】GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n【5】Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","source":"_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention.md","raw":"---\ntitle: '稀疏注意力计算:sliding window attention'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - sliding window attention\n  - sparse attention\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: c61d17e3\ndate: 2024-03-12 17:26:00\n---\n\n【本文已在同名微信公众号/知乎/个人博客同步上线】  \n\nLLM的长文本能力现在已经是各个大模型巨头的必争之地。  \n\n我们之前在[《LLM长上下文的问题》](http://www.linsight.cn/c4da56c0.html)简单介绍了目前把大模型理解和生成能力推广到32k+/128k+的主流方法，在[《理解Attention:从起源到MHA,MQA和GQA》](http://www.linsight.cn/3dc22f96.html)一文中也解析了MQA和GQA通过节省KV缓存的方式，支持模型在长上下文情况下推理加速的方案。  \n\n在这讲一下另一种（理论有损）提升注意力计算效率的方法：SWA（sliding window attention）。  \n\n一些效果受到广泛关注的模型，如Qwen系列和Mistral就使用了SWA。  \n\n关于Mistral：  \n\nMistral AI是法国一家AI独角兽公司，2023年5月才成立，但是在2023年9月和12月就分别推出了Mistral 7B和MoE模型Mistral 8x7B并开源。  \n\n2024年2月，微软也投资了它。  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n它在2024年2月发布的Mistral Large，支持多语言 & 32k的上下文长度，在MMLU上也是获得了直逼GPT4的效果  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\n（大家也因此对Mistral寄予了厚望，希望它能成为大模型行业的鲶鱼，激活一下OPENAI和META加速一下开源。）  \n\n# SWA\n\n虽然SWA的思路最早不是Mistral提出的，我们还是先以Mistral 7B为例来看下SWA的具体做法。  \n\n## Mistral 7B\n\n2023年10月，Mistral发布了Mistral 7B的[技术报告](https://arxiv.org/pdf/2310.06825.pdf)。其中开篇就说到，相比Llama，Mistral在结构上做了一些改动，除了GQA，另一个用于支持长文本下高效推理的改动就是SWA。  \n\n来看下Mistral 7B的模型结构参数  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistral使用了kv组数=8的GQA，intermediate size相比Llama2（11008）大一些，其他基本没有太大变化。  \n\n## 计算量和缓存\n\n对于原始的causal attention，其注意力矩阵是一个下三角矩阵，这样每个token都能看到自己和在自己前面的token。  \n\n这样随着输入长度 $s$ 增大，这个下三角矩阵中1的元素数量以 $s^2$ 的速度增长，带来的是计算量和所需的KV Cache以平方的速度增长。  \n\n（我们知道计算量/缓存和长度 $s$ 成平方关系，这里放一些更具体的推算细节，已经熟悉的朋友可以跳过）\n\n（1）计算量\n\n对于两个这样大小的矩阵相乘： $[m,n]\\times[n,p]$ ，输出矩阵大小为 $[m,p]$，共有 $m\\times p$ 个元素，每个元素需要 $n$ 次乘法和 $n$ 次加法，因此一次矩阵乘法有 $2mpn$ 个floating point operations（FLOPs）。  \n\n计算量上，按[《Training Compute-Optimal Large Language Models》](https://arxiv.org/pdf/2203.15556.pdf)的算法来。  \n\n对于一般MHA，输入长度为 $s$ ，层数为 $L$ ，模型hidden size为 $d_{model}$ ，每个头的维度为 $d_{q}$ ， 头的数量为 $n_{q}$（这里假设有 $d_{model} = n_{q}\\times d_{q}$ ），各个operation的FLOPs如下  \n\n<center>\n\n| Operation | FLOPs（MHA） |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Softmax | $n_{q}\\times 3\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax项中，对一个 $[1,s]$ 的向量做softmax，计算量为 $3s$ （一个 $s$ 是算每个元素的exp，一个 $s$ 是求和算分母，一个 $s$ 是算除法），而对 $[s,s]$ 的矩阵做softmax，则计算量为  $3s^2$ ，每个头都要计算一遍，因此再乘以 $n_{q}$ 。\n\n（这里忽略了其他一些operation，比如scaling，dropout等，有兴趣的朋友可以自己推算一下）\n\n顺便算下对于Mistral 7B这样使用了GQA的情况。  \n\n其实只有第一项的KV有变化，其他都没变。假设kv头的数量为 $n_{kv}$，则有\n\n<center>\n\n| Operation | FLOPs（GQA） |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\n从上面的推算可以看到QK logits、Softmax和Reduction三项是和长度 $s$ 成平方关系的，其他则是线性关系。  \n\n（2）缓存\n\nKV Cache需要缓存的参数量为  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n如果使用的是半精度浮点数，那么总共所需的空间就是\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n对于Mistral 7B，在输入长度为16k的情况下，所需的KV_Cache约为2G。  \n\n看来虽然用了GQA，但是在长文本（16k+）的情况下计算量和显存还是颇有压力。\n\n## SWA思路\n\n看来要提升attention计算效率，需要想办法减小上面推算中的 $s$ ，但是怎么在减小 $s$ 的同时，还能保持模型长上下文的理解和生成能力呢？\n\n来看一下，CNN中的感受野  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n如上图，假设模型有3层，每层卷积核大小为 $3\\times 3$ （实际上CNN里卷积操作就是一个sliding window）。  \n\n那对于layer 3，每一个像素能看到layer 2中的一个 $3\\times 3$ 的区域，layer 2中其他较远的像素就看到不了。  \n\n但我们再往前推，layer 2里的每个像素也可以看到layer 1中的一个 $3\\times 3$ 区域，那么layer 2中的 $3\\times 3$ 区域就可以看到layer 1中一个 $5\\times 5$ 的区域，相当于layer 3中一个像素可以<u>**间接**</u>看到一个 $5\\times 5$ 的输入。  \n\n以此类推，如果我们再增加一层layer 4，那么layer 4中一个像素就能获取输入层（layer 1） 一个 $7\\times 7$ 区域的信息。  \n\n虽然每层只能多看周围一格的信息，但是只要我们层数够多，理论上靠近输出端的层想看多远就能看多远。  \n\n值得注意的一点是，我们一般认为模型低层部分提取比较基础的特征，而高层会提取高级的语义特征。  \n\n在CNN里，前几层提取的可能更多是关于简单的边界、颜色、形状等基础特征，而后面的层则提取较复杂的语义特征，比如在分类任务中会是和分类类别相关的花纹、物体大小、风格等特征。  \n\n如果我们把模型设计成，最后一层的一个像素刚好要到第一层才能接收到全局信息（在其它层都只能看到局部），那对于图像边缘的语义特征识别能力可能会受到一些限制。  \n\n具体来说，假设我们做猫和狗的图像分类任务，如果这个时候决定性的特征出现在图像最边缘几个像素里，那这种情况下的错误率会比特征出现在图像中间时要高。  \n\n而对于语言模型，一般情况下，越远距离的信息，对当前位置的重要性越低，因此只要我们的窗口大小不要太过极限小，问题应该都还不大。\n\n看下Mistral的SWA具体是怎么做的  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\n左边是正常的causal attention，每个位置能看到自己和前面的位置，attention mask是个下三角矩阵。  \n\n中间则是SWA的attention mask，这里的窗口大小为3。包括自己在内，每个位置只能往前看3个输入。  \n\n同CNN的感受野一样，随着层数的堆叠，模型理论上能处理的最远距离也逐层线性递增。只是LLM里递增的方向是单向的，只能往前。  \n\nMistral 7B使用了4096的窗口大小，模型层数为32，则最终输出的”感受野“为 $4096\\times 32=131,072$ 达到131k的长度。  \n\n前面我们推算了attention的计算量，其中QK logits、Softmax和Reduction三项是和长度 $s$ 成平方关系。在使用了SWA之后，理论上，这几个operation仅使用4k的计算量，就能获得131k的上下文效果。当输入长度为131k时，除去已经缓存部分的数值，新的输入计算量相差 $32\\times 32=1024$ 倍。  \n\n而缓存和上下文长度 $s$ 成线性关系，当上下文长度为131k时，最大也能节省 $31/32$ 的显存。  \n\n即SWA在上下文长度在4k以下时，和普通causal attention一样；当上下文长度超过4k时，则相对节省资源，长度越大，节省的比例越高。\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\n实际使用中，Mistral通过把SWA实现在FlashAttention和xFormers中，对于16k的上下文长度，获得了2倍的速度提升。  \n\n## 和KV Cache的配合实现\n\n在不使用sliding window的情况下，随着自回归推理的进行，KV Cache是只增不减的。  \n\n而在使用SWA的情况下，超出窗口长度的kv就可以不用再缓存了，因此使用一个轮转替换的策略。  \n\n比如窗口大小 $W=4$ ，则当第5个token需要缓存是，直接替换掉第1个token，这样就可以保持kv缓存有一个最大值（为窗口大小），而不会无限增长。  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\n这样便于我们估计硬件设备所能支持的throughput，也不会因为少量超长的case而造成堵塞，在工程上有利于提高硬件利用率，降低成本。  \n\n## 长Prompt的分块\n\n更近一步，考虑到我们使用RAG或者funciton call的时候，都会使用比较长的，固定的prompt来知道模型的行为。  \n\n比如GPT4就被诱导说出它接收到的长system prompt（当然未必真的就是OPENAI用的）  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: 【{message idx}†{link text}】.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\n除了预先计算好system prompt的kv值，并保存在缓存中方便每次用户输入使用外，如果system prompt很长（比sliding window大），还可以通过对system prompt的kv值进行切分来进一步优化计算。\n\n比如窗口大小 $W=4$，system prompt大小为9时，就可以把system prompt的kv缓存切成 [4,4,1] 三块。  \n\n第一块由于和当前的输入距离超过了一个window的大小，所以是完全看不见的，对应的attention mask全为0，因此可以完全忽略。  \n\n第二块的attention mask则是一个上三角矩阵，当前的输入需要用到这部分信息。  \n\n第三块是一个下三角矩阵（的左边部分），包含了当前的输入在内。  \n\n在推理的时候，我们只需要用到第二块和第三块的内容，这就节省了缓存的操作。  \n\n而且无论prompt有多长，只要我们按窗口大小分块，一定只会用到最后两块。  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\n（实际上现在推理框架基本上都有FlashAttention/PagedAttention等技术加持，能够进一步节省资源，提高效率，这个后续再开一篇讲）\n\nMistral 7B整体的效果上的效果相比Llama是有优势的，部分任务甚至超过了Llama 34B。  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral认为大语言模型压缩知识的能力实际超过我们的认知，7B这个规模的效果还有提升空间。  \n\n# Sparse Attention\n\nSWA实际上是一种sparse attention，而sparse attention也有许多工作做了深入探索。  \n\n这里简单说一小部分，有机会再完整梳理一遍sparse attention的理论和实践。  \n\n## Longformer\n\n前面提到，Mistral并不是第一个使用SWA的。  \n\n2020年，[《Longformer: The Long-Document Transformer》](https://arxiv.org/pdf/2004.05150.pdf)就提出包含SWA在内的一系列sparse attention的做法。  \n\n从文章名字就看到出来，Longformer主要目的也是为了解决长上下文的问题。  \n\n{% asset_img longformer_attention.png longformer %}  \n\n上图中的（b）就是SWA，只是用在Bert中的时候它是双向的。  \n\n在SWA的基础上，还可以进行空洞滑窗（dilated sliding window），在不增加计算量的情况下，提升感受野。这也是从空洞卷积（下图）来的灵感了。    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\n还可以更进一步优化attention。无论是SWA还是dilated sliding window，每个位置都只能看到局部的信息。  \n\n但是实际上有些位置就是对全局信息有很高的需求。  \n\n在Bert中，[CLS] token就常常作为分类token或者相似度向量使用，这种情况下就需要它能获取整个上下文的完整信息。  \n\n而在GPT中，instruction，或者说prompt的部分也对全局信息有更高要求，因为我们希望在整个对话过程中，模型都能遵循我们给出的规则。  \n\n对于这些token，我们让它可以看到其他所有位置，使用完整的global attention，而其他位置则使用sliding window，如（d）中所示。  \n\n## Big Bird\n\n无独有偶，同样在2020年，和Longformer差不多在同一时期，也有另外一个通过sparse attention来优化长文本效果的工作，[《Big Bird: Transformers for Longer Sequences》](https://arxiv.org/abs/2007.14062)。  \n\n其中sliding window和global attention结合的思路和Longformer相似。Big Bird还额外加入了一个random attention的做法。  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n上图中 $r=2$ 即每个位置使用2个随机注意力。\n\n# 小结  \n\nSWA在优化长上下文的计算效率上有明显的收益。而在模型效果上，目前基本没有看到不可接受的损失。对长上下文有需求的业务，值得探索。  \n\n除了SWA，sparse attention还有许多其他探索。目前来看，这些做法都有一定的理论基础，效果也不错。但是阻碍这些方案大规模使用的一个原因就是<big>**工程实现**</big>，比如如何高效计算global + local attention，在flash attention中能够支持random attention，这都是要考虑的内容。\n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n【1】Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n【2】Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n【3】Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n【4】GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n【5】Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","slug":"cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention","published":1,"updated":"2024-03-20T11:38:30.908Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9z001c314kasf352ne","content":"<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>\n<p>LLM的长文本能力现在已经是各个大模型巨头的必争之地。</p>\n<p>我们之前在<a href=\"http://www.linsight.cn/c4da56c0.html\">《LLM长上下文的问题》</a>简单介绍了目前把大模型理解和生成能力推广到32k+/128k+的主流方法，在<a href=\"http://www.linsight.cn/3dc22f96.html\">《理解Attention:从起源到MHA,MQA和GQA》</a>一文中也解析了MQA和GQA通过节省KV缓存的方式，支持模型在长上下文情况下推理加速的方案。</p>\n<p>在这讲一下另一种（理论有损）提升注意力计算效率的方法：SWA（sliding\nwindow attention）。</p>\n<p>一些效果受到广泛关注的模型，如Qwen系列和Mistral就使用了SWA。</p>\n<p>关于Mistral：</p>\n<p>Mistral\nAI是法国一家AI独角兽公司，2023年5月才成立，但是在2023年9月和12月就分别推出了Mistral\n7B和MoE模型Mistral 8x7B并开源。</p>\n<p>2024年2月，微软也投资了它。</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>它在2024年2月发布的Mistral Large，支持多语言 &amp;\n32k的上下文长度，在MMLU上也是获得了直逼GPT4的效果</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>（大家也因此对Mistral寄予了厚望，希望它能成为大模型行业的鲶鱼，激活一下OPENAI和META加速一下开源。）</p>\n<h1 id=\"swa\">SWA</h1>\n<p>虽然SWA的思路最早不是Mistral提出的，我们还是先以Mistral\n7B为例来看下SWA的具体做法。</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>2023年10月，Mistral发布了Mistral 7B的<a href=\"https://arxiv.org/pdf/2310.06825.pdf\">技术报告</a>。其中开篇就说到，相比Llama，Mistral在结构上做了一些改动，除了GQA，另一个用于支持长文本下高效推理的改动就是SWA。</p>\n<p>来看下Mistral 7B的模型结构参数</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistral使用了kv组数=8的GQA，intermediate\nsize相比Llama2（11008）大一些，其他基本没有太大变化。</p>\n<h2 id=\"计算量和缓存\">计算量和缓存</h2>\n<p>对于原始的causal\nattention，其注意力矩阵是一个下三角矩阵，这样每个token都能看到自己和在自己前面的token。</p>\n<p>这样随着输入长度 <span class=\"math inline\">\\(s\\)</span>\n增大，这个下三角矩阵中1的元素数量以 <span class=\"math inline\">\\(s^2\\)</span> 的速度增长，带来的是计算量和所需的KV\nCache以平方的速度增长。</p>\n<p>（我们知道计算量/缓存和长度 <span class=\"math inline\">\\(s\\)</span>\n成平方关系，这里放一些更具体的推算细节，已经熟悉的朋友可以跳过）</p>\n<p>（1）计算量</p>\n<p>对于两个这样大小的矩阵相乘： <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span> ，输出矩阵大小为 <span class=\"math inline\">\\([m,p]\\)</span>，共有 <span class=\"math inline\">\\(m\\times p\\)</span> 个元素，每个元素需要 <span class=\"math inline\">\\(n\\)</span> 次乘法和 <span class=\"math inline\">\\(n\\)</span> 次加法，因此一次矩阵乘法有 <span class=\"math inline\">\\(2mpn\\)</span> 个floating point\noperations（FLOPs）。</p>\n<p>计算量上，按<a href=\"https://arxiv.org/pdf/2203.15556.pdf\">《Training\nCompute-Optimal Large Language Models》</a>的算法来。</p>\n<p>对于一般MHA，输入长度为 <span class=\"math inline\">\\(s\\)</span>\n，层数为 <span class=\"math inline\">\\(L\\)</span> ，模型hidden size为\n<span class=\"math inline\">\\(d_{model}\\)</span> ，每个头的维度为 <span class=\"math inline\">\\(d_{q}\\)</span> ， 头的数量为 <span class=\"math inline\">\\(n_{q}\\)</span>（这里假设有 <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\n），各个operation的FLOPs如下</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPs（MHA）</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n3\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax项中，对一个 <span class=\"math inline\">\\([1,s]\\)</span>\n的向量做softmax，计算量为 <span class=\"math inline\">\\(3s\\)</span> （一个\n<span class=\"math inline\">\\(s\\)</span> 是算每个元素的exp，一个 <span class=\"math inline\">\\(s\\)</span> 是求和算分母，一个 <span class=\"math inline\">\\(s\\)</span> 是算除法），而对 <span class=\"math inline\">\\([s,s]\\)</span> 的矩阵做softmax，则计算量为 <span class=\"math inline\">\\(3s^2\\)</span> ，每个头都要计算一遍，因此再乘以\n<span class=\"math inline\">\\(n_{q}\\)</span> 。</p>\n<p>（这里忽略了其他一些operation，比如scaling，dropout等，有兴趣的朋友可以自己推算一下）</p>\n<p>顺便算下对于Mistral 7B这样使用了GQA的情况。</p>\n<p>其实只有第一项的KV有变化，其他都没变。假设kv头的数量为 <span class=\"math inline\">\\(n_{kv}\\)</span>，则有</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPs（GQA）</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>从上面的推算可以看到QK logits、Softmax和Reduction三项是和长度 <span class=\"math inline\">\\(s\\)</span> 成平方关系的，其他则是线性关系。</p>\n<p>（2）缓存</p>\n<p>KV Cache需要缓存的参数量为</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>如果使用的是半精度浮点数，那么总共所需的空间就是</p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>对于Mistral 7B，在输入长度为16k的情况下，所需的KV_Cache约为2G。</p>\n<p>看来虽然用了GQA，但是在长文本（16k+）的情况下计算量和显存还是颇有压力。</p>\n<h2 id=\"swa思路\">SWA思路</h2>\n<p>看来要提升attention计算效率，需要想办法减小上面推算中的 <span class=\"math inline\">\\(s\\)</span> ，但是怎么在减小 <span class=\"math inline\">\\(s\\)</span>\n的同时，还能保持模型长上下文的理解和生成能力呢？</p>\n<p>来看一下，CNN中的感受野</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>如上图，假设模型有3层，每层卷积核大小为 <span class=\"math inline\">\\(3\\times 3\\)</span>\n（实际上CNN里卷积操作就是一个sliding window）。</p>\n<p>那对于layer 3，每一个像素能看到layer 2中的一个 <span class=\"math inline\">\\(3\\times 3\\)</span> 的区域，layer\n2中其他较远的像素就看到不了。</p>\n<p>但我们再往前推，layer 2里的每个像素也可以看到layer 1中的一个 <span class=\"math inline\">\\(3\\times 3\\)</span> 区域，那么layer 2中的 <span class=\"math inline\">\\(3\\times 3\\)</span> 区域就可以看到layer 1中一个\n<span class=\"math inline\">\\(5\\times 5\\)</span> 的区域，相当于layer\n3中一个像素可以<u><strong>间接</strong></u>看到一个 <span class=\"math inline\">\\(5\\times 5\\)</span> 的输入。</p>\n<p>以此类推，如果我们再增加一层layer 4，那么layer\n4中一个像素就能获取输入层（layer 1） 一个 <span class=\"math inline\">\\(7\\times 7\\)</span> 区域的信息。</p>\n<p>虽然每层只能多看周围一格的信息，但是只要我们层数够多，理论上靠近输出端的层想看多远就能看多远。</p>\n<p>值得注意的一点是，我们一般认为模型低层部分提取比较基础的特征，而高层会提取高级的语义特征。</p>\n<p>在CNN里，前几层提取的可能更多是关于简单的边界、颜色、形状等基础特征，而后面的层则提取较复杂的语义特征，比如在分类任务中会是和分类类别相关的花纹、物体大小、风格等特征。</p>\n<p>如果我们把模型设计成，最后一层的一个像素刚好要到第一层才能接收到全局信息（在其它层都只能看到局部），那对于图像边缘的语义特征识别能力可能会受到一些限制。</p>\n<p>具体来说，假设我们做猫和狗的图像分类任务，如果这个时候决定性的特征出现在图像最边缘几个像素里，那这种情况下的错误率会比特征出现在图像中间时要高。</p>\n<p>而对于语言模型，一般情况下，越远距离的信息，对当前位置的重要性越低，因此只要我们的窗口大小不要太过极限小，问题应该都还不大。</p>\n<p>看下Mistral的SWA具体是怎么做的</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>左边是正常的causal\nattention，每个位置能看到自己和前面的位置，attention\nmask是个下三角矩阵。</p>\n<p>中间则是SWA的attention\nmask，这里的窗口大小为3。包括自己在内，每个位置只能往前看3个输入。</p>\n<p>同CNN的感受野一样，随着层数的堆叠，模型理论上能处理的最远距离也逐层线性递增。只是LLM里递增的方向是单向的，只能往前。</p>\n<p>Mistral 7B使用了4096的窗口大小，模型层数为32，则最终输出的”感受野“为\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n达到131k的长度。</p>\n<p>前面我们推算了attention的计算量，其中QK\nlogits、Softmax和Reduction三项是和长度 <span class=\"math inline\">\\(s\\)</span>\n成平方关系。在使用了SWA之后，理论上，这几个operation仅使用4k的计算量，就能获得131k的上下文效果。当输入长度为131k时，除去已经缓存部分的数值，新的输入计算量相差\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> 倍。</p>\n<p>而缓存和上下文长度 <span class=\"math inline\">\\(s\\)</span>\n成线性关系，当上下文长度为131k时，最大也能节省 <span class=\"math inline\">\\(31/32\\)</span> 的显存。</p>\n<p>即SWA在上下文长度在4k以下时，和普通causal\nattention一样；当上下文长度超过4k时，则相对节省资源，长度越大，节省的比例越高。</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>实际使用中，Mistral通过把SWA实现在FlashAttention和xFormers中，对于16k的上下文长度，获得了2倍的速度提升。</p>\n<h2 id=\"和kv-cache的配合实现\">和KV Cache的配合实现</h2>\n<p>在不使用sliding window的情况下，随着自回归推理的进行，KV\nCache是只增不减的。</p>\n<p>而在使用SWA的情况下，超出窗口长度的kv就可以不用再缓存了，因此使用一个轮转替换的策略。</p>\n<p>比如窗口大小 <span class=\"math inline\">\\(W=4\\)</span>\n，则当第5个token需要缓存是，直接替换掉第1个token，这样就可以保持kv缓存有一个最大值（为窗口大小），而不会无限增长。</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>这样便于我们估计硬件设备所能支持的throughput，也不会因为少量超长的case而造成堵塞，在工程上有利于提高硬件利用率，降低成本。</p>\n<h2 id=\"长prompt的分块\">长Prompt的分块</h2>\n<p>更近一步，考虑到我们使用RAG或者funciton\ncall的时候，都会使用比较长的，固定的prompt来知道模型的行为。</p>\n<p>比如GPT4就被诱导说出它接收到的长system\nprompt（当然未必真的就是OPENAI用的）</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: 【{message idx}†{link text}】. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>除了预先计算好system\nprompt的kv值，并保存在缓存中方便每次用户输入使用外，如果system\nprompt很长（比sliding window大），还可以通过对system\nprompt的kv值进行切分来进一步优化计算。</p>\n<p>比如窗口大小 <span class=\"math inline\">\\(W=4\\)</span>，system\nprompt大小为9时，就可以把system prompt的kv缓存切成 [4,4,1] 三块。</p>\n<p>第一块由于和当前的输入距离超过了一个window的大小，所以是完全看不见的，对应的attention\nmask全为0，因此可以完全忽略。</p>\n<p>第二块的attention\nmask则是一个上三角矩阵，当前的输入需要用到这部分信息。</p>\n<p>第三块是一个下三角矩阵（的左边部分），包含了当前的输入在内。</p>\n<p>在推理的时候，我们只需要用到第二块和第三块的内容，这就节省了缓存的操作。</p>\n<p>而且无论prompt有多长，只要我们按窗口大小分块，一定只会用到最后两块。</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>（实际上现在推理框架基本上都有FlashAttention/PagedAttention等技术加持，能够进一步节省资源，提高效率，这个后续再开一篇讲）</p>\n<p>Mistral\n7B整体的效果上的效果相比Llama是有优势的，部分任务甚至超过了Llama\n34B。</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral认为大语言模型压缩知识的能力实际超过我们的认知，7B这个规模的效果还有提升空间。</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWA实际上是一种sparse attention，而sparse\nattention也有许多工作做了深入探索。</p>\n<p>这里简单说一小部分，有机会再完整梳理一遍sparse\nattention的理论和实践。</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>前面提到，Mistral并不是第一个使用SWA的。</p>\n<p>2020年，<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">《Longformer:\nThe Long-Document Transformer》</a>就提出包含SWA在内的一系列sparse\nattention的做法。</p>\n<p>从文章名字就看到出来，Longformer主要目的也是为了解决长上下文的问题。</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>上图中的（b）就是SWA，只是用在Bert中的时候它是双向的。</p>\n<p>在SWA的基础上，还可以进行空洞滑窗（dilated sliding\nwindow），在不增加计算量的情况下，提升感受野。这也是从空洞卷积（下图）来的灵感了。</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>还可以更进一步优化attention。无论是SWA还是dilated sliding\nwindow，每个位置都只能看到局部的信息。</p>\n<p>但是实际上有些位置就是对全局信息有很高的需求。</p>\n<p>在Bert中，[CLS]\ntoken就常常作为分类token或者相似度向量使用，这种情况下就需要它能获取整个上下文的完整信息。</p>\n<p>而在GPT中，instruction，或者说prompt的部分也对全局信息有更高要求，因为我们希望在整个对话过程中，模型都能遵循我们给出的规则。</p>\n<p>对于这些token，我们让它可以看到其他所有位置，使用完整的global\nattention，而其他位置则使用sliding window，如（d）中所示。</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>无独有偶，同样在2020年，和Longformer差不多在同一时期，也有另外一个通过sparse\nattention来优化长文本效果的工作，<a href=\"https://arxiv.org/abs/2007.14062\">《Big Bird: Transformers for\nLonger Sequences》</a>。</p>\n<p>其中sliding window和global attention结合的思路和Longformer相似。Big\nBird还额外加入了一个random attention的做法。</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p>上图中 <span class=\"math inline\">\\(r=2\\)</span>\n即每个位置使用2个随机注意力。</p>\n<h1 id=\"小结\">小结</h1>\n<p>SWA在优化长上下文的计算效率上有明显的收益。而在模型效果上，目前基本没有看到不可接受的损失。对长上下文有需求的业务，值得探索。</p>\n<p>除了SWA，sparse\nattention还有许多其他探索。目前来看，这些做法都有一定的理论基础，效果也不错。但是阻碍这些方案大规模使用的一个原因就是<big><strong>工程实现</strong></big>，比如如何高效计算global\n+ local attention，在flash attention中能够支持random\nattention，这都是要考虑的内容。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n【2】Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n【3】Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n【4】GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n【5】Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n","length":10783,"excerpt":"","more":"<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>\n<p>LLM的长文本能力现在已经是各个大模型巨头的必争之地。</p>\n<p>我们之前在<a href=\"http://www.linsight.cn/c4da56c0.html\">《LLM长上下文的问题》</a>简单介绍了目前把大模型理解和生成能力推广到32k+/128k+的主流方法，在<a href=\"http://www.linsight.cn/3dc22f96.html\">《理解Attention:从起源到MHA,MQA和GQA》</a>一文中也解析了MQA和GQA通过节省KV缓存的方式，支持模型在长上下文情况下推理加速的方案。</p>\n<p>在这讲一下另一种（理论有损）提升注意力计算效率的方法：SWA（sliding\nwindow attention）。</p>\n<p>一些效果受到广泛关注的模型，如Qwen系列和Mistral就使用了SWA。</p>\n<p>关于Mistral：</p>\n<p>Mistral\nAI是法国一家AI独角兽公司，2023年5月才成立，但是在2023年9月和12月就分别推出了Mistral\n7B和MoE模型Mistral 8x7B并开源。</p>\n<p>2024年2月，微软也投资了它。</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>它在2024年2月发布的Mistral Large，支持多语言 &amp;\n32k的上下文长度，在MMLU上也是获得了直逼GPT4的效果</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>（大家也因此对Mistral寄予了厚望，希望它能成为大模型行业的鲶鱼，激活一下OPENAI和META加速一下开源。）</p>\n<h1 id=\"swa\">SWA</h1>\n<p>虽然SWA的思路最早不是Mistral提出的，我们还是先以Mistral\n7B为例来看下SWA的具体做法。</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>2023年10月，Mistral发布了Mistral 7B的<a href=\"https://arxiv.org/pdf/2310.06825.pdf\">技术报告</a>。其中开篇就说到，相比Llama，Mistral在结构上做了一些改动，除了GQA，另一个用于支持长文本下高效推理的改动就是SWA。</p>\n<p>来看下Mistral 7B的模型结构参数</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistral使用了kv组数=8的GQA，intermediate\nsize相比Llama2（11008）大一些，其他基本没有太大变化。</p>\n<h2 id=\"计算量和缓存\">计算量和缓存</h2>\n<p>对于原始的causal\nattention，其注意力矩阵是一个下三角矩阵，这样每个token都能看到自己和在自己前面的token。</p>\n<p>这样随着输入长度 <span class=\"math inline\">\\(s\\)</span>\n增大，这个下三角矩阵中1的元素数量以 <span class=\"math inline\">\\(s^2\\)</span> 的速度增长，带来的是计算量和所需的KV\nCache以平方的速度增长。</p>\n<p>（我们知道计算量/缓存和长度 <span class=\"math inline\">\\(s\\)</span>\n成平方关系，这里放一些更具体的推算细节，已经熟悉的朋友可以跳过）</p>\n<p>（1）计算量</p>\n<p>对于两个这样大小的矩阵相乘： <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span> ，输出矩阵大小为 <span class=\"math inline\">\\([m,p]\\)</span>，共有 <span class=\"math inline\">\\(m\\times p\\)</span> 个元素，每个元素需要 <span class=\"math inline\">\\(n\\)</span> 次乘法和 <span class=\"math inline\">\\(n\\)</span> 次加法，因此一次矩阵乘法有 <span class=\"math inline\">\\(2mpn\\)</span> 个floating point\noperations（FLOPs）。</p>\n<p>计算量上，按<a href=\"https://arxiv.org/pdf/2203.15556.pdf\">《Training\nCompute-Optimal Large Language Models》</a>的算法来。</p>\n<p>对于一般MHA，输入长度为 <span class=\"math inline\">\\(s\\)</span>\n，层数为 <span class=\"math inline\">\\(L\\)</span> ，模型hidden size为\n<span class=\"math inline\">\\(d_{model}\\)</span> ，每个头的维度为 <span class=\"math inline\">\\(d_{q}\\)</span> ， 头的数量为 <span class=\"math inline\">\\(n_{q}\\)</span>（这里假设有 <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\n），各个operation的FLOPs如下</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPs（MHA）</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n3\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax项中，对一个 <span class=\"math inline\">\\([1,s]\\)</span>\n的向量做softmax，计算量为 <span class=\"math inline\">\\(3s\\)</span> （一个\n<span class=\"math inline\">\\(s\\)</span> 是算每个元素的exp，一个 <span class=\"math inline\">\\(s\\)</span> 是求和算分母，一个 <span class=\"math inline\">\\(s\\)</span> 是算除法），而对 <span class=\"math inline\">\\([s,s]\\)</span> 的矩阵做softmax，则计算量为 <span class=\"math inline\">\\(3s^2\\)</span> ，每个头都要计算一遍，因此再乘以\n<span class=\"math inline\">\\(n_{q}\\)</span> 。</p>\n<p>（这里忽略了其他一些operation，比如scaling，dropout等，有兴趣的朋友可以自己推算一下）</p>\n<p>顺便算下对于Mistral 7B这样使用了GQA的情况。</p>\n<p>其实只有第一项的KV有变化，其他都没变。假设kv头的数量为 <span class=\"math inline\">\\(n_{kv}\\)</span>，则有</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPs（GQA）</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>从上面的推算可以看到QK logits、Softmax和Reduction三项是和长度 <span class=\"math inline\">\\(s\\)</span> 成平方关系的，其他则是线性关系。</p>\n<p>（2）缓存</p>\n<p>KV Cache需要缓存的参数量为</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>如果使用的是半精度浮点数，那么总共所需的空间就是</p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>对于Mistral 7B，在输入长度为16k的情况下，所需的KV_Cache约为2G。</p>\n<p>看来虽然用了GQA，但是在长文本（16k+）的情况下计算量和显存还是颇有压力。</p>\n<h2 id=\"swa思路\">SWA思路</h2>\n<p>看来要提升attention计算效率，需要想办法减小上面推算中的 <span class=\"math inline\">\\(s\\)</span> ，但是怎么在减小 <span class=\"math inline\">\\(s\\)</span>\n的同时，还能保持模型长上下文的理解和生成能力呢？</p>\n<p>来看一下，CNN中的感受野</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>如上图，假设模型有3层，每层卷积核大小为 <span class=\"math inline\">\\(3\\times 3\\)</span>\n（实际上CNN里卷积操作就是一个sliding window）。</p>\n<p>那对于layer 3，每一个像素能看到layer 2中的一个 <span class=\"math inline\">\\(3\\times 3\\)</span> 的区域，layer\n2中其他较远的像素就看到不了。</p>\n<p>但我们再往前推，layer 2里的每个像素也可以看到layer 1中的一个 <span class=\"math inline\">\\(3\\times 3\\)</span> 区域，那么layer 2中的 <span class=\"math inline\">\\(3\\times 3\\)</span> 区域就可以看到layer 1中一个\n<span class=\"math inline\">\\(5\\times 5\\)</span> 的区域，相当于layer\n3中一个像素可以<u><strong>间接</strong></u>看到一个 <span class=\"math inline\">\\(5\\times 5\\)</span> 的输入。</p>\n<p>以此类推，如果我们再增加一层layer 4，那么layer\n4中一个像素就能获取输入层（layer 1） 一个 <span class=\"math inline\">\\(7\\times 7\\)</span> 区域的信息。</p>\n<p>虽然每层只能多看周围一格的信息，但是只要我们层数够多，理论上靠近输出端的层想看多远就能看多远。</p>\n<p>值得注意的一点是，我们一般认为模型低层部分提取比较基础的特征，而高层会提取高级的语义特征。</p>\n<p>在CNN里，前几层提取的可能更多是关于简单的边界、颜色、形状等基础特征，而后面的层则提取较复杂的语义特征，比如在分类任务中会是和分类类别相关的花纹、物体大小、风格等特征。</p>\n<p>如果我们把模型设计成，最后一层的一个像素刚好要到第一层才能接收到全局信息（在其它层都只能看到局部），那对于图像边缘的语义特征识别能力可能会受到一些限制。</p>\n<p>具体来说，假设我们做猫和狗的图像分类任务，如果这个时候决定性的特征出现在图像最边缘几个像素里，那这种情况下的错误率会比特征出现在图像中间时要高。</p>\n<p>而对于语言模型，一般情况下，越远距离的信息，对当前位置的重要性越低，因此只要我们的窗口大小不要太过极限小，问题应该都还不大。</p>\n<p>看下Mistral的SWA具体是怎么做的</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>左边是正常的causal\nattention，每个位置能看到自己和前面的位置，attention\nmask是个下三角矩阵。</p>\n<p>中间则是SWA的attention\nmask，这里的窗口大小为3。包括自己在内，每个位置只能往前看3个输入。</p>\n<p>同CNN的感受野一样，随着层数的堆叠，模型理论上能处理的最远距离也逐层线性递增。只是LLM里递增的方向是单向的，只能往前。</p>\n<p>Mistral 7B使用了4096的窗口大小，模型层数为32，则最终输出的”感受野“为\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n达到131k的长度。</p>\n<p>前面我们推算了attention的计算量，其中QK\nlogits、Softmax和Reduction三项是和长度 <span class=\"math inline\">\\(s\\)</span>\n成平方关系。在使用了SWA之后，理论上，这几个operation仅使用4k的计算量，就能获得131k的上下文效果。当输入长度为131k时，除去已经缓存部分的数值，新的输入计算量相差\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> 倍。</p>\n<p>而缓存和上下文长度 <span class=\"math inline\">\\(s\\)</span>\n成线性关系，当上下文长度为131k时，最大也能节省 <span class=\"math inline\">\\(31/32\\)</span> 的显存。</p>\n<p>即SWA在上下文长度在4k以下时，和普通causal\nattention一样；当上下文长度超过4k时，则相对节省资源，长度越大，节省的比例越高。</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>实际使用中，Mistral通过把SWA实现在FlashAttention和xFormers中，对于16k的上下文长度，获得了2倍的速度提升。</p>\n<h2 id=\"和kv-cache的配合实现\">和KV Cache的配合实现</h2>\n<p>在不使用sliding window的情况下，随着自回归推理的进行，KV\nCache是只增不减的。</p>\n<p>而在使用SWA的情况下，超出窗口长度的kv就可以不用再缓存了，因此使用一个轮转替换的策略。</p>\n<p>比如窗口大小 <span class=\"math inline\">\\(W=4\\)</span>\n，则当第5个token需要缓存是，直接替换掉第1个token，这样就可以保持kv缓存有一个最大值（为窗口大小），而不会无限增长。</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>这样便于我们估计硬件设备所能支持的throughput，也不会因为少量超长的case而造成堵塞，在工程上有利于提高硬件利用率，降低成本。</p>\n<h2 id=\"长prompt的分块\">长Prompt的分块</h2>\n<p>更近一步，考虑到我们使用RAG或者funciton\ncall的时候，都会使用比较长的，固定的prompt来知道模型的行为。</p>\n<p>比如GPT4就被诱导说出它接收到的长system\nprompt（当然未必真的就是OPENAI用的）</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: 【{message idx}†{link text}】. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>除了预先计算好system\nprompt的kv值，并保存在缓存中方便每次用户输入使用外，如果system\nprompt很长（比sliding window大），还可以通过对system\nprompt的kv值进行切分来进一步优化计算。</p>\n<p>比如窗口大小 <span class=\"math inline\">\\(W=4\\)</span>，system\nprompt大小为9时，就可以把system prompt的kv缓存切成 [4,4,1] 三块。</p>\n<p>第一块由于和当前的输入距离超过了一个window的大小，所以是完全看不见的，对应的attention\nmask全为0，因此可以完全忽略。</p>\n<p>第二块的attention\nmask则是一个上三角矩阵，当前的输入需要用到这部分信息。</p>\n<p>第三块是一个下三角矩阵（的左边部分），包含了当前的输入在内。</p>\n<p>在推理的时候，我们只需要用到第二块和第三块的内容，这就节省了缓存的操作。</p>\n<p>而且无论prompt有多长，只要我们按窗口大小分块，一定只会用到最后两块。</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>（实际上现在推理框架基本上都有FlashAttention/PagedAttention等技术加持，能够进一步节省资源，提高效率，这个后续再开一篇讲）</p>\n<p>Mistral\n7B整体的效果上的效果相比Llama是有优势的，部分任务甚至超过了Llama\n34B。</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral认为大语言模型压缩知识的能力实际超过我们的认知，7B这个规模的效果还有提升空间。</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWA实际上是一种sparse attention，而sparse\nattention也有许多工作做了深入探索。</p>\n<p>这里简单说一小部分，有机会再完整梳理一遍sparse\nattention的理论和实践。</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>前面提到，Mistral并不是第一个使用SWA的。</p>\n<p>2020年，<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">《Longformer:\nThe Long-Document Transformer》</a>就提出包含SWA在内的一系列sparse\nattention的做法。</p>\n<p>从文章名字就看到出来，Longformer主要目的也是为了解决长上下文的问题。</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>上图中的（b）就是SWA，只是用在Bert中的时候它是双向的。</p>\n<p>在SWA的基础上，还可以进行空洞滑窗（dilated sliding\nwindow），在不增加计算量的情况下，提升感受野。这也是从空洞卷积（下图）来的灵感了。</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>还可以更进一步优化attention。无论是SWA还是dilated sliding\nwindow，每个位置都只能看到局部的信息。</p>\n<p>但是实际上有些位置就是对全局信息有很高的需求。</p>\n<p>在Bert中，[CLS]\ntoken就常常作为分类token或者相似度向量使用，这种情况下就需要它能获取整个上下文的完整信息。</p>\n<p>而在GPT中，instruction，或者说prompt的部分也对全局信息有更高要求，因为我们希望在整个对话过程中，模型都能遵循我们给出的规则。</p>\n<p>对于这些token，我们让它可以看到其他所有位置，使用完整的global\nattention，而其他位置则使用sliding window，如（d）中所示。</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>无独有偶，同样在2020年，和Longformer差不多在同一时期，也有另外一个通过sparse\nattention来优化长文本效果的工作，<a href=\"https://arxiv.org/abs/2007.14062\">《Big Bird: Transformers for\nLonger Sequences》</a>。</p>\n<p>其中sliding window和global attention结合的思路和Longformer相似。Big\nBird还额外加入了一个random attention的做法。</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p>上图中 <span class=\"math inline\">\\(r=2\\)</span>\n即每个位置使用2个随机注意力。</p>\n<h1 id=\"小结\">小结</h1>\n<p>SWA在优化长上下文的计算效率上有明显的收益。而在模型效果上，目前基本没有看到不可接受的损失。对长上下文有需求的业务，值得探索。</p>\n<p>除了SWA，sparse\nattention还有许多其他探索。目前来看，这些做法都有一定的理论基础，效果也不错。但是阻碍这些方案大规模使用的一个原因就是<big><strong>工程实现</strong></big>，比如如何高效计算global\n+ local attention，在flash attention中能够支持random\nattention，这都是要考虑的内容。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n【2】Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n【3】Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n【4】GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n【5】Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n"},{"title":"transformer中normalization的二三事","abbrlink":"6a40bfa5","date":"2024-03-19T13:06:12.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\nNormalization在模型中，相对于attention这种经常被魔改的结构，受到的关注度似乎没那么高，但它对模型能否顺利训练，却有很关键的作用。  \n\n在此简单梳理下normalization相关的背景和内容，也分析一下在transformer发展上的相关内容。  \n\n这部分内容感觉目前还有些存在争议的地方，如果有不同意见欢迎讨论。  \n\n# why normalization\n\nnormalization，也叫「归一化」、「正则化」、「规范化」、「标准化」等，可以说已经是神经网络不可以或缺的一环。  \n\n使用的话，现在基本只需几行代码就能实现。但要用得好，还是需要了解一下它作用的机制。  \n\n## 从输入数据看normalization\n\n假设我们有一个二元损失函数 $Loss(x_1,x_2)=x_1^2+x_2^2+b$ ，那在三维空间画出来的损失平面大概是这样  \n\n{% asset_img lossfunc_surface.jpeg loss function surface %}  \n\n在这样一个平面上，使用梯度下降法，梯度方向是垂直于当前位置等高线的切线方向的。  \n\n如果这个损失函数的等高线是一系列完美的同心圆，那么无论我们起点在哪里，梯度下降的时候都会以垂直切线方向，沿着圆心一路奔去。  \n\n这种情况下优化很快，控制好学习率不要跳过minimum就可以（也可以用自适应优化器来控制速度）。  \n\n但是实际上我们的损失平面很难那么完美。损失函数的等高线更可能是个椭圆（或者更复杂的形状）。  \n\n{% asset_img ellipse_1.png ellipse %}  \n\n这样我们梯度下降是方向就要经常修正，训练效率就会受影响。  \n\n如果这个椭圆很扁或者我们的训练参数不太好，可能会出现反复震荡收敛缓慢的情况。  \n\n{% asset_img ellipse_2.png ellipse %}  \n\n损失在这个狭窄的山谷中反复横跳。  \n\n那损失函数等高线什么时候会是椭圆形？  \n\n假设我们现在有两个输入变量，以米为单位的身高 $x_{1}$，和以元为单位的月工资收入 $x_{2}$。（这里对量纲的使用也会改变数值，如米->厘米）  \n\n如果我们用这两个自变量训练模型，我们会发现，身高取值范围基本是在0.x米~2.x米，而工资的取值范围是0到几百几千几万或者几十万以及更多。  \n\n而模型的一个主要操作就是对输入特征进行线性组合。  \n\n这时模型的输出值会更大地受到 $x_{2}$ 的影响，因为它的变化更大，取值范围也更大。  \n\n这时损失函数在不同变量维度的变化速度相差很多，损失函数就会出现椭圆形等高线的情况。  \n\n既然由于量纲和取值范围的问题，会导致训练困难，那最直接方法就是规定一个标准范围，所有输入变量，不管原来是什么范围，现在都归一化到标准范围里来。  \n\n这就是最朴素的输入normalization的思想。  \n\n输入的normalization有很多种做法  \n$$x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}\\sigma$$  \n\n其中 $\\mu$ 为均值，$\\sigma$ 为方差。  \n\n第三种，均值方差归一化，也叫Z-score normalization，应该是我们用得比较多的。  \n\n这样我们通过对输入进行一些操作，把「椭圆」拉成了「圆」，解决输入参数范围带来的一些训练问题。  \n\n除了针对均值、方差、最大值、最小值的归一化，对输入还有一些其他的处理，如PCA等，就暂不展开。  \n\n## 缓解ICS...吗？\n\n机器学习里有一个叫i.i.d.（independent and identical distribution，独立同分布）的假设：独立，每次抽样之间是没有关系的，不会相互影响；同分布，即每次抽样，样本都服从同样的一个分布。  \n\n为什么需要i.i.d.？  \n\n由于机器学习依赖于使用现有数据来训练模型，进而对未来的数据做出预测和模拟，因此这一过程本质上是在历史数据的基础上，通过模型来推测未来的数据走向。  \n\n这就要求我们使用的历史数据必须具备整体的代表性。以便从现有数据（经验）中提炼出规律，对未知数据进行决策。  \n\n如果用于训练的数据缺乏总体代表性，即仅代表特殊情况，那么得出的规律可能不准确或错误，因为这些规律是基于个别案例推测出来的，就不具备泛化性。  \n\n当然并不是所有机器学习都需要i.i.d.，但是有i.i.d.的话，可以简化很多事情，让模型学习起来更容易快速。  \n\n对于输入，通过合理的抽样和处理（前面提到的PCA就可以用来解耦特征间的关联，达到“独立”的效果），我们可以得到输入的i.i.d.的条件，但这只是针对输入。  \n\n在多层的神经网络中，上一层的输出会作为下一层的输入。  \n\n而在训练过程中，由于上层的模型参数在不断学习变化，则上层输出的分布也在不断变化，靠后的层实际上要学习不断的变化的分布，这就很不i.i.d.，那靠后面的层的学习速度和效果就会收到影响，调参也变得困难，模型也难以加深。  \n\n这个问题就是ICS，internal covariate shift。  \n\n那有没有办法保证上一层的分布不要变化呢？  \n\n一个「可能」的方案就是normalization。我们通过把上一层的输出映射到一个固定的分布上，来稳定给下一层的输入，这样就降低了学习难度。  \n\n但也有一些工作表明normalization（batchnorm）的作用机制和ICS的关系并不大，这个观点下面在batchnorm部分说。  \n\n当然ICS的问题也可以通过改变初始化策略、调控训练超参如学习率等方法来优化，但是这样做的效率并不是很高。  \n\n## 远离激活函数饱和区\n\n神经网络中还有一个重要组件，非线性激活函数，比如常用的sigmoid。\n\n{% asset_img sigmoid.png sigmoid %}  \n\n当输入 > 6 或者 < -6 的时候，sigmoid函数的梯度已经变得非常小，也就是进入了饱和区。  \n\n这种情况下训练就变得困难。  \n\nICS就会加剧梯度消失的情况。在没有normalization的情况下，分布不断变化，后面层的参数变化激烈，导致输出值更容易进入到左右两端，更容易进入到激活函数的饱和区。  \n\n而normalization能把部分输出值拉回到梯度正常的范围内，一定程度缓解了梯度消失的问题，使训练可以正常进行下去。  \n\n# batchnorm\n\n神经网络中使用的normalization有很多种，这里不一一展开，只梳理一下最重要的batchnorm和layernorm两类。  \n\n## batchnorm算法\n\n假设输入数据的形状是 $[B,C]$ ，其中 $B$ 是batch size，$C$ 是特征向量维度。  \n\n这 $C$ 个输入特征每个都有不同的含义，如我们前面的例子，第一个元素可能是身高，第二个元素可能是月收入，因此做normalization的时候这 $C$ 个特征分别来做。  \n\n具体来说，对于第 $i$ 个特征维度，首先计算整个batch内的均值  \n\n$$\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n$$  \n\n再计算这个维度上的方差  \n\n$$\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n$$  \n\n得到均值和方差之后，对batch内维度上的所有值进行Z-score normalization  \n\n$$\nx_{i,j}'=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n$$  \n\n其中 $\\epsilon$ 是为了防止分母为0。这个在实际代码中挺重要的，忘记加可能会出问题。  \n\n经过这样的变换之后，在 $C$ 个特征维度上就是均值为0，方差为1的分布了。  \n\n但是到这还没结束。  \n\n每个维度的数值全部归一化之后，对于激活函数来说，更集中在中间的部分，而这部分的非线性特征并不强（比如上面的sigmoid），这样非线性激活层近似了一个线性变换，这样就降低了模型的学习能力。  \n\n且无论输入是什么，最终输出都会被强行拉到这样一个“平均”的值，也极大抑制了模型的表达能力。  \n\n所以为了保证模型的能力，也保证非线性能力的获得，对每个特征，又增加两个可学习的参数， 缩放参数 $\\gamma$ 和位移参数 $\\beta$ 。  \n\n$$\ny_{i,j} = \\gamma_{i} x_{i,j}' + \\beta_{i}\n$$  \n\n这样每个特征值就有机会从“线性区”移动到“非线性区”，把被归一化削弱的非线性能力找了回来。  \n\n并且通过这样一个归一化再重新缩放移动的操作，解耦了上层输出分布和下层输入，本来下层参数要去适应上层分布变化，现在只需要通过每个batchnorm层中的 $\\gamma$ 和 $\\beta$ 直接学习就行了，训练变得简单了。\n\n[《Batch Normalization: Accelerating Deep Network》](https://zhuanlan.zhihu.com/p/340856414)给出的算法如下  \n\n{% asset_img bn_algo.png batch norm %}  \n\n## CNN中的batchnorm  \n\nbatchnorm最主要的应用还是在CNN模型中。  \n\n假设CNN中feature map的size是 $[B,C,H,W]$ ，其中 $B$ 是batch size，$C$ 是channel数（也是卷积核数量），$H$ 和 $W$ 分别是特征图的高和宽。  \n\n如果按照前面batchnorm的算法，那应该有 $C\\times H\\times W$ 组特征，每组特征有 $B$ 个，对每组内的 $B$ 进行归一化，再进行放缩和平移。  \n\n但是实际上，CNN中卷积是一个滑动窗口，对于同一个channel下的 $H\\times W$ 个特征值其实都来自于同一个卷积核的计算，这 $H\\times W$ 也属于一个“batch”，它们要放在一起进行归一化。  \n\n也就是对于卷积核来说，真正的batch数是 $B\\times H\\times W$ ，而只有 $C$ 组特征值，因此也只有 $C$ 个 $\\gamma$ 和 $\\beta$ 。  \n\nbatchnorm原文中，batchnorm放在了relu后面，作者认为这样使得进入激活函数的分布会更加稳定，顺便对于fc层，由于batchnorm和fc都有bias项，还可以省略掉其中一个而不影响效果。  \n\nbtw，一般来说，batchnorm初始化的时候，把 $\\gamma$ 设为1（不缩放），把 $\\beta$ 设为0（不平移），在训练中让模型从相当于没有batchnorm开始慢慢学习这两个参数。  \n\n## 训练和推理  \n\n现在我们知道在训练时，batchnorm对一个mini-batch计算均值和方差来进行归一化，再进行缩放和移动。  \n\n$\\gamma$ 和 $\\beta$ 属于模型学出来的参数，只要训练结束这两个向量就固定了，在推理的时候直接使用即可。  \n\n但是推理时，均值和方差怎么计算呢。推理的时候可能是一个sample，也可能是任意个sample作为一个batch，和训练的时候一样计算肯定不合适。  \n\n我们需要在训练的时候就为推理做准备：训练的时候，模型会遍历整个训练集，因此理论上可以统计出整个训练集的均值和方差，然后把这个大量样本统计出来的均值和方差当做真实分布的均值和方差，在推理的时候使用。（回想i.i.d.）  \n\n当时又有一个问题，训练集可能会很大，有百万甚至千万的数据，在训练的数据记录下所有层所有特征来计算均值和方差显然效率不高，因此用一个近似的方法：\n\nmoving_mean = momentum × moving_mean + (1.0 − momentum) × mean  \n\nmoving_var = momentum × moving_var + (1.0 − momentum) × var  \n\n通过把多个batch的均值和方差进行移动平均的方式来逼近整个训练集的均值和方差。  \n\nmomentum为动量参数，在 TF/Keras 中，该值为0.99，在 Pytorch 中，这个值为0.9。  \n\n小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动。  \n \n大的momentum值对应慢的更新速度，如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。  \n\n如果batch size比较小，每个mini batch和全局差异较大，就不应该用太大的momentum。  \n\n理论上，训练步数越长是会越靠近真实分布的，实际上，因为每个batch并不能代表整个训练集的分布，所以最后的值是在真实分布附近波动。\n\n这里还引入另外一个问题，如果batch size太小，每个mini batch统计的均值和方差和全局的值偏差相对会比较大，对模型收敛的稳定性有影响，因此一般来说，使用batchnorm的话，batch size不能太小，如下图  \n\n{% asset_img bs_bn.png batch size的影响 %}  \n\n小结一下，batchnorm的优点是解耦了上层输出和下层输入的分布，既缓解了进入激活函数饱和区带来的梯度消失的情况，又保留了模型的表达能力。每一层的输入尺度相对固定，提供了更好的尺度不变形，使模型训练更稳定。    \n\n同时每个batch分别进行归一化，相当于引入了一些随机噪音，使得模型不容易过拟合到某些微小的特征上，相当于进行了一定的正则化，将损失平面变得相对平滑。\n\n但是同时也引入了新的超参（如momentum），另外也依赖batch size的大小，过小的batch size可能会带来问题。\n\n## batchnorm起作用的真正原因？\n\n虽然batchnorm原文认为batchnorm在一定程度上是缓解了ICS，但是2018年的《How Does Batch Normalization Help Optimization?》提出了不同观点。  \n\n为了探究batchnorm的效果，是否是因为优化了ICS（或者说和优化了ICS有多大关系），做了一个这样的实验：在batchnorm后面又通过加入随机噪音来引入“covariate shift”，并和没有加噪音，以及没有加batchnorm的模型效果进行对比，如下图  \n\n{% asset_img bn_ics.png ICS %}  \n\n结果发现，即使人工加强了ICS的情况，但是只要用了batchnorm，效果依然比不用好；而人工引入ICS的模型，在效果上并没有多大影响。  \n\n这就说明缓解ICS并不是batchnorm有效的真正原因。  \n\n那batchnorm到底有没有缓解到ICS呢？  \n\n要测量ICS的变化，就要先定义ICS。  \n\n对于网络中的每一层，ICS被定义为在前一层参数更新后，当前层输入分布的变化。这种变化可以通过比较更新前后的梯度来量化。\n\n{% asset_img ics_define.png ICS 定义 %}  \n\n具体来说，对于每一层i，作者们计算了以下两个梯度之间的L2范数差异：\n\n$G_{t,i}$ ，在时间t，使用当前所有层的参数（包括前一层的参数）计算的梯度。  \n\n$G_{t,i}'$ ，在时间t，使用更新后的前一层参数计算的梯度，而其他层的参数保持不变。  \n\n这个差异直观上表明了「上一层参数变化，下一层需要在多大程度上来变化，以适应新的分布」。  \n\n理想来说，ICS越小，上一层参数更新对当前层的分布影响越小，梯度变化程度应该越小。\n\n{% asset_img ics_measure.png ICS measure %}  \n\n但是从结果上来看，使用了batchnorm并不能有效减少这个变化，甚至还有所增加。  \n\n这也说明batchnorm实际上并不能真正缓解ICS的情况。  \n\n那batchnorm起效果的真正原因是什么？  \n\n作者认为主要是batchnorm使得损失函数更加平滑，直观上来说就是减少了很多坑坑洼洼的位置，使得训练更不容易陷入到局部最小值中去。  \n\n# layernorm\n\n看完batchnorm，再来看layernorm。  \n\n## 理解layernorm\n\nlayernorm，不要被名字骗了，这里的layer指的不是模型的层，而是数值的layer。  \n\n对于二维的输入，batchnorm实在batch维度上做归一化，而layernorm是在特征维度做归一化  \n\n{% asset_img bn_and_ln.png bn和ln %}  \n\n对于非NLP数据而言，相比batchnorm，layernorm归一化的维度似乎解释性没那么强。batchnorm对同一个特征，比如身高计算均值是有意义的，而layernorm在不同的特征，比如身高、工资、温度做归一化，好像并没有可靠的物理意义。  \n\nlayernorm最主要的应用就是NLP的模型，包括RNN和transfomrer模型。  \n\n在transformer中，一般输入的形状是 $[B,S,H]$ ，$S$ 是序列长度，每个样本的长度可能不同，因此在这个维度需要使用padding（一般是zero-padding）来把batch内的数据处理成一样长。  \n\n比如这样一批文本输入\n\n```\n我  爱  中  国\n你  好\n谢  谢  你\n```\n\n为了使模型能够统一处理，会pad成\n\n```\n我  爱  中  国\n你  好  [P] [P]\n谢  谢  你  [P]\n```\n\n一般来说，我们认为由于有padding的存在，做batchnorm并不合适。  \n\n比如上面的例子，对“中”，“[P]”，“你”做归一化，由于 [P] 的存在，实际的batch size只有2，并且和 [P] 做normalization也对训练没什么帮助。  \n\n而且对于文本数据，batch内同一个位置上的字是不同的，对完全没有关系的字进行归一化也并没有什么意义。  \n\n也就是说，哪怕没有 [P] 的存在，比如对第一个token“我”，“你”，“谢”做归一化，直觉上也不太有物理意义。  \n\n因此使用layernorm，在 $H$ 维度上进行normalization，同时有 $H$ 个$\\gamma$ 和 $\\beta$ 需要学习。  \n\n相当于计算每一句输入内，每个token所有特征之间的均值和方差来进行归一化。  \n\n## 为什么transformer用layernorm  \n\n和batchnorm不同的是，由于layernorm不需要再batch维度上计算均值和方差，所以不存在训练和推理的时候不一样的地方，不用保存一个全局的均值和方差供推理的时候使用。  \n\n而由于layernorm和batch无关，也就不会受到batch size大小的影响。  \n\n除了以上的原因，也有一些工作深入探究了在nlp任务上layernorm和batchnorm的区别。  \n\n如《PowerNorm: Rethinking Batch Normalization in Transformers》就研究了transformer中BN为啥表现不太好。  \n\n研究了训练中的四个统计量：batch的均值和方差，以及他们的梯度的均值和方差。对于batch的均值和方差，计算了他们和running statistics（就是用移动平均法累积的均值和方差，见前面的文章）的欧氏距离。发现NLP任务上（IWSLT14）batch的均值和方差一直震荡，偏离全局的running statistics，而CV任务也相对稳定。  \n\n对于他们梯度的均值和方差，研究了其magnitude（绝对值），在CV任务上震荡更小，且训练完成后，也没有离群点。  \n\n总结来说，transformer中BN表现不太好的原因可能在于CV和NLP数据特性的不同，对于NLP数据，前向和反向传播中，batch统计量及其梯度都不太稳定。  \n\n更重要的是，实际效果就是layernorm在NLP的效果比batchnorm好，效果好，这是最重要的原因。  \n\n## RMSnorm\n\n19年《Root Mean Square Layer Normalization》提出了normalization变体RMSnorm，主要针对layernorm来改进。  \n\n简单地说，RMSnorm就是在标准layernorm的基础上，省略了平移，只进行缩放。  \n\n{% asset_img rmsnorm.png RMSnorm %}  \n\n作者认为标准layernorm计算效率并不高  \n\n{% asset_img rmsnorm_eff.png RMSnorm效率 %}  \n\n作者用一个GRU模型做实验，对比是否添加layernorm的结果，发现在相同时间和相同步骤下，有layernorm的模型，都没有无layernorm的模型收敛得快。  \n\n并且layernorm的平移对梯度方差的减小没有贡献，因此作者直接舍弃了中心化和平移两步，只对数据进行方差归一化和缩放。  \n\n更近一步，作者提出pRMSnorm，只对数据中前p%的数值进行处理，这样就能进一步加速训练，而效果也基本不太受影响。  \n\n{% asset_img prmsnorm.png prmsnorm %}  \n\nRMSnorm现在被很多主流的大模型所采样了。  \n\n# post-norm & pre-norm\n\n## 二者对比\nlayernorm在模型里放哪也有讲究。  \n\n原始的transformer模型使用的post-norm，而《On Layer Normalization in the Transformer Architecture》则认为pre-norm更好。  \n\npost-norm和pre-norm分别是下面这样\n\n{% asset_img postnorm_prenorm.png postnorm and prenorm %}  \n\npost-norm是在残差和主干相加之后进行归一化，而pre-norm则是在主干先归一化再和残差相加。  \n\npost-norm和pre-norm对比，目前大家比较接受的结论是，pre-norm更容易训练，因此可以叠加更多的层，但是在层数不是特别多的情况下，post-norm最终的收敛效果会比pre-norm要好。  \n\n模型中，第 $l$ 层的输出是第 $l+1$ 层的输入，对于post-norm有  \n\n$$\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n$$\n\n而对于pre-norm则是  \n\n$$\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n$$\n\n参考苏剑林在《为什么Pre Norm的效果不如Post Norm？》中的分析，认为 $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$ 的方差，由于有norm的存在，是不随层数变化的。  \n\n当 $l$ 比较大时，$x_{l}、x_{l+1}$ 的差距较小，因此 $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$ 和 $\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))$ 的差距也很小，这时有  \n\n$$\\begin{aligned}\n&\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1})) \\\\\n&{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right) \\\\\n&=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}$$\n\n相当于 $l$ 层和 $l+1$ 层的效果接近于一个更宽的 $l$ 层的效果。  \n\n也就是使用pre-norm的时候，模型的深度有水分，表面看起来有 $l$ 层，实际在效果上，等效于post-norm的浅层模型。  \n\n从模型结构上看，恒等分支永远有一部分不用经过normalization，这部分能够直接把梯度回传到最前面，这也是pre-norm能够训练“层数更多”的模型的原因--缓解了梯度消失。  \n\n正常来说，模型深度对最终效果的影响，是大于模型宽度的。  \n\n而post-norm，在残差分支之后做归一化，对参数正则化的效果更好（loss平面更平滑），且它每norm一次就削弱一次恒等分支的权重，所以post-norm相对pre-norm，是更突出残差分支的，因此它的层数更加“足秤”，训练好之后效果更优。  \n\n## 和warmup的关系  \n\n《On Layer Normalization in the Transformer Architecture》（认为pre-norm更好）还分析指出，使用post-norm的transformer，在初始化时候，靠近输出层的部分梯度期望很大，所以模型在开始训练的时候很依赖warmup的策略，通过缓慢提升学习率来稳定训练过程。  \n\n使用warmup引入了新的超参，调参更为麻烦点。  \n\n而实验表明，使用pre-norm的transformer在不需要warmup的情况下，也能收敛到post-norm+warmup的相同水平，而post-norm不加warmup效果就差点了。  \n\n{% asset_img warmup_effect.png warmup影响 %}  \n\n## Deepnorm  \n\n2022年，《DeepNet: Scaling Transformers to 1,000 Layers》对transformer训练不稳定的原因进行了深入分析，发现模型更新过大是导致不稳定的主要原因。  \n\n为了解决这个问题，他们提出了Deepnorm，可以限制模型更新的大小。  \n\n{% asset_img deepnorm.png deepnorm %}  \n\n其中 $\\alpha>1$ 是根据模型参数定的常数。这里相比post-norm提升了恒等分支的权重，使训练更容易进行。  \n\n另外，还用了一个 $\\beta$ 参数，把 $G_{l}$ 中的模型参数进行了缩小，以此来稳定模型的训练。  \n\n实验结果上，deepnorm结合了pre-norm的容易训练，和post-norm的收敛效果好的特点，能够把百层、浅层的模型训到比较好的效果。  \n\n{% asset_img deepnorm_result.png deepnorm result %}  \n\n参数过程相比post-norm稳定了很多。  \n\n## Realformer--residual attention  \n\npost-norm和pre-norm实际上改变的是模型残差分支和恒等分支怎么排布的问题，而《RealFormer: Transformer Likes Residual Attention》则提出了另外一种做法  \n\n{% asset_img realformer.png realformer %}  \n\nRealFormer的核心是在其标准Transformer编码器的每一层中引入了残差连接。这些残差连接将前一层的原始注意力分数（即在应用Softmax之前的分数）与当前层计算出的注意力分数相结合。这样做的结果是，当前层的注意力分数在计算时会考虑到前一层的信息。\n\n每个多头注意力模块都会接收来自前一层的残差注意力分数作为额外输入。这意味着每个注意力头不仅考虑了当前层内的输入序列，而且还直接利用了前一层的注意力信息。\n\n{% asset_img realformer_attention.png realformer attention %}  \n\n其中 $Prev'$ 是来自上一层softmax之前的权重矩阵（多头注意力的话，则是对应的头的值），而 $\\frac{Q^{\\prime}K^{\\prime T}}{\\sqrt{d_k}}+Prev'$ 则是传给下一层的attention的。  \n\n# 小结\n\n本篇粗略梳理了一下关于normalization，batchnorm，以及layernorm在transformer的一些使用情况。\n\n目前主流的大模型使用的是rmsnorm + prenorm，也有使用其他变体的。  \n\n关于normalization，依然留有一些探索空间。    \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n往期文章\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)\n\n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)\n\n***\n\n# Reference  \n【1】https://www.zhihu.com/question/487766088  \n【2】Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization https://arxiv.org/abs/2001.06838  \n【3】Transformer中的归一化(一)：什么是归一化&为什么要归一化 https://zhuanlan.zhihu.com/p/476102712  \n【4】Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://arxiv.org/abs/1502.03167  \n【5】How Does Batch Normalization Help Optimization? https://arxiv.org/abs/1805.11604  \n【6】Batch Normalization: Accelerating Deep Network https://zhuanlan.zhihu.com/p/340856414  \n【7】Layer Normalization https://arxiv.org/abs/1607.06450  \n【8】详解深度学习中的Normalization，BN/LN/WN https://zhuanlan.zhihu.com/p/33173246  \n【9】Transformer中的归一化(四)：BatchNormalization的原理、作用和实现 https://zhuanlan.zhihu.com/p/481277619  \n【10】Layer Normalization https://arxiv.org/abs/1607.06450  \n【11】PowerNorm: Rethinking Batch Normalization in Transformers https://arxiv.org/abs/2003.07845  \n【12】Root Mean Square Layer Normalization https://arxiv.org/abs/1910.07467  \n【13】On Layer Normalization in the Transformer Architecture https://arxiv.org/abs/2002.04745  \n【14】为什么Pre Norm的效果不如Post Norm？ https://spaces.ac.cn/archives/9009  \n【15】Understanding the Difficulty of Training Transformers https://arxiv.org/abs/2004.08249  \n【16】RealFormer: Transformer Likes Residual Attention https://arxiv.org/abs/2012.11747  \n【17】DeepNet: Scaling Transformers to 1,000 Layers https://arxiv.org/abs/2203.00555  \n\n","source":"_posts/cs/nlp/2024/03/关于Transformer的normalization.md","raw":"---\ntitle: transformer中normalization的二三事\nabbrlink: 6a40bfa5\ndate: 2024-03-19 21:06:12\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - layernorm\n  - post-norm\n  - pre-norm\n  - normalization\n  - batchnorm\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\nNormalization在模型中，相对于attention这种经常被魔改的结构，受到的关注度似乎没那么高，但它对模型能否顺利训练，却有很关键的作用。  \n\n在此简单梳理下normalization相关的背景和内容，也分析一下在transformer发展上的相关内容。  \n\n这部分内容感觉目前还有些存在争议的地方，如果有不同意见欢迎讨论。  \n\n# why normalization\n\nnormalization，也叫「归一化」、「正则化」、「规范化」、「标准化」等，可以说已经是神经网络不可以或缺的一环。  \n\n使用的话，现在基本只需几行代码就能实现。但要用得好，还是需要了解一下它作用的机制。  \n\n## 从输入数据看normalization\n\n假设我们有一个二元损失函数 $Loss(x_1,x_2)=x_1^2+x_2^2+b$ ，那在三维空间画出来的损失平面大概是这样  \n\n{% asset_img lossfunc_surface.jpeg loss function surface %}  \n\n在这样一个平面上，使用梯度下降法，梯度方向是垂直于当前位置等高线的切线方向的。  \n\n如果这个损失函数的等高线是一系列完美的同心圆，那么无论我们起点在哪里，梯度下降的时候都会以垂直切线方向，沿着圆心一路奔去。  \n\n这种情况下优化很快，控制好学习率不要跳过minimum就可以（也可以用自适应优化器来控制速度）。  \n\n但是实际上我们的损失平面很难那么完美。损失函数的等高线更可能是个椭圆（或者更复杂的形状）。  \n\n{% asset_img ellipse_1.png ellipse %}  \n\n这样我们梯度下降是方向就要经常修正，训练效率就会受影响。  \n\n如果这个椭圆很扁或者我们的训练参数不太好，可能会出现反复震荡收敛缓慢的情况。  \n\n{% asset_img ellipse_2.png ellipse %}  \n\n损失在这个狭窄的山谷中反复横跳。  \n\n那损失函数等高线什么时候会是椭圆形？  \n\n假设我们现在有两个输入变量，以米为单位的身高 $x_{1}$，和以元为单位的月工资收入 $x_{2}$。（这里对量纲的使用也会改变数值，如米->厘米）  \n\n如果我们用这两个自变量训练模型，我们会发现，身高取值范围基本是在0.x米~2.x米，而工资的取值范围是0到几百几千几万或者几十万以及更多。  \n\n而模型的一个主要操作就是对输入特征进行线性组合。  \n\n这时模型的输出值会更大地受到 $x_{2}$ 的影响，因为它的变化更大，取值范围也更大。  \n\n这时损失函数在不同变量维度的变化速度相差很多，损失函数就会出现椭圆形等高线的情况。  \n\n既然由于量纲和取值范围的问题，会导致训练困难，那最直接方法就是规定一个标准范围，所有输入变量，不管原来是什么范围，现在都归一化到标准范围里来。  \n\n这就是最朴素的输入normalization的思想。  \n\n输入的normalization有很多种做法  \n$$x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}\\sigma$$  \n\n其中 $\\mu$ 为均值，$\\sigma$ 为方差。  \n\n第三种，均值方差归一化，也叫Z-score normalization，应该是我们用得比较多的。  \n\n这样我们通过对输入进行一些操作，把「椭圆」拉成了「圆」，解决输入参数范围带来的一些训练问题。  \n\n除了针对均值、方差、最大值、最小值的归一化，对输入还有一些其他的处理，如PCA等，就暂不展开。  \n\n## 缓解ICS...吗？\n\n机器学习里有一个叫i.i.d.（independent and identical distribution，独立同分布）的假设：独立，每次抽样之间是没有关系的，不会相互影响；同分布，即每次抽样，样本都服从同样的一个分布。  \n\n为什么需要i.i.d.？  \n\n由于机器学习依赖于使用现有数据来训练模型，进而对未来的数据做出预测和模拟，因此这一过程本质上是在历史数据的基础上，通过模型来推测未来的数据走向。  \n\n这就要求我们使用的历史数据必须具备整体的代表性。以便从现有数据（经验）中提炼出规律，对未知数据进行决策。  \n\n如果用于训练的数据缺乏总体代表性，即仅代表特殊情况，那么得出的规律可能不准确或错误，因为这些规律是基于个别案例推测出来的，就不具备泛化性。  \n\n当然并不是所有机器学习都需要i.i.d.，但是有i.i.d.的话，可以简化很多事情，让模型学习起来更容易快速。  \n\n对于输入，通过合理的抽样和处理（前面提到的PCA就可以用来解耦特征间的关联，达到“独立”的效果），我们可以得到输入的i.i.d.的条件，但这只是针对输入。  \n\n在多层的神经网络中，上一层的输出会作为下一层的输入。  \n\n而在训练过程中，由于上层的模型参数在不断学习变化，则上层输出的分布也在不断变化，靠后的层实际上要学习不断的变化的分布，这就很不i.i.d.，那靠后面的层的学习速度和效果就会收到影响，调参也变得困难，模型也难以加深。  \n\n这个问题就是ICS，internal covariate shift。  \n\n那有没有办法保证上一层的分布不要变化呢？  \n\n一个「可能」的方案就是normalization。我们通过把上一层的输出映射到一个固定的分布上，来稳定给下一层的输入，这样就降低了学习难度。  \n\n但也有一些工作表明normalization（batchnorm）的作用机制和ICS的关系并不大，这个观点下面在batchnorm部分说。  \n\n当然ICS的问题也可以通过改变初始化策略、调控训练超参如学习率等方法来优化，但是这样做的效率并不是很高。  \n\n## 远离激活函数饱和区\n\n神经网络中还有一个重要组件，非线性激活函数，比如常用的sigmoid。\n\n{% asset_img sigmoid.png sigmoid %}  \n\n当输入 > 6 或者 < -6 的时候，sigmoid函数的梯度已经变得非常小，也就是进入了饱和区。  \n\n这种情况下训练就变得困难。  \n\nICS就会加剧梯度消失的情况。在没有normalization的情况下，分布不断变化，后面层的参数变化激烈，导致输出值更容易进入到左右两端，更容易进入到激活函数的饱和区。  \n\n而normalization能把部分输出值拉回到梯度正常的范围内，一定程度缓解了梯度消失的问题，使训练可以正常进行下去。  \n\n# batchnorm\n\n神经网络中使用的normalization有很多种，这里不一一展开，只梳理一下最重要的batchnorm和layernorm两类。  \n\n## batchnorm算法\n\n假设输入数据的形状是 $[B,C]$ ，其中 $B$ 是batch size，$C$ 是特征向量维度。  \n\n这 $C$ 个输入特征每个都有不同的含义，如我们前面的例子，第一个元素可能是身高，第二个元素可能是月收入，因此做normalization的时候这 $C$ 个特征分别来做。  \n\n具体来说，对于第 $i$ 个特征维度，首先计算整个batch内的均值  \n\n$$\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n$$  \n\n再计算这个维度上的方差  \n\n$$\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n$$  \n\n得到均值和方差之后，对batch内维度上的所有值进行Z-score normalization  \n\n$$\nx_{i,j}'=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n$$  \n\n其中 $\\epsilon$ 是为了防止分母为0。这个在实际代码中挺重要的，忘记加可能会出问题。  \n\n经过这样的变换之后，在 $C$ 个特征维度上就是均值为0，方差为1的分布了。  \n\n但是到这还没结束。  \n\n每个维度的数值全部归一化之后，对于激活函数来说，更集中在中间的部分，而这部分的非线性特征并不强（比如上面的sigmoid），这样非线性激活层近似了一个线性变换，这样就降低了模型的学习能力。  \n\n且无论输入是什么，最终输出都会被强行拉到这样一个“平均”的值，也极大抑制了模型的表达能力。  \n\n所以为了保证模型的能力，也保证非线性能力的获得，对每个特征，又增加两个可学习的参数， 缩放参数 $\\gamma$ 和位移参数 $\\beta$ 。  \n\n$$\ny_{i,j} = \\gamma_{i} x_{i,j}' + \\beta_{i}\n$$  \n\n这样每个特征值就有机会从“线性区”移动到“非线性区”，把被归一化削弱的非线性能力找了回来。  \n\n并且通过这样一个归一化再重新缩放移动的操作，解耦了上层输出分布和下层输入，本来下层参数要去适应上层分布变化，现在只需要通过每个batchnorm层中的 $\\gamma$ 和 $\\beta$ 直接学习就行了，训练变得简单了。\n\n[《Batch Normalization: Accelerating Deep Network》](https://zhuanlan.zhihu.com/p/340856414)给出的算法如下  \n\n{% asset_img bn_algo.png batch norm %}  \n\n## CNN中的batchnorm  \n\nbatchnorm最主要的应用还是在CNN模型中。  \n\n假设CNN中feature map的size是 $[B,C,H,W]$ ，其中 $B$ 是batch size，$C$ 是channel数（也是卷积核数量），$H$ 和 $W$ 分别是特征图的高和宽。  \n\n如果按照前面batchnorm的算法，那应该有 $C\\times H\\times W$ 组特征，每组特征有 $B$ 个，对每组内的 $B$ 进行归一化，再进行放缩和平移。  \n\n但是实际上，CNN中卷积是一个滑动窗口，对于同一个channel下的 $H\\times W$ 个特征值其实都来自于同一个卷积核的计算，这 $H\\times W$ 也属于一个“batch”，它们要放在一起进行归一化。  \n\n也就是对于卷积核来说，真正的batch数是 $B\\times H\\times W$ ，而只有 $C$ 组特征值，因此也只有 $C$ 个 $\\gamma$ 和 $\\beta$ 。  \n\nbatchnorm原文中，batchnorm放在了relu后面，作者认为这样使得进入激活函数的分布会更加稳定，顺便对于fc层，由于batchnorm和fc都有bias项，还可以省略掉其中一个而不影响效果。  \n\nbtw，一般来说，batchnorm初始化的时候，把 $\\gamma$ 设为1（不缩放），把 $\\beta$ 设为0（不平移），在训练中让模型从相当于没有batchnorm开始慢慢学习这两个参数。  \n\n## 训练和推理  \n\n现在我们知道在训练时，batchnorm对一个mini-batch计算均值和方差来进行归一化，再进行缩放和移动。  \n\n$\\gamma$ 和 $\\beta$ 属于模型学出来的参数，只要训练结束这两个向量就固定了，在推理的时候直接使用即可。  \n\n但是推理时，均值和方差怎么计算呢。推理的时候可能是一个sample，也可能是任意个sample作为一个batch，和训练的时候一样计算肯定不合适。  \n\n我们需要在训练的时候就为推理做准备：训练的时候，模型会遍历整个训练集，因此理论上可以统计出整个训练集的均值和方差，然后把这个大量样本统计出来的均值和方差当做真实分布的均值和方差，在推理的时候使用。（回想i.i.d.）  \n\n当时又有一个问题，训练集可能会很大，有百万甚至千万的数据，在训练的数据记录下所有层所有特征来计算均值和方差显然效率不高，因此用一个近似的方法：\n\nmoving_mean = momentum × moving_mean + (1.0 − momentum) × mean  \n\nmoving_var = momentum × moving_var + (1.0 − momentum) × var  \n\n通过把多个batch的均值和方差进行移动平均的方式来逼近整个训练集的均值和方差。  \n\nmomentum为动量参数，在 TF/Keras 中，该值为0.99，在 Pytorch 中，这个值为0.9。  \n\n小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动。  \n \n大的momentum值对应慢的更新速度，如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。  \n\n如果batch size比较小，每个mini batch和全局差异较大，就不应该用太大的momentum。  \n\n理论上，训练步数越长是会越靠近真实分布的，实际上，因为每个batch并不能代表整个训练集的分布，所以最后的值是在真实分布附近波动。\n\n这里还引入另外一个问题，如果batch size太小，每个mini batch统计的均值和方差和全局的值偏差相对会比较大，对模型收敛的稳定性有影响，因此一般来说，使用batchnorm的话，batch size不能太小，如下图  \n\n{% asset_img bs_bn.png batch size的影响 %}  \n\n小结一下，batchnorm的优点是解耦了上层输出和下层输入的分布，既缓解了进入激活函数饱和区带来的梯度消失的情况，又保留了模型的表达能力。每一层的输入尺度相对固定，提供了更好的尺度不变形，使模型训练更稳定。    \n\n同时每个batch分别进行归一化，相当于引入了一些随机噪音，使得模型不容易过拟合到某些微小的特征上，相当于进行了一定的正则化，将损失平面变得相对平滑。\n\n但是同时也引入了新的超参（如momentum），另外也依赖batch size的大小，过小的batch size可能会带来问题。\n\n## batchnorm起作用的真正原因？\n\n虽然batchnorm原文认为batchnorm在一定程度上是缓解了ICS，但是2018年的《How Does Batch Normalization Help Optimization?》提出了不同观点。  \n\n为了探究batchnorm的效果，是否是因为优化了ICS（或者说和优化了ICS有多大关系），做了一个这样的实验：在batchnorm后面又通过加入随机噪音来引入“covariate shift”，并和没有加噪音，以及没有加batchnorm的模型效果进行对比，如下图  \n\n{% asset_img bn_ics.png ICS %}  \n\n结果发现，即使人工加强了ICS的情况，但是只要用了batchnorm，效果依然比不用好；而人工引入ICS的模型，在效果上并没有多大影响。  \n\n这就说明缓解ICS并不是batchnorm有效的真正原因。  \n\n那batchnorm到底有没有缓解到ICS呢？  \n\n要测量ICS的变化，就要先定义ICS。  \n\n对于网络中的每一层，ICS被定义为在前一层参数更新后，当前层输入分布的变化。这种变化可以通过比较更新前后的梯度来量化。\n\n{% asset_img ics_define.png ICS 定义 %}  \n\n具体来说，对于每一层i，作者们计算了以下两个梯度之间的L2范数差异：\n\n$G_{t,i}$ ，在时间t，使用当前所有层的参数（包括前一层的参数）计算的梯度。  \n\n$G_{t,i}'$ ，在时间t，使用更新后的前一层参数计算的梯度，而其他层的参数保持不变。  \n\n这个差异直观上表明了「上一层参数变化，下一层需要在多大程度上来变化，以适应新的分布」。  \n\n理想来说，ICS越小，上一层参数更新对当前层的分布影响越小，梯度变化程度应该越小。\n\n{% asset_img ics_measure.png ICS measure %}  \n\n但是从结果上来看，使用了batchnorm并不能有效减少这个变化，甚至还有所增加。  \n\n这也说明batchnorm实际上并不能真正缓解ICS的情况。  \n\n那batchnorm起效果的真正原因是什么？  \n\n作者认为主要是batchnorm使得损失函数更加平滑，直观上来说就是减少了很多坑坑洼洼的位置，使得训练更不容易陷入到局部最小值中去。  \n\n# layernorm\n\n看完batchnorm，再来看layernorm。  \n\n## 理解layernorm\n\nlayernorm，不要被名字骗了，这里的layer指的不是模型的层，而是数值的layer。  \n\n对于二维的输入，batchnorm实在batch维度上做归一化，而layernorm是在特征维度做归一化  \n\n{% asset_img bn_and_ln.png bn和ln %}  \n\n对于非NLP数据而言，相比batchnorm，layernorm归一化的维度似乎解释性没那么强。batchnorm对同一个特征，比如身高计算均值是有意义的，而layernorm在不同的特征，比如身高、工资、温度做归一化，好像并没有可靠的物理意义。  \n\nlayernorm最主要的应用就是NLP的模型，包括RNN和transfomrer模型。  \n\n在transformer中，一般输入的形状是 $[B,S,H]$ ，$S$ 是序列长度，每个样本的长度可能不同，因此在这个维度需要使用padding（一般是zero-padding）来把batch内的数据处理成一样长。  \n\n比如这样一批文本输入\n\n```\n我  爱  中  国\n你  好\n谢  谢  你\n```\n\n为了使模型能够统一处理，会pad成\n\n```\n我  爱  中  国\n你  好  [P] [P]\n谢  谢  你  [P]\n```\n\n一般来说，我们认为由于有padding的存在，做batchnorm并不合适。  \n\n比如上面的例子，对“中”，“[P]”，“你”做归一化，由于 [P] 的存在，实际的batch size只有2，并且和 [P] 做normalization也对训练没什么帮助。  \n\n而且对于文本数据，batch内同一个位置上的字是不同的，对完全没有关系的字进行归一化也并没有什么意义。  \n\n也就是说，哪怕没有 [P] 的存在，比如对第一个token“我”，“你”，“谢”做归一化，直觉上也不太有物理意义。  \n\n因此使用layernorm，在 $H$ 维度上进行normalization，同时有 $H$ 个$\\gamma$ 和 $\\beta$ 需要学习。  \n\n相当于计算每一句输入内，每个token所有特征之间的均值和方差来进行归一化。  \n\n## 为什么transformer用layernorm  \n\n和batchnorm不同的是，由于layernorm不需要再batch维度上计算均值和方差，所以不存在训练和推理的时候不一样的地方，不用保存一个全局的均值和方差供推理的时候使用。  \n\n而由于layernorm和batch无关，也就不会受到batch size大小的影响。  \n\n除了以上的原因，也有一些工作深入探究了在nlp任务上layernorm和batchnorm的区别。  \n\n如《PowerNorm: Rethinking Batch Normalization in Transformers》就研究了transformer中BN为啥表现不太好。  \n\n研究了训练中的四个统计量：batch的均值和方差，以及他们的梯度的均值和方差。对于batch的均值和方差，计算了他们和running statistics（就是用移动平均法累积的均值和方差，见前面的文章）的欧氏距离。发现NLP任务上（IWSLT14）batch的均值和方差一直震荡，偏离全局的running statistics，而CV任务也相对稳定。  \n\n对于他们梯度的均值和方差，研究了其magnitude（绝对值），在CV任务上震荡更小，且训练完成后，也没有离群点。  \n\n总结来说，transformer中BN表现不太好的原因可能在于CV和NLP数据特性的不同，对于NLP数据，前向和反向传播中，batch统计量及其梯度都不太稳定。  \n\n更重要的是，实际效果就是layernorm在NLP的效果比batchnorm好，效果好，这是最重要的原因。  \n\n## RMSnorm\n\n19年《Root Mean Square Layer Normalization》提出了normalization变体RMSnorm，主要针对layernorm来改进。  \n\n简单地说，RMSnorm就是在标准layernorm的基础上，省略了平移，只进行缩放。  \n\n{% asset_img rmsnorm.png RMSnorm %}  \n\n作者认为标准layernorm计算效率并不高  \n\n{% asset_img rmsnorm_eff.png RMSnorm效率 %}  \n\n作者用一个GRU模型做实验，对比是否添加layernorm的结果，发现在相同时间和相同步骤下，有layernorm的模型，都没有无layernorm的模型收敛得快。  \n\n并且layernorm的平移对梯度方差的减小没有贡献，因此作者直接舍弃了中心化和平移两步，只对数据进行方差归一化和缩放。  \n\n更近一步，作者提出pRMSnorm，只对数据中前p%的数值进行处理，这样就能进一步加速训练，而效果也基本不太受影响。  \n\n{% asset_img prmsnorm.png prmsnorm %}  \n\nRMSnorm现在被很多主流的大模型所采样了。  \n\n# post-norm & pre-norm\n\n## 二者对比\nlayernorm在模型里放哪也有讲究。  \n\n原始的transformer模型使用的post-norm，而《On Layer Normalization in the Transformer Architecture》则认为pre-norm更好。  \n\npost-norm和pre-norm分别是下面这样\n\n{% asset_img postnorm_prenorm.png postnorm and prenorm %}  \n\npost-norm是在残差和主干相加之后进行归一化，而pre-norm则是在主干先归一化再和残差相加。  \n\npost-norm和pre-norm对比，目前大家比较接受的结论是，pre-norm更容易训练，因此可以叠加更多的层，但是在层数不是特别多的情况下，post-norm最终的收敛效果会比pre-norm要好。  \n\n模型中，第 $l$ 层的输出是第 $l+1$ 层的输入，对于post-norm有  \n\n$$\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n$$\n\n而对于pre-norm则是  \n\n$$\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n$$\n\n参考苏剑林在《为什么Pre Norm的效果不如Post Norm？》中的分析，认为 $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$ 的方差，由于有norm的存在，是不随层数变化的。  \n\n当 $l$ 比较大时，$x_{l}、x_{l+1}$ 的差距较小，因此 $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$ 和 $\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))$ 的差距也很小，这时有  \n\n$$\\begin{aligned}\n&\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1})) \\\\\n&{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right) \\\\\n&=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}$$\n\n相当于 $l$ 层和 $l+1$ 层的效果接近于一个更宽的 $l$ 层的效果。  \n\n也就是使用pre-norm的时候，模型的深度有水分，表面看起来有 $l$ 层，实际在效果上，等效于post-norm的浅层模型。  \n\n从模型结构上看，恒等分支永远有一部分不用经过normalization，这部分能够直接把梯度回传到最前面，这也是pre-norm能够训练“层数更多”的模型的原因--缓解了梯度消失。  \n\n正常来说，模型深度对最终效果的影响，是大于模型宽度的。  \n\n而post-norm，在残差分支之后做归一化，对参数正则化的效果更好（loss平面更平滑），且它每norm一次就削弱一次恒等分支的权重，所以post-norm相对pre-norm，是更突出残差分支的，因此它的层数更加“足秤”，训练好之后效果更优。  \n\n## 和warmup的关系  \n\n《On Layer Normalization in the Transformer Architecture》（认为pre-norm更好）还分析指出，使用post-norm的transformer，在初始化时候，靠近输出层的部分梯度期望很大，所以模型在开始训练的时候很依赖warmup的策略，通过缓慢提升学习率来稳定训练过程。  \n\n使用warmup引入了新的超参，调参更为麻烦点。  \n\n而实验表明，使用pre-norm的transformer在不需要warmup的情况下，也能收敛到post-norm+warmup的相同水平，而post-norm不加warmup效果就差点了。  \n\n{% asset_img warmup_effect.png warmup影响 %}  \n\n## Deepnorm  \n\n2022年，《DeepNet: Scaling Transformers to 1,000 Layers》对transformer训练不稳定的原因进行了深入分析，发现模型更新过大是导致不稳定的主要原因。  \n\n为了解决这个问题，他们提出了Deepnorm，可以限制模型更新的大小。  \n\n{% asset_img deepnorm.png deepnorm %}  \n\n其中 $\\alpha>1$ 是根据模型参数定的常数。这里相比post-norm提升了恒等分支的权重，使训练更容易进行。  \n\n另外，还用了一个 $\\beta$ 参数，把 $G_{l}$ 中的模型参数进行了缩小，以此来稳定模型的训练。  \n\n实验结果上，deepnorm结合了pre-norm的容易训练，和post-norm的收敛效果好的特点，能够把百层、浅层的模型训到比较好的效果。  \n\n{% asset_img deepnorm_result.png deepnorm result %}  \n\n参数过程相比post-norm稳定了很多。  \n\n## Realformer--residual attention  \n\npost-norm和pre-norm实际上改变的是模型残差分支和恒等分支怎么排布的问题，而《RealFormer: Transformer Likes Residual Attention》则提出了另外一种做法  \n\n{% asset_img realformer.png realformer %}  \n\nRealFormer的核心是在其标准Transformer编码器的每一层中引入了残差连接。这些残差连接将前一层的原始注意力分数（即在应用Softmax之前的分数）与当前层计算出的注意力分数相结合。这样做的结果是，当前层的注意力分数在计算时会考虑到前一层的信息。\n\n每个多头注意力模块都会接收来自前一层的残差注意力分数作为额外输入。这意味着每个注意力头不仅考虑了当前层内的输入序列，而且还直接利用了前一层的注意力信息。\n\n{% asset_img realformer_attention.png realformer attention %}  \n\n其中 $Prev'$ 是来自上一层softmax之前的权重矩阵（多头注意力的话，则是对应的头的值），而 $\\frac{Q^{\\prime}K^{\\prime T}}{\\sqrt{d_k}}+Prev'$ 则是传给下一层的attention的。  \n\n# 小结\n\n本篇粗略梳理了一下关于normalization，batchnorm，以及layernorm在transformer的一些使用情况。\n\n目前主流的大模型使用的是rmsnorm + prenorm，也有使用其他变体的。  \n\n关于normalization，依然留有一些探索空间。    \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n往期文章\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)\n\n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)\n\n***\n\n# Reference  \n【1】https://www.zhihu.com/question/487766088  \n【2】Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization https://arxiv.org/abs/2001.06838  \n【3】Transformer中的归一化(一)：什么是归一化&为什么要归一化 https://zhuanlan.zhihu.com/p/476102712  \n【4】Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://arxiv.org/abs/1502.03167  \n【5】How Does Batch Normalization Help Optimization? https://arxiv.org/abs/1805.11604  \n【6】Batch Normalization: Accelerating Deep Network https://zhuanlan.zhihu.com/p/340856414  \n【7】Layer Normalization https://arxiv.org/abs/1607.06450  \n【8】详解深度学习中的Normalization，BN/LN/WN https://zhuanlan.zhihu.com/p/33173246  \n【9】Transformer中的归一化(四)：BatchNormalization的原理、作用和实现 https://zhuanlan.zhihu.com/p/481277619  \n【10】Layer Normalization https://arxiv.org/abs/1607.06450  \n【11】PowerNorm: Rethinking Batch Normalization in Transformers https://arxiv.org/abs/2003.07845  \n【12】Root Mean Square Layer Normalization https://arxiv.org/abs/1910.07467  \n【13】On Layer Normalization in the Transformer Architecture https://arxiv.org/abs/2002.04745  \n【14】为什么Pre Norm的效果不如Post Norm？ https://spaces.ac.cn/archives/9009  \n【15】Understanding the Difficulty of Training Transformers https://arxiv.org/abs/2004.08249  \n【16】RealFormer: Transformer Likes Residual Attention https://arxiv.org/abs/2012.11747  \n【17】DeepNet: Scaling Transformers to 1,000 Layers https://arxiv.org/abs/2203.00555  \n\n","slug":"cs/nlp/2024/03/关于Transformer的normalization","published":1,"updated":"2024-04-07T06:36:14.847Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9z001e314khbudet36","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>Normalization在模型中，相对于attention这种经常被魔改的结构，受到的关注度似乎没那么高，但它对模型能否顺利训练，却有很关键的作用。</p>\n<p>在此简单梳理下normalization相关的背景和内容，也分析一下在transformer发展上的相关内容。</p>\n<p>这部分内容感觉目前还有些存在争议的地方，如果有不同意见欢迎讨论。</p>\n<h1 id=\"why-normalization\">why normalization</h1>\n<p>normalization，也叫「归一化」、「正则化」、「规范化」、「标准化」等，可以说已经是神经网络不可以或缺的一环。</p>\n<p>使用的话，现在基本只需几行代码就能实现。但要用得好，还是需要了解一下它作用的机制。</p>\n<h2 id=\"从输入数据看normalization\">从输入数据看normalization</h2>\n<p>假设我们有一个二元损失函数 <span class=\"math inline\">\\(Loss(x_1,x_2)=x_1^2+x_2^2+b\\)</span>\n，那在三维空间画出来的损失平面大概是这样</p>\n<img src=\"/6a40bfa5/lossfunc_surface.jpeg\" class title=\"loss function surface\">\n<p>在这样一个平面上，使用梯度下降法，梯度方向是垂直于当前位置等高线的切线方向的。</p>\n<p>如果这个损失函数的等高线是一系列完美的同心圆，那么无论我们起点在哪里，梯度下降的时候都会以垂直切线方向，沿着圆心一路奔去。</p>\n<p>这种情况下优化很快，控制好学习率不要跳过minimum就可以（也可以用自适应优化器来控制速度）。</p>\n<p>但是实际上我们的损失平面很难那么完美。损失函数的等高线更可能是个椭圆（或者更复杂的形状）。</p>\n<img src=\"/6a40bfa5/ellipse_1.png\" class title=\"ellipse\">\n<p>这样我们梯度下降是方向就要经常修正，训练效率就会受影响。</p>\n<p>如果这个椭圆很扁或者我们的训练参数不太好，可能会出现反复震荡收敛缓慢的情况。</p>\n<img src=\"/6a40bfa5/ellipse_2.png\" class title=\"ellipse\">\n<p>损失在这个狭窄的山谷中反复横跳。</p>\n<p>那损失函数等高线什么时候会是椭圆形？</p>\n<p>假设我们现在有两个输入变量，以米为单位的身高 <span class=\"math inline\">\\(x_{1}\\)</span>，和以元为单位的月工资收入 <span class=\"math inline\">\\(x_{2}\\)</span>。（这里对量纲的使用也会改变数值，如米-&gt;厘米）</p>\n<p>如果我们用这两个自变量训练模型，我们会发现，身高取值范围基本是在0.x米~2.x米，而工资的取值范围是0到几百几千几万或者几十万以及更多。</p>\n<p>而模型的一个主要操作就是对输入特征进行线性组合。</p>\n<p>这时模型的输出值会更大地受到 <span class=\"math inline\">\\(x_{2}\\)</span>\n的影响，因为它的变化更大，取值范围也更大。</p>\n<p>这时损失函数在不同变量维度的变化速度相差很多，损失函数就会出现椭圆形等高线的情况。</p>\n<p>既然由于量纲和取值范围的问题，会导致训练困难，那最直接方法就是规定一个标准范围，所有输入变量，不管原来是什么范围，现在都归一化到标准范围里来。</p>\n<p>这就是最朴素的输入normalization的思想。</p>\n<p>输入的normalization有很多种做法<br>\n<span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}\\sigma\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\mu\\)</span> 为均值，<span class=\"math inline\">\\(\\sigma\\)</span> 为方差。</p>\n<p>第三种，均值方差归一化，也叫Z-score\nnormalization，应该是我们用得比较多的。</p>\n<p>这样我们通过对输入进行一些操作，把「椭圆」拉成了「圆」，解决输入参数范围带来的一些训练问题。</p>\n<p>除了针对均值、方差、最大值、最小值的归一化，对输入还有一些其他的处理，如PCA等，就暂不展开。</p>\n<h2 id=\"缓解ics...吗\">缓解ICS...吗？</h2>\n<p>机器学习里有一个叫i.i.d.（independent and identical\ndistribution，独立同分布）的假设：独立，每次抽样之间是没有关系的，不会相互影响；同分布，即每次抽样，样本都服从同样的一个分布。</p>\n<p>为什么需要i.i.d.？</p>\n<p>由于机器学习依赖于使用现有数据来训练模型，进而对未来的数据做出预测和模拟，因此这一过程本质上是在历史数据的基础上，通过模型来推测未来的数据走向。</p>\n<p>这就要求我们使用的历史数据必须具备整体的代表性。以便从现有数据（经验）中提炼出规律，对未知数据进行决策。</p>\n<p>如果用于训练的数据缺乏总体代表性，即仅代表特殊情况，那么得出的规律可能不准确或错误，因为这些规律是基于个别案例推测出来的，就不具备泛化性。</p>\n<p>当然并不是所有机器学习都需要i.i.d.，但是有i.i.d.的话，可以简化很多事情，让模型学习起来更容易快速。</p>\n<p>对于输入，通过合理的抽样和处理（前面提到的PCA就可以用来解耦特征间的关联，达到“独立”的效果），我们可以得到输入的i.i.d.的条件，但这只是针对输入。</p>\n<p>在多层的神经网络中，上一层的输出会作为下一层的输入。</p>\n<p>而在训练过程中，由于上层的模型参数在不断学习变化，则上层输出的分布也在不断变化，靠后的层实际上要学习不断的变化的分布，这就很不i.i.d.，那靠后面的层的学习速度和效果就会收到影响，调参也变得困难，模型也难以加深。</p>\n<p>这个问题就是ICS，internal covariate shift。</p>\n<p>那有没有办法保证上一层的分布不要变化呢？</p>\n<p>一个「可能」的方案就是normalization。我们通过把上一层的输出映射到一个固定的分布上，来稳定给下一层的输入，这样就降低了学习难度。</p>\n<p>但也有一些工作表明normalization（batchnorm）的作用机制和ICS的关系并不大，这个观点下面在batchnorm部分说。</p>\n<p>当然ICS的问题也可以通过改变初始化策略、调控训练超参如学习率等方法来优化，但是这样做的效率并不是很高。</p>\n<h2 id=\"远离激活函数饱和区\">远离激活函数饱和区</h2>\n<p>神经网络中还有一个重要组件，非线性激活函数，比如常用的sigmoid。</p>\n<img src=\"/6a40bfa5/sigmoid.png\" class title=\"sigmoid\">\n<p>当输入 &gt; 6 或者 &lt; -6\n的时候，sigmoid函数的梯度已经变得非常小，也就是进入了饱和区。</p>\n<p>这种情况下训练就变得困难。</p>\n<p>ICS就会加剧梯度消失的情况。在没有normalization的情况下，分布不断变化，后面层的参数变化激烈，导致输出值更容易进入到左右两端，更容易进入到激活函数的饱和区。</p>\n<p>而normalization能把部分输出值拉回到梯度正常的范围内，一定程度缓解了梯度消失的问题，使训练可以正常进行下去。</p>\n<h1 id=\"batchnorm\">batchnorm</h1>\n<p>神经网络中使用的normalization有很多种，这里不一一展开，只梳理一下最重要的batchnorm和layernorm两类。</p>\n<h2 id=\"batchnorm算法\">batchnorm算法</h2>\n<p>假设输入数据的形状是 <span class=\"math inline\">\\([B,C]\\)</span>\n，其中 <span class=\"math inline\">\\(B\\)</span> 是batch size，<span class=\"math inline\">\\(C\\)</span> 是特征向量维度。</p>\n<p>这 <span class=\"math inline\">\\(C\\)</span>\n个输入特征每个都有不同的含义，如我们前面的例子，第一个元素可能是身高，第二个元素可能是月收入，因此做normalization的时候这\n<span class=\"math inline\">\\(C\\)</span> 个特征分别来做。</p>\n<p>具体来说，对于第 <span class=\"math inline\">\\(i\\)</span>\n个特征维度，首先计算整个batch内的均值</p>\n<p><span class=\"math display\">\\[\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n\\]</span></p>\n<p>再计算这个维度上的方差</p>\n<p><span class=\"math display\">\\[\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n\\]</span></p>\n<p>得到均值和方差之后，对batch内维度上的所有值进行Z-score\nnormalization</p>\n<p><span class=\"math display\">\\[\nx_{i,j}&#39;=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\epsilon\\)</span>\n是为了防止分母为0。这个在实际代码中挺重要的，忘记加可能会出问题。</p>\n<p>经过这样的变换之后，在 <span class=\"math inline\">\\(C\\)</span>\n个特征维度上就是均值为0，方差为1的分布了。</p>\n<p>但是到这还没结束。</p>\n<p>每个维度的数值全部归一化之后，对于激活函数来说，更集中在中间的部分，而这部分的非线性特征并不强（比如上面的sigmoid），这样非线性激活层近似了一个线性变换，这样就降低了模型的学习能力。</p>\n<p>且无论输入是什么，最终输出都会被强行拉到这样一个“平均”的值，也极大抑制了模型的表达能力。</p>\n<p>所以为了保证模型的能力，也保证非线性能力的获得，对每个特征，又增加两个可学习的参数，\n缩放参数 <span class=\"math inline\">\\(\\gamma\\)</span> 和位移参数 <span class=\"math inline\">\\(\\beta\\)</span> 。</p>\n<p><span class=\"math display\">\\[\ny_{i,j} = \\gamma_{i} x_{i,j}&#39; + \\beta_{i}\n\\]</span></p>\n<p>这样每个特征值就有机会从“线性区”移动到“非线性区”，把被归一化削弱的非线性能力找了回来。</p>\n<p>并且通过这样一个归一化再重新缩放移动的操作，解耦了上层输出分布和下层输入，本来下层参数要去适应上层分布变化，现在只需要通过每个batchnorm层中的\n<span class=\"math inline\">\\(\\gamma\\)</span> 和 <span class=\"math inline\">\\(\\beta\\)</span>\n直接学习就行了，训练变得简单了。</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/340856414\">《Batch\nNormalization: Accelerating Deep Network》</a>给出的算法如下</p>\n<img src=\"/6a40bfa5/bn_algo.png\" class title=\"batch norm\">\n<h2 id=\"cnn中的batchnorm\">CNN中的batchnorm</h2>\n<p>batchnorm最主要的应用还是在CNN模型中。</p>\n<p>假设CNN中feature map的size是 <span class=\"math inline\">\\([B,C,H,W]\\)</span> ，其中 <span class=\"math inline\">\\(B\\)</span> 是batch size，<span class=\"math inline\">\\(C\\)</span> 是channel数（也是卷积核数量），<span class=\"math inline\">\\(H\\)</span> 和 <span class=\"math inline\">\\(W\\)</span> 分别是特征图的高和宽。</p>\n<p>如果按照前面batchnorm的算法，那应该有 <span class=\"math inline\">\\(C\\times H\\times W\\)</span> 组特征，每组特征有\n<span class=\"math inline\">\\(B\\)</span> 个，对每组内的 <span class=\"math inline\">\\(B\\)</span> 进行归一化，再进行放缩和平移。</p>\n<p>但是实际上，CNN中卷积是一个滑动窗口，对于同一个channel下的 <span class=\"math inline\">\\(H\\times W\\)</span>\n个特征值其实都来自于同一个卷积核的计算，这 <span class=\"math inline\">\\(H\\times W\\)</span>\n也属于一个“batch”，它们要放在一起进行归一化。</p>\n<p>也就是对于卷积核来说，真正的batch数是 <span class=\"math inline\">\\(B\\times H\\times W\\)</span> ，而只有 <span class=\"math inline\">\\(C\\)</span> 组特征值，因此也只有 <span class=\"math inline\">\\(C\\)</span> 个 <span class=\"math inline\">\\(\\gamma\\)</span> 和 <span class=\"math inline\">\\(\\beta\\)</span> 。</p>\n<p>batchnorm原文中，batchnorm放在了relu后面，作者认为这样使得进入激活函数的分布会更加稳定，顺便对于fc层，由于batchnorm和fc都有bias项，还可以省略掉其中一个而不影响效果。</p>\n<p>btw，一般来说，batchnorm初始化的时候，把 <span class=\"math inline\">\\(\\gamma\\)</span> 设为1（不缩放），把 <span class=\"math inline\">\\(\\beta\\)</span>\n设为0（不平移），在训练中让模型从相当于没有batchnorm开始慢慢学习这两个参数。</p>\n<h2 id=\"训练和推理\">训练和推理</h2>\n<p>现在我们知道在训练时，batchnorm对一个mini-batch计算均值和方差来进行归一化，再进行缩放和移动。</p>\n<p><span class=\"math inline\">\\(\\gamma\\)</span> 和 <span class=\"math inline\">\\(\\beta\\)</span>\n属于模型学出来的参数，只要训练结束这两个向量就固定了，在推理的时候直接使用即可。</p>\n<p>但是推理时，均值和方差怎么计算呢。推理的时候可能是一个sample，也可能是任意个sample作为一个batch，和训练的时候一样计算肯定不合适。</p>\n<p>我们需要在训练的时候就为推理做准备：训练的时候，模型会遍历整个训练集，因此理论上可以统计出整个训练集的均值和方差，然后把这个大量样本统计出来的均值和方差当做真实分布的均值和方差，在推理的时候使用。（回想i.i.d.）</p>\n<p>当时又有一个问题，训练集可能会很大，有百万甚至千万的数据，在训练的数据记录下所有层所有特征来计算均值和方差显然效率不高，因此用一个近似的方法：</p>\n<p>moving_mean = momentum × moving_mean + (1.0 − momentum) × mean</p>\n<p>moving_var = momentum × moving_var + (1.0 − momentum) × var</p>\n<p>通过把多个batch的均值和方差进行移动平均的方式来逼近整个训练集的均值和方差。</p>\n<p>momentum为动量参数，在 TF/Keras 中，该值为0.99，在 Pytorch\n中，这个值为0.9。</p>\n<p>小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动。</p>\n<p>大的momentum值对应慢的更新速度，如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。</p>\n<p>如果batch size比较小，每个mini\nbatch和全局差异较大，就不应该用太大的momentum。</p>\n<p>理论上，训练步数越长是会越靠近真实分布的，实际上，因为每个batch并不能代表整个训练集的分布，所以最后的值是在真实分布附近波动。</p>\n<p>这里还引入另外一个问题，如果batch size太小，每个mini\nbatch统计的均值和方差和全局的值偏差相对会比较大，对模型收敛的稳定性有影响，因此一般来说，使用batchnorm的话，batch\nsize不能太小，如下图</p>\n<img src=\"/6a40bfa5/bs_bn.png\" class title=\"batch size的影响\">\n<p>小结一下，batchnorm的优点是解耦了上层输出和下层输入的分布，既缓解了进入激活函数饱和区带来的梯度消失的情况，又保留了模型的表达能力。每一层的输入尺度相对固定，提供了更好的尺度不变形，使模型训练更稳定。</p>\n<p>同时每个batch分别进行归一化，相当于引入了一些随机噪音，使得模型不容易过拟合到某些微小的特征上，相当于进行了一定的正则化，将损失平面变得相对平滑。</p>\n<p>但是同时也引入了新的超参（如momentum），另外也依赖batch\nsize的大小，过小的batch size可能会带来问题。</p>\n<h2 id=\"batchnorm起作用的真正原因\">batchnorm起作用的真正原因？</h2>\n<p>虽然batchnorm原文认为batchnorm在一定程度上是缓解了ICS，但是2018年的《How\nDoes Batch Normalization Help Optimization?》提出了不同观点。</p>\n<p>为了探究batchnorm的效果，是否是因为优化了ICS（或者说和优化了ICS有多大关系），做了一个这样的实验：在batchnorm后面又通过加入随机噪音来引入“covariate\nshift”，并和没有加噪音，以及没有加batchnorm的模型效果进行对比，如下图</p>\n<img src=\"/6a40bfa5/bn_ics.png\" class title=\"ICS\">\n<p>结果发现，即使人工加强了ICS的情况，但是只要用了batchnorm，效果依然比不用好；而人工引入ICS的模型，在效果上并没有多大影响。</p>\n<p>这就说明缓解ICS并不是batchnorm有效的真正原因。</p>\n<p>那batchnorm到底有没有缓解到ICS呢？</p>\n<p>要测量ICS的变化，就要先定义ICS。</p>\n<p>对于网络中的每一层，ICS被定义为在前一层参数更新后，当前层输入分布的变化。这种变化可以通过比较更新前后的梯度来量化。</p>\n<img src=\"/6a40bfa5/ics_define.png\" class title=\"ICS 定义\">\n<p>具体来说，对于每一层i，作者们计算了以下两个梯度之间的L2范数差异：</p>\n<p><span class=\"math inline\">\\(G_{t,i}\\)</span>\n，在时间t，使用当前所有层的参数（包括前一层的参数）计算的梯度。</p>\n<p><span class=\"math inline\">\\(G_{t,i}&#39;\\)</span>\n，在时间t，使用更新后的前一层参数计算的梯度，而其他层的参数保持不变。</p>\n<p>这个差异直观上表明了「上一层参数变化，下一层需要在多大程度上来变化，以适应新的分布」。</p>\n<p>理想来说，ICS越小，上一层参数更新对当前层的分布影响越小，梯度变化程度应该越小。</p>\n<img src=\"/6a40bfa5/ics_measure.png\" class title=\"ICS measure\">\n<p>但是从结果上来看，使用了batchnorm并不能有效减少这个变化，甚至还有所增加。</p>\n<p>这也说明batchnorm实际上并不能真正缓解ICS的情况。</p>\n<p>那batchnorm起效果的真正原因是什么？</p>\n<p>作者认为主要是batchnorm使得损失函数更加平滑，直观上来说就是减少了很多坑坑洼洼的位置，使得训练更不容易陷入到局部最小值中去。</p>\n<h1 id=\"layernorm\">layernorm</h1>\n<p>看完batchnorm，再来看layernorm。</p>\n<h2 id=\"理解layernorm\">理解layernorm</h2>\n<p>layernorm，不要被名字骗了，这里的layer指的不是模型的层，而是数值的layer。</p>\n<p>对于二维的输入，batchnorm实在batch维度上做归一化，而layernorm是在特征维度做归一化</p>\n<img src=\"/6a40bfa5/bn_and_ln.png\" class title=\"bn和ln\">\n<p>对于非NLP数据而言，相比batchnorm，layernorm归一化的维度似乎解释性没那么强。batchnorm对同一个特征，比如身高计算均值是有意义的，而layernorm在不同的特征，比如身高、工资、温度做归一化，好像并没有可靠的物理意义。</p>\n<p>layernorm最主要的应用就是NLP的模型，包括RNN和transfomrer模型。</p>\n<p>在transformer中，一般输入的形状是 <span class=\"math inline\">\\([B,S,H]\\)</span> ，<span class=\"math inline\">\\(S\\)</span>\n是序列长度，每个样本的长度可能不同，因此在这个维度需要使用padding（一般是zero-padding）来把batch内的数据处理成一样长。</p>\n<p>比如这样一批文本输入</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">我  爱  中  国</span><br><span class=\"line\">你  好</span><br><span class=\"line\">谢  谢  你</span><br></pre></td></tr></table></figure>\n<p>为了使模型能够统一处理，会pad成</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">我  爱  中  国</span><br><span class=\"line\">你  好  [P] [P]</span><br><span class=\"line\">谢  谢  你  [P]</span><br></pre></td></tr></table></figure>\n<p>一般来说，我们认为由于有padding的存在，做batchnorm并不合适。</p>\n<p>比如上面的例子，对“中”，“[P]”，“你”做归一化，由于 [P]\n的存在，实际的batch size只有2，并且和 [P]\n做normalization也对训练没什么帮助。</p>\n<p>而且对于文本数据，batch内同一个位置上的字是不同的，对完全没有关系的字进行归一化也并没有什么意义。</p>\n<p>也就是说，哪怕没有 [P]\n的存在，比如对第一个token“我”，“你”，“谢”做归一化，直觉上也不太有物理意义。</p>\n<p>因此使用layernorm，在 <span class=\"math inline\">\\(H\\)</span>\n维度上进行normalization，同时有 <span class=\"math inline\">\\(H\\)</span>\n个<span class=\"math inline\">\\(\\gamma\\)</span> 和 <span class=\"math inline\">\\(\\beta\\)</span> 需要学习。</p>\n<p>相当于计算每一句输入内，每个token所有特征之间的均值和方差来进行归一化。</p>\n<h2 id=\"为什么transformer用layernorm\">为什么transformer用layernorm</h2>\n<p>和batchnorm不同的是，由于layernorm不需要再batch维度上计算均值和方差，所以不存在训练和推理的时候不一样的地方，不用保存一个全局的均值和方差供推理的时候使用。</p>\n<p>而由于layernorm和batch无关，也就不会受到batch size大小的影响。</p>\n<p>除了以上的原因，也有一些工作深入探究了在nlp任务上layernorm和batchnorm的区别。</p>\n<p>如《PowerNorm: Rethinking Batch Normalization in\nTransformers》就研究了transformer中BN为啥表现不太好。</p>\n<p>研究了训练中的四个统计量：batch的均值和方差，以及他们的梯度的均值和方差。对于batch的均值和方差，计算了他们和running\nstatistics（就是用移动平均法累积的均值和方差，见前面的文章）的欧氏距离。发现NLP任务上（IWSLT14）batch的均值和方差一直震荡，偏离全局的running\nstatistics，而CV任务也相对稳定。</p>\n<p>对于他们梯度的均值和方差，研究了其magnitude（绝对值），在CV任务上震荡更小，且训练完成后，也没有离群点。</p>\n<p>总结来说，transformer中BN表现不太好的原因可能在于CV和NLP数据特性的不同，对于NLP数据，前向和反向传播中，batch统计量及其梯度都不太稳定。</p>\n<p>更重要的是，实际效果就是layernorm在NLP的效果比batchnorm好，效果好，这是最重要的原因。</p>\n<h2 id=\"rmsnorm\">RMSnorm</h2>\n<p>19年《Root Mean Square Layer\nNormalization》提出了normalization变体RMSnorm，主要针对layernorm来改进。</p>\n<p>简单地说，RMSnorm就是在标准layernorm的基础上，省略了平移，只进行缩放。</p>\n<img src=\"/6a40bfa5/rmsnorm.png\" class title=\"RMSnorm\">\n<p>作者认为标准layernorm计算效率并不高</p>\n<img src=\"/6a40bfa5/rmsnorm_eff.png\" class title=\"RMSnorm效率\">\n<p>作者用一个GRU模型做实验，对比是否添加layernorm的结果，发现在相同时间和相同步骤下，有layernorm的模型，都没有无layernorm的模型收敛得快。</p>\n<p>并且layernorm的平移对梯度方差的减小没有贡献，因此作者直接舍弃了中心化和平移两步，只对数据进行方差归一化和缩放。</p>\n<p>更近一步，作者提出pRMSnorm，只对数据中前p%的数值进行处理，这样就能进一步加速训练，而效果也基本不太受影响。</p>\n<img src=\"/6a40bfa5/prmsnorm.png\" class title=\"prmsnorm\">\n<p>RMSnorm现在被很多主流的大模型所采样了。</p>\n<h1 id=\"post-norm-pre-norm\">post-norm &amp; pre-norm</h1>\n<h2 id=\"二者对比\">二者对比</h2>\n<p>layernorm在模型里放哪也有讲究。</p>\n<p>原始的transformer模型使用的post-norm，而《On Layer Normalization in\nthe Transformer Architecture》则认为pre-norm更好。</p>\n<p>post-norm和pre-norm分别是下面这样</p>\n<img src=\"/6a40bfa5/postnorm_prenorm.png\" class title=\"postnorm and prenorm\">\n<p>post-norm是在残差和主干相加之后进行归一化，而pre-norm则是在主干先归一化再和残差相加。</p>\n<p>post-norm和pre-norm对比，目前大家比较接受的结论是，pre-norm更容易训练，因此可以叠加更多的层，但是在层数不是特别多的情况下，post-norm最终的收敛效果会比pre-norm要好。</p>\n<p>模型中，第 <span class=\"math inline\">\\(l\\)</span> 层的输出是第 <span class=\"math inline\">\\(l+1\\)</span> 层的输入，对于post-norm有</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n\\]</span></p>\n<p>而对于pre-norm则是</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n\\]</span></p>\n<p>参考苏剑林在《为什么Pre Norm的效果不如Post Norm？》中的分析，认为\n<span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>\n的方差，由于有norm的存在，是不随层数变化的。</p>\n<p>当 <span class=\"math inline\">\\(l\\)</span> 比较大时，<span class=\"math inline\">\\(x_{l}、x_{l+1}\\)</span> 的差距较小，因此 <span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span> 和 <span class=\"math inline\">\\(\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))\\)</span>\n的差距也很小，这时有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1}))\n\\\\\n&amp;{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right)\n\\\\\n&amp;=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}\\]</span></p>\n<p>相当于 <span class=\"math inline\">\\(l\\)</span> 层和 <span class=\"math inline\">\\(l+1\\)</span> 层的效果接近于一个更宽的 <span class=\"math inline\">\\(l\\)</span> 层的效果。</p>\n<p>也就是使用pre-norm的时候，模型的深度有水分，表面看起来有 <span class=\"math inline\">\\(l\\)</span>\n层，实际在效果上，等效于post-norm的浅层模型。</p>\n<p>从模型结构上看，恒等分支永远有一部分不用经过normalization，这部分能够直接把梯度回传到最前面，这也是pre-norm能够训练“层数更多”的模型的原因--缓解了梯度消失。</p>\n<p>正常来说，模型深度对最终效果的影响，是大于模型宽度的。</p>\n<p>而post-norm，在残差分支之后做归一化，对参数正则化的效果更好（loss平面更平滑），且它每norm一次就削弱一次恒等分支的权重，所以post-norm相对pre-norm，是更突出残差分支的，因此它的层数更加“足秤”，训练好之后效果更优。</p>\n<h2 id=\"和warmup的关系\">和warmup的关系</h2>\n<p>《On Layer Normalization in the Transformer\nArchitecture》（认为pre-norm更好）还分析指出，使用post-norm的transformer，在初始化时候，靠近输出层的部分梯度期望很大，所以模型在开始训练的时候很依赖warmup的策略，通过缓慢提升学习率来稳定训练过程。</p>\n<p>使用warmup引入了新的超参，调参更为麻烦点。</p>\n<p>而实验表明，使用pre-norm的transformer在不需要warmup的情况下，也能收敛到post-norm+warmup的相同水平，而post-norm不加warmup效果就差点了。</p>\n<img src=\"/6a40bfa5/warmup_effect.png\" class title=\"warmup影响\">\n<h2 id=\"deepnorm\">Deepnorm</h2>\n<p>2022年，《DeepNet: Scaling Transformers to 1,000\nLayers》对transformer训练不稳定的原因进行了深入分析，发现模型更新过大是导致不稳定的主要原因。</p>\n<p>为了解决这个问题，他们提出了Deepnorm，可以限制模型更新的大小。</p>\n<img src=\"/6a40bfa5/deepnorm.png\" class title=\"deepnorm\">\n<p>其中 <span class=\"math inline\">\\(\\alpha&gt;1\\)</span>\n是根据模型参数定的常数。这里相比post-norm提升了恒等分支的权重，使训练更容易进行。</p>\n<p>另外，还用了一个 <span class=\"math inline\">\\(\\beta\\)</span> 参数，把\n<span class=\"math inline\">\\(G_{l}\\)</span>\n中的模型参数进行了缩小，以此来稳定模型的训练。</p>\n<p>实验结果上，deepnorm结合了pre-norm的容易训练，和post-norm的收敛效果好的特点，能够把百层、浅层的模型训到比较好的效果。</p>\n<img src=\"/6a40bfa5/deepnorm_result.png\" class title=\"deepnorm result\">\n<p>参数过程相比post-norm稳定了很多。</p>\n<h2 id=\"realformer--residual-attention\">Realformer--residual\nattention</h2>\n<p>post-norm和pre-norm实际上改变的是模型残差分支和恒等分支怎么排布的问题，而《RealFormer:\nTransformer Likes Residual Attention》则提出了另外一种做法</p>\n<img src=\"/6a40bfa5/realformer.png\" class title=\"realformer\">\n<p>RealFormer的核心是在其标准Transformer编码器的每一层中引入了残差连接。这些残差连接将前一层的原始注意力分数（即在应用Softmax之前的分数）与当前层计算出的注意力分数相结合。这样做的结果是，当前层的注意力分数在计算时会考虑到前一层的信息。</p>\n<p>每个多头注意力模块都会接收来自前一层的残差注意力分数作为额外输入。这意味着每个注意力头不仅考虑了当前层内的输入序列，而且还直接利用了前一层的注意力信息。</p>\n<img src=\"/6a40bfa5/realformer_attention.png\" class title=\"realformer attention\">\n<p>其中 <span class=\"math inline\">\\(Prev&#39;\\)</span>\n是来自上一层softmax之前的权重矩阵（多头注意力的话，则是对应的头的值），而\n<span class=\"math inline\">\\(\\frac{Q^{\\prime}K^{\\prime\nT}}{\\sqrt{d_k}}+Prev&#39;\\)</span> 则是传给下一层的attention的。</p>\n<h1 id=\"小结\">小结</h1>\n<p>本篇粗略梳理了一下关于normalization，batchnorm，以及layernorm在transformer的一些使用情况。</p>\n<p>目前主流的大模型使用的是rmsnorm + prenorm，也有使用其他变体的。</p>\n<p>关于normalization，依然留有一些探索空间。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>往期文章</p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】https://www.zhihu.com/question/487766088<br>\n【2】Towards Stabilizing Batch Statistics in Backward Propagation of\nBatch Normalization https://arxiv.org/abs/2001.06838<br>\n【3】Transformer中的归一化(一)：什么是归一化&amp;为什么要归一化\nhttps://zhuanlan.zhihu.com/p/476102712<br>\n【4】Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift https://arxiv.org/abs/1502.03167<br>\n【5】How Does Batch Normalization Help Optimization?\nhttps://arxiv.org/abs/1805.11604<br>\n【6】Batch Normalization: Accelerating Deep Network\nhttps://zhuanlan.zhihu.com/p/340856414<br>\n【7】Layer Normalization https://arxiv.org/abs/1607.06450<br>\n【8】详解深度学习中的Normalization，BN/LN/WN\nhttps://zhuanlan.zhihu.com/p/33173246<br>\n【9】Transformer中的归一化(四)：BatchNormalization的原理、作用和实现\nhttps://zhuanlan.zhihu.com/p/481277619<br>\n【10】Layer Normalization https://arxiv.org/abs/1607.06450<br>\n【11】PowerNorm: Rethinking Batch Normalization in Transformers\nhttps://arxiv.org/abs/2003.07845<br>\n【12】Root Mean Square Layer Normalization\nhttps://arxiv.org/abs/1910.07467<br>\n【13】On Layer Normalization in the Transformer Architecture\nhttps://arxiv.org/abs/2002.04745<br>\n【14】为什么Pre Norm的效果不如Post Norm？\nhttps://spaces.ac.cn/archives/9009<br>\n【15】Understanding the Difficulty of Training Transformers\nhttps://arxiv.org/abs/2004.08249<br>\n【16】RealFormer: Transformer Likes Residual Attention\nhttps://arxiv.org/abs/2012.11747<br>\n【17】DeepNet: Scaling Transformers to 1,000 Layers\nhttps://arxiv.org/abs/2203.00555</p>\n","length":11986,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>Normalization在模型中，相对于attention这种经常被魔改的结构，受到的关注度似乎没那么高，但它对模型能否顺利训练，却有很关键的作用。</p>\n<p>在此简单梳理下normalization相关的背景和内容，也分析一下在transformer发展上的相关内容。</p>\n<p>这部分内容感觉目前还有些存在争议的地方，如果有不同意见欢迎讨论。</p>\n<h1 id=\"why-normalization\">why normalization</h1>\n<p>normalization，也叫「归一化」、「正则化」、「规范化」、「标准化」等，可以说已经是神经网络不可以或缺的一环。</p>\n<p>使用的话，现在基本只需几行代码就能实现。但要用得好，还是需要了解一下它作用的机制。</p>\n<h2 id=\"从输入数据看normalization\">从输入数据看normalization</h2>\n<p>假设我们有一个二元损失函数 <span class=\"math inline\">\\(Loss(x_1,x_2)=x_1^2+x_2^2+b\\)</span>\n，那在三维空间画出来的损失平面大概是这样</p>\n<img src=\"/6a40bfa5/lossfunc_surface.jpeg\" class title=\"loss function surface\">\n<p>在这样一个平面上，使用梯度下降法，梯度方向是垂直于当前位置等高线的切线方向的。</p>\n<p>如果这个损失函数的等高线是一系列完美的同心圆，那么无论我们起点在哪里，梯度下降的时候都会以垂直切线方向，沿着圆心一路奔去。</p>\n<p>这种情况下优化很快，控制好学习率不要跳过minimum就可以（也可以用自适应优化器来控制速度）。</p>\n<p>但是实际上我们的损失平面很难那么完美。损失函数的等高线更可能是个椭圆（或者更复杂的形状）。</p>\n<img src=\"/6a40bfa5/ellipse_1.png\" class title=\"ellipse\">\n<p>这样我们梯度下降是方向就要经常修正，训练效率就会受影响。</p>\n<p>如果这个椭圆很扁或者我们的训练参数不太好，可能会出现反复震荡收敛缓慢的情况。</p>\n<img src=\"/6a40bfa5/ellipse_2.png\" class title=\"ellipse\">\n<p>损失在这个狭窄的山谷中反复横跳。</p>\n<p>那损失函数等高线什么时候会是椭圆形？</p>\n<p>假设我们现在有两个输入变量，以米为单位的身高 <span class=\"math inline\">\\(x_{1}\\)</span>，和以元为单位的月工资收入 <span class=\"math inline\">\\(x_{2}\\)</span>。（这里对量纲的使用也会改变数值，如米-&gt;厘米）</p>\n<p>如果我们用这两个自变量训练模型，我们会发现，身高取值范围基本是在0.x米~2.x米，而工资的取值范围是0到几百几千几万或者几十万以及更多。</p>\n<p>而模型的一个主要操作就是对输入特征进行线性组合。</p>\n<p>这时模型的输出值会更大地受到 <span class=\"math inline\">\\(x_{2}\\)</span>\n的影响，因为它的变化更大，取值范围也更大。</p>\n<p>这时损失函数在不同变量维度的变化速度相差很多，损失函数就会出现椭圆形等高线的情况。</p>\n<p>既然由于量纲和取值范围的问题，会导致训练困难，那最直接方法就是规定一个标准范围，所有输入变量，不管原来是什么范围，现在都归一化到标准范围里来。</p>\n<p>这就是最朴素的输入normalization的思想。</p>\n<p>输入的normalization有很多种做法<br>\n<span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}\\sigma\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\mu\\)</span> 为均值，<span class=\"math inline\">\\(\\sigma\\)</span> 为方差。</p>\n<p>第三种，均值方差归一化，也叫Z-score\nnormalization，应该是我们用得比较多的。</p>\n<p>这样我们通过对输入进行一些操作，把「椭圆」拉成了「圆」，解决输入参数范围带来的一些训练问题。</p>\n<p>除了针对均值、方差、最大值、最小值的归一化，对输入还有一些其他的处理，如PCA等，就暂不展开。</p>\n<h2 id=\"缓解ics...吗\">缓解ICS...吗？</h2>\n<p>机器学习里有一个叫i.i.d.（independent and identical\ndistribution，独立同分布）的假设：独立，每次抽样之间是没有关系的，不会相互影响；同分布，即每次抽样，样本都服从同样的一个分布。</p>\n<p>为什么需要i.i.d.？</p>\n<p>由于机器学习依赖于使用现有数据来训练模型，进而对未来的数据做出预测和模拟，因此这一过程本质上是在历史数据的基础上，通过模型来推测未来的数据走向。</p>\n<p>这就要求我们使用的历史数据必须具备整体的代表性。以便从现有数据（经验）中提炼出规律，对未知数据进行决策。</p>\n<p>如果用于训练的数据缺乏总体代表性，即仅代表特殊情况，那么得出的规律可能不准确或错误，因为这些规律是基于个别案例推测出来的，就不具备泛化性。</p>\n<p>当然并不是所有机器学习都需要i.i.d.，但是有i.i.d.的话，可以简化很多事情，让模型学习起来更容易快速。</p>\n<p>对于输入，通过合理的抽样和处理（前面提到的PCA就可以用来解耦特征间的关联，达到“独立”的效果），我们可以得到输入的i.i.d.的条件，但这只是针对输入。</p>\n<p>在多层的神经网络中，上一层的输出会作为下一层的输入。</p>\n<p>而在训练过程中，由于上层的模型参数在不断学习变化，则上层输出的分布也在不断变化，靠后的层实际上要学习不断的变化的分布，这就很不i.i.d.，那靠后面的层的学习速度和效果就会收到影响，调参也变得困难，模型也难以加深。</p>\n<p>这个问题就是ICS，internal covariate shift。</p>\n<p>那有没有办法保证上一层的分布不要变化呢？</p>\n<p>一个「可能」的方案就是normalization。我们通过把上一层的输出映射到一个固定的分布上，来稳定给下一层的输入，这样就降低了学习难度。</p>\n<p>但也有一些工作表明normalization（batchnorm）的作用机制和ICS的关系并不大，这个观点下面在batchnorm部分说。</p>\n<p>当然ICS的问题也可以通过改变初始化策略、调控训练超参如学习率等方法来优化，但是这样做的效率并不是很高。</p>\n<h2 id=\"远离激活函数饱和区\">远离激活函数饱和区</h2>\n<p>神经网络中还有一个重要组件，非线性激活函数，比如常用的sigmoid。</p>\n<img src=\"/6a40bfa5/sigmoid.png\" class title=\"sigmoid\">\n<p>当输入 &gt; 6 或者 &lt; -6\n的时候，sigmoid函数的梯度已经变得非常小，也就是进入了饱和区。</p>\n<p>这种情况下训练就变得困难。</p>\n<p>ICS就会加剧梯度消失的情况。在没有normalization的情况下，分布不断变化，后面层的参数变化激烈，导致输出值更容易进入到左右两端，更容易进入到激活函数的饱和区。</p>\n<p>而normalization能把部分输出值拉回到梯度正常的范围内，一定程度缓解了梯度消失的问题，使训练可以正常进行下去。</p>\n<h1 id=\"batchnorm\">batchnorm</h1>\n<p>神经网络中使用的normalization有很多种，这里不一一展开，只梳理一下最重要的batchnorm和layernorm两类。</p>\n<h2 id=\"batchnorm算法\">batchnorm算法</h2>\n<p>假设输入数据的形状是 <span class=\"math inline\">\\([B,C]\\)</span>\n，其中 <span class=\"math inline\">\\(B\\)</span> 是batch size，<span class=\"math inline\">\\(C\\)</span> 是特征向量维度。</p>\n<p>这 <span class=\"math inline\">\\(C\\)</span>\n个输入特征每个都有不同的含义，如我们前面的例子，第一个元素可能是身高，第二个元素可能是月收入，因此做normalization的时候这\n<span class=\"math inline\">\\(C\\)</span> 个特征分别来做。</p>\n<p>具体来说，对于第 <span class=\"math inline\">\\(i\\)</span>\n个特征维度，首先计算整个batch内的均值</p>\n<p><span class=\"math display\">\\[\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n\\]</span></p>\n<p>再计算这个维度上的方差</p>\n<p><span class=\"math display\">\\[\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n\\]</span></p>\n<p>得到均值和方差之后，对batch内维度上的所有值进行Z-score\nnormalization</p>\n<p><span class=\"math display\">\\[\nx_{i,j}&#39;=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\epsilon\\)</span>\n是为了防止分母为0。这个在实际代码中挺重要的，忘记加可能会出问题。</p>\n<p>经过这样的变换之后，在 <span class=\"math inline\">\\(C\\)</span>\n个特征维度上就是均值为0，方差为1的分布了。</p>\n<p>但是到这还没结束。</p>\n<p>每个维度的数值全部归一化之后，对于激活函数来说，更集中在中间的部分，而这部分的非线性特征并不强（比如上面的sigmoid），这样非线性激活层近似了一个线性变换，这样就降低了模型的学习能力。</p>\n<p>且无论输入是什么，最终输出都会被强行拉到这样一个“平均”的值，也极大抑制了模型的表达能力。</p>\n<p>所以为了保证模型的能力，也保证非线性能力的获得，对每个特征，又增加两个可学习的参数，\n缩放参数 <span class=\"math inline\">\\(\\gamma\\)</span> 和位移参数 <span class=\"math inline\">\\(\\beta\\)</span> 。</p>\n<p><span class=\"math display\">\\[\ny_{i,j} = \\gamma_{i} x_{i,j}&#39; + \\beta_{i}\n\\]</span></p>\n<p>这样每个特征值就有机会从“线性区”移动到“非线性区”，把被归一化削弱的非线性能力找了回来。</p>\n<p>并且通过这样一个归一化再重新缩放移动的操作，解耦了上层输出分布和下层输入，本来下层参数要去适应上层分布变化，现在只需要通过每个batchnorm层中的\n<span class=\"math inline\">\\(\\gamma\\)</span> 和 <span class=\"math inline\">\\(\\beta\\)</span>\n直接学习就行了，训练变得简单了。</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/340856414\">《Batch\nNormalization: Accelerating Deep Network》</a>给出的算法如下</p>\n<img src=\"/6a40bfa5/bn_algo.png\" class title=\"batch norm\">\n<h2 id=\"cnn中的batchnorm\">CNN中的batchnorm</h2>\n<p>batchnorm最主要的应用还是在CNN模型中。</p>\n<p>假设CNN中feature map的size是 <span class=\"math inline\">\\([B,C,H,W]\\)</span> ，其中 <span class=\"math inline\">\\(B\\)</span> 是batch size，<span class=\"math inline\">\\(C\\)</span> 是channel数（也是卷积核数量），<span class=\"math inline\">\\(H\\)</span> 和 <span class=\"math inline\">\\(W\\)</span> 分别是特征图的高和宽。</p>\n<p>如果按照前面batchnorm的算法，那应该有 <span class=\"math inline\">\\(C\\times H\\times W\\)</span> 组特征，每组特征有\n<span class=\"math inline\">\\(B\\)</span> 个，对每组内的 <span class=\"math inline\">\\(B\\)</span> 进行归一化，再进行放缩和平移。</p>\n<p>但是实际上，CNN中卷积是一个滑动窗口，对于同一个channel下的 <span class=\"math inline\">\\(H\\times W\\)</span>\n个特征值其实都来自于同一个卷积核的计算，这 <span class=\"math inline\">\\(H\\times W\\)</span>\n也属于一个“batch”，它们要放在一起进行归一化。</p>\n<p>也就是对于卷积核来说，真正的batch数是 <span class=\"math inline\">\\(B\\times H\\times W\\)</span> ，而只有 <span class=\"math inline\">\\(C\\)</span> 组特征值，因此也只有 <span class=\"math inline\">\\(C\\)</span> 个 <span class=\"math inline\">\\(\\gamma\\)</span> 和 <span class=\"math inline\">\\(\\beta\\)</span> 。</p>\n<p>batchnorm原文中，batchnorm放在了relu后面，作者认为这样使得进入激活函数的分布会更加稳定，顺便对于fc层，由于batchnorm和fc都有bias项，还可以省略掉其中一个而不影响效果。</p>\n<p>btw，一般来说，batchnorm初始化的时候，把 <span class=\"math inline\">\\(\\gamma\\)</span> 设为1（不缩放），把 <span class=\"math inline\">\\(\\beta\\)</span>\n设为0（不平移），在训练中让模型从相当于没有batchnorm开始慢慢学习这两个参数。</p>\n<h2 id=\"训练和推理\">训练和推理</h2>\n<p>现在我们知道在训练时，batchnorm对一个mini-batch计算均值和方差来进行归一化，再进行缩放和移动。</p>\n<p><span class=\"math inline\">\\(\\gamma\\)</span> 和 <span class=\"math inline\">\\(\\beta\\)</span>\n属于模型学出来的参数，只要训练结束这两个向量就固定了，在推理的时候直接使用即可。</p>\n<p>但是推理时，均值和方差怎么计算呢。推理的时候可能是一个sample，也可能是任意个sample作为一个batch，和训练的时候一样计算肯定不合适。</p>\n<p>我们需要在训练的时候就为推理做准备：训练的时候，模型会遍历整个训练集，因此理论上可以统计出整个训练集的均值和方差，然后把这个大量样本统计出来的均值和方差当做真实分布的均值和方差，在推理的时候使用。（回想i.i.d.）</p>\n<p>当时又有一个问题，训练集可能会很大，有百万甚至千万的数据，在训练的数据记录下所有层所有特征来计算均值和方差显然效率不高，因此用一个近似的方法：</p>\n<p>moving_mean = momentum × moving_mean + (1.0 − momentum) × mean</p>\n<p>moving_var = momentum × moving_var + (1.0 − momentum) × var</p>\n<p>通过把多个batch的均值和方差进行移动平均的方式来逼近整个训练集的均值和方差。</p>\n<p>momentum为动量参数，在 TF/Keras 中，该值为0.99，在 Pytorch\n中，这个值为0.9。</p>\n<p>小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动。</p>\n<p>大的momentum值对应慢的更新速度，如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。</p>\n<p>如果batch size比较小，每个mini\nbatch和全局差异较大，就不应该用太大的momentum。</p>\n<p>理论上，训练步数越长是会越靠近真实分布的，实际上，因为每个batch并不能代表整个训练集的分布，所以最后的值是在真实分布附近波动。</p>\n<p>这里还引入另外一个问题，如果batch size太小，每个mini\nbatch统计的均值和方差和全局的值偏差相对会比较大，对模型收敛的稳定性有影响，因此一般来说，使用batchnorm的话，batch\nsize不能太小，如下图</p>\n<img src=\"/6a40bfa5/bs_bn.png\" class title=\"batch size的影响\">\n<p>小结一下，batchnorm的优点是解耦了上层输出和下层输入的分布，既缓解了进入激活函数饱和区带来的梯度消失的情况，又保留了模型的表达能力。每一层的输入尺度相对固定，提供了更好的尺度不变形，使模型训练更稳定。</p>\n<p>同时每个batch分别进行归一化，相当于引入了一些随机噪音，使得模型不容易过拟合到某些微小的特征上，相当于进行了一定的正则化，将损失平面变得相对平滑。</p>\n<p>但是同时也引入了新的超参（如momentum），另外也依赖batch\nsize的大小，过小的batch size可能会带来问题。</p>\n<h2 id=\"batchnorm起作用的真正原因\">batchnorm起作用的真正原因？</h2>\n<p>虽然batchnorm原文认为batchnorm在一定程度上是缓解了ICS，但是2018年的《How\nDoes Batch Normalization Help Optimization?》提出了不同观点。</p>\n<p>为了探究batchnorm的效果，是否是因为优化了ICS（或者说和优化了ICS有多大关系），做了一个这样的实验：在batchnorm后面又通过加入随机噪音来引入“covariate\nshift”，并和没有加噪音，以及没有加batchnorm的模型效果进行对比，如下图</p>\n<img src=\"/6a40bfa5/bn_ics.png\" class title=\"ICS\">\n<p>结果发现，即使人工加强了ICS的情况，但是只要用了batchnorm，效果依然比不用好；而人工引入ICS的模型，在效果上并没有多大影响。</p>\n<p>这就说明缓解ICS并不是batchnorm有效的真正原因。</p>\n<p>那batchnorm到底有没有缓解到ICS呢？</p>\n<p>要测量ICS的变化，就要先定义ICS。</p>\n<p>对于网络中的每一层，ICS被定义为在前一层参数更新后，当前层输入分布的变化。这种变化可以通过比较更新前后的梯度来量化。</p>\n<img src=\"/6a40bfa5/ics_define.png\" class title=\"ICS 定义\">\n<p>具体来说，对于每一层i，作者们计算了以下两个梯度之间的L2范数差异：</p>\n<p><span class=\"math inline\">\\(G_{t,i}\\)</span>\n，在时间t，使用当前所有层的参数（包括前一层的参数）计算的梯度。</p>\n<p><span class=\"math inline\">\\(G_{t,i}&#39;\\)</span>\n，在时间t，使用更新后的前一层参数计算的梯度，而其他层的参数保持不变。</p>\n<p>这个差异直观上表明了「上一层参数变化，下一层需要在多大程度上来变化，以适应新的分布」。</p>\n<p>理想来说，ICS越小，上一层参数更新对当前层的分布影响越小，梯度变化程度应该越小。</p>\n<img src=\"/6a40bfa5/ics_measure.png\" class title=\"ICS measure\">\n<p>但是从结果上来看，使用了batchnorm并不能有效减少这个变化，甚至还有所增加。</p>\n<p>这也说明batchnorm实际上并不能真正缓解ICS的情况。</p>\n<p>那batchnorm起效果的真正原因是什么？</p>\n<p>作者认为主要是batchnorm使得损失函数更加平滑，直观上来说就是减少了很多坑坑洼洼的位置，使得训练更不容易陷入到局部最小值中去。</p>\n<h1 id=\"layernorm\">layernorm</h1>\n<p>看完batchnorm，再来看layernorm。</p>\n<h2 id=\"理解layernorm\">理解layernorm</h2>\n<p>layernorm，不要被名字骗了，这里的layer指的不是模型的层，而是数值的layer。</p>\n<p>对于二维的输入，batchnorm实在batch维度上做归一化，而layernorm是在特征维度做归一化</p>\n<img src=\"/6a40bfa5/bn_and_ln.png\" class title=\"bn和ln\">\n<p>对于非NLP数据而言，相比batchnorm，layernorm归一化的维度似乎解释性没那么强。batchnorm对同一个特征，比如身高计算均值是有意义的，而layernorm在不同的特征，比如身高、工资、温度做归一化，好像并没有可靠的物理意义。</p>\n<p>layernorm最主要的应用就是NLP的模型，包括RNN和transfomrer模型。</p>\n<p>在transformer中，一般输入的形状是 <span class=\"math inline\">\\([B,S,H]\\)</span> ，<span class=\"math inline\">\\(S\\)</span>\n是序列长度，每个样本的长度可能不同，因此在这个维度需要使用padding（一般是zero-padding）来把batch内的数据处理成一样长。</p>\n<p>比如这样一批文本输入</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">我  爱  中  国</span><br><span class=\"line\">你  好</span><br><span class=\"line\">谢  谢  你</span><br></pre></td></tr></table></figure>\n<p>为了使模型能够统一处理，会pad成</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">我  爱  中  国</span><br><span class=\"line\">你  好  [P] [P]</span><br><span class=\"line\">谢  谢  你  [P]</span><br></pre></td></tr></table></figure>\n<p>一般来说，我们认为由于有padding的存在，做batchnorm并不合适。</p>\n<p>比如上面的例子，对“中”，“[P]”，“你”做归一化，由于 [P]\n的存在，实际的batch size只有2，并且和 [P]\n做normalization也对训练没什么帮助。</p>\n<p>而且对于文本数据，batch内同一个位置上的字是不同的，对完全没有关系的字进行归一化也并没有什么意义。</p>\n<p>也就是说，哪怕没有 [P]\n的存在，比如对第一个token“我”，“你”，“谢”做归一化，直觉上也不太有物理意义。</p>\n<p>因此使用layernorm，在 <span class=\"math inline\">\\(H\\)</span>\n维度上进行normalization，同时有 <span class=\"math inline\">\\(H\\)</span>\n个<span class=\"math inline\">\\(\\gamma\\)</span> 和 <span class=\"math inline\">\\(\\beta\\)</span> 需要学习。</p>\n<p>相当于计算每一句输入内，每个token所有特征之间的均值和方差来进行归一化。</p>\n<h2 id=\"为什么transformer用layernorm\">为什么transformer用layernorm</h2>\n<p>和batchnorm不同的是，由于layernorm不需要再batch维度上计算均值和方差，所以不存在训练和推理的时候不一样的地方，不用保存一个全局的均值和方差供推理的时候使用。</p>\n<p>而由于layernorm和batch无关，也就不会受到batch size大小的影响。</p>\n<p>除了以上的原因，也有一些工作深入探究了在nlp任务上layernorm和batchnorm的区别。</p>\n<p>如《PowerNorm: Rethinking Batch Normalization in\nTransformers》就研究了transformer中BN为啥表现不太好。</p>\n<p>研究了训练中的四个统计量：batch的均值和方差，以及他们的梯度的均值和方差。对于batch的均值和方差，计算了他们和running\nstatistics（就是用移动平均法累积的均值和方差，见前面的文章）的欧氏距离。发现NLP任务上（IWSLT14）batch的均值和方差一直震荡，偏离全局的running\nstatistics，而CV任务也相对稳定。</p>\n<p>对于他们梯度的均值和方差，研究了其magnitude（绝对值），在CV任务上震荡更小，且训练完成后，也没有离群点。</p>\n<p>总结来说，transformer中BN表现不太好的原因可能在于CV和NLP数据特性的不同，对于NLP数据，前向和反向传播中，batch统计量及其梯度都不太稳定。</p>\n<p>更重要的是，实际效果就是layernorm在NLP的效果比batchnorm好，效果好，这是最重要的原因。</p>\n<h2 id=\"rmsnorm\">RMSnorm</h2>\n<p>19年《Root Mean Square Layer\nNormalization》提出了normalization变体RMSnorm，主要针对layernorm来改进。</p>\n<p>简单地说，RMSnorm就是在标准layernorm的基础上，省略了平移，只进行缩放。</p>\n<img src=\"/6a40bfa5/rmsnorm.png\" class title=\"RMSnorm\">\n<p>作者认为标准layernorm计算效率并不高</p>\n<img src=\"/6a40bfa5/rmsnorm_eff.png\" class title=\"RMSnorm效率\">\n<p>作者用一个GRU模型做实验，对比是否添加layernorm的结果，发现在相同时间和相同步骤下，有layernorm的模型，都没有无layernorm的模型收敛得快。</p>\n<p>并且layernorm的平移对梯度方差的减小没有贡献，因此作者直接舍弃了中心化和平移两步，只对数据进行方差归一化和缩放。</p>\n<p>更近一步，作者提出pRMSnorm，只对数据中前p%的数值进行处理，这样就能进一步加速训练，而效果也基本不太受影响。</p>\n<img src=\"/6a40bfa5/prmsnorm.png\" class title=\"prmsnorm\">\n<p>RMSnorm现在被很多主流的大模型所采样了。</p>\n<h1 id=\"post-norm-pre-norm\">post-norm &amp; pre-norm</h1>\n<h2 id=\"二者对比\">二者对比</h2>\n<p>layernorm在模型里放哪也有讲究。</p>\n<p>原始的transformer模型使用的post-norm，而《On Layer Normalization in\nthe Transformer Architecture》则认为pre-norm更好。</p>\n<p>post-norm和pre-norm分别是下面这样</p>\n<img src=\"/6a40bfa5/postnorm_prenorm.png\" class title=\"postnorm and prenorm\">\n<p>post-norm是在残差和主干相加之后进行归一化，而pre-norm则是在主干先归一化再和残差相加。</p>\n<p>post-norm和pre-norm对比，目前大家比较接受的结论是，pre-norm更容易训练，因此可以叠加更多的层，但是在层数不是特别多的情况下，post-norm最终的收敛效果会比pre-norm要好。</p>\n<p>模型中，第 <span class=\"math inline\">\\(l\\)</span> 层的输出是第 <span class=\"math inline\">\\(l+1\\)</span> 层的输入，对于post-norm有</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n\\]</span></p>\n<p>而对于pre-norm则是</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n\\]</span></p>\n<p>参考苏剑林在《为什么Pre Norm的效果不如Post Norm？》中的分析，认为\n<span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>\n的方差，由于有norm的存在，是不随层数变化的。</p>\n<p>当 <span class=\"math inline\">\\(l\\)</span> 比较大时，<span class=\"math inline\">\\(x_{l}、x_{l+1}\\)</span> 的差距较小，因此 <span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span> 和 <span class=\"math inline\">\\(\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))\\)</span>\n的差距也很小，这时有</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1}))\n\\\\\n&amp;{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right)\n\\\\\n&amp;=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}\\]</span></p>\n<p>相当于 <span class=\"math inline\">\\(l\\)</span> 层和 <span class=\"math inline\">\\(l+1\\)</span> 层的效果接近于一个更宽的 <span class=\"math inline\">\\(l\\)</span> 层的效果。</p>\n<p>也就是使用pre-norm的时候，模型的深度有水分，表面看起来有 <span class=\"math inline\">\\(l\\)</span>\n层，实际在效果上，等效于post-norm的浅层模型。</p>\n<p>从模型结构上看，恒等分支永远有一部分不用经过normalization，这部分能够直接把梯度回传到最前面，这也是pre-norm能够训练“层数更多”的模型的原因--缓解了梯度消失。</p>\n<p>正常来说，模型深度对最终效果的影响，是大于模型宽度的。</p>\n<p>而post-norm，在残差分支之后做归一化，对参数正则化的效果更好（loss平面更平滑），且它每norm一次就削弱一次恒等分支的权重，所以post-norm相对pre-norm，是更突出残差分支的，因此它的层数更加“足秤”，训练好之后效果更优。</p>\n<h2 id=\"和warmup的关系\">和warmup的关系</h2>\n<p>《On Layer Normalization in the Transformer\nArchitecture》（认为pre-norm更好）还分析指出，使用post-norm的transformer，在初始化时候，靠近输出层的部分梯度期望很大，所以模型在开始训练的时候很依赖warmup的策略，通过缓慢提升学习率来稳定训练过程。</p>\n<p>使用warmup引入了新的超参，调参更为麻烦点。</p>\n<p>而实验表明，使用pre-norm的transformer在不需要warmup的情况下，也能收敛到post-norm+warmup的相同水平，而post-norm不加warmup效果就差点了。</p>\n<img src=\"/6a40bfa5/warmup_effect.png\" class title=\"warmup影响\">\n<h2 id=\"deepnorm\">Deepnorm</h2>\n<p>2022年，《DeepNet: Scaling Transformers to 1,000\nLayers》对transformer训练不稳定的原因进行了深入分析，发现模型更新过大是导致不稳定的主要原因。</p>\n<p>为了解决这个问题，他们提出了Deepnorm，可以限制模型更新的大小。</p>\n<img src=\"/6a40bfa5/deepnorm.png\" class title=\"deepnorm\">\n<p>其中 <span class=\"math inline\">\\(\\alpha&gt;1\\)</span>\n是根据模型参数定的常数。这里相比post-norm提升了恒等分支的权重，使训练更容易进行。</p>\n<p>另外，还用了一个 <span class=\"math inline\">\\(\\beta\\)</span> 参数，把\n<span class=\"math inline\">\\(G_{l}\\)</span>\n中的模型参数进行了缩小，以此来稳定模型的训练。</p>\n<p>实验结果上，deepnorm结合了pre-norm的容易训练，和post-norm的收敛效果好的特点，能够把百层、浅层的模型训到比较好的效果。</p>\n<img src=\"/6a40bfa5/deepnorm_result.png\" class title=\"deepnorm result\">\n<p>参数过程相比post-norm稳定了很多。</p>\n<h2 id=\"realformer--residual-attention\">Realformer--residual\nattention</h2>\n<p>post-norm和pre-norm实际上改变的是模型残差分支和恒等分支怎么排布的问题，而《RealFormer:\nTransformer Likes Residual Attention》则提出了另外一种做法</p>\n<img src=\"/6a40bfa5/realformer.png\" class title=\"realformer\">\n<p>RealFormer的核心是在其标准Transformer编码器的每一层中引入了残差连接。这些残差连接将前一层的原始注意力分数（即在应用Softmax之前的分数）与当前层计算出的注意力分数相结合。这样做的结果是，当前层的注意力分数在计算时会考虑到前一层的信息。</p>\n<p>每个多头注意力模块都会接收来自前一层的残差注意力分数作为额外输入。这意味着每个注意力头不仅考虑了当前层内的输入序列，而且还直接利用了前一层的注意力信息。</p>\n<img src=\"/6a40bfa5/realformer_attention.png\" class title=\"realformer attention\">\n<p>其中 <span class=\"math inline\">\\(Prev&#39;\\)</span>\n是来自上一层softmax之前的权重矩阵（多头注意力的话，则是对应的头的值），而\n<span class=\"math inline\">\\(\\frac{Q^{\\prime}K^{\\prime\nT}}{\\sqrt{d_k}}+Prev&#39;\\)</span> 则是传给下一层的attention的。</p>\n<h1 id=\"小结\">小结</h1>\n<p>本篇粗略梳理了一下关于normalization，batchnorm，以及layernorm在transformer的一些使用情况。</p>\n<p>目前主流的大模型使用的是rmsnorm + prenorm，也有使用其他变体的。</p>\n<p>关于normalization，依然留有一些探索空间。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>往期文章</p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】https://www.zhihu.com/question/487766088<br>\n【2】Towards Stabilizing Batch Statistics in Backward Propagation of\nBatch Normalization https://arxiv.org/abs/2001.06838<br>\n【3】Transformer中的归一化(一)：什么是归一化&amp;为什么要归一化\nhttps://zhuanlan.zhihu.com/p/476102712<br>\n【4】Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift https://arxiv.org/abs/1502.03167<br>\n【5】How Does Batch Normalization Help Optimization?\nhttps://arxiv.org/abs/1805.11604<br>\n【6】Batch Normalization: Accelerating Deep Network\nhttps://zhuanlan.zhihu.com/p/340856414<br>\n【7】Layer Normalization https://arxiv.org/abs/1607.06450<br>\n【8】详解深度学习中的Normalization，BN/LN/WN\nhttps://zhuanlan.zhihu.com/p/33173246<br>\n【9】Transformer中的归一化(四)：BatchNormalization的原理、作用和实现\nhttps://zhuanlan.zhihu.com/p/481277619<br>\n【10】Layer Normalization https://arxiv.org/abs/1607.06450<br>\n【11】PowerNorm: Rethinking Batch Normalization in Transformers\nhttps://arxiv.org/abs/2003.07845<br>\n【12】Root Mean Square Layer Normalization\nhttps://arxiv.org/abs/1910.07467<br>\n【13】On Layer Normalization in the Transformer Architecture\nhttps://arxiv.org/abs/2002.04745<br>\n【14】为什么Pre Norm的效果不如Post Norm？\nhttps://spaces.ac.cn/archives/9009<br>\n【15】Understanding the Difficulty of Training Transformers\nhttps://arxiv.org/abs/2004.08249<br>\n【16】RealFormer: Transformer Likes Residual Attention\nhttps://arxiv.org/abs/2012.11747<br>\n【17】DeepNet: Scaling Transformers to 1,000 Layers\nhttps://arxiv.org/abs/2203.00555</p>\n"},{"title":"Yi技术报告-划重点看细节","abbrlink":"41b6a819","date":"2024-03-26T08:51:08.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n01.AI（零一万物），是李开复带队孵化的AI公司。2023年11月初，01.AI发布并开源了Yi-6B、Yi-34B base模型，同一周内，又开源了Yi-6B-200K和Yi-34B-200K base模型。Yi号称是从零预训练的双语模型。接下来的几个月，01.AI陆续推出了chat模型、多模态能力，Yi-9B、长上下文的记忆和检索能力等优化。  \n\n从2023年11发布起，个人就有测试和使用Yi的模型。在SuperCLUE/CMMLU等一些榜单数据的实测上，发现Yi的效果确实不错。实际工作使用里，Yi的效果基本也都能排在同时期中文（开源）大模型里的第一梯队。  \n\n2024年3月，Yi终于发布了技术报告，在此来梳理一下报告中的重点内容和值得关注的细节信息。  \n\n# TL;DR\n\n先给出核心内容的总结：  \n\n- Yi-34B模型int4量化之后，相比float16损失<1%，可以跑在RTX4090上（24G显存）\n- 模型结构不需要太多变化，LLAMA2标准结构已经足够训出很好的效果\n- 3.1T的预训练数据远比scaling law建议的1T大，但是效果更好，并且模型还没饱和，继续增大数据量还能提升\n- 微调数据质量很重要，由算法人员直接标注，只要<10k的数据量就足够了\n- 4k长度的基础预训练模型已经具备长文本能力，只需用长文本数据继续预训练，更新百步就有很好效果\n- 总之，数据要精心设计，数据质量要高，数据量要大\n\n# 模型\n\n## 规模选择  \n\nYi目前有6B、9B、34B三个规模，其中34B是主力模型。  \n\n选择34B，而不是更大规模的原因，是这个规模能在24G显存的消费级显卡（如RTX4090）上运行。  \n\n使用int4量化之后的34B模型可以运行在24G显存的GPU上。  \n\n参考[《Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases》](https://arxiv.org/abs/2301.12017)的量化方法，Yi-34B的int8量化模型相比bf16模型，几乎可以做到效果无损（差距<1%），而int4量化模型在大部分任务的损失也完全可以接受，具体效果如下表。  \n\n{% asset_img eval.png Yi效果 %}  \n\n训练数据总共是3.1T token，这比DeepMind的scaling law所建议的1TB要大不少。目前能接触到的这个规模的模型，数据量基本都<2T。  \n\n（即提出Chinchilla模型的[《Training Compute-Optimal Large Language Models》](https://arxiv.org/abs/2203.15556)的scaling law）  \n\n也就是从scaling law的角度来说，Yi是overtrain的。  \n\n<big>**但是Yi实践结果证明，较小模型+更大规模的高质量数据，是可以获得进一步效果提升的，这也就让我们获得了高性价比的推理模型--34B推理成本+大训练投入，就能得到接近普通70B规模的推理效果。**</big>  \n\n## 模型结构  \n\n结构上，基于标准LLAMA2模型，做了一些变化  \n\n- 注意力机制：LLAMA2只在70B用了GQA，Yi全系列都用了GQA，具体参数如下表  \n- 位置编码：RoPE，参考RoPE ABF（《Effective long-context scaling of foundation models》），base扩大到10M，用于支持长上下文。  \n- 激活函数：使用SwiGLU，参考《GLU Variants Improve Transformer》  \n\n并且把activation size从4h降为8/3h，这里的说法是补偿了GQA带来的参数下降  \n\n> We use SwiGLU as Yi’s post-attention layer, reducing its activation size from 4h to 8/3h (h denotes hidden size) to be consistent with the normal post-attention layer. This adjustment also compensates for the reduction in parameter resulted from GQA, making the overall parameter count comparible of existing 7B and 34B models.  \n\n{% asset_img model.png Yi模型结构 %}  \n\n关于模型结构，一个结论就是<big>**“虽然做了很多模型结构上的实验，但是最终发现，标准的结构就足以训出足够好的模型”**</big>  \n\n## tokenizer  \n\n- 用BPE，词表大小为64000，这个大小平衡了计算效率和表达能力；  \n- 其中数字全是单个的digit，让模型能更好地理解数字数据；  \n- 对于OOV的词，会降级用unicode编码 ； \n- 保留全角标点符号，不转为半角；  \n  \n另外，优先考虑英语的LLM在tokenizer会使用虚拟前缀（文本开头的空格）来泛化句子不同位置相同的单词。Yi不这么做，因为即使是在英语语境中，这种假设并不总是成立，比如对于以引号开头的句子，而且在中文语境中，这么做没有明显效果。  \n\n# 数据  \n\n数据，是LLM最核心的部分，没有之一。Yi最核心的工作就是提升数据数量和质量。  \n\n{% asset_img cover.png 数据 %}  \n\n## 预训练数据  \n\n预训练数据整体处理流程如下  \n\n{% asset_img pretrain_data_pipeline.png 预训练数据处理流程 %}  \n\n1. 语料获取 & 语言分类  \n\n从网络爬虫开始，爬取中英文这两种语言的网站，对网站内容进行解析。  \n\n并参考CCNeT（《CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data》）的做法，进行语言识别。  \n\n2. 规则过滤器 Heuristic Rule Filters  \n\n目的是快速过滤掉明显的低质量数据。基于这些规则来过滤掉：  \n\n- URL、域名、单词黑名单和乱码文本；  \n- 文档长度、特殊符号的比例，以及短行、连续行或不完整行的比例；  \n- 重复的单词模式、n-gram或段落，参考《Scaling Language Models: Methods, Analysis & Insights from Training Gopher》的做法，阈值则是来参考《CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages》；\n- 数据脱敏：识别并匿名化个人信息（Personal Identifiable Information，PII），如电子邮件地址和电话号码。\n\n3. 可训练过滤器 Learned Filters  \n\n对于不好用规则处理的，就用模型来学习模式，并进行清洗。共有4个scorer：  \n\n- Perplexity Scorer：参照《CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data》，用kenlm库，把高于平均perplexity的内容丢弃；\n- Quality Scorer：识别如维基百科这样的高质量内容，丢弃低质量内容；\n- Document Coherence Scorer：用于发现句子、段落零散不连贯的文本，要么分割，要么直接丢弃；\n- Safety Scorer：识别并删除暴力、色情、涉政内容\n\n4. 基于聚类的过滤 Cluster-based Filters  \n\n用聚类的方法，把所有文档进行分类。一方面用于给数据混合策略做参考，一方面如果整个类别的质量太差，就直接抛弃类别内的所有数据。  \n\n5. 去重\n\n参考《The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only》，做文档级的minhash去重，以及子文档级的完全匹配去重。  \n\n最终获得的数据分布如下  \n\n{% asset_img pretrain_data_dist.png 预训练数据分布 %}  \n\n虽然数据规模一定要够，但是也不能因此就放弃数据质量，否则只能是garbage in，garbage out\n\n> we prefer 3T tokens over sophasticated engineering over 10T tokens without extensive filtering\n\n这句话大概表示清洗前的数据有10T，这也是一个信息，符合质量的数据可能只有3成  \n\n## 微调数据 \n\n对于微调数据，一句话：Quality is All You Need。  \n\n一共只有<10k条SFT数据，每条数据都通过人工多次打磨，这比大数量但质量一般的数据的效果好。  \n\n这思路和《Gemini: A family of highly capable multimodal models.》、《Llama 2: Open Foundation and Fine-Tuned Chat Models》、《Lima: Less is more for alignment》一致，而和FLAN（《Scaling instruction-finetuned language models》）以及UltraChat（《Enhancing chat language models by scaling high-quality instructional conversations》）这样更关注数据量的做法不同。  \n\n具体做法上有：  \n\n- 对于<big>**prompt distribution selection**</big>：参考《Wizardlm: Empowering large language models to follow complex instructions》，开发复合指令，并通过指令进化，逐步增加指令的复杂度。这种做法显著减少了SFT数据量。  \n- 对于<big>**CoT data formatting**</big>：参考《Take a step back: Evoking reasoning via abstraction in large language models》，采用了“Step-Back”的模式。即通过抽象化处理，让模型学习在深入探讨原始、具体的问题之前，制定更高层次的解决方案。  \n- 对于<big>**response formatting**</big>：使用从《Lima: Less is more for alignment》扩展的默认样式。总体而言，response的结构为introduction-body-conclusion的格式，“where the body is usually a list of bullet point”。  \n- 在缓解<big>**幻觉**</big>问题上，思路是确保response中的知识不由模型内部产生，对应的做法是把会导致模型进行记忆的response删掉。（但是这个具体标准是什么，有没有了解的朋友说下看法？）  \n- 在缓解<big>**生成重复**</big>的问题上，则是直接把response中包含重复的部分都重写了。（核心还是洗数据，一条条打磨）  \n- 数据<big>**多样性**</big>很重要，因此参考《#instag: Instruction tagging for analyzing supervised fine-tuning of large language models》建立了一个打标系统，并设计一个注重多样性的采样算法，平衡了各个领域数据的分布。  \n- 为了找到最佳的数据<big>**配比**</big>，参考《How abilities in large language models are affected by supervised fine-tuning data composition》，使用近似网络搜索（approximate grid search），对每个领域以{1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64}的比例进行实验和人工测评，找到最佳的组合方式。  \n- 除了内容，<big>**数据格式**</big>对效果也有很大影响。参OPENAI的ChatML格式（[https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md](https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md)），这种结构化的格式使模型能够区分各种信息类型，如system prompt、user input和bot response。\n\nSFT数据质量能极大影响模型的效果，随着数据量的增加，高质量数据能带来更多提升，如下图\n\n{% asset_img sft.png SFT %}  \n\n# 训练  \n\n## infra\n\n从数据处理到模型训练都需要大集群大算力的支持。Yi构建了支持全栈数据处理、预训练、微调和服务的基础设施。包括：  \n\n(1) 自动管理和监控计算资源的能力；\n(2) 通过优化并行策略、内核效率和长上下文支持提高训练速度；\n(3) 统一微调框架，支持异构分布式训练后端，例如在DPO中同时使用Megatron和DeepSpeed进行多个模型的训练；\n(4) 通过各种LLM服务加速技术（如量化、continuous batching 和 paged attention）降低部署成本。\n\n总之这部分工作还是很多的，比如由于经常有硬件坏，坏的硬件会被自动从资源池移除；任务失败时，会自动跟踪重启。给算法人员考法UI等。  \n\n## 预训练  \n\n训了4k基础模型。（暂时没有给出更多细节）  \n\n## 微调  \n\n超参如下  \n\n- AdamW：beta=[0.9,0.999]，epsilon = 1e-8  \n- seq_len = 4096  \n- batch size = 64  \n- constant lr = 1e-5，weight decay = 0.1  \n- gradient clip = 1.0  \n- max step = 300\n- 参考《Neftune: Noisy embeddings improve instruction finetuning》，对于6B模型 noise scale = 5，对于34B模型 noise scale = 45\n\n# 模型评测\n\n## 基模型评测\n\n1. 基础能力评测  \n\n对其他开源模型，保持和公开的设置相同做法获取结果。Yi使用贪婪解码，没有进行任何后处理，结果如下表  \n\n{% asset_img base_model_eval.png Base模型效果 %}  \n\n在数学和代码能力上，和GPT3.5、GPT4还存在一些差距，而这些能力是可以通过继续预训练和微调来持续提升的。Yi最初的设计并没有针对这些能力，因此没有在预训练数据中包含特别多相关数据，后续会有计划增加这部分能力的提升。  \n\n而和其他开源模型相比，在代码和数学以外的任务，Yi基本上做到了跟大一倍模型的效果相近，甚至更好的水平。  \n\n2. 观察  \n\n- 模型规模带来的增益：尽管Yi-34B和Yi-6B使用了相同的预训练语料，但Yi-34B的性能相比Yi-6B有了质的提升。更大的模型尺寸在代码和数学基准测试上带来了明显的增益。  \n- 数据质量：高质量预训练数据的小型模型，如Yi-34B或Qwen-14B，通常表现优于尺寸更大但（可能）数据质量较低的模型，例如Falcon-180B。\n- GPT-4与开源LLM之间的差距：开源LLM在多种基准测试上的性能仍然落后于GPT-4和GPT-3.5。然而，具有代表性的双语LLM，例如Qwen-14B和Yi-34B，可以在包括C-Eval、CMMLU和Gaokao在内的中文知识相关基准测试上匹配甚至超过GPT-4的性能。然而，在BBH、代码（HumanEval）和数学（MATH）等推理相关基准测试上，仍然存在巨大的差距。  \n\n\n3. In-Context Learning能力的测试  \n\nYi进一步研究了in-context learning的能力，即根据少数展示的输入-输出示例，推断underlying function的能力。  \n\n考虑的任务是推断加权和的线性系数。具体来说，定义 y = w1x1 + w2x2 + ... + wnxn。  \n\n少量示例展示是 x1, x2, ..., xn, y，要求模型预测给定一组新输入 x 的 y。  \n\n这就要求模型隐式地推断出 w1, w2, ..., wn。  \n\n评测上，使用（a）模型预测的 y 与真实值 y∗ 之间的绝对差，即 |y − y∗| 作为连续度量，以及使用（b）精确匹配 y == y∗ 作为不连续度量。  \n\n模型在算术上的效果正常，因此可以认为这样的测试不受算术能力的影响，而能直接看模型是否具备根据给定的实例进行underlying function推理的能力。  \n\n实验发现，当问题比较简单时（系数是[1,-1]），Yi-34B和LLAMA-70B效果比较好（看下图）。  \n\n当问题更复杂点（系数是[1，1，1，1，1]），只有LLAMA-70B和Mistral 8*7B这样的大模型表现出了涌现的能力。  \n\n{% asset_img ict.png ICT %}  \n\n## Chat模型评测  \n\n1. 自动评测\n\n评测的任务和base模型相同，分别采用zero-shot和few-shot，效果依然不错，具体结果如下\n\n{% asset_img eval.png Yi效果 %}  \n\n报告强调，如Goodhart’s principle所说，当一个指标变成目标，就不再是一个好指标。因此这里的测试只是为了确认微调没有使得模型的知识能力下降，而不会专门去针对任务做优化。  \n\n结果上，Yi-34B-Chat数学能力不错，而Yi-6B-Chat并没有展现出强大的数学能力。推测较小的模型可能需要更多的数据在SFT阶段激活其相应的能力。  \n\n2. 人工评测\n\n{% asset_img third_party.png 三方评测 %}  \n\n# 能力扩展  \n\nbase模型的基础上，做了3个能力扩展：长上下文、多模态、深度扩展。  \n\n## 长上下文能力  \n\n报告中认为，4k的base模型已经具备了长文本（200k）的能力。只要用少量数据，进行继续预训练来释放这个能力，再用轻量级的SFT来调整格式，就能获得足够好的长文本能力。  \n\n长文本的继续预训练中，依然使用完整的attention，而不是线性attention或者sparse attention。  \n\n继续预训练的数据，混合了（1）原始预训练数据（2）length-upsampled long-context data长文本数据，长文本数据主要来自书籍（3）多文档问答的人造数据。  \n\n多文档问答数据的应答中，在最终答案之前会对和答案相关的段落进行复述（recitation），以此来提升模型长文本关联的能力。  \n\n这部分的数据工作主要参考《Data engineering for scaling language models to 128k context》和《Paraphrasing the original text makes high accuracy long-context qa》。  \n\n最终用了5B token的长文本数据，batch size=4M（token），只更新了100个step（这里没明白100步是怎么来，不应该是5B/4M=1250？有没有明白的朋友指点一下）。  \n\n> We continue pretrain the model on 5B tokens with 4M batch size, which translate to 100 optimization steps. Aligning with the concurrent work from Fu et al. [22], we observe that such light-weight continue pretraining is already able to enable a strong performance on Needle-in-a-Haystack test, as we will show in Figure 6.\n\n这个做法与《Data engineering for scaling language models to 128k context》一致，这样轻量级的微调已经足够在“大海捞针”任务做得很好。  \n\n而微调的数据，也混合了短的SFT数据，以及长的文本问答数据。  \n\n这些文本问答数据都是人工用模型造出来的。  \n\n具体的做法是，随机抽一些文档，然后从中随机选择一个或者多个段落，让一个训练好的模型根据这些段落造出问题和答案。  \n\n一个重要的细节是复述和改写：在给出答案之前，我们要求模型复述或改写原文段落。这种数据格式鼓励模型的检索行为，从而抑制其虚构行为：面对一个问题，模型更倾向于使用输入中的信息来构建答案，而不是使用其内部知识，后者可能与问题相关但不准确。\n\n使用以上所述的轻量级训练，已经可以在“大海捞针”任务做得很好，几乎能够做到全绿。  \n\n{% asset_img long_context_result.png 大海捞针效果 %}  \n\n## 多模态  \n\nViT部分由CLIP ViT-H/14 model初始化，后面的transformer由Yi-Chat初始化  \n\n{% asset_img multimodal.png 多模态 %}  \n\n3步训练：  \n\n（1）使用224^2的图像来训练ViT和projection模块的参数。这一训练利用了包含1亿个图像-文本对的数据集，这些数据来自LAION-400M。主要目标是增强ViT在架构中的知识获取能力，并实现ViT与LLM之间更好的对齐。  \n\n（2）将ViT的图像分辨率提升到448^2，目的是进一步推动模型识别复杂视觉细节的能力。在这个阶段使用的数据集包括从LAION-400M中提取的2000万个图像-文本对。此外，还融入了来自不同来源的大约480万个图像-文本对，例如CLLaVA、LLaVAR、Flickr、VQAv2、RefCOCO、Visual7w等。  \n\n（3）整个模型的参数一起训练。主要目标是提高模型在多模态聊天交互方面的熟练度，从而赋予它能够无缝融合和解释视觉与语言输入的能力。为此，训练数据集涵盖了多种来源，总共大约有100万张图像-文本对，包括GQA、VizWiz VQA、TextCaps、OCR-VQA、Visual Genome、ShareGPT4V等等。为了确保数据平衡，对任何单一来源的最大数据量设定了上限，将其限制在不超过50,000对。  \n\n使用128张A100，6B训了3天，34B训10天。  \n\n## Depth Upscaling 深度扩展\n\n目标是把32层的6B扩展到48层的9B模型。  \n\n参考《Scaling large language models with simple yet effective depth up-scaling》，通过复制中间的12-28层共16层，把层数扩展为48层。  \n\n实验表明，要确定复制哪些层，可以通过测量输入和每层输出的cosine similarity来衡量。  \n\n这种方法使得模型能在不额外训练的情况下，和原模型性能最接近，损失最少。  \n\n{% asset_img 9B.png 9B模型 %}  \n\n这说明复制的这些层并不会很大地改变原模型的激活值。  \n\n除了层数增加，Depth Upscaling还要做继续预训练，才能提升效果。  \n\n继续预训练使用约800B token，训练过程分为两个阶段。  \n\n其中约70%的数据是最近收集并精心挑选的。在最后阶段增强了代码的比例以提高代码性能。  \n\n训练保持constant lr = 3e-5，并在模型损失达到平台期时，从4M token开始逐渐增加batch size大小。  \n\n这种增加batch size的方法，以及保持所有其他参数与Yi-6B基础模型配置一致，继续预训练很重要。  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n往期文章\n\n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)\n\n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)\n\n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n","source":"_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节.md","raw":"---\ntitle: Yi技术报告-划重点看细节\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 技术报告\n  - 多模态\n  - 长上下文\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 41b6a819\ndate: 2024-03-26 16:51:08\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n01.AI（零一万物），是李开复带队孵化的AI公司。2023年11月初，01.AI发布并开源了Yi-6B、Yi-34B base模型，同一周内，又开源了Yi-6B-200K和Yi-34B-200K base模型。Yi号称是从零预训练的双语模型。接下来的几个月，01.AI陆续推出了chat模型、多模态能力，Yi-9B、长上下文的记忆和检索能力等优化。  \n\n从2023年11发布起，个人就有测试和使用Yi的模型。在SuperCLUE/CMMLU等一些榜单数据的实测上，发现Yi的效果确实不错。实际工作使用里，Yi的效果基本也都能排在同时期中文（开源）大模型里的第一梯队。  \n\n2024年3月，Yi终于发布了技术报告，在此来梳理一下报告中的重点内容和值得关注的细节信息。  \n\n# TL;DR\n\n先给出核心内容的总结：  \n\n- Yi-34B模型int4量化之后，相比float16损失<1%，可以跑在RTX4090上（24G显存）\n- 模型结构不需要太多变化，LLAMA2标准结构已经足够训出很好的效果\n- 3.1T的预训练数据远比scaling law建议的1T大，但是效果更好，并且模型还没饱和，继续增大数据量还能提升\n- 微调数据质量很重要，由算法人员直接标注，只要<10k的数据量就足够了\n- 4k长度的基础预训练模型已经具备长文本能力，只需用长文本数据继续预训练，更新百步就有很好效果\n- 总之，数据要精心设计，数据质量要高，数据量要大\n\n# 模型\n\n## 规模选择  \n\nYi目前有6B、9B、34B三个规模，其中34B是主力模型。  \n\n选择34B，而不是更大规模的原因，是这个规模能在24G显存的消费级显卡（如RTX4090）上运行。  \n\n使用int4量化之后的34B模型可以运行在24G显存的GPU上。  \n\n参考[《Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases》](https://arxiv.org/abs/2301.12017)的量化方法，Yi-34B的int8量化模型相比bf16模型，几乎可以做到效果无损（差距<1%），而int4量化模型在大部分任务的损失也完全可以接受，具体效果如下表。  \n\n{% asset_img eval.png Yi效果 %}  \n\n训练数据总共是3.1T token，这比DeepMind的scaling law所建议的1TB要大不少。目前能接触到的这个规模的模型，数据量基本都<2T。  \n\n（即提出Chinchilla模型的[《Training Compute-Optimal Large Language Models》](https://arxiv.org/abs/2203.15556)的scaling law）  \n\n也就是从scaling law的角度来说，Yi是overtrain的。  \n\n<big>**但是Yi实践结果证明，较小模型+更大规模的高质量数据，是可以获得进一步效果提升的，这也就让我们获得了高性价比的推理模型--34B推理成本+大训练投入，就能得到接近普通70B规模的推理效果。**</big>  \n\n## 模型结构  \n\n结构上，基于标准LLAMA2模型，做了一些变化  \n\n- 注意力机制：LLAMA2只在70B用了GQA，Yi全系列都用了GQA，具体参数如下表  \n- 位置编码：RoPE，参考RoPE ABF（《Effective long-context scaling of foundation models》），base扩大到10M，用于支持长上下文。  \n- 激活函数：使用SwiGLU，参考《GLU Variants Improve Transformer》  \n\n并且把activation size从4h降为8/3h，这里的说法是补偿了GQA带来的参数下降  \n\n> We use SwiGLU as Yi’s post-attention layer, reducing its activation size from 4h to 8/3h (h denotes hidden size) to be consistent with the normal post-attention layer. This adjustment also compensates for the reduction in parameter resulted from GQA, making the overall parameter count comparible of existing 7B and 34B models.  \n\n{% asset_img model.png Yi模型结构 %}  \n\n关于模型结构，一个结论就是<big>**“虽然做了很多模型结构上的实验，但是最终发现，标准的结构就足以训出足够好的模型”**</big>  \n\n## tokenizer  \n\n- 用BPE，词表大小为64000，这个大小平衡了计算效率和表达能力；  \n- 其中数字全是单个的digit，让模型能更好地理解数字数据；  \n- 对于OOV的词，会降级用unicode编码 ； \n- 保留全角标点符号，不转为半角；  \n  \n另外，优先考虑英语的LLM在tokenizer会使用虚拟前缀（文本开头的空格）来泛化句子不同位置相同的单词。Yi不这么做，因为即使是在英语语境中，这种假设并不总是成立，比如对于以引号开头的句子，而且在中文语境中，这么做没有明显效果。  \n\n# 数据  \n\n数据，是LLM最核心的部分，没有之一。Yi最核心的工作就是提升数据数量和质量。  \n\n{% asset_img cover.png 数据 %}  \n\n## 预训练数据  \n\n预训练数据整体处理流程如下  \n\n{% asset_img pretrain_data_pipeline.png 预训练数据处理流程 %}  \n\n1. 语料获取 & 语言分类  \n\n从网络爬虫开始，爬取中英文这两种语言的网站，对网站内容进行解析。  \n\n并参考CCNeT（《CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data》）的做法，进行语言识别。  \n\n2. 规则过滤器 Heuristic Rule Filters  \n\n目的是快速过滤掉明显的低质量数据。基于这些规则来过滤掉：  \n\n- URL、域名、单词黑名单和乱码文本；  \n- 文档长度、特殊符号的比例，以及短行、连续行或不完整行的比例；  \n- 重复的单词模式、n-gram或段落，参考《Scaling Language Models: Methods, Analysis & Insights from Training Gopher》的做法，阈值则是来参考《CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages》；\n- 数据脱敏：识别并匿名化个人信息（Personal Identifiable Information，PII），如电子邮件地址和电话号码。\n\n3. 可训练过滤器 Learned Filters  \n\n对于不好用规则处理的，就用模型来学习模式，并进行清洗。共有4个scorer：  \n\n- Perplexity Scorer：参照《CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data》，用kenlm库，把高于平均perplexity的内容丢弃；\n- Quality Scorer：识别如维基百科这样的高质量内容，丢弃低质量内容；\n- Document Coherence Scorer：用于发现句子、段落零散不连贯的文本，要么分割，要么直接丢弃；\n- Safety Scorer：识别并删除暴力、色情、涉政内容\n\n4. 基于聚类的过滤 Cluster-based Filters  \n\n用聚类的方法，把所有文档进行分类。一方面用于给数据混合策略做参考，一方面如果整个类别的质量太差，就直接抛弃类别内的所有数据。  \n\n5. 去重\n\n参考《The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only》，做文档级的minhash去重，以及子文档级的完全匹配去重。  \n\n最终获得的数据分布如下  \n\n{% asset_img pretrain_data_dist.png 预训练数据分布 %}  \n\n虽然数据规模一定要够，但是也不能因此就放弃数据质量，否则只能是garbage in，garbage out\n\n> we prefer 3T tokens over sophasticated engineering over 10T tokens without extensive filtering\n\n这句话大概表示清洗前的数据有10T，这也是一个信息，符合质量的数据可能只有3成  \n\n## 微调数据 \n\n对于微调数据，一句话：Quality is All You Need。  \n\n一共只有<10k条SFT数据，每条数据都通过人工多次打磨，这比大数量但质量一般的数据的效果好。  \n\n这思路和《Gemini: A family of highly capable multimodal models.》、《Llama 2: Open Foundation and Fine-Tuned Chat Models》、《Lima: Less is more for alignment》一致，而和FLAN（《Scaling instruction-finetuned language models》）以及UltraChat（《Enhancing chat language models by scaling high-quality instructional conversations》）这样更关注数据量的做法不同。  \n\n具体做法上有：  \n\n- 对于<big>**prompt distribution selection**</big>：参考《Wizardlm: Empowering large language models to follow complex instructions》，开发复合指令，并通过指令进化，逐步增加指令的复杂度。这种做法显著减少了SFT数据量。  \n- 对于<big>**CoT data formatting**</big>：参考《Take a step back: Evoking reasoning via abstraction in large language models》，采用了“Step-Back”的模式。即通过抽象化处理，让模型学习在深入探讨原始、具体的问题之前，制定更高层次的解决方案。  \n- 对于<big>**response formatting**</big>：使用从《Lima: Less is more for alignment》扩展的默认样式。总体而言，response的结构为introduction-body-conclusion的格式，“where the body is usually a list of bullet point”。  \n- 在缓解<big>**幻觉**</big>问题上，思路是确保response中的知识不由模型内部产生，对应的做法是把会导致模型进行记忆的response删掉。（但是这个具体标准是什么，有没有了解的朋友说下看法？）  \n- 在缓解<big>**生成重复**</big>的问题上，则是直接把response中包含重复的部分都重写了。（核心还是洗数据，一条条打磨）  \n- 数据<big>**多样性**</big>很重要，因此参考《#instag: Instruction tagging for analyzing supervised fine-tuning of large language models》建立了一个打标系统，并设计一个注重多样性的采样算法，平衡了各个领域数据的分布。  \n- 为了找到最佳的数据<big>**配比**</big>，参考《How abilities in large language models are affected by supervised fine-tuning data composition》，使用近似网络搜索（approximate grid search），对每个领域以{1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64}的比例进行实验和人工测评，找到最佳的组合方式。  \n- 除了内容，<big>**数据格式**</big>对效果也有很大影响。参OPENAI的ChatML格式（[https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md](https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md)），这种结构化的格式使模型能够区分各种信息类型，如system prompt、user input和bot response。\n\nSFT数据质量能极大影响模型的效果，随着数据量的增加，高质量数据能带来更多提升，如下图\n\n{% asset_img sft.png SFT %}  \n\n# 训练  \n\n## infra\n\n从数据处理到模型训练都需要大集群大算力的支持。Yi构建了支持全栈数据处理、预训练、微调和服务的基础设施。包括：  \n\n(1) 自动管理和监控计算资源的能力；\n(2) 通过优化并行策略、内核效率和长上下文支持提高训练速度；\n(3) 统一微调框架，支持异构分布式训练后端，例如在DPO中同时使用Megatron和DeepSpeed进行多个模型的训练；\n(4) 通过各种LLM服务加速技术（如量化、continuous batching 和 paged attention）降低部署成本。\n\n总之这部分工作还是很多的，比如由于经常有硬件坏，坏的硬件会被自动从资源池移除；任务失败时，会自动跟踪重启。给算法人员考法UI等。  \n\n## 预训练  \n\n训了4k基础模型。（暂时没有给出更多细节）  \n\n## 微调  \n\n超参如下  \n\n- AdamW：beta=[0.9,0.999]，epsilon = 1e-8  \n- seq_len = 4096  \n- batch size = 64  \n- constant lr = 1e-5，weight decay = 0.1  \n- gradient clip = 1.0  \n- max step = 300\n- 参考《Neftune: Noisy embeddings improve instruction finetuning》，对于6B模型 noise scale = 5，对于34B模型 noise scale = 45\n\n# 模型评测\n\n## 基模型评测\n\n1. 基础能力评测  \n\n对其他开源模型，保持和公开的设置相同做法获取结果。Yi使用贪婪解码，没有进行任何后处理，结果如下表  \n\n{% asset_img base_model_eval.png Base模型效果 %}  \n\n在数学和代码能力上，和GPT3.5、GPT4还存在一些差距，而这些能力是可以通过继续预训练和微调来持续提升的。Yi最初的设计并没有针对这些能力，因此没有在预训练数据中包含特别多相关数据，后续会有计划增加这部分能力的提升。  \n\n而和其他开源模型相比，在代码和数学以外的任务，Yi基本上做到了跟大一倍模型的效果相近，甚至更好的水平。  \n\n2. 观察  \n\n- 模型规模带来的增益：尽管Yi-34B和Yi-6B使用了相同的预训练语料，但Yi-34B的性能相比Yi-6B有了质的提升。更大的模型尺寸在代码和数学基准测试上带来了明显的增益。  \n- 数据质量：高质量预训练数据的小型模型，如Yi-34B或Qwen-14B，通常表现优于尺寸更大但（可能）数据质量较低的模型，例如Falcon-180B。\n- GPT-4与开源LLM之间的差距：开源LLM在多种基准测试上的性能仍然落后于GPT-4和GPT-3.5。然而，具有代表性的双语LLM，例如Qwen-14B和Yi-34B，可以在包括C-Eval、CMMLU和Gaokao在内的中文知识相关基准测试上匹配甚至超过GPT-4的性能。然而，在BBH、代码（HumanEval）和数学（MATH）等推理相关基准测试上，仍然存在巨大的差距。  \n\n\n3. In-Context Learning能力的测试  \n\nYi进一步研究了in-context learning的能力，即根据少数展示的输入-输出示例，推断underlying function的能力。  \n\n考虑的任务是推断加权和的线性系数。具体来说，定义 y = w1x1 + w2x2 + ... + wnxn。  \n\n少量示例展示是 x1, x2, ..., xn, y，要求模型预测给定一组新输入 x 的 y。  \n\n这就要求模型隐式地推断出 w1, w2, ..., wn。  \n\n评测上，使用（a）模型预测的 y 与真实值 y∗ 之间的绝对差，即 |y − y∗| 作为连续度量，以及使用（b）精确匹配 y == y∗ 作为不连续度量。  \n\n模型在算术上的效果正常，因此可以认为这样的测试不受算术能力的影响，而能直接看模型是否具备根据给定的实例进行underlying function推理的能力。  \n\n实验发现，当问题比较简单时（系数是[1,-1]），Yi-34B和LLAMA-70B效果比较好（看下图）。  \n\n当问题更复杂点（系数是[1，1，1，1，1]），只有LLAMA-70B和Mistral 8*7B这样的大模型表现出了涌现的能力。  \n\n{% asset_img ict.png ICT %}  \n\n## Chat模型评测  \n\n1. 自动评测\n\n评测的任务和base模型相同，分别采用zero-shot和few-shot，效果依然不错，具体结果如下\n\n{% asset_img eval.png Yi效果 %}  \n\n报告强调，如Goodhart’s principle所说，当一个指标变成目标，就不再是一个好指标。因此这里的测试只是为了确认微调没有使得模型的知识能力下降，而不会专门去针对任务做优化。  \n\n结果上，Yi-34B-Chat数学能力不错，而Yi-6B-Chat并没有展现出强大的数学能力。推测较小的模型可能需要更多的数据在SFT阶段激活其相应的能力。  \n\n2. 人工评测\n\n{% asset_img third_party.png 三方评测 %}  \n\n# 能力扩展  \n\nbase模型的基础上，做了3个能力扩展：长上下文、多模态、深度扩展。  \n\n## 长上下文能力  \n\n报告中认为，4k的base模型已经具备了长文本（200k）的能力。只要用少量数据，进行继续预训练来释放这个能力，再用轻量级的SFT来调整格式，就能获得足够好的长文本能力。  \n\n长文本的继续预训练中，依然使用完整的attention，而不是线性attention或者sparse attention。  \n\n继续预训练的数据，混合了（1）原始预训练数据（2）length-upsampled long-context data长文本数据，长文本数据主要来自书籍（3）多文档问答的人造数据。  \n\n多文档问答数据的应答中，在最终答案之前会对和答案相关的段落进行复述（recitation），以此来提升模型长文本关联的能力。  \n\n这部分的数据工作主要参考《Data engineering for scaling language models to 128k context》和《Paraphrasing the original text makes high accuracy long-context qa》。  \n\n最终用了5B token的长文本数据，batch size=4M（token），只更新了100个step（这里没明白100步是怎么来，不应该是5B/4M=1250？有没有明白的朋友指点一下）。  \n\n> We continue pretrain the model on 5B tokens with 4M batch size, which translate to 100 optimization steps. Aligning with the concurrent work from Fu et al. [22], we observe that such light-weight continue pretraining is already able to enable a strong performance on Needle-in-a-Haystack test, as we will show in Figure 6.\n\n这个做法与《Data engineering for scaling language models to 128k context》一致，这样轻量级的微调已经足够在“大海捞针”任务做得很好。  \n\n而微调的数据，也混合了短的SFT数据，以及长的文本问答数据。  \n\n这些文本问答数据都是人工用模型造出来的。  \n\n具体的做法是，随机抽一些文档，然后从中随机选择一个或者多个段落，让一个训练好的模型根据这些段落造出问题和答案。  \n\n一个重要的细节是复述和改写：在给出答案之前，我们要求模型复述或改写原文段落。这种数据格式鼓励模型的检索行为，从而抑制其虚构行为：面对一个问题，模型更倾向于使用输入中的信息来构建答案，而不是使用其内部知识，后者可能与问题相关但不准确。\n\n使用以上所述的轻量级训练，已经可以在“大海捞针”任务做得很好，几乎能够做到全绿。  \n\n{% asset_img long_context_result.png 大海捞针效果 %}  \n\n## 多模态  \n\nViT部分由CLIP ViT-H/14 model初始化，后面的transformer由Yi-Chat初始化  \n\n{% asset_img multimodal.png 多模态 %}  \n\n3步训练：  \n\n（1）使用224^2的图像来训练ViT和projection模块的参数。这一训练利用了包含1亿个图像-文本对的数据集，这些数据来自LAION-400M。主要目标是增强ViT在架构中的知识获取能力，并实现ViT与LLM之间更好的对齐。  \n\n（2）将ViT的图像分辨率提升到448^2，目的是进一步推动模型识别复杂视觉细节的能力。在这个阶段使用的数据集包括从LAION-400M中提取的2000万个图像-文本对。此外，还融入了来自不同来源的大约480万个图像-文本对，例如CLLaVA、LLaVAR、Flickr、VQAv2、RefCOCO、Visual7w等。  \n\n（3）整个模型的参数一起训练。主要目标是提高模型在多模态聊天交互方面的熟练度，从而赋予它能够无缝融合和解释视觉与语言输入的能力。为此，训练数据集涵盖了多种来源，总共大约有100万张图像-文本对，包括GQA、VizWiz VQA、TextCaps、OCR-VQA、Visual Genome、ShareGPT4V等等。为了确保数据平衡，对任何单一来源的最大数据量设定了上限，将其限制在不超过50,000对。  \n\n使用128张A100，6B训了3天，34B训10天。  \n\n## Depth Upscaling 深度扩展\n\n目标是把32层的6B扩展到48层的9B模型。  \n\n参考《Scaling large language models with simple yet effective depth up-scaling》，通过复制中间的12-28层共16层，把层数扩展为48层。  \n\n实验表明，要确定复制哪些层，可以通过测量输入和每层输出的cosine similarity来衡量。  \n\n这种方法使得模型能在不额外训练的情况下，和原模型性能最接近，损失最少。  \n\n{% asset_img 9B.png 9B模型 %}  \n\n这说明复制的这些层并不会很大地改变原模型的激活值。  \n\n除了层数增加，Depth Upscaling还要做继续预训练，才能提升效果。  \n\n继续预训练使用约800B token，训练过程分为两个阶段。  \n\n其中约70%的数据是最近收集并精心挑选的。在最后阶段增强了代码的比例以提高代码性能。  \n\n训练保持constant lr = 3e-5，并在模型损失达到平台期时，从4M token开始逐渐增加batch size大小。  \n\n这种增加batch size的方法，以及保持所有其他参数与Yi-6B基础模型配置一致，继续预训练很重要。  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n往期文章\n\n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)\n\n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)\n\n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n","slug":"cs/nlp/2024/03/Yi技术报告-划重点看细节","published":1,"updated":"2024-03-29T11:53:37.115Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4k9z001g314k1is3ehig","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>01.AI（零一万物），是李开复带队孵化的AI公司。2023年11月初，01.AI发布并开源了Yi-6B、Yi-34B\nbase模型，同一周内，又开源了Yi-6B-200K和Yi-34B-200K\nbase模型。Yi号称是从零预训练的双语模型。接下来的几个月，01.AI陆续推出了chat模型、多模态能力，Yi-9B、长上下文的记忆和检索能力等优化。</p>\n<p>从2023年11发布起，个人就有测试和使用Yi的模型。在SuperCLUE/CMMLU等一些榜单数据的实测上，发现Yi的效果确实不错。实际工作使用里，Yi的效果基本也都能排在同时期中文（开源）大模型里的第一梯队。</p>\n<p>2024年3月，Yi终于发布了技术报告，在此来梳理一下报告中的重点内容和值得关注的细节信息。</p>\n<h1 id=\"tldr\">TL;DR</h1>\n<p>先给出核心内容的总结：</p>\n<ul>\n<li>Yi-34B模型int4量化之后，相比float16损失&lt;1%，可以跑在RTX4090上（24G显存）</li>\n<li>模型结构不需要太多变化，LLAMA2标准结构已经足够训出很好的效果</li>\n<li>3.1T的预训练数据远比scaling\nlaw建议的1T大，但是效果更好，并且模型还没饱和，继续增大数据量还能提升</li>\n<li>微调数据质量很重要，由算法人员直接标注，只要&lt;10k的数据量就足够了</li>\n<li>4k长度的基础预训练模型已经具备长文本能力，只需用长文本数据继续预训练，更新百步就有很好效果</li>\n<li>总之，数据要精心设计，数据质量要高，数据量要大</li>\n</ul>\n<h1 id=\"模型\">模型</h1>\n<h2 id=\"规模选择\">规模选择</h2>\n<p>Yi目前有6B、9B、34B三个规模，其中34B是主力模型。</p>\n<p>选择34B，而不是更大规模的原因，是这个规模能在24G显存的消费级显卡（如RTX4090）上运行。</p>\n<p>使用int4量化之后的34B模型可以运行在24G显存的GPU上。</p>\n<p>参考<a href=\"https://arxiv.org/abs/2301.12017\">《Understanding INT4\nQuantization for Language Models: Latency Speedup, Composability, and\nFailure\nCases》</a>的量化方法，Yi-34B的int8量化模型相比bf16模型，几乎可以做到效果无损（差距&lt;1%），而int4量化模型在大部分任务的损失也完全可以接受，具体效果如下表。</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi效果\">\n<p>训练数据总共是3.1T token，这比DeepMind的scaling\nlaw所建议的1TB要大不少。目前能接触到的这个规模的模型，数据量基本都&lt;2T。</p>\n<p>（即提出Chinchilla模型的<a href=\"https://arxiv.org/abs/2203.15556\">《Training Compute-Optimal Large\nLanguage Models》</a>的scaling law）</p>\n<p>也就是从scaling law的角度来说，Yi是overtrain的。</p>\n<p><big><strong>但是Yi实践结果证明，较小模型+更大规模的高质量数据，是可以获得进一步效果提升的，这也就让我们获得了高性价比的推理模型--34B推理成本+大训练投入，就能得到接近普通70B规模的推理效果。</strong></big></p>\n<h2 id=\"模型结构\">模型结构</h2>\n<p>结构上，基于标准LLAMA2模型，做了一些变化</p>\n<ul>\n<li>注意力机制：LLAMA2只在70B用了GQA，Yi全系列都用了GQA，具体参数如下表<br>\n</li>\n<li>位置编码：RoPE，参考RoPE ABF（《Effective long-context scaling of\nfoundation models》），base扩大到10M，用于支持长上下文。<br>\n</li>\n<li>激活函数：使用SwiGLU，参考《GLU Variants Improve Transformer》</li>\n</ul>\n<p>并且把activation\nsize从4h降为8/3h，这里的说法是补偿了GQA带来的参数下降</p>\n<blockquote>\n<p>We use SwiGLU as Yi’s post-attention layer, reducing its activation\nsize from 4h to 8/3h (h denotes hidden size) to be consistent with the\nnormal post-attention layer. This adjustment also compensates for the\nreduction in parameter resulted from GQA, making the overall parameter\ncount comparible of existing 7B and 34B models.</p>\n</blockquote>\n<img src=\"/41b6a819/model.png\" class title=\"Yi模型结构\">\n<p>关于模型结构，一个结论就是<big><strong>“虽然做了很多模型结构上的实验，但是最终发现，标准的结构就足以训出足够好的模型”</strong></big></p>\n<h2 id=\"tokenizer\">tokenizer</h2>\n<ul>\n<li>用BPE，词表大小为64000，这个大小平衡了计算效率和表达能力；<br>\n</li>\n<li>其中数字全是单个的digit，让模型能更好地理解数字数据；<br>\n</li>\n<li>对于OOV的词，会降级用unicode编码 ；</li>\n<li>保留全角标点符号，不转为半角；</li>\n</ul>\n<p>另外，优先考虑英语的LLM在tokenizer会使用虚拟前缀（文本开头的空格）来泛化句子不同位置相同的单词。Yi不这么做，因为即使是在英语语境中，这种假设并不总是成立，比如对于以引号开头的句子，而且在中文语境中，这么做没有明显效果。</p>\n<h1 id=\"数据\">数据</h1>\n<p>数据，是LLM最核心的部分，没有之一。Yi最核心的工作就是提升数据数量和质量。</p>\n<img src=\"/41b6a819/cover.png\" class title=\"数据\">\n<h2 id=\"预训练数据\">预训练数据</h2>\n<p>预训练数据整体处理流程如下</p>\n<img src=\"/41b6a819/pretrain_data_pipeline.png\" class title=\"预训练数据处理流程\">\n<ol type=\"1\">\n<li>语料获取 &amp; 语言分类</li>\n</ol>\n<p>从网络爬虫开始，爬取中英文这两种语言的网站，对网站内容进行解析。</p>\n<p>并参考CCNeT（《CCNet: Extracting High Quality Monolingual Datasets\nfrom Web Crawl Data》）的做法，进行语言识别。</p>\n<ol start=\"2\" type=\"1\">\n<li>规则过滤器 Heuristic Rule Filters</li>\n</ol>\n<p>目的是快速过滤掉明显的低质量数据。基于这些规则来过滤掉：</p>\n<ul>\n<li>URL、域名、单词黑名单和乱码文本；<br>\n</li>\n<li>文档长度、特殊符号的比例，以及短行、连续行或不完整行的比例；<br>\n</li>\n<li>重复的单词模式、n-gram或段落，参考《Scaling Language Models:\nMethods, Analysis &amp; Insights from Training\nGopher》的做法，阈值则是来参考《CulturaX: A Cleaned, Enormous, and\nMultilingual Dataset for Large Language Models in 167 Languages》；</li>\n<li>数据脱敏：识别并匿名化个人信息（Personal Identifiable\nInformation，PII），如电子邮件地址和电话号码。</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li>可训练过滤器 Learned Filters</li>\n</ol>\n<p>对于不好用规则处理的，就用模型来学习模式，并进行清洗。共有4个scorer：</p>\n<ul>\n<li>Perplexity Scorer：参照《CCNet: Extracting High Quality Monolingual\nDatasets from Web Crawl\nData》，用kenlm库，把高于平均perplexity的内容丢弃；</li>\n<li>Quality\nScorer：识别如维基百科这样的高质量内容，丢弃低质量内容；</li>\n<li>Document Coherence\nScorer：用于发现句子、段落零散不连贯的文本，要么分割，要么直接丢弃；</li>\n<li>Safety Scorer：识别并删除暴力、色情、涉政内容</li>\n</ul>\n<ol start=\"4\" type=\"1\">\n<li>基于聚类的过滤 Cluster-based Filters</li>\n</ol>\n<p>用聚类的方法，把所有文档进行分类。一方面用于给数据混合策略做参考，一方面如果整个类别的质量太差，就直接抛弃类别内的所有数据。</p>\n<ol start=\"5\" type=\"1\">\n<li>去重</li>\n</ol>\n<p>参考《The RefinedWeb Dataset for Falcon LLM: Outperforming Curated\nCorpora with Web Data, and Web Data\nOnly》，做文档级的minhash去重，以及子文档级的完全匹配去重。</p>\n<p>最终获得的数据分布如下</p>\n<img src=\"/41b6a819/pretrain_data_dist.png\" class title=\"预训练数据分布\">\n<p>虽然数据规模一定要够，但是也不能因此就放弃数据质量，否则只能是garbage\nin，garbage out</p>\n<blockquote>\n<p>we prefer 3T tokens over sophasticated engineering over 10T tokens\nwithout extensive filtering</p>\n</blockquote>\n<p>这句话大概表示清洗前的数据有10T，这也是一个信息，符合质量的数据可能只有3成</p>\n<h2 id=\"微调数据\">微调数据</h2>\n<p>对于微调数据，一句话：Quality is All You Need。</p>\n<p>一共只有&lt;10k条SFT数据，每条数据都通过人工多次打磨，这比大数量但质量一般的数据的效果好。</p>\n<p>这思路和《Gemini: A family of highly capable multimodal\nmodels.》、《Llama 2: Open Foundation and Fine-Tuned Chat\nModels》、《Lima: Less is more for alignment》一致，而和FLAN（《Scaling\ninstruction-finetuned language models》）以及UltraChat（《Enhancing chat\nlanguage models by scaling high-quality instructional\nconversations》）这样更关注数据量的做法不同。</p>\n<p>具体做法上有：</p>\n<ul>\n<li>对于<big><strong>prompt distribution\nselection</strong></big>：参考《Wizardlm: Empowering large language\nmodels to follow complex\ninstructions》，开发复合指令，并通过指令进化，逐步增加指令的复杂度。这种做法显著减少了SFT数据量。<br>\n</li>\n<li>对于<big><strong>CoT data formatting</strong></big>：参考《Take a\nstep back: Evoking reasoning via abstraction in large language\nmodels》，采用了“Step-Back”的模式。即通过抽象化处理，让模型学习在深入探讨原始、具体的问题之前，制定更高层次的解决方案。<br>\n</li>\n<li>对于<big><strong>response formatting</strong></big>：使用从《Lima:\nLess is more for\nalignment》扩展的默认样式。总体而言，response的结构为introduction-body-conclusion的格式，“where\nthe body is usually a list of bullet point”。<br>\n</li>\n<li>在缓解<big><strong>幻觉</strong></big>问题上，思路是确保response中的知识不由模型内部产生，对应的做法是把会导致模型进行记忆的response删掉。（但是这个具体标准是什么，有没有了解的朋友说下看法？）<br>\n</li>\n<li>在缓解<big><strong>生成重复</strong></big>的问题上，则是直接把response中包含重复的部分都重写了。（核心还是洗数据，一条条打磨）<br>\n</li>\n<li>数据<big><strong>多样性</strong></big>很重要，因此参考《#instag:\nInstruction tagging for analyzing supervised fine-tuning of large\nlanguage\nmodels》建立了一个打标系统，并设计一个注重多样性的采样算法，平衡了各个领域数据的分布。<br>\n</li>\n<li>为了找到最佳的数据<big><strong>配比</strong></big>，参考《How\nabilities in large language models are affected by supervised\nfine-tuning data composition》，使用近似网络搜索（approximate grid\nsearch），对每个领域以{1, 1/2, 1/4, 1/8, 1/16, 1/32,\n1/64}的比例进行实验和人工测评，找到最佳的组合方式。<br>\n</li>\n<li>除了内容，<big><strong>数据格式</strong></big>对效果也有很大影响。参OPENAI的ChatML格式（<a href=\"https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md\">https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md</a>），这种结构化的格式使模型能够区分各种信息类型，如system\nprompt、user input和bot response。</li>\n</ul>\n<p>SFT数据质量能极大影响模型的效果，随着数据量的增加，高质量数据能带来更多提升，如下图</p>\n<img src=\"/41b6a819/sft.png\" class title=\"SFT\">\n<h1 id=\"训练\">训练</h1>\n<h2 id=\"infra\">infra</h2>\n<p>从数据处理到模型训练都需要大集群大算力的支持。Yi构建了支持全栈数据处理、预训练、微调和服务的基础设施。包括：</p>\n<ol type=\"1\">\n<li>自动管理和监控计算资源的能力；</li>\n<li>通过优化并行策略、内核效率和长上下文支持提高训练速度；</li>\n<li>统一微调框架，支持异构分布式训练后端，例如在DPO中同时使用Megatron和DeepSpeed进行多个模型的训练；</li>\n<li>通过各种LLM服务加速技术（如量化、continuous batching 和 paged\nattention）降低部署成本。</li>\n</ol>\n<p>总之这部分工作还是很多的，比如由于经常有硬件坏，坏的硬件会被自动从资源池移除；任务失败时，会自动跟踪重启。给算法人员考法UI等。</p>\n<h2 id=\"预训练\">预训练</h2>\n<p>训了4k基础模型。（暂时没有给出更多细节）</p>\n<h2 id=\"微调\">微调</h2>\n<p>超参如下</p>\n<ul>\n<li>AdamW：beta=[0.9,0.999]，epsilon = 1e-8<br>\n</li>\n<li>seq_len = 4096<br>\n</li>\n<li>batch size = 64<br>\n</li>\n<li>constant lr = 1e-5，weight decay = 0.1<br>\n</li>\n<li>gradient clip = 1.0<br>\n</li>\n<li>max step = 300</li>\n<li>参考《Neftune: Noisy embeddings improve instruction\nfinetuning》，对于6B模型 noise scale = 5，对于34B模型 noise scale =\n45</li>\n</ul>\n<h1 id=\"模型评测\">模型评测</h1>\n<h2 id=\"基模型评测\">基模型评测</h2>\n<ol type=\"1\">\n<li>基础能力评测</li>\n</ol>\n<p>对其他开源模型，保持和公开的设置相同做法获取结果。Yi使用贪婪解码，没有进行任何后处理，结果如下表</p>\n<img src=\"/41b6a819/base_model_eval.png\" class title=\"Base模型效果\">\n<p>在数学和代码能力上，和GPT3.5、GPT4还存在一些差距，而这些能力是可以通过继续预训练和微调来持续提升的。Yi最初的设计并没有针对这些能力，因此没有在预训练数据中包含特别多相关数据，后续会有计划增加这部分能力的提升。</p>\n<p>而和其他开源模型相比，在代码和数学以外的任务，Yi基本上做到了跟大一倍模型的效果相近，甚至更好的水平。</p>\n<ol start=\"2\" type=\"1\">\n<li>观察</li>\n</ol>\n<ul>\n<li>模型规模带来的增益：尽管Yi-34B和Yi-6B使用了相同的预训练语料，但Yi-34B的性能相比Yi-6B有了质的提升。更大的模型尺寸在代码和数学基准测试上带来了明显的增益。<br>\n</li>\n<li>数据质量：高质量预训练数据的小型模型，如Yi-34B或Qwen-14B，通常表现优于尺寸更大但（可能）数据质量较低的模型，例如Falcon-180B。</li>\n<li>GPT-4与开源LLM之间的差距：开源LLM在多种基准测试上的性能仍然落后于GPT-4和GPT-3.5。然而，具有代表性的双语LLM，例如Qwen-14B和Yi-34B，可以在包括C-Eval、CMMLU和Gaokao在内的中文知识相关基准测试上匹配甚至超过GPT-4的性能。然而，在BBH、代码（HumanEval）和数学（MATH）等推理相关基准测试上，仍然存在巨大的差距。</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li>In-Context Learning能力的测试</li>\n</ol>\n<p>Yi进一步研究了in-context\nlearning的能力，即根据少数展示的输入-输出示例，推断underlying\nfunction的能力。</p>\n<p>考虑的任务是推断加权和的线性系数。具体来说，定义 y = w1x1 + w2x2 +\n... + wnxn。</p>\n<p>少量示例展示是 x1, x2, ..., xn, y，要求模型预测给定一组新输入 x 的\ny。</p>\n<p>这就要求模型隐式地推断出 w1, w2, ..., wn。</p>\n<p>评测上，使用（a）模型预测的 y 与真实值 y∗ 之间的绝对差，即 |y − y∗|\n作为连续度量，以及使用（b）精确匹配 y == y∗ 作为不连续度量。</p>\n<p>模型在算术上的效果正常，因此可以认为这样的测试不受算术能力的影响，而能直接看模型是否具备根据给定的实例进行underlying\nfunction推理的能力。</p>\n<p>实验发现，当问题比较简单时（系数是[1,-1]），Yi-34B和LLAMA-70B效果比较好（看下图）。</p>\n<p>当问题更复杂点（系数是[1，1，1，1，1]），只有LLAMA-70B和Mistral\n8*7B这样的大模型表现出了涌现的能力。</p>\n<img src=\"/41b6a819/ict.png\" class title=\"ICT\">\n<h2 id=\"chat模型评测\">Chat模型评测</h2>\n<ol type=\"1\">\n<li>自动评测</li>\n</ol>\n<p>评测的任务和base模型相同，分别采用zero-shot和few-shot，效果依然不错，具体结果如下</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi效果\">\n<p>报告强调，如Goodhart’s\nprinciple所说，当一个指标变成目标，就不再是一个好指标。因此这里的测试只是为了确认微调没有使得模型的知识能力下降，而不会专门去针对任务做优化。</p>\n<p>结果上，Yi-34B-Chat数学能力不错，而Yi-6B-Chat并没有展现出强大的数学能力。推测较小的模型可能需要更多的数据在SFT阶段激活其相应的能力。</p>\n<ol start=\"2\" type=\"1\">\n<li>人工评测</li>\n</ol>\n<img src=\"/41b6a819/third_party.png\" class title=\"三方评测\">\n<h1 id=\"能力扩展\">能力扩展</h1>\n<p>base模型的基础上，做了3个能力扩展：长上下文、多模态、深度扩展。</p>\n<h2 id=\"长上下文能力\">长上下文能力</h2>\n<p>报告中认为，4k的base模型已经具备了长文本（200k）的能力。只要用少量数据，进行继续预训练来释放这个能力，再用轻量级的SFT来调整格式，就能获得足够好的长文本能力。</p>\n<p>长文本的继续预训练中，依然使用完整的attention，而不是线性attention或者sparse\nattention。</p>\n<p>继续预训练的数据，混合了（1）原始预训练数据（2）length-upsampled\nlong-context\ndata长文本数据，长文本数据主要来自书籍（3）多文档问答的人造数据。</p>\n<p>多文档问答数据的应答中，在最终答案之前会对和答案相关的段落进行复述（recitation），以此来提升模型长文本关联的能力。</p>\n<p>这部分的数据工作主要参考《Data engineering for scaling language\nmodels to 128k context》和《Paraphrasing the original text makes high\naccuracy long-context qa》。</p>\n<p>最终用了5B token的长文本数据，batch\nsize=4M（token），只更新了100个step（这里没明白100步是怎么来，不应该是5B/4M=1250？有没有明白的朋友指点一下）。</p>\n<blockquote>\n<p>We continue pretrain the model on 5B tokens with 4M batch size, which\ntranslate to 100 optimization steps. Aligning with the concurrent work\nfrom Fu et al. [22], we observe that such light-weight continue\npretraining is already able to enable a strong performance on\nNeedle-in-a-Haystack test, as we will show in Figure 6.</p>\n</blockquote>\n<p>这个做法与《Data engineering for scaling language models to 128k\ncontext》一致，这样轻量级的微调已经足够在“大海捞针”任务做得很好。</p>\n<p>而微调的数据，也混合了短的SFT数据，以及长的文本问答数据。</p>\n<p>这些文本问答数据都是人工用模型造出来的。</p>\n<p>具体的做法是，随机抽一些文档，然后从中随机选择一个或者多个段落，让一个训练好的模型根据这些段落造出问题和答案。</p>\n<p>一个重要的细节是复述和改写：在给出答案之前，我们要求模型复述或改写原文段落。这种数据格式鼓励模型的检索行为，从而抑制其虚构行为：面对一个问题，模型更倾向于使用输入中的信息来构建答案，而不是使用其内部知识，后者可能与问题相关但不准确。</p>\n<p>使用以上所述的轻量级训练，已经可以在“大海捞针”任务做得很好，几乎能够做到全绿。</p>\n<img src=\"/41b6a819/long_context_result.png\" class title=\"大海捞针效果\">\n<h2 id=\"多模态\">多模态</h2>\n<p>ViT部分由CLIP ViT-H/14\nmodel初始化，后面的transformer由Yi-Chat初始化</p>\n<img src=\"/41b6a819/multimodal.png\" class title=\"多模态\">\n<p>3步训练：</p>\n<p>（1）使用224^2的图像来训练ViT和projection模块的参数。这一训练利用了包含1亿个图像-文本对的数据集，这些数据来自LAION-400M。主要目标是增强ViT在架构中的知识获取能力，并实现ViT与LLM之间更好的对齐。</p>\n<p>（2）将ViT的图像分辨率提升到448^2，目的是进一步推动模型识别复杂视觉细节的能力。在这个阶段使用的数据集包括从LAION-400M中提取的2000万个图像-文本对。此外，还融入了来自不同来源的大约480万个图像-文本对，例如CLLaVA、LLaVAR、Flickr、VQAv2、RefCOCO、Visual7w等。</p>\n<p>（3）整个模型的参数一起训练。主要目标是提高模型在多模态聊天交互方面的熟练度，从而赋予它能够无缝融合和解释视觉与语言输入的能力。为此，训练数据集涵盖了多种来源，总共大约有100万张图像-文本对，包括GQA、VizWiz\nVQA、TextCaps、OCR-VQA、Visual\nGenome、ShareGPT4V等等。为了确保数据平衡，对任何单一来源的最大数据量设定了上限，将其限制在不超过50,000对。</p>\n<p>使用128张A100，6B训了3天，34B训10天。</p>\n<h2 id=\"depth-upscaling-深度扩展\">Depth Upscaling 深度扩展</h2>\n<p>目标是把32层的6B扩展到48层的9B模型。</p>\n<p>参考《Scaling large language models with simple yet effective depth\nup-scaling》，通过复制中间的12-28层共16层，把层数扩展为48层。</p>\n<p>实验表明，要确定复制哪些层，可以通过测量输入和每层输出的cosine\nsimilarity来衡量。</p>\n<p>这种方法使得模型能在不额外训练的情况下，和原模型性能最接近，损失最少。</p>\n<img src=\"/41b6a819/9B.png\" class title=\"9B模型\">\n<p>这说明复制的这些层并不会很大地改变原模型的激活值。</p>\n<p>除了层数增加，Depth Upscaling还要做继续预训练，才能提升效果。</p>\n<p>继续预训练使用约800B token，训练过程分为两个阶段。</p>\n<p>其中约70%的数据是最近收集并精心挑选的。在最后阶段增强了代码的比例以提高代码性能。</p>\n<p>训练保持constant lr = 3e-5，并在模型损失达到平台期时，从4M\ntoken开始逐渐增加batch size大小。</p>\n<p>这种增加batch\nsize的方法，以及保持所有其他参数与Yi-6B基础模型配置一致，继续预训练很重要。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>往期文章</p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a></p>\n<p><a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n","length":8965,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>01.AI（零一万物），是李开复带队孵化的AI公司。2023年11月初，01.AI发布并开源了Yi-6B、Yi-34B\nbase模型，同一周内，又开源了Yi-6B-200K和Yi-34B-200K\nbase模型。Yi号称是从零预训练的双语模型。接下来的几个月，01.AI陆续推出了chat模型、多模态能力，Yi-9B、长上下文的记忆和检索能力等优化。</p>\n<p>从2023年11发布起，个人就有测试和使用Yi的模型。在SuperCLUE/CMMLU等一些榜单数据的实测上，发现Yi的效果确实不错。实际工作使用里，Yi的效果基本也都能排在同时期中文（开源）大模型里的第一梯队。</p>\n<p>2024年3月，Yi终于发布了技术报告，在此来梳理一下报告中的重点内容和值得关注的细节信息。</p>\n<h1 id=\"tldr\">TL;DR</h1>\n<p>先给出核心内容的总结：</p>\n<ul>\n<li>Yi-34B模型int4量化之后，相比float16损失&lt;1%，可以跑在RTX4090上（24G显存）</li>\n<li>模型结构不需要太多变化，LLAMA2标准结构已经足够训出很好的效果</li>\n<li>3.1T的预训练数据远比scaling\nlaw建议的1T大，但是效果更好，并且模型还没饱和，继续增大数据量还能提升</li>\n<li>微调数据质量很重要，由算法人员直接标注，只要&lt;10k的数据量就足够了</li>\n<li>4k长度的基础预训练模型已经具备长文本能力，只需用长文本数据继续预训练，更新百步就有很好效果</li>\n<li>总之，数据要精心设计，数据质量要高，数据量要大</li>\n</ul>\n<h1 id=\"模型\">模型</h1>\n<h2 id=\"规模选择\">规模选择</h2>\n<p>Yi目前有6B、9B、34B三个规模，其中34B是主力模型。</p>\n<p>选择34B，而不是更大规模的原因，是这个规模能在24G显存的消费级显卡（如RTX4090）上运行。</p>\n<p>使用int4量化之后的34B模型可以运行在24G显存的GPU上。</p>\n<p>参考<a href=\"https://arxiv.org/abs/2301.12017\">《Understanding INT4\nQuantization for Language Models: Latency Speedup, Composability, and\nFailure\nCases》</a>的量化方法，Yi-34B的int8量化模型相比bf16模型，几乎可以做到效果无损（差距&lt;1%），而int4量化模型在大部分任务的损失也完全可以接受，具体效果如下表。</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi效果\">\n<p>训练数据总共是3.1T token，这比DeepMind的scaling\nlaw所建议的1TB要大不少。目前能接触到的这个规模的模型，数据量基本都&lt;2T。</p>\n<p>（即提出Chinchilla模型的<a href=\"https://arxiv.org/abs/2203.15556\">《Training Compute-Optimal Large\nLanguage Models》</a>的scaling law）</p>\n<p>也就是从scaling law的角度来说，Yi是overtrain的。</p>\n<p><big><strong>但是Yi实践结果证明，较小模型+更大规模的高质量数据，是可以获得进一步效果提升的，这也就让我们获得了高性价比的推理模型--34B推理成本+大训练投入，就能得到接近普通70B规模的推理效果。</strong></big></p>\n<h2 id=\"模型结构\">模型结构</h2>\n<p>结构上，基于标准LLAMA2模型，做了一些变化</p>\n<ul>\n<li>注意力机制：LLAMA2只在70B用了GQA，Yi全系列都用了GQA，具体参数如下表<br>\n</li>\n<li>位置编码：RoPE，参考RoPE ABF（《Effective long-context scaling of\nfoundation models》），base扩大到10M，用于支持长上下文。<br>\n</li>\n<li>激活函数：使用SwiGLU，参考《GLU Variants Improve Transformer》</li>\n</ul>\n<p>并且把activation\nsize从4h降为8/3h，这里的说法是补偿了GQA带来的参数下降</p>\n<blockquote>\n<p>We use SwiGLU as Yi’s post-attention layer, reducing its activation\nsize from 4h to 8/3h (h denotes hidden size) to be consistent with the\nnormal post-attention layer. This adjustment also compensates for the\nreduction in parameter resulted from GQA, making the overall parameter\ncount comparible of existing 7B and 34B models.</p>\n</blockquote>\n<img src=\"/41b6a819/model.png\" class title=\"Yi模型结构\">\n<p>关于模型结构，一个结论就是<big><strong>“虽然做了很多模型结构上的实验，但是最终发现，标准的结构就足以训出足够好的模型”</strong></big></p>\n<h2 id=\"tokenizer\">tokenizer</h2>\n<ul>\n<li>用BPE，词表大小为64000，这个大小平衡了计算效率和表达能力；<br>\n</li>\n<li>其中数字全是单个的digit，让模型能更好地理解数字数据；<br>\n</li>\n<li>对于OOV的词，会降级用unicode编码 ；</li>\n<li>保留全角标点符号，不转为半角；</li>\n</ul>\n<p>另外，优先考虑英语的LLM在tokenizer会使用虚拟前缀（文本开头的空格）来泛化句子不同位置相同的单词。Yi不这么做，因为即使是在英语语境中，这种假设并不总是成立，比如对于以引号开头的句子，而且在中文语境中，这么做没有明显效果。</p>\n<h1 id=\"数据\">数据</h1>\n<p>数据，是LLM最核心的部分，没有之一。Yi最核心的工作就是提升数据数量和质量。</p>\n<img src=\"/41b6a819/cover.png\" class title=\"数据\">\n<h2 id=\"预训练数据\">预训练数据</h2>\n<p>预训练数据整体处理流程如下</p>\n<img src=\"/41b6a819/pretrain_data_pipeline.png\" class title=\"预训练数据处理流程\">\n<ol type=\"1\">\n<li>语料获取 &amp; 语言分类</li>\n</ol>\n<p>从网络爬虫开始，爬取中英文这两种语言的网站，对网站内容进行解析。</p>\n<p>并参考CCNeT（《CCNet: Extracting High Quality Monolingual Datasets\nfrom Web Crawl Data》）的做法，进行语言识别。</p>\n<ol start=\"2\" type=\"1\">\n<li>规则过滤器 Heuristic Rule Filters</li>\n</ol>\n<p>目的是快速过滤掉明显的低质量数据。基于这些规则来过滤掉：</p>\n<ul>\n<li>URL、域名、单词黑名单和乱码文本；<br>\n</li>\n<li>文档长度、特殊符号的比例，以及短行、连续行或不完整行的比例；<br>\n</li>\n<li>重复的单词模式、n-gram或段落，参考《Scaling Language Models:\nMethods, Analysis &amp; Insights from Training\nGopher》的做法，阈值则是来参考《CulturaX: A Cleaned, Enormous, and\nMultilingual Dataset for Large Language Models in 167 Languages》；</li>\n<li>数据脱敏：识别并匿名化个人信息（Personal Identifiable\nInformation，PII），如电子邮件地址和电话号码。</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li>可训练过滤器 Learned Filters</li>\n</ol>\n<p>对于不好用规则处理的，就用模型来学习模式，并进行清洗。共有4个scorer：</p>\n<ul>\n<li>Perplexity Scorer：参照《CCNet: Extracting High Quality Monolingual\nDatasets from Web Crawl\nData》，用kenlm库，把高于平均perplexity的内容丢弃；</li>\n<li>Quality\nScorer：识别如维基百科这样的高质量内容，丢弃低质量内容；</li>\n<li>Document Coherence\nScorer：用于发现句子、段落零散不连贯的文本，要么分割，要么直接丢弃；</li>\n<li>Safety Scorer：识别并删除暴力、色情、涉政内容</li>\n</ul>\n<ol start=\"4\" type=\"1\">\n<li>基于聚类的过滤 Cluster-based Filters</li>\n</ol>\n<p>用聚类的方法，把所有文档进行分类。一方面用于给数据混合策略做参考，一方面如果整个类别的质量太差，就直接抛弃类别内的所有数据。</p>\n<ol start=\"5\" type=\"1\">\n<li>去重</li>\n</ol>\n<p>参考《The RefinedWeb Dataset for Falcon LLM: Outperforming Curated\nCorpora with Web Data, and Web Data\nOnly》，做文档级的minhash去重，以及子文档级的完全匹配去重。</p>\n<p>最终获得的数据分布如下</p>\n<img src=\"/41b6a819/pretrain_data_dist.png\" class title=\"预训练数据分布\">\n<p>虽然数据规模一定要够，但是也不能因此就放弃数据质量，否则只能是garbage\nin，garbage out</p>\n<blockquote>\n<p>we prefer 3T tokens over sophasticated engineering over 10T tokens\nwithout extensive filtering</p>\n</blockquote>\n<p>这句话大概表示清洗前的数据有10T，这也是一个信息，符合质量的数据可能只有3成</p>\n<h2 id=\"微调数据\">微调数据</h2>\n<p>对于微调数据，一句话：Quality is All You Need。</p>\n<p>一共只有&lt;10k条SFT数据，每条数据都通过人工多次打磨，这比大数量但质量一般的数据的效果好。</p>\n<p>这思路和《Gemini: A family of highly capable multimodal\nmodels.》、《Llama 2: Open Foundation and Fine-Tuned Chat\nModels》、《Lima: Less is more for alignment》一致，而和FLAN（《Scaling\ninstruction-finetuned language models》）以及UltraChat（《Enhancing chat\nlanguage models by scaling high-quality instructional\nconversations》）这样更关注数据量的做法不同。</p>\n<p>具体做法上有：</p>\n<ul>\n<li>对于<big><strong>prompt distribution\nselection</strong></big>：参考《Wizardlm: Empowering large language\nmodels to follow complex\ninstructions》，开发复合指令，并通过指令进化，逐步增加指令的复杂度。这种做法显著减少了SFT数据量。<br>\n</li>\n<li>对于<big><strong>CoT data formatting</strong></big>：参考《Take a\nstep back: Evoking reasoning via abstraction in large language\nmodels》，采用了“Step-Back”的模式。即通过抽象化处理，让模型学习在深入探讨原始、具体的问题之前，制定更高层次的解决方案。<br>\n</li>\n<li>对于<big><strong>response formatting</strong></big>：使用从《Lima:\nLess is more for\nalignment》扩展的默认样式。总体而言，response的结构为introduction-body-conclusion的格式，“where\nthe body is usually a list of bullet point”。<br>\n</li>\n<li>在缓解<big><strong>幻觉</strong></big>问题上，思路是确保response中的知识不由模型内部产生，对应的做法是把会导致模型进行记忆的response删掉。（但是这个具体标准是什么，有没有了解的朋友说下看法？）<br>\n</li>\n<li>在缓解<big><strong>生成重复</strong></big>的问题上，则是直接把response中包含重复的部分都重写了。（核心还是洗数据，一条条打磨）<br>\n</li>\n<li>数据<big><strong>多样性</strong></big>很重要，因此参考《#instag:\nInstruction tagging for analyzing supervised fine-tuning of large\nlanguage\nmodels》建立了一个打标系统，并设计一个注重多样性的采样算法，平衡了各个领域数据的分布。<br>\n</li>\n<li>为了找到最佳的数据<big><strong>配比</strong></big>，参考《How\nabilities in large language models are affected by supervised\nfine-tuning data composition》，使用近似网络搜索（approximate grid\nsearch），对每个领域以{1, 1/2, 1/4, 1/8, 1/16, 1/32,\n1/64}的比例进行实验和人工测评，找到最佳的组合方式。<br>\n</li>\n<li>除了内容，<big><strong>数据格式</strong></big>对效果也有很大影响。参OPENAI的ChatML格式（<a href=\"https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md\">https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md</a>），这种结构化的格式使模型能够区分各种信息类型，如system\nprompt、user input和bot response。</li>\n</ul>\n<p>SFT数据质量能极大影响模型的效果，随着数据量的增加，高质量数据能带来更多提升，如下图</p>\n<img src=\"/41b6a819/sft.png\" class title=\"SFT\">\n<h1 id=\"训练\">训练</h1>\n<h2 id=\"infra\">infra</h2>\n<p>从数据处理到模型训练都需要大集群大算力的支持。Yi构建了支持全栈数据处理、预训练、微调和服务的基础设施。包括：</p>\n<ol type=\"1\">\n<li>自动管理和监控计算资源的能力；</li>\n<li>通过优化并行策略、内核效率和长上下文支持提高训练速度；</li>\n<li>统一微调框架，支持异构分布式训练后端，例如在DPO中同时使用Megatron和DeepSpeed进行多个模型的训练；</li>\n<li>通过各种LLM服务加速技术（如量化、continuous batching 和 paged\nattention）降低部署成本。</li>\n</ol>\n<p>总之这部分工作还是很多的，比如由于经常有硬件坏，坏的硬件会被自动从资源池移除；任务失败时，会自动跟踪重启。给算法人员考法UI等。</p>\n<h2 id=\"预训练\">预训练</h2>\n<p>训了4k基础模型。（暂时没有给出更多细节）</p>\n<h2 id=\"微调\">微调</h2>\n<p>超参如下</p>\n<ul>\n<li>AdamW：beta=[0.9,0.999]，epsilon = 1e-8<br>\n</li>\n<li>seq_len = 4096<br>\n</li>\n<li>batch size = 64<br>\n</li>\n<li>constant lr = 1e-5，weight decay = 0.1<br>\n</li>\n<li>gradient clip = 1.0<br>\n</li>\n<li>max step = 300</li>\n<li>参考《Neftune: Noisy embeddings improve instruction\nfinetuning》，对于6B模型 noise scale = 5，对于34B模型 noise scale =\n45</li>\n</ul>\n<h1 id=\"模型评测\">模型评测</h1>\n<h2 id=\"基模型评测\">基模型评测</h2>\n<ol type=\"1\">\n<li>基础能力评测</li>\n</ol>\n<p>对其他开源模型，保持和公开的设置相同做法获取结果。Yi使用贪婪解码，没有进行任何后处理，结果如下表</p>\n<img src=\"/41b6a819/base_model_eval.png\" class title=\"Base模型效果\">\n<p>在数学和代码能力上，和GPT3.5、GPT4还存在一些差距，而这些能力是可以通过继续预训练和微调来持续提升的。Yi最初的设计并没有针对这些能力，因此没有在预训练数据中包含特别多相关数据，后续会有计划增加这部分能力的提升。</p>\n<p>而和其他开源模型相比，在代码和数学以外的任务，Yi基本上做到了跟大一倍模型的效果相近，甚至更好的水平。</p>\n<ol start=\"2\" type=\"1\">\n<li>观察</li>\n</ol>\n<ul>\n<li>模型规模带来的增益：尽管Yi-34B和Yi-6B使用了相同的预训练语料，但Yi-34B的性能相比Yi-6B有了质的提升。更大的模型尺寸在代码和数学基准测试上带来了明显的增益。<br>\n</li>\n<li>数据质量：高质量预训练数据的小型模型，如Yi-34B或Qwen-14B，通常表现优于尺寸更大但（可能）数据质量较低的模型，例如Falcon-180B。</li>\n<li>GPT-4与开源LLM之间的差距：开源LLM在多种基准测试上的性能仍然落后于GPT-4和GPT-3.5。然而，具有代表性的双语LLM，例如Qwen-14B和Yi-34B，可以在包括C-Eval、CMMLU和Gaokao在内的中文知识相关基准测试上匹配甚至超过GPT-4的性能。然而，在BBH、代码（HumanEval）和数学（MATH）等推理相关基准测试上，仍然存在巨大的差距。</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li>In-Context Learning能力的测试</li>\n</ol>\n<p>Yi进一步研究了in-context\nlearning的能力，即根据少数展示的输入-输出示例，推断underlying\nfunction的能力。</p>\n<p>考虑的任务是推断加权和的线性系数。具体来说，定义 y = w1x1 + w2x2 +\n... + wnxn。</p>\n<p>少量示例展示是 x1, x2, ..., xn, y，要求模型预测给定一组新输入 x 的\ny。</p>\n<p>这就要求模型隐式地推断出 w1, w2, ..., wn。</p>\n<p>评测上，使用（a）模型预测的 y 与真实值 y∗ 之间的绝对差，即 |y − y∗|\n作为连续度量，以及使用（b）精确匹配 y == y∗ 作为不连续度量。</p>\n<p>模型在算术上的效果正常，因此可以认为这样的测试不受算术能力的影响，而能直接看模型是否具备根据给定的实例进行underlying\nfunction推理的能力。</p>\n<p>实验发现，当问题比较简单时（系数是[1,-1]），Yi-34B和LLAMA-70B效果比较好（看下图）。</p>\n<p>当问题更复杂点（系数是[1，1，1，1，1]），只有LLAMA-70B和Mistral\n8*7B这样的大模型表现出了涌现的能力。</p>\n<img src=\"/41b6a819/ict.png\" class title=\"ICT\">\n<h2 id=\"chat模型评测\">Chat模型评测</h2>\n<ol type=\"1\">\n<li>自动评测</li>\n</ol>\n<p>评测的任务和base模型相同，分别采用zero-shot和few-shot，效果依然不错，具体结果如下</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi效果\">\n<p>报告强调，如Goodhart’s\nprinciple所说，当一个指标变成目标，就不再是一个好指标。因此这里的测试只是为了确认微调没有使得模型的知识能力下降，而不会专门去针对任务做优化。</p>\n<p>结果上，Yi-34B-Chat数学能力不错，而Yi-6B-Chat并没有展现出强大的数学能力。推测较小的模型可能需要更多的数据在SFT阶段激活其相应的能力。</p>\n<ol start=\"2\" type=\"1\">\n<li>人工评测</li>\n</ol>\n<img src=\"/41b6a819/third_party.png\" class title=\"三方评测\">\n<h1 id=\"能力扩展\">能力扩展</h1>\n<p>base模型的基础上，做了3个能力扩展：长上下文、多模态、深度扩展。</p>\n<h2 id=\"长上下文能力\">长上下文能力</h2>\n<p>报告中认为，4k的base模型已经具备了长文本（200k）的能力。只要用少量数据，进行继续预训练来释放这个能力，再用轻量级的SFT来调整格式，就能获得足够好的长文本能力。</p>\n<p>长文本的继续预训练中，依然使用完整的attention，而不是线性attention或者sparse\nattention。</p>\n<p>继续预训练的数据，混合了（1）原始预训练数据（2）length-upsampled\nlong-context\ndata长文本数据，长文本数据主要来自书籍（3）多文档问答的人造数据。</p>\n<p>多文档问答数据的应答中，在最终答案之前会对和答案相关的段落进行复述（recitation），以此来提升模型长文本关联的能力。</p>\n<p>这部分的数据工作主要参考《Data engineering for scaling language\nmodels to 128k context》和《Paraphrasing the original text makes high\naccuracy long-context qa》。</p>\n<p>最终用了5B token的长文本数据，batch\nsize=4M（token），只更新了100个step（这里没明白100步是怎么来，不应该是5B/4M=1250？有没有明白的朋友指点一下）。</p>\n<blockquote>\n<p>We continue pretrain the model on 5B tokens with 4M batch size, which\ntranslate to 100 optimization steps. Aligning with the concurrent work\nfrom Fu et al. [22], we observe that such light-weight continue\npretraining is already able to enable a strong performance on\nNeedle-in-a-Haystack test, as we will show in Figure 6.</p>\n</blockquote>\n<p>这个做法与《Data engineering for scaling language models to 128k\ncontext》一致，这样轻量级的微调已经足够在“大海捞针”任务做得很好。</p>\n<p>而微调的数据，也混合了短的SFT数据，以及长的文本问答数据。</p>\n<p>这些文本问答数据都是人工用模型造出来的。</p>\n<p>具体的做法是，随机抽一些文档，然后从中随机选择一个或者多个段落，让一个训练好的模型根据这些段落造出问题和答案。</p>\n<p>一个重要的细节是复述和改写：在给出答案之前，我们要求模型复述或改写原文段落。这种数据格式鼓励模型的检索行为，从而抑制其虚构行为：面对一个问题，模型更倾向于使用输入中的信息来构建答案，而不是使用其内部知识，后者可能与问题相关但不准确。</p>\n<p>使用以上所述的轻量级训练，已经可以在“大海捞针”任务做得很好，几乎能够做到全绿。</p>\n<img src=\"/41b6a819/long_context_result.png\" class title=\"大海捞针效果\">\n<h2 id=\"多模态\">多模态</h2>\n<p>ViT部分由CLIP ViT-H/14\nmodel初始化，后面的transformer由Yi-Chat初始化</p>\n<img src=\"/41b6a819/multimodal.png\" class title=\"多模态\">\n<p>3步训练：</p>\n<p>（1）使用224^2的图像来训练ViT和projection模块的参数。这一训练利用了包含1亿个图像-文本对的数据集，这些数据来自LAION-400M。主要目标是增强ViT在架构中的知识获取能力，并实现ViT与LLM之间更好的对齐。</p>\n<p>（2）将ViT的图像分辨率提升到448^2，目的是进一步推动模型识别复杂视觉细节的能力。在这个阶段使用的数据集包括从LAION-400M中提取的2000万个图像-文本对。此外，还融入了来自不同来源的大约480万个图像-文本对，例如CLLaVA、LLaVAR、Flickr、VQAv2、RefCOCO、Visual7w等。</p>\n<p>（3）整个模型的参数一起训练。主要目标是提高模型在多模态聊天交互方面的熟练度，从而赋予它能够无缝融合和解释视觉与语言输入的能力。为此，训练数据集涵盖了多种来源，总共大约有100万张图像-文本对，包括GQA、VizWiz\nVQA、TextCaps、OCR-VQA、Visual\nGenome、ShareGPT4V等等。为了确保数据平衡，对任何单一来源的最大数据量设定了上限，将其限制在不超过50,000对。</p>\n<p>使用128张A100，6B训了3天，34B训10天。</p>\n<h2 id=\"depth-upscaling-深度扩展\">Depth Upscaling 深度扩展</h2>\n<p>目标是把32层的6B扩展到48层的9B模型。</p>\n<p>参考《Scaling large language models with simple yet effective depth\nup-scaling》，通过复制中间的12-28层共16层，把层数扩展为48层。</p>\n<p>实验表明，要确定复制哪些层，可以通过测量输入和每层输出的cosine\nsimilarity来衡量。</p>\n<p>这种方法使得模型能在不额外训练的情况下，和原模型性能最接近，损失最少。</p>\n<img src=\"/41b6a819/9B.png\" class title=\"9B模型\">\n<p>这说明复制的这些层并不会很大地改变原模型的激活值。</p>\n<p>除了层数增加，Depth Upscaling还要做继续预训练，才能提升效果。</p>\n<p>继续预训练使用约800B token，训练过程分为两个阶段。</p>\n<p>其中约70%的数据是最近收集并精心挑选的。在最后阶段增强了代码的比例以提高代码性能。</p>\n<p>训练保持constant lr = 3e-5，并在模型损失达到平台期时，从4M\ntoken开始逐渐增加batch size大小。</p>\n<p>这种增加batch\nsize的方法，以及保持所有其他参数与Yi-6B基础模型配置一致，继续预训练很重要。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>往期文章</p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a></p>\n<p><a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n"},{"title":"大模型算法题(1)","abbrlink":"3345028a","date":"2024-03-17T02:46:09.000Z","_content":"\n![](/images/cover.png)  \n\n往期回顾\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n\n***\n\n【本文已在同名微信公众号/知乎/个人博客同步上线】  \n\n本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错误，欢迎指正~\n\n# 1、在Transformer模型中，为什么scaled dot-product attention在计算QK内积之后要除以根号d？  \n\n简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。如果不对attention值进行scaling，也可以通过在参数初始化时将方差除以根号d ，同样可以起到预防softmax饱和的效果。\n\n# 2、Transformer自注意力计算中，为什么Q和K要使用不同的权重矩阵进行线性变换投影，为什么不使用同一个变换矩阵，或者不进行变换？  \n\n1、如果Q和K一样，则矩阵乘积的结果是一个对称矩阵，这样减弱了模型的表达能力。  \n\n2、如果Q和K一样，乘积结果的对称矩阵中，对角线的值会比较大，导致每个位置过分关注自己。  \n\n3、使用不同的投影矩阵，参数增多，可以增强模型表达能力。  \n\n# 3、Transformer模型中，注意力计算后面使用了两个FFN层，为什么第一个FFN层先把维度提升，第二个FFN层再把维度降回原大小？  \n\n1、提升维度：类似SVM kernel，通过提升维度可以识别一些在低维无法识别的特征。  \n\n2、提升维度：更大的可训练参数，提升模型的容量。  \n\n3、降回原维度：方便多层注意力层和残差模块进行拼接，而无需进行额外的处理。  \n\n# 4、MQA(Multi-Query Attention)和GQA(Grouped-Query Attention)相比MHA(Multi-Head Attention)，计算量变化如何，主要带来了什么优化？  \n\n1、MQA和GQA虽然可训练参数量比MHA少，但是计算量和MHA相比变化不大，主要在生成KV时有少量降低。  \n\n2、Decoder-only的大模型由于causal attention的存在，使用了KV缓存加速推理。MQA和GQA能减少KV头的数量，节省了缓存，使得在输入长度较长时也能把KV放进缓存。  \n\n# 5、为什么现在主流的LLM模型基本都是Decoder-only的结构？单向注意力模型为什么效果比双向注意力效果好？  \n\n1、双向Attention在多层模型训练中容易退化成低秩矩阵，限制了模型容量；而Decoder-only模型使用了下三角注意力矩阵，使得训练过程中矩阵是满秩，建模能力更强。  \n\n2、单向注意力模型相比双向注意力模型在训练的时候难度更大，能迫使模型学到更多信息。  \n\n3、Causal Attention天然具有位置编码的功能，而双向Attention即使交换两个token的位置也基本不影响表示，对语序区分能力较弱。  \n\n4、工程上，单向模型支持KV Cache等，对于对话场景效率友好。  \n\n5、轨迹依赖，基模型训练成本高，业界倾向于沿着已经成功的模型继续开发。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***\n\n往期回顾\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  ","source":"_posts/cs/nlp/2024/03/大模型算法题-1.md","raw":"---\ntitle: 大模型算法题(1)\ntags:\n  - NLP\n  - LLM\n  - 算法题\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 3345028a\ndate: 2024-03-17 10:46:09\n---\n\n![](/images/cover.png)  \n\n往期回顾\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n\n***\n\n【本文已在同名微信公众号/知乎/个人博客同步上线】  \n\n本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错误，欢迎指正~\n\n# 1、在Transformer模型中，为什么scaled dot-product attention在计算QK内积之后要除以根号d？  \n\n简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。如果不对attention值进行scaling，也可以通过在参数初始化时将方差除以根号d ，同样可以起到预防softmax饱和的效果。\n\n# 2、Transformer自注意力计算中，为什么Q和K要使用不同的权重矩阵进行线性变换投影，为什么不使用同一个变换矩阵，或者不进行变换？  \n\n1、如果Q和K一样，则矩阵乘积的结果是一个对称矩阵，这样减弱了模型的表达能力。  \n\n2、如果Q和K一样，乘积结果的对称矩阵中，对角线的值会比较大，导致每个位置过分关注自己。  \n\n3、使用不同的投影矩阵，参数增多，可以增强模型表达能力。  \n\n# 3、Transformer模型中，注意力计算后面使用了两个FFN层，为什么第一个FFN层先把维度提升，第二个FFN层再把维度降回原大小？  \n\n1、提升维度：类似SVM kernel，通过提升维度可以识别一些在低维无法识别的特征。  \n\n2、提升维度：更大的可训练参数，提升模型的容量。  \n\n3、降回原维度：方便多层注意力层和残差模块进行拼接，而无需进行额外的处理。  \n\n# 4、MQA(Multi-Query Attention)和GQA(Grouped-Query Attention)相比MHA(Multi-Head Attention)，计算量变化如何，主要带来了什么优化？  \n\n1、MQA和GQA虽然可训练参数量比MHA少，但是计算量和MHA相比变化不大，主要在生成KV时有少量降低。  \n\n2、Decoder-only的大模型由于causal attention的存在，使用了KV缓存加速推理。MQA和GQA能减少KV头的数量，节省了缓存，使得在输入长度较长时也能把KV放进缓存。  \n\n# 5、为什么现在主流的LLM模型基本都是Decoder-only的结构？单向注意力模型为什么效果比双向注意力效果好？  \n\n1、双向Attention在多层模型训练中容易退化成低秩矩阵，限制了模型容量；而Decoder-only模型使用了下三角注意力矩阵，使得训练过程中矩阵是满秩，建模能力更强。  \n\n2、单向注意力模型相比双向注意力模型在训练的时候难度更大，能迫使模型学到更多信息。  \n\n3、Causal Attention天然具有位置编码的功能，而双向Attention即使交换两个token的位置也基本不影响表示，对语序区分能力较弱。  \n\n4、工程上，单向模型支持KV Cache等，对于对话场景效率友好。  \n\n5、轨迹依赖，基模型训练成本高，业界倾向于沿着已经成功的模型继续开发。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***\n\n往期回顾\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  ","slug":"cs/nlp/2024/03/大模型算法题-1","published":1,"updated":"2024-03-17T14:16:49.511Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4ka0001i314k07psb31l","content":"<p><img src=\"/images/cover.png\"></p>\n<p>往期回顾</p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n<hr>\n<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>\n<p>本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错误，欢迎指正</sub></p>\n<h1 id=\"在transformer模型中为什么scaled-dot-product-attention在计算qk内积之后要除以根号d\">1、在Transformer模型中，为什么scaled\ndot-product attention在计算QK内积之后要除以根号d？</h1>\n<p>简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。如果不对attention值进行scaling，也可以通过在参数初始化时将方差除以根号d\n，同样可以起到预防softmax饱和的效果。</p>\n<h1 id=\"transformer自注意力计算中为什么q和k要使用不同的权重矩阵进行线性变换投影为什么不使用同一个变换矩阵或者不进行变换\">2、Transformer自注意力计算中，为什么Q和K要使用不同的权重矩阵进行线性变换投影，为什么不使用同一个变换矩阵，或者不进行变换？</h1>\n<p>1、如果Q和K一样，则矩阵乘积的结果是一个对称矩阵，这样减弱了模型的表达能力。</p>\n<p>2、如果Q和K一样，乘积结果的对称矩阵中，对角线的值会比较大，导致每个位置过分关注自己。</p>\n<p>3、使用不同的投影矩阵，参数增多，可以增强模型表达能力。</p>\n<h1 id=\"transformer模型中注意力计算后面使用了两个ffn层为什么第一个ffn层先把维度提升第二个ffn层再把维度降回原大小\">3、Transformer模型中，注意力计算后面使用了两个FFN层，为什么第一个FFN层先把维度提升，第二个FFN层再把维度降回原大小？</h1>\n<p>1、提升维度：类似SVM\nkernel，通过提升维度可以识别一些在低维无法识别的特征。</p>\n<p>2、提升维度：更大的可训练参数，提升模型的容量。</p>\n<p>3、降回原维度：方便多层注意力层和残差模块进行拼接，而无需进行额外的处理。</p>\n<h1 id=\"mqamulti-query-attention和gqagrouped-query-attention相比mhamulti-head-attention计算量变化如何主要带来了什么优化\">4、MQA(Multi-Query\nAttention)和GQA(Grouped-Query Attention)相比MHA(Multi-Head\nAttention)，计算量变化如何，主要带来了什么优化？</h1>\n<p>1、MQA和GQA虽然可训练参数量比MHA少，但是计算量和MHA相比变化不大，主要在生成KV时有少量降低。</p>\n<p>2、Decoder-only的大模型由于causal\nattention的存在，使用了KV缓存加速推理。MQA和GQA能减少KV头的数量，节省了缓存，使得在输入长度较长时也能把KV放进缓存。</p>\n<h1 id=\"为什么现在主流的llm模型基本都是decoder-only的结构单向注意力模型为什么效果比双向注意力效果好\">5、为什么现在主流的LLM模型基本都是Decoder-only的结构？单向注意力模型为什么效果比双向注意力效果好？</h1>\n<p>1、双向Attention在多层模型训练中容易退化成低秩矩阵，限制了模型容量；而Decoder-only模型使用了下三角注意力矩阵，使得训练过程中矩阵是满秩，建模能力更强。</p>\n<p>2、单向注意力模型相比双向注意力模型在训练的时候难度更大，能迫使模型学到更多信息。</p>\n<p>3、Causal\nAttention天然具有位置编码的功能，而双向Attention即使交换两个token的位置也基本不影响表示，对语序区分能力较弱。</p>\n<p>4、工程上，单向模型支持KV Cache等，对于对话场景效率友好。</p>\n<p>5、轨迹依赖，基模型训练成本高，业界倾向于沿着已经成功的模型继续开发。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>往期回顾</p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n","length":1412,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p>往期回顾</p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n<hr>\n<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>\n<p>本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错误，欢迎指正</sub></p>\n<h1 id=\"在transformer模型中为什么scaled-dot-product-attention在计算qk内积之后要除以根号d\">1、在Transformer模型中，为什么scaled\ndot-product attention在计算QK内积之后要除以根号d？</h1>\n<p>简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。如果不对attention值进行scaling，也可以通过在参数初始化时将方差除以根号d\n，同样可以起到预防softmax饱和的效果。</p>\n<h1 id=\"transformer自注意力计算中为什么q和k要使用不同的权重矩阵进行线性变换投影为什么不使用同一个变换矩阵或者不进行变换\">2、Transformer自注意力计算中，为什么Q和K要使用不同的权重矩阵进行线性变换投影，为什么不使用同一个变换矩阵，或者不进行变换？</h1>\n<p>1、如果Q和K一样，则矩阵乘积的结果是一个对称矩阵，这样减弱了模型的表达能力。</p>\n<p>2、如果Q和K一样，乘积结果的对称矩阵中，对角线的值会比较大，导致每个位置过分关注自己。</p>\n<p>3、使用不同的投影矩阵，参数增多，可以增强模型表达能力。</p>\n<h1 id=\"transformer模型中注意力计算后面使用了两个ffn层为什么第一个ffn层先把维度提升第二个ffn层再把维度降回原大小\">3、Transformer模型中，注意力计算后面使用了两个FFN层，为什么第一个FFN层先把维度提升，第二个FFN层再把维度降回原大小？</h1>\n<p>1、提升维度：类似SVM\nkernel，通过提升维度可以识别一些在低维无法识别的特征。</p>\n<p>2、提升维度：更大的可训练参数，提升模型的容量。</p>\n<p>3、降回原维度：方便多层注意力层和残差模块进行拼接，而无需进行额外的处理。</p>\n<h1 id=\"mqamulti-query-attention和gqagrouped-query-attention相比mhamulti-head-attention计算量变化如何主要带来了什么优化\">4、MQA(Multi-Query\nAttention)和GQA(Grouped-Query Attention)相比MHA(Multi-Head\nAttention)，计算量变化如何，主要带来了什么优化？</h1>\n<p>1、MQA和GQA虽然可训练参数量比MHA少，但是计算量和MHA相比变化不大，主要在生成KV时有少量降低。</p>\n<p>2、Decoder-only的大模型由于causal\nattention的存在，使用了KV缓存加速推理。MQA和GQA能减少KV头的数量，节省了缓存，使得在输入长度较长时也能把KV放进缓存。</p>\n<h1 id=\"为什么现在主流的llm模型基本都是decoder-only的结构单向注意力模型为什么效果比双向注意力效果好\">5、为什么现在主流的LLM模型基本都是Decoder-only的结构？单向注意力模型为什么效果比双向注意力效果好？</h1>\n<p>1、双向Attention在多层模型训练中容易退化成低秩矩阵，限制了模型容量；而Decoder-only模型使用了下三角注意力矩阵，使得训练过程中矩阵是满秩，建模能力更强。</p>\n<p>2、单向注意力模型相比双向注意力模型在训练的时候难度更大，能迫使模型学到更多信息。</p>\n<p>3、Causal\nAttention天然具有位置编码的功能，而双向Attention即使交换两个token的位置也基本不影响表示，对语序区分能力较弱。</p>\n<p>4、工程上，单向模型支持KV Cache等，对于对话场景效率友好。</p>\n<p>5、轨迹依赖，基模型训练成本高，业界倾向于沿着已经成功的模型继续开发。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>往期回顾</p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n"},{"title":"大模型算法题(2)","abbrlink":"ad0bba9d","date":"2024-03-24T03:24:47.000Z","_content":"\n![](/images/cover.png)  \n\n往期文章\n\n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)\n\n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错误，欢迎指正~\n\n***  \n\n# 1、在Bert中，词向量token embedding和(绝对)位置编码position encoding为什么可以直接相加？  \n\n1、两个向量相加，理论上其效果等价于维度拼接concat+线性变换，而相加的操作在实现上更为方便。  \n\n2、高维空间中(如768维)，两个随机向量近似为正交关系。模型在高维度有能力区分出所有组合的情况。假设共有2万个词向量，500个位置，则模型需要在768维空间区分1000万个点，即使768维每个维度只能取1和-1也具备足够的区分能力。  \n\n3、词向量和位置编码可以认为都是一个one-hot向量经过一层线性变换层得到的。两个向量相加等价于把它们的one-hot编码拼接后进行线性变换。  \n\n4、没有使用相乘则是出于工程考虑。相加相比相乘结果更为稳定，方便训练。  \n\n# 2、LoRA和全参数训练在计算量和显存上相比如何？为什么LoRA能提升大模型训练效率？  \n\n1、计算量上：LoRA训练时，在主干模型的（部分）全连接层增加了LoRA旁路，前向和后向的计算量都在主干模型的基础上，增加了旁路部分的计算，因此相比全参数训练，略有增加。  \n\n2、显存上：训练时，显存主要有①模型参数②梯度③中间激活值④优化器参数四个部分。模型参数/梯度/激活值相比全参数训练也略微增加；而优化器则不需要再存储原模型参数的部分，只需要存储LoRA旁路部分，这部分节省较多显存。  \n\n3、使用LoRA能提升训练效率主要是因为（1）优化器部分的显存需要减少了，可以增大batch（2）优化器参数减少了，分布式训练中多卡之间的通信量减少了（3）（optional）主干模型由于不用更新，可以进一步量化到int8/int4等。  \n\n# 3、为什么模型需要normalization（batchnorm/layernorm等）？  \n\n1、输入数据包含多个特征，特征之间有不同的量纲和范围（如身高180和年龄18岁），通过normalization进行归一化再经过模型进行线性/非线性组合，能够防止部分特征占据主导，部分特征被忽略。  \n\n2、batchnorm论文认为：模型一般有多层，前一层的输出是后一层的输入，而训练中前一层的参数更新会导致后一层的输入数据分布变化导致ICS（internal covariate shift），这样后面的层就不得不频繁剧烈更新适应分布变化，导致分布偏移进入激活函数饱和区而出现梯度消失，另外分布变化也是对i.i.d.条件的破坏。使用normalization可以保持分布的稳定，减小方差，使模型训练可以正常进行。  \n\n3.《How Does Batch Normalization Help Optimization?》设计了实验测量使用batchnorm前后的ICS，发现batchnorm实际上并没有缓解ICS，甚至有所增加。而batchnorm能优化模型训练的原因更多是使得损失函数平面更加光滑，而便于梯度下降收敛。  \n\n# 4、Transformer中pre-norm和post-norm各有什么优缺点?  \n\n1.原始的Transformer用的是post-norm，它在残差之后进行归一化（add & norm），对参数正则化的效果更强，模型更为鲁棒；post-norm对每个通路都进行了归一化，使得梯度在回传的时候容易出现消失。  \n\n2.Pre-norm相对于post-norm，残差部分存在不经过归一化的通路，因此能够缓解梯度消失，能够训练更深的网络。但是模型的等效“深度”受到影响，L+1层网络近似于一个L层的宽网络。  \n\n3.也就是说，在层数较少，post-norm和pre-norm都能正常收敛的情况下，post-norm的效果更好一些；但是pre-norm更适合用于训练更深的网络。  \n\n# 5、对于使用Multi-Head Attention的模型，假设hidden size=D，注意力头数量为h，每个头维度为d（假设有D=d×h），输入上下文长度为s，batch size=1，计算self-attention模块各个部分的计算量（Float Operations）。  \n\n1.QKV线性变换：6 × s × D^2（矩阵乘法，每个位置有加法和乘法两个运算，因此每个位置需要2D次计算）  \n\n2.QK内积：h × 2 × d × s^2（h组矩阵分别计算）  \n\n3.scaling：h × s^2  \n\n4.softmax：h × 3 × s^2（softmax是按列进行的，每列要计算s个exp，s个exp结果的求和，以及s次除法）  \n\n5.reduction（权重矩阵乘以V）：h × 2 × d × s^2  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n往期文章\n\n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)\n\n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n","source":"_posts/cs/nlp/2024/03/大模型算法题-2.md","raw":"---\ntitle: 大模型算法题(2)\ntags:\n  - NLP\n  - LLM\n  - 算法题\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: ad0bba9d\ndate: 2024-03-24 11:24:47\n---\n\n![](/images/cover.png)  \n\n往期文章\n\n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)\n\n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错误，欢迎指正~\n\n***  \n\n# 1、在Bert中，词向量token embedding和(绝对)位置编码position encoding为什么可以直接相加？  \n\n1、两个向量相加，理论上其效果等价于维度拼接concat+线性变换，而相加的操作在实现上更为方便。  \n\n2、高维空间中(如768维)，两个随机向量近似为正交关系。模型在高维度有能力区分出所有组合的情况。假设共有2万个词向量，500个位置，则模型需要在768维空间区分1000万个点，即使768维每个维度只能取1和-1也具备足够的区分能力。  \n\n3、词向量和位置编码可以认为都是一个one-hot向量经过一层线性变换层得到的。两个向量相加等价于把它们的one-hot编码拼接后进行线性变换。  \n\n4、没有使用相乘则是出于工程考虑。相加相比相乘结果更为稳定，方便训练。  \n\n# 2、LoRA和全参数训练在计算量和显存上相比如何？为什么LoRA能提升大模型训练效率？  \n\n1、计算量上：LoRA训练时，在主干模型的（部分）全连接层增加了LoRA旁路，前向和后向的计算量都在主干模型的基础上，增加了旁路部分的计算，因此相比全参数训练，略有增加。  \n\n2、显存上：训练时，显存主要有①模型参数②梯度③中间激活值④优化器参数四个部分。模型参数/梯度/激活值相比全参数训练也略微增加；而优化器则不需要再存储原模型参数的部分，只需要存储LoRA旁路部分，这部分节省较多显存。  \n\n3、使用LoRA能提升训练效率主要是因为（1）优化器部分的显存需要减少了，可以增大batch（2）优化器参数减少了，分布式训练中多卡之间的通信量减少了（3）（optional）主干模型由于不用更新，可以进一步量化到int8/int4等。  \n\n# 3、为什么模型需要normalization（batchnorm/layernorm等）？  \n\n1、输入数据包含多个特征，特征之间有不同的量纲和范围（如身高180和年龄18岁），通过normalization进行归一化再经过模型进行线性/非线性组合，能够防止部分特征占据主导，部分特征被忽略。  \n\n2、batchnorm论文认为：模型一般有多层，前一层的输出是后一层的输入，而训练中前一层的参数更新会导致后一层的输入数据分布变化导致ICS（internal covariate shift），这样后面的层就不得不频繁剧烈更新适应分布变化，导致分布偏移进入激活函数饱和区而出现梯度消失，另外分布变化也是对i.i.d.条件的破坏。使用normalization可以保持分布的稳定，减小方差，使模型训练可以正常进行。  \n\n3.《How Does Batch Normalization Help Optimization?》设计了实验测量使用batchnorm前后的ICS，发现batchnorm实际上并没有缓解ICS，甚至有所增加。而batchnorm能优化模型训练的原因更多是使得损失函数平面更加光滑，而便于梯度下降收敛。  \n\n# 4、Transformer中pre-norm和post-norm各有什么优缺点?  \n\n1.原始的Transformer用的是post-norm，它在残差之后进行归一化（add & norm），对参数正则化的效果更强，模型更为鲁棒；post-norm对每个通路都进行了归一化，使得梯度在回传的时候容易出现消失。  \n\n2.Pre-norm相对于post-norm，残差部分存在不经过归一化的通路，因此能够缓解梯度消失，能够训练更深的网络。但是模型的等效“深度”受到影响，L+1层网络近似于一个L层的宽网络。  \n\n3.也就是说，在层数较少，post-norm和pre-norm都能正常收敛的情况下，post-norm的效果更好一些；但是pre-norm更适合用于训练更深的网络。  \n\n# 5、对于使用Multi-Head Attention的模型，假设hidden size=D，注意力头数量为h，每个头维度为d（假设有D=d×h），输入上下文长度为s，batch size=1，计算self-attention模块各个部分的计算量（Float Operations）。  \n\n1.QKV线性变换：6 × s × D^2（矩阵乘法，每个位置有加法和乘法两个运算，因此每个位置需要2D次计算）  \n\n2.QK内积：h × 2 × d × s^2（h组矩阵分别计算）  \n\n3.scaling：h × s^2  \n\n4.softmax：h × 3 × s^2（softmax是按列进行的，每列要计算s个exp，s个exp结果的求和，以及s次除法）  \n\n5.reduction（权重矩阵乘以V）：h × 2 × d × s^2  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n往期文章\n\n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)\n\n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n\n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)\n\n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n","slug":"cs/nlp/2024/03/大模型算法题-2","published":1,"updated":"2024-03-24T04:16:09.176Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4ka0001l314k8x1o3924","content":"<p><img src=\"/images/cover.png\"></p>\n<p>往期文章</p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错误，欢迎指正</sub></p>\n<hr>\n<h1 id=\"在bert中词向量token-embedding和绝对位置编码position-encoding为什么可以直接相加\">1、在Bert中，词向量token\nembedding和(绝对)位置编码position encoding为什么可以直接相加？</h1>\n<p>1、两个向量相加，理论上其效果等价于维度拼接concat+线性变换，而相加的操作在实现上更为方便。</p>\n<p>2、高维空间中(如768维)，两个随机向量近似为正交关系。模型在高维度有能力区分出所有组合的情况。假设共有2万个词向量，500个位置，则模型需要在768维空间区分1000万个点，即使768维每个维度只能取1和-1也具备足够的区分能力。</p>\n<p>3、词向量和位置编码可以认为都是一个one-hot向量经过一层线性变换层得到的。两个向量相加等价于把它们的one-hot编码拼接后进行线性变换。</p>\n<p>4、没有使用相乘则是出于工程考虑。相加相比相乘结果更为稳定，方便训练。</p>\n<h1 id=\"lora和全参数训练在计算量和显存上相比如何为什么lora能提升大模型训练效率\">2、LoRA和全参数训练在计算量和显存上相比如何？为什么LoRA能提升大模型训练效率？</h1>\n<p>1、计算量上：LoRA训练时，在主干模型的（部分）全连接层增加了LoRA旁路，前向和后向的计算量都在主干模型的基础上，增加了旁路部分的计算，因此相比全参数训练，略有增加。</p>\n<p>2、显存上：训练时，显存主要有①模型参数②梯度③中间激活值④优化器参数四个部分。模型参数/梯度/激活值相比全参数训练也略微增加；而优化器则不需要再存储原模型参数的部分，只需要存储LoRA旁路部分，这部分节省较多显存。</p>\n<p>3、使用LoRA能提升训练效率主要是因为（1）优化器部分的显存需要减少了，可以增大batch（2）优化器参数减少了，分布式训练中多卡之间的通信量减少了（3）（optional）主干模型由于不用更新，可以进一步量化到int8/int4等。</p>\n<h1 id=\"为什么模型需要normalizationbatchnormlayernorm等\">3、为什么模型需要normalization（batchnorm/layernorm等）？</h1>\n<p>1、输入数据包含多个特征，特征之间有不同的量纲和范围（如身高180和年龄18岁），通过normalization进行归一化再经过模型进行线性/非线性组合，能够防止部分特征占据主导，部分特征被忽略。</p>\n<p>2、batchnorm论文认为：模型一般有多层，前一层的输出是后一层的输入，而训练中前一层的参数更新会导致后一层的输入数据分布变化导致ICS（internal\ncovariate\nshift），这样后面的层就不得不频繁剧烈更新适应分布变化，导致分布偏移进入激活函数饱和区而出现梯度消失，另外分布变化也是对i.i.d.条件的破坏。使用normalization可以保持分布的稳定，减小方差，使模型训练可以正常进行。</p>\n<p>3.《How Does Batch Normalization Help\nOptimization?》设计了实验测量使用batchnorm前后的ICS，发现batchnorm实际上并没有缓解ICS，甚至有所增加。而batchnorm能优化模型训练的原因更多是使得损失函数平面更加光滑，而便于梯度下降收敛。</p>\n<h1 id=\"transformer中pre-norm和post-norm各有什么优缺点\">4、Transformer中pre-norm和post-norm各有什么优缺点?</h1>\n<p>1.原始的Transformer用的是post-norm，它在残差之后进行归一化（add &amp;\nnorm），对参数正则化的效果更强，模型更为鲁棒；post-norm对每个通路都进行了归一化，使得梯度在回传的时候容易出现消失。</p>\n<p>2.Pre-norm相对于post-norm，残差部分存在不经过归一化的通路，因此能够缓解梯度消失，能够训练更深的网络。但是模型的等效“深度”受到影响，L+1层网络近似于一个L层的宽网络。</p>\n<p>3.也就是说，在层数较少，post-norm和pre-norm都能正常收敛的情况下，post-norm的效果更好一些；但是pre-norm更适合用于训练更深的网络。</p>\n<h1 id=\"对于使用multi-head-attention的模型假设hidden-sized注意力头数量为h每个头维度为d假设有ddh输入上下文长度为sbatch-size1计算self-attention模块各个部分的计算量float-operations\">5、对于使用Multi-Head\nAttention的模型，假设hidden\nsize=D，注意力头数量为h，每个头维度为d（假设有D=d×h），输入上下文长度为s，batch\nsize=1，计算self-attention模块各个部分的计算量（Float\nOperations）。</h1>\n<p>1.QKV线性变换：6 × s ×\nD^2（矩阵乘法，每个位置有加法和乘法两个运算，因此每个位置需要2D次计算）</p>\n<p>2.QK内积：h × 2 × d × s^2（h组矩阵分别计算）</p>\n<p>3.scaling：h × s^2</p>\n<p>4.softmax：h × 3 ×\ns^2（softmax是按列进行的，每列要计算s个exp，s个exp结果的求和，以及s次除法）</p>\n<p>5.reduction（权重矩阵乘以V）：h × 2 × d × s^2</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>往期文章</p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a></p>\n","length":2220,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p>往期文章</p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错误，欢迎指正</sub></p>\n<hr>\n<h1 id=\"在bert中词向量token-embedding和绝对位置编码position-encoding为什么可以直接相加\">1、在Bert中，词向量token\nembedding和(绝对)位置编码position encoding为什么可以直接相加？</h1>\n<p>1、两个向量相加，理论上其效果等价于维度拼接concat+线性变换，而相加的操作在实现上更为方便。</p>\n<p>2、高维空间中(如768维)，两个随机向量近似为正交关系。模型在高维度有能力区分出所有组合的情况。假设共有2万个词向量，500个位置，则模型需要在768维空间区分1000万个点，即使768维每个维度只能取1和-1也具备足够的区分能力。</p>\n<p>3、词向量和位置编码可以认为都是一个one-hot向量经过一层线性变换层得到的。两个向量相加等价于把它们的one-hot编码拼接后进行线性变换。</p>\n<p>4、没有使用相乘则是出于工程考虑。相加相比相乘结果更为稳定，方便训练。</p>\n<h1 id=\"lora和全参数训练在计算量和显存上相比如何为什么lora能提升大模型训练效率\">2、LoRA和全参数训练在计算量和显存上相比如何？为什么LoRA能提升大模型训练效率？</h1>\n<p>1、计算量上：LoRA训练时，在主干模型的（部分）全连接层增加了LoRA旁路，前向和后向的计算量都在主干模型的基础上，增加了旁路部分的计算，因此相比全参数训练，略有增加。</p>\n<p>2、显存上：训练时，显存主要有①模型参数②梯度③中间激活值④优化器参数四个部分。模型参数/梯度/激活值相比全参数训练也略微增加；而优化器则不需要再存储原模型参数的部分，只需要存储LoRA旁路部分，这部分节省较多显存。</p>\n<p>3、使用LoRA能提升训练效率主要是因为（1）优化器部分的显存需要减少了，可以增大batch（2）优化器参数减少了，分布式训练中多卡之间的通信量减少了（3）（optional）主干模型由于不用更新，可以进一步量化到int8/int4等。</p>\n<h1 id=\"为什么模型需要normalizationbatchnormlayernorm等\">3、为什么模型需要normalization（batchnorm/layernorm等）？</h1>\n<p>1、输入数据包含多个特征，特征之间有不同的量纲和范围（如身高180和年龄18岁），通过normalization进行归一化再经过模型进行线性/非线性组合，能够防止部分特征占据主导，部分特征被忽略。</p>\n<p>2、batchnorm论文认为：模型一般有多层，前一层的输出是后一层的输入，而训练中前一层的参数更新会导致后一层的输入数据分布变化导致ICS（internal\ncovariate\nshift），这样后面的层就不得不频繁剧烈更新适应分布变化，导致分布偏移进入激活函数饱和区而出现梯度消失，另外分布变化也是对i.i.d.条件的破坏。使用normalization可以保持分布的稳定，减小方差，使模型训练可以正常进行。</p>\n<p>3.《How Does Batch Normalization Help\nOptimization?》设计了实验测量使用batchnorm前后的ICS，发现batchnorm实际上并没有缓解ICS，甚至有所增加。而batchnorm能优化模型训练的原因更多是使得损失函数平面更加光滑，而便于梯度下降收敛。</p>\n<h1 id=\"transformer中pre-norm和post-norm各有什么优缺点\">4、Transformer中pre-norm和post-norm各有什么优缺点?</h1>\n<p>1.原始的Transformer用的是post-norm，它在残差之后进行归一化（add &amp;\nnorm），对参数正则化的效果更强，模型更为鲁棒；post-norm对每个通路都进行了归一化，使得梯度在回传的时候容易出现消失。</p>\n<p>2.Pre-norm相对于post-norm，残差部分存在不经过归一化的通路，因此能够缓解梯度消失，能够训练更深的网络。但是模型的等效“深度”受到影响，L+1层网络近似于一个L层的宽网络。</p>\n<p>3.也就是说，在层数较少，post-norm和pre-norm都能正常收敛的情况下，post-norm的效果更好一些；但是pre-norm更适合用于训练更深的网络。</p>\n<h1 id=\"对于使用multi-head-attention的模型假设hidden-sized注意力头数量为h每个头维度为d假设有ddh输入上下文长度为sbatch-size1计算self-attention模块各个部分的计算量float-operations\">5、对于使用Multi-Head\nAttention的模型，假设hidden\nsize=D，注意力头数量为h，每个头维度为d（假设有D=d×h），输入上下文长度为s，batch\nsize=1，计算self-attention模块各个部分的计算量（Float\nOperations）。</h1>\n<p>1.QKV线性变换：6 × s ×\nD^2（矩阵乘法，每个位置有加法和乘法两个运算，因此每个位置需要2D次计算）</p>\n<p>2.QK内积：h × 2 × d × s^2（h组矩阵分别计算）</p>\n<p>3.scaling：h × s^2</p>\n<p>4.softmax：h × 3 ×\ns^2（softmax是按列进行的，每列要计算s个exp，s个exp结果的求和，以及s次除法）</p>\n<p>5.reduction（权重矩阵乘以V）：h × 2 × d × s^2</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>往期文章</p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a></p>\n"},{"title":"大模型算法题(5)","abbrlink":"336f2f3e","date":"2024-05-04T07:47:14.000Z","_content":"\n![](/images/cover.png)  \n\n【往期文章】\n\n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~  \n\n如有错漏，欢迎指正~\n\n***  \n\n# 1.使用半精度训练时，bf16和fp16格式有什么异同？  \n\n二者都是占用16bit空间。  \n\nfp16由1个符号位、5个指数位和10个尾数位组成。fp16在表达小数时具有较高的精度，但表示的最大范围相对bf16比较小。相比bf16，在表达较大的数时更容易出现上溢的情况。  \n\nbf16由1个符号位、8个指数位和7个尾数位组成。相比于fp16，bf16牺牲了一些尾数位以增加指数位，扩大了表达的范围，但是精度降低了，因此对于对精度需求比较高的模型，模型可能效果不如fp16。  \n\n模型训练时使用bf16和fp16都可以降低内存使用和传输量，提高训练效率。  \n\n{% asset_img bfloat16.jpeg bf16 %}  \n\n\n# 2.支持模型长上下文的方案「NTK-aware interpolation」的思路是什么？  \n\n1.在NTK插值之前，线性插值通过在原模型训练的两个位置编码中间，插入新的位置编码，使得同样的取值范围可以容纳更多位置。  \n\n2.而NTK插值则是一种非线性插值的方法。它通过仅改变RoPE的base，使得位置编码中不同频率的信号有不同的表现，具体来说就是“高频外推，低频内插”。高频信号使用外推，防止分辨率太低，而低频信号沿用插值的方式，实现方便。  \n\n# 3.LLM长度外推方案NTK-by-parts的思路是什么？  \n\nNTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，都认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。对于波长远小于上下文长度的分量（如波长<=1/32上下文），就不插值只外推；而对于波长大于等于上下文长度的分量，就只外推不插值；对于介于两者之间的分量，就使用外推和插值的加权和。  \n\n使用一个斜坡函数来定义NTK-by-parts的分段插值方法，如下所示  \n\n{% asset_img ntk_by_parts.png NTK-by-parts %}  \n\n# 4.LLM长度外推方案YaRN是怎做的？  \n\nPI/NTK/NTK-by-parts主要的做法都是使用插值，而随着插值进行，token之间的距离变得更近（因为现在每一个位置旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖，也就是都集中在某个区间。  \n\n换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。  \n\n可以通过在softmax之前，将中间注意力矩阵乘以温度 t>1来缓解这个问题。由于RoPE被编码为一个旋转矩阵，就可以简单地给旋转矩阵乘以一个系数根号t来实现，这样可以不必修改注意力的代码。  \n\nYaRN结合NTK-by-parts和这个温度系数，对attention score进行调整。  \n\n{% asset_img yarn.png YaRN %}  \n\n# 5.对于使用Group-Query Attention的模型，假设hidden size=D，Q的注意力头数量为h，每个头维度为d（假设有D=d×h），kv组数为n，输入上下文长度为s，batch size=b，模型层数为L，计算推理时kv cache所需的空间。  \n\nkv cache缓存的是经过投影变换之后的K和V矩阵。  \n\n对于GQA，每层有n组K和V，每组的特征维度和Q的每个头的特征维度相同，为D/h。则每层每组K和V数据量为sD/h，整个模型共有2LnsD/h个数据，因此整个batch需要缓存2bLnsD/h个数据。\n如果使用的是半精度浮点数，每个浮点需要两个字节，因此共需要4bLnsD/h字节的空间。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  ","source":"_posts/cs/nlp/2024/05/大模型算法题-5.md","raw":"---\ntitle: 大模型算法题(5)\nabbrlink: 336f2f3e\ndate: 2024-05-04 15:47:14\ntags:\n  - NLP\n  - LLM\n  - 算法题\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n![](/images/cover.png)  \n\n【往期文章】\n\n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~  \n\n如有错漏，欢迎指正~\n\n***  \n\n# 1.使用半精度训练时，bf16和fp16格式有什么异同？  \n\n二者都是占用16bit空间。  \n\nfp16由1个符号位、5个指数位和10个尾数位组成。fp16在表达小数时具有较高的精度，但表示的最大范围相对bf16比较小。相比bf16，在表达较大的数时更容易出现上溢的情况。  \n\nbf16由1个符号位、8个指数位和7个尾数位组成。相比于fp16，bf16牺牲了一些尾数位以增加指数位，扩大了表达的范围，但是精度降低了，因此对于对精度需求比较高的模型，模型可能效果不如fp16。  \n\n模型训练时使用bf16和fp16都可以降低内存使用和传输量，提高训练效率。  \n\n{% asset_img bfloat16.jpeg bf16 %}  \n\n\n# 2.支持模型长上下文的方案「NTK-aware interpolation」的思路是什么？  \n\n1.在NTK插值之前，线性插值通过在原模型训练的两个位置编码中间，插入新的位置编码，使得同样的取值范围可以容纳更多位置。  \n\n2.而NTK插值则是一种非线性插值的方法。它通过仅改变RoPE的base，使得位置编码中不同频率的信号有不同的表现，具体来说就是“高频外推，低频内插”。高频信号使用外推，防止分辨率太低，而低频信号沿用插值的方式，实现方便。  \n\n# 3.LLM长度外推方案NTK-by-parts的思路是什么？  \n\nNTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，都认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。对于波长远小于上下文长度的分量（如波长<=1/32上下文），就不插值只外推；而对于波长大于等于上下文长度的分量，就只外推不插值；对于介于两者之间的分量，就使用外推和插值的加权和。  \n\n使用一个斜坡函数来定义NTK-by-parts的分段插值方法，如下所示  \n\n{% asset_img ntk_by_parts.png NTK-by-parts %}  \n\n# 4.LLM长度外推方案YaRN是怎做的？  \n\nPI/NTK/NTK-by-parts主要的做法都是使用插值，而随着插值进行，token之间的距离变得更近（因为现在每一个位置旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖，也就是都集中在某个区间。  \n\n换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。  \n\n可以通过在softmax之前，将中间注意力矩阵乘以温度 t>1来缓解这个问题。由于RoPE被编码为一个旋转矩阵，就可以简单地给旋转矩阵乘以一个系数根号t来实现，这样可以不必修改注意力的代码。  \n\nYaRN结合NTK-by-parts和这个温度系数，对attention score进行调整。  \n\n{% asset_img yarn.png YaRN %}  \n\n# 5.对于使用Group-Query Attention的模型，假设hidden size=D，Q的注意力头数量为h，每个头维度为d（假设有D=d×h），kv组数为n，输入上下文长度为s，batch size=b，模型层数为L，计算推理时kv cache所需的空间。  \n\nkv cache缓存的是经过投影变换之后的K和V矩阵。  \n\n对于GQA，每层有n组K和V，每组的特征维度和Q的每个头的特征维度相同，为D/h。则每层每组K和V数据量为sD/h，整个模型共有2LnsD/h个数据，因此整个batch需要缓存2bLnsD/h个数据。\n如果使用的是半精度浮点数，每个浮点需要两个字节，因此共需要4bLnsD/h字节的空间。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n[大模型算法题(3)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(4)](http://www.linsight.cn/1736008.html)  \n[大模型算法题(5)](http://www.linsight.cn/336f2f3e.html)  ","slug":"cs/nlp/2024/05/大模型算法题-5","published":1,"updated":"2024-05-10T06:50:19.009Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4ka9009r314k85ui4tt8","content":"<p><img src=\"/images/cover.png\"></p>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~</p>\n<p>如有错漏，欢迎指正~</p>\n<hr>\n<h1 id=\"使用半精度训练时bf16和fp16格式有什么异同\">1.使用半精度训练时，bf16和fp16格式有什么异同？</h1>\n<p>二者都是占用16bit空间。</p>\n<p>fp16由1个符号位、5个指数位和10个尾数位组成。fp16在表达小数时具有较高的精度，但表示的最大范围相对bf16比较小。相比bf16，在表达较大的数时更容易出现上溢的情况。</p>\n<p>bf16由1个符号位、8个指数位和7个尾数位组成。相比于fp16，bf16牺牲了一些尾数位以增加指数位，扩大了表达的范围，但是精度降低了，因此对于对精度需求比较高的模型，模型可能效果不如fp16。</p>\n<p>模型训练时使用bf16和fp16都可以降低内存使用和传输量，提高训练效率。</p>\n<img src=\"/336f2f3e/bfloat16.jpeg\" class title=\"bf16\">\n<h1 id=\"支持模型长上下文的方案ntk-aware-interpolation的思路是什么\">2.支持模型长上下文的方案「NTK-aware\ninterpolation」的思路是什么？</h1>\n<p>1.在NTK插值之前，线性插值通过在原模型训练的两个位置编码中间，插入新的位置编码，使得同样的取值范围可以容纳更多位置。</p>\n<p>2.而NTK插值则是一种非线性插值的方法。它通过仅改变RoPE的base，使得位置编码中不同频率的信号有不同的表现，具体来说就是“高频外推，低频内插”。高频信号使用外推，防止分辨率太低，而低频信号沿用插值的方式，实现方便。</p>\n<h1 id=\"llm长度外推方案ntk-by-parts的思路是什么\">3.LLM长度外推方案NTK-by-parts的思路是什么？</h1>\n<p>NTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，都认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。对于波长远小于上下文长度的分量（如波长&lt;=1/32上下文），就不插值只外推；而对于波长大于等于上下文长度的分量，就只外推不插值；对于介于两者之间的分量，就使用外推和插值的加权和。</p>\n<p>使用一个斜坡函数来定义NTK-by-parts的分段插值方法，如下所示</p>\n<img src=\"/336f2f3e/ntk_by_parts.png\" class title=\"NTK-by-parts\">\n<h1 id=\"llm长度外推方案yarn是怎做的\">4.LLM长度外推方案YaRN是怎做的？</h1>\n<p>PI/NTK/NTK-by-parts主要的做法都是使用插值，而随着插值进行，token之间的距离变得更近（因为现在每一个位置旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖，也就是都集中在某个区间。</p>\n<p>换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。</p>\n<p>可以通过在softmax之前，将中间注意力矩阵乘以温度\nt&gt;1来缓解这个问题。由于RoPE被编码为一个旋转矩阵，就可以简单地给旋转矩阵乘以一个系数根号t来实现，这样可以不必修改注意力的代码。</p>\n<p>YaRN结合NTK-by-parts和这个温度系数，对attention score进行调整。</p>\n<img src=\"/336f2f3e/yarn.png\" class title=\"YaRN\">\n<h1 id=\"对于使用group-query-attention的模型假设hidden-sizedq的注意力头数量为h每个头维度为d假设有ddhkv组数为n输入上下文长度为sbatch-sizeb模型层数为l计算推理时kv-cache所需的空间\">5.对于使用Group-Query\nAttention的模型，假设hidden\nsize=D，Q的注意力头数量为h，每个头维度为d（假设有D=d×h），kv组数为n，输入上下文长度为s，batch\nsize=b，模型层数为L，计算推理时kv cache所需的空间。</h1>\n<p>kv cache缓存的是经过投影变换之后的K和V矩阵。</p>\n<p>对于GQA，每层有n组K和V，每组的特征维度和Q的每个头的特征维度相同，为D/h。则每层每组K和V数据量为sD/h，整个模型共有2LnsD/h个数据，因此整个batch需要缓存2bLnsD/h个数据。\n如果使用的是半精度浮点数，每个浮点需要两个字节，因此共需要4bLnsD/h字节的空间。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n","length":1925,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n<hr>\n<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~</p>\n<p>如有错漏，欢迎指正~</p>\n<hr>\n<h1 id=\"使用半精度训练时bf16和fp16格式有什么异同\">1.使用半精度训练时，bf16和fp16格式有什么异同？</h1>\n<p>二者都是占用16bit空间。</p>\n<p>fp16由1个符号位、5个指数位和10个尾数位组成。fp16在表达小数时具有较高的精度，但表示的最大范围相对bf16比较小。相比bf16，在表达较大的数时更容易出现上溢的情况。</p>\n<p>bf16由1个符号位、8个指数位和7个尾数位组成。相比于fp16，bf16牺牲了一些尾数位以增加指数位，扩大了表达的范围，但是精度降低了，因此对于对精度需求比较高的模型，模型可能效果不如fp16。</p>\n<p>模型训练时使用bf16和fp16都可以降低内存使用和传输量，提高训练效率。</p>\n<img src=\"/336f2f3e/bfloat16.jpeg\" class title=\"bf16\">\n<h1 id=\"支持模型长上下文的方案ntk-aware-interpolation的思路是什么\">2.支持模型长上下文的方案「NTK-aware\ninterpolation」的思路是什么？</h1>\n<p>1.在NTK插值之前，线性插值通过在原模型训练的两个位置编码中间，插入新的位置编码，使得同样的取值范围可以容纳更多位置。</p>\n<p>2.而NTK插值则是一种非线性插值的方法。它通过仅改变RoPE的base，使得位置编码中不同频率的信号有不同的表现，具体来说就是“高频外推，低频内插”。高频信号使用外推，防止分辨率太低，而低频信号沿用插值的方式，实现方便。</p>\n<h1 id=\"llm长度外推方案ntk-by-parts的思路是什么\">3.LLM长度外推方案NTK-by-parts的思路是什么？</h1>\n<p>NTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，都认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。对于波长远小于上下文长度的分量（如波长&lt;=1/32上下文），就不插值只外推；而对于波长大于等于上下文长度的分量，就只外推不插值；对于介于两者之间的分量，就使用外推和插值的加权和。</p>\n<p>使用一个斜坡函数来定义NTK-by-parts的分段插值方法，如下所示</p>\n<img src=\"/336f2f3e/ntk_by_parts.png\" class title=\"NTK-by-parts\">\n<h1 id=\"llm长度外推方案yarn是怎做的\">4.LLM长度外推方案YaRN是怎做的？</h1>\n<p>PI/NTK/NTK-by-parts主要的做法都是使用插值，而随着插值进行，token之间的距离变得更近（因为现在每一个位置旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖，也就是都集中在某个区间。</p>\n<p>换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。</p>\n<p>可以通过在softmax之前，将中间注意力矩阵乘以温度\nt&gt;1来缓解这个问题。由于RoPE被编码为一个旋转矩阵，就可以简单地给旋转矩阵乘以一个系数根号t来实现，这样可以不必修改注意力的代码。</p>\n<p>YaRN结合NTK-by-parts和这个温度系数，对attention score进行调整。</p>\n<img src=\"/336f2f3e/yarn.png\" class title=\"YaRN\">\n<h1 id=\"对于使用group-query-attention的模型假设hidden-sizedq的注意力头数量为h每个头维度为d假设有ddhkv组数为n输入上下文长度为sbatch-sizeb模型层数为l计算推理时kv-cache所需的空间\">5.对于使用Group-Query\nAttention的模型，假设hidden\nsize=D，Q的注意力头数量为h，每个头维度为d（假设有D=d×h），kv组数为n，输入上下文长度为s，batch\nsize=b，模型层数为L，计算推理时kv cache所需的空间。</h1>\n<p>kv cache缓存的是经过投影变换之后的K和V矩阵。</p>\n<p>对于GQA，每层有n组K和V，每组的特征维度和Q的每个头的特征维度相同，为D/h。则每层每组K和V数据量为sD/h，整个模型共有2LnsD/h个数据，因此整个batch需要缓存2bLnsD/h个数据。\n如果使用的是半精度浮点数，每个浮点需要两个字节，因此共需要4bLnsD/h字节的空间。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">大模型算法题(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">大模型算法题(5)</a></p>\n"},{"title":"MoE模型的前世今生","abbrlink":"44e38c1b","date":"2024-03-30T01:56:05.000Z","_content":"\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n2024年3、4月这段时间，很多MoE模型扎堆发布，包括Qwen1.5-MoE、DBRX、Jamba和Mistral等。  \n\n下面这个表格列出了部分近期发布的MoE工作  \n\n<center>\n\n| 模型 | 发布时间 | 备注 |\n| :----: | :----: | :----: |\n| GPT4 | 2023年3月 | 23年6月George Hotz爆料GPT4是8×220B模型 |\n| Mistral-8×7B | 2023年12月 | Mistral AI，开源 |\n| LLAMA-MoE | 2023年12月 | github开源项目 |\n| DeepSeek-MoE | 2024年1月 | 幻方量化，国内首个开源MoE模型，有技术报告 |\n| abab6 |2024年1月 | MiniMax，号称千亿MoE，无开源，无细节发布 |\n| 天工2.0 | 2024年2月 | 昆仑万维，无开源，无细节发布 |\n| Step-2 | 2024年3月 | 阶跃星辰，无开源，无细节发布 |\n| MM1 | 2024年3月 | 苹果，多模态MoE，无开源，有技术报告 |\n| Grok-1 | 2024年3月 | X，开源 |\n| Qwen1.5-MoE-A2.7B| 2024年3月 | 阿里巴巴，开源 |\n| DBRX | 2024年3月 | Databricks，开源 |\n| Jamba | 2024年3月 | AI21，开源 |\n| Mistral-8×22B | 2024年4月 | Mistral AI，开源 |\n| WizardLM-2-8×22B | 2024年4月 | 微软，开源 |\n| 天工3.0 | 2024年4月 | 昆仑万维，400BMoE |\n| Arctic | 2024年4月 | Snowflake，480B，Dense-MoE Hybrid，开源 |\n\n</center>  \n\nMoE模型目前风头正劲，就连前不久小米汽车发布会上，雷总也弄了个多模态MoE大模型做汽车智能中控  \n\n{% asset_img xiaomi_moe.jpg 小米汽车多模态MoE模型 %}  \n\n相信今年接下来的这段时间，MoE还会给我们带来更多的大新闻。  \n\n本篇将初步梳理MoE相关的一些经典工作和几个近期发布的中文MoE模型，从背景、思路和效果来了解MoE模型。  \n\n到文章发出的2024年4月为止，个人认为DeepSeek-MoE和Qwen1.5-MoE是中文领域做得比较好的两个工作，赶时间的朋友可以优先关注这两个工作。\n\n# 时间线  \n\n这里先对后面会涉及的MoE相关工作，大致按时间线梳理一下，也列出一些关键信息包括模型结构、模型规模等。  \n\n（很多经典的MoE工作都出自Google）\n\n## 上古时代  \n\n首先是很多MoE相关论文都会引用的，发表在1991年的论文[《Adaptive Mixtures of Local Experts》](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)，这篇文章出自Geoffrey Hinton和Michael I. Jordan两位大神之手。虽然在更早的时候就有MoE相关概念的工作，如原文所提到的，1988年这个概念就有了  \n\n>This idea was first presented by Jacobs and Hinton at the Connectionist Summer School in Pittsburg in 1988.  \n\n但是大部分MoE文章还是认为是这个工作奠定了MoE的基础。  \n\n## RNN时代  \n\n时隔二十多年，Google在2017年1月发布了[《Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer》](https://arxiv.org/abs/1701.06538)，把MoE带进了LSTM，训出了最大137B参数，专家数达到128k的LSTM模型。  \n\n## Transformer时代  \n\n1. 2020年6月，Google发布[《GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding》](https://arxiv.org/abs/2006.16668)，把MoE应用在encoder-decoder结构的transformer模型上，每两层将一个FFN层替换成一个MoE层，训出了模型参数量从12.5B到600B的一系列MoE模型，每层最大专家数也达到2048个。  \n\n2. 2021年1月，Google发布[《Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity》](https://arxiv.org/abs/2101.03961) ，在T5（encoder-decoder结构）的基础上，把FFN层替换成MoE层，并简化了routing策略，训出了最大1.6T参数量的switch transformer。Switch Transformers对scaling、蒸馏等做了很多详细的探索，影响深远，是很重要的一个工作。  \n\n3. 2022年2月，Google发布[《ST-MoE: Designing Stable and Transferable Sparse Expert Models》](https://arxiv.org/abs/2202.08906)，也是一个基于encoder-decoder结构的MoE模型，最大模型有269B的总参数，32B的激活参数。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，个人认为其重要程度相比Switch Transformer都有过之而无不及。  \n\n## GPT时代  \n\n1. 2021年12月，Google发布了GLaM，[《GLaM: Efficient Scaling of Language Models with Mixture-of-Experts》](https://arxiv.org/abs/2112.06905)，训出了最大为1.2T参数量的decoder-only模型。（从encoder-decoder到decoder-only，可以看到Google内部在模型结构方向上也有很多不同的尝试）  \n\n2. 2024年1月，幻方量化发布[《DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models》](https://arxiv.org/abs/2401.06066)，对在23年12月开源的DeepSeekMoE，给出了一些细节。  \n\n3. 2024年，Databricks的DBRX、阿里的Qwen1.5-MoE-A2.7B、Mistral AI的Mistral-8x22B等陆续发布。  \n\n# 奠基工作  \n\nGeoffrey Hinton和Michael I. Jordan的[《Adaptive Mixtures of Local Experts》](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)是大多数MoE论文都会引用的最早工作。  \n\n1. 思路  \n\n这篇文章大致的思路是这样的：对于比较复杂的任务，一般可以拆分为多个子任务。比如要求计算输入文本中有多少个动词和名词，那就可以拆分为“数动词”和“数名词”这两个子任务。  \n\n而一个模型如果要同时学习多个子任务，多个子任务相互之间就会互相影响，模型的学习就会比较缓慢、困难，最终的学习效果也不好。  \n\n因此这篇文章提出了一种由多个分开的子网络组成的监督学习方法。这些分开的网络，在训练过程中，分别学习处理整个训练数据集中的一个子集，也就是一个子任务。这个思路就是现代MoE的思路，每个子网络（也就是一个expert）学习处理一部分内容。  \n\n文章里把这个MoE的方法应用于vowel discrimination task，即元音辨别任务，验证了MoE设计的有效性。元音辨别指的是语音学中区分不同元音的能力，在语音学中，模型需要学习辨别不同的元音因素，以便准确地理解和识别语音输入。通过让多个子模型分别学习分别学习不同元音（a、e、i、o、u）辨别的子任务，最终效果得到了提升。  \n\n2. 模型设计  \n\n下图展示的就是这个MoE的思路：各个expert network和gating network接收同样的输入，每个expert给出各自的处理结果；而gating network输出每个expert的权重，就像一个开关一样，控制着每个expert对当前输入的打开程度，只是这个开关不是离散的，而是stochastic的，给出的不是true和false，而是权重。  \n\n{% asset_img vanilla_moe.png Vanilla MoE %}  \n\n3. 损失函数优化  \n\n实际上，MoE这个idea在这篇文章之前就有了。如论文中所提，Jacobs和Hinton在1988就讨论过。但是之前的工作在loss的设计上，和ensemble更相近，多个expert之间更倾向于合作，每个expert会学习其他expert的residual部分。  \n\n具体来说，对于case $c$，假设第 $d^c$ 是对应的ground truth，第 $i$ 个expert的输出是 $o_{i}^c$，$p_{i}^c$ 是gating network给第 $i$ 个expert分配的权重，那么以前的工作所使用的损失函数 $E^{c}$ 计算如下\n\n$$E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}$$\n\n这样的损失计算方式，是把期望输出和所有expert输出的混合结果进行比较。  \n\n这样做的结果是，在训练过程中，每个expert学习的其实是其他expert的组合结果所剩下的残差。这样的学习目标并不能很好迫使每个expert单独输出好的结果，因此不能得到稀疏的模型。  \n\n从另一个角度来看，这个损失计算把所有专家耦合在了一起。即当一个expert的输出发生了变化，所有expert的组合结果也会变化，其他所有的expert也需要做相应的改动来适应这个变化。因此各个expert之间更加倾向于合作，而不是相互竞争并单独给出好的结果，让gating network输出稀疏的结果。  \n\n虽然可以使用如增加辅助损失函数的做法，迫使模型给出稀疏激活的结果，但是这样相当于增加了很强的先验正则化，对模型最终效果也是有损害的。  \n\n而Hinton和Jordan在这个工作里，提出更简单的做法是对loss计算进行修改，使得各个expert之间的关系从合作变成竞争。  \n\n假设gating network每次随机选择一个expert，损失计算如下  \n\n$$E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$\n\n在这个损失函数中，每个expert的输出结果会单独和期望结果进行对比，这就要求每个expert单独给出完整的结果，而不是仅学习其他expert的残差。  \n\n这样的loss计算具有localization的特性，即如果一个训练case错了，那么会被修改的主要是被gating network选中且出错的expert，以及负责分配权重的gating network，而不会很大地影响其他expert。  \n\n此外，localization还体现在，每个expert只会负责处理输入空间中某个特定子空间的向量，而不是完整的输入空间。  \n\n这样一来，不同的expert之间不会直接相互影响，虽然还是有间接的影响，比如某个expert的输出变了，gating network可能会分配新的权重，但是至少不会改变其他expert error的符号（+，-），即优化的方向。  \n\n最终的结果是，对于给定的输入，这样的系统会倾向于以高权重分配单一一个expert来预测结果（但其他权重还不是真正的0，不是真正的稀疏）。  \n\n4. 实操技巧\n\n上面提出的这个loss计算，理论上没有问题，实际上也能训练，但是为了得到更好的效果，作者把原loss计算作了如下变化：先指数化再求和，最后再取对数，得到了优化loss。看下变化前后的对比  \n\n$$\\text{原loss：}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$  \n\n$$\\text{优化loss：}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}$$  \n\n这样做有什么好处呢？来对比一下原loss函数和优化后的loss函数的求导结果  \n\n$$\\text{原loss导数：}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\n$$\\text{优化loss导数：}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\n相比原loss函数的导数，优化后的loss函数的导数，把当前第 $i$ 个expert的表现，和其他expert联系起来了。这样能够更好地衡量expert $i$ 对当前case的处理结果好坏。特别是在训练初期，gating network的权重是近似平均分配的，那么使用原loss函数的结果是，对当前case效果最好的expert，学习速度是最慢的（因为loss最小）；而优化的loss函数则可以让当前最好的expert的学习速度最快。相当于让“有天赋”的专家在对应的子任务上尽快提高水平。这样就强化了localization的特征，使得各个expert更快拟合到自己擅长的部分，加速训练。  \n\n（BTW，优化后的这个loss导数，和现在的对比学习形式上看起来也很相似）  \n\n这个工作在今天看来不很复杂，但是思路还是很踏实有效的，给MoE奠定了基础。  \n\n# LSTM MoE  \n\nGoogle在2017年1月发布了\n[《OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER》](https://arxiv.org/abs/1701.06538)，把MoE应用到了LSTM上，训出了最大137B的LSTM模型。这样规模的模型哪怕放在7年后的今天，也是巨无霸的存在，需要解决很多工程问题。  \n\n相比1991年的工作，这里做到了真正的稀疏激活，从而可以在实际计算量较少的情况下，训练巨大的模型。  \n\n## 背景\n\n虽然当时Transformer还没出来，大规模模型的竞赛也还不像今天这么激烈，但是在多个领域中（文本、图像、音频），已经有不少工作反复证实了一件事：模型容量越大，能训出来的效果越好，上限越高。但是模型越大，需要的训练数据也就越多，二者共同作用下，就造成了训练开销基本是随着模型增大，以平方关系在增长。  \n\n在这个背景下就出现一些conditional computation，条件计算的工作来解决这个问题。conditional computation就是根据输入，有选择地只激活部分网络模块。那么MoE其实就是一种条件计算的实现。由于不用激活全部参数，训练所需的计算量就大大减小，整体计算成本就不用以平方速度增长。  \n\n虽然理论上计算量的成本下来了，不过实操起来还是会遇到几个问题：  \n\n- 训练的时候，在MoE结构下，每个expert的batch size比整个模型的batch size小了。  \n比如模型的batch size是32，一共有16个expert，那实际上一次迭代平均每个expert只能分到2个训练样本。而batch size对训练效率影响是很大的，大的batch size摊小了参数传输和更新的成本。如果直接增大模型的batch size，又会受显存和通讯效率的限制。  \n- 训练数据量不足。  \n要训大模型就需要大量的数据，让模型参数充分学习。在当时的背景下，大规模的NLP数据是比较缺的。当然如今数据集多了很多，特别是预训练数据，这个问题现在来看没有那么突出了。  \n- 损失函数的设计。  \n如何使用合适的损失函数来训练模型，提升效果，并且使得模型的负载比较均衡，这是一个不容易解决的问题。  \n- 集群通讯问题。  \n一个GPU集群的计算能力可能比设备间网络带宽的总和高出数千倍，因此设备间的通讯很可能成为训练效率的瓶颈。为了计算效率，就要使得设备内计算量和所需的通讯量的比值，达到相应的比例。  \n- GPU计算特点。  \nGPU做数学计算很快，但是并不擅长做branching（if/else），因此MoE的工作基本上都是用gating network来控制参数的激活。这个严格来说不算是新的挑战了，应该说是根据计算设备沿用下来的设计。  \n\n要解决好这些问题，才能训出比较好的模型来。  \n\n## 模型设计\n\n1. 整体结构  \n\n先看下模型结构的设计。  \n\n论文里使用的是两个LSTM层，中间夹着一个MoE层，最上面和最下面分别还有一个embedding层和一个任务输出层，结构如下图所示  \n\n{% asset_img rnn_moe.png LSTM MoE %}  \n\n每个expert是一个简单的feed-forward neural network。一共有n个expert，gating network输出是一个稀疏的n维向量  \n\n$$\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}$$  \n\n$E_{i}(x)$ 是第 $i$ 个expert的输出，$G(x)_{i}$ 是gating network给出的第 $i$ 个expert的权重。  \n\n如果 $G(x)_{i}$ 为0，就不用计算对应的那个expert了，节省了计算。  \n\n如果expert的数量特别多，可以用two-level hierarchical MoE，即使用两层gating network，第一层的gating network先选择一个包含一批expert的分支，每个分支又有一个单独的gating network来选择具体的expert。类似word2vec训练所用的hierarchical softmax。这样做可以节省一些计算。  \n\n2. gating network  \n\n那具体gating network怎么设计呢？  \n\n如果对输入进行线性变换，再简单加上一个softmax，那得到的是一个非稀疏的gating function  \n\n$$\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot W_g)\\end{aligned}$$  \n\n在这个基础上，使用一个topk函数，只保留最大的k个值，其他都设为﹣∞（softmax之后变成0），这样就能只选择部分expert，得到了稀疏性。  \n\n论文提到，虽然理论上这个形式的sparsity（topk）会造成gating function的不连续，不过在实操中暂时没有遇到相关问题。  \n\n在这个基础上，在输入再加上一个Gaussian noise，这个noise的大小由另外一个可学习的参数来控制。整体的计算公式如下  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$KeepTopK(v,k)_i=\\begin{cases}v_i&\\text{if }v_i\\text{ is in the top }k\\text{ elements of }v.\\\\-\\infty&\\text{otherwise.}\\end{cases}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\n其中用来调整noise的非线性函数softplus是个类似ReLU的激活函数，但是更为光滑，函数图像如下  \n\n{% asset_img softplus.png softplus %}  \n\n这里添加噪声的原因和负载均衡有关，下面来分析下负载均衡。  \n\n## 负载均衡  \n\n在MoE模型训练的实验中观察到，如果不对gating network进行干预，任由模型自由学习，那么最终模型会倾向于收敛到“总是选那几个固定的expert”的状态，而其他expert几乎不会被使用。这就是负载不均衡的状态，如果这些专家分布在不同的计算设备上，结果就是有些设备输入排队特别长，而有些设备基本处于闲置状态，这明显不是我们想要的。  \n\n这种负载不均衡的状态有自我加强的属性，因为一旦开始出现部分专家被较多选中激活，这些专家就会得到更充分的训练，从而获得更好的效果，进而又提升被选中激活的概率。  \n\n针对这种情况，之前有一些工作使用hard constraint来缓解，比如当某个expert激活次数达到上限，就把它从候选集合中移除。hard constraint明显会对模型效果有影响。而这篇论文使用的是一种soft constraint。  \n\n具体来说，对于每个expert，定义了一个它在当前这批输入数据里的重要性指标，如以下公式所示  \n\n$$Importance(X)=\\sum_{x\\in X}G(x)$$  \n\n$G(x)$ 是gating network给出的权重，是一个维度等于expert数量的向量。  \n\n基于这个重要性指标，论文定义了一个辅助损失 $L_{importance}$，训练时和模型的交叉熵损失加到一起。$L_{importance}$ 的计算方式如下  \n\n$$L_{importance}(X)=w_{importance}\\cdot CV(Importance(X))^2$$  \n\n其中权重 $w_{importance}$ 是手动设置的超参，实验的推荐值是0.1，CV是coefficient of variation。  \n\ncoefficient of variation离散系数，是概率分布离散程度的一个归一化量度，定义为标准差 $\\sigma$ 和 均值 $\\mu$ 的比值。  \n\n对于MoE来说，确定激活的expert数之后，均值是固定的。如果expert的gating很不平衡，标准差就会很大，离散系数也会很大，使得 $L_{importance}$ 变大。  \n\n但是这里还是有问题，虽然均衡的负载可以推导出 $L_{importance}$ 较小的结论，但是 $L_{importance}$ 较小却不能保证负载均衡。也就是说 $L_{importance}$ 较小只是负载均衡一个必要不充分条件。  \n\n比如一个expert可能以很高的权重被分配到一个样本，而另一个expert可能以不太高的权重被分配到好几个样本。这种情况下对所有输入数据的gating权重进行求和，仍然可能呈现出均匀的表象（离散系数比较小），但这并不符合我们的要求。  \n\n为了解决这个问题，需要额外再加上一个损失 $L_{load}$ 。这里就要用到添加在每个expert输出上的随机噪音了。  \n\n我们想要各个expert的负载均衡，也就是每个专家需要处理的样本数基本一致，但是分配到各个专家的样本数是个离散值，因此没有办法直接用于back propagation，而 $L_{load}$ 就是对各个expert负载的一个平滑评估。  \n\n回想一下前面在设计MoE的时候，定义了 $H(x)$ 为KeepTopK函数的输入  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\n那么这里先定义一个 $kth\\_excluding(H(x),k,i)$，表示在除去 $H(x)$ 中的第 $i$ 个分量之后，排在第 $k$ 大的值。基于这个，再定义 $P(x,i)$ 为：固定其他分量已经选取好的noise，重新给第 $i$ 个分量再添加一次noise，结果比 $kth\\_excluding(H(x),k,i)$ 大的概率，公式如下  \n\n$$\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\\\>kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}$$  \n\n通过这个noise，我们把“第 $i$ 个专家是否处理这个输入”的离散值，变成“第 $i$ 个专家处理这个输入的概率”这样一个平滑的估计，$P(x,i)$ 就表示这个概率。这个概率可以简化写成  \n\n$$\\begin{aligned}P(x,i)&=\\Phi\\Big(\\frac{(x\\cdot W_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot W_{noise})_i)}\\Big)\\end{aligned}$$  \n\n其中 $\\Phi$ 是标准正态分布的CDF。  \n\n接下来就可以把第 $i$ 个expert的负载定义为  \n\n$$\\begin{aligned}Load(X)_i=\\sum_{x\\in X}P(x,i)\\end{aligned}$$  \n\n有了每个expert的负载衡量，就可以和前面第一个负载均衡损失一样，计算新的负载均衡损失了  \n\n$$L_{load}(X)=w_{load}\\cdot CV(Load(X))^2$$  \n\n$w_{load}$ 是手动设置的超参，实验的推荐值是0.1。  \n\n相比前面的 $L_{importance}(X)$，$Load(X)$ 是对负载是否均衡更细粒度的评估。  \n\n论文中提到一个细节，在刚开始训练的时候，希望模型分配的expert尽量均衡，因此把 $W_g$ 和  $W_{noise}$ 都设为0，这样相当于没有信号，也没有噪音。  \n\n最终使用负载均衡之后的效果如下  \n\n{% asset_img rnn_moe_load_function.png 负载平衡效果 %}  \n\n使用这两个负载均衡损失之后，能达到接近完全平均分配的效果。  \n\n## 实验  \n\n1. 解决工程问题  \n\n针对前面提出的一些工程问题，论文给出一些方案  \n\n（1）batch size减小  \n\n由于稀疏激活的原因，每个expert的batch size会变小。假设每次在n个expert中选择k个，模型训练的batch size为b，那么每个expert的batch size就是kb/n。论文通过以下这几种方法来提升每个expert的batch size：  \n- 混合使用数据并行和模型并行。本来在使用数据并行的情况下，每个模型副本是异步处理各自的数据的。而这里做了优化，各个副本的batch是同步处理的，这样就可以把多个模型副本的batch组合起来。对于非MoE部分的参数，依然使用标准的数据并行机制；而对于每个expert，则在整个集群中只保留一个副本。如果模型分布在d个设备上，那每个expert就能得到一个kbd/n的batch size。\n- 对于LSTM模型，在时间步上展开，就能把batch size提升相应的倍数。\n\n（2）集群通讯问题  \n\n另一个挑战就是平衡集群计算量和通讯量的关系。  \n\n对于每个expert来说，主要的通讯就是input和output的传输。而每个专家的主要计算量就是两个全连接层，大小分别为[input_size, hidden_size]和[hidden_size, output_size]。对于GPU来说，计算速度可能是通讯速度的1000倍，那我们就需要把计算量设计得足够大。最简单的做法就是把hidden_size提高，使得每个expert的内部计算量比通讯量大1000倍，以保证通讯不会成为训练的瓶颈。  \n\n2. 模型容量 & 参数效率  \n\n为了验证模型容量提升带来的收益，以及MoE模型的参数效率（即和dense模型同样推理计算量下能达到的效果），训练了包含4/32/256个expert的flat MoE模型，和包含256/1024/4096个expert的hierarchical MoE模型。每个expert大约是1M参数量，对于所有flat模型都是激活4个expert，而对于hierarchical MoE是每层gating激活2个。  \n\n效果如下图。左边的图显示，随着模型容量提升，测试的ppl有明显下降。右边的图将相近模型容量的dense模型和MoE模型的效果放在一起对比，可以看到MoE模型在相同模型容量下，效果更好\n\n{% asset_img rnn_moe_perf.png 效果 %}  \n\n3. 更大的模型  \n\n前面几个模型训练用的数据量不是很大，模型最大也只有4B左右，训练不久就出现diminishing returns。  \n\n为了验证更大数据集 + 更大模型的收益，在100B token的语料上，分别训了包含32, 256, 1024，4096, 16384, 65536, 和131072个expert的MoE模型，最大的模型达到了137B的参数量。  \n\n各个模型对比如下表。整体来看，增加数据和模型容量，是可以继续获得提升的。  \n\n{% asset_img rnn_moe_137b.png 137模型效果 %}  \n\n从这里还可以看出，在专家数量不太多时，提升专家数量效果有提升，但是收益会慢慢减小，甚至会出现专家数量太多，效果反而下降的情况。  \n\n4. Expert Specialization  \n\n按照MoE的设计思路，不同的专家应该学习到不同的子任务，但是实际上是否是这样呢？\n\n论文里把模型中不同的专家分配到token拿出看，发现确实有比较强的specialization效果，不同的专家处理不同的内容，如下所示  \n\n{% asset_img rnn_moe_specilized.png RNN MoE 专门化 %}  \n\n# GShard\n\n1. 简介\n\n2018年，随着Bert的发布，transformer结构彻底火了起来。2020年6月，Google发布《GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding》，把MoE用到了encoder-decoder结构的transformer模型上。MoE开始变成我们现在熟悉的样子了。  \n\nGShard这个工作做了很多的实验，训了很多规模巨大的MoE模型，最大的达到了600B。训练的一系列模型的参数如下表  \n\n{% asset_img gshard_moe_family.png GShard MoE family %}  \n\n在expert数量的设计上，延续上面LSMT MoE工作的思路 -- expert越多，效果越好。（站在24年这个时间节点来看，太多的expert未必适合；但是也不能说这个思路一定错误，毕竟事物的发展是螺旋式的，就像ChatGPT出来之前大多数人都在魔改各种Bert，而GPT已经坐了几年冷板凳了。）  \n\nGShard论文中很大的篇幅在介绍工程实现和优化，这也是MoE模型训练最大的痛点。关于工程框架的内容比较硬核，因此这里不会展开讲太多，而是关注在模型算法层面上。  \n\n2. 模型设计\n\n先来看下模型设计。  \n\nGoogle在那段时间走的是encoder-decoder transfomer的技术路线，因此GShard也是基于encoder-decoder transfomer的模型结构。  \n\nGShard的模型设计是，在encoder和decoder中，每两层把其中一个FFN层替换成MoE层。对于总共有N层的模型，则有N/2个MoE层，如下图  \n\n{% asset_img gshard_model.png GShard模型结构 %}  \n\n每层会选择最多top-2 expert来激活。为什么是最多，后面解释。  \n\nGShard在上面这篇LSTM MoE论文的基础上，改进了gating function和auxiliary loss function。  \n\n从公式来看，MoE层的具体计算如下\n\n$$\\begin{aligned}\n\\mathcal{G}_{s,E}& =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)& =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}& =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s) \n\\end{aligned}$$\n\n其中 $x_s$ 是MoE的输入token，$w_i$ 和 $w_o$ 分别是输入输出的线性变换矩阵。向量$\\mathcal{G}_{s}$ 就是gating function的输出。\n\nGShard在gating function的设计上提出了两个要求：（1）负载均衡（2）高效扩展。  \n\n负载均衡和前面讲的一样，很好理解。而为什么要高效扩展，因为如果要对N个token分别进行E个expert的分配，在N能达到百万甚至千万级别，而E也有几百上千的情况下，就需要一个高效的分布式实现，以免其他计算资源等待gating function。  \n\n为了满足这些要求，gating function提出了以下机制  \n\n（1）专家容量 expert capacity  \n\n为了确保负载平衡，我们不希望有少量expert需要处理很多token，因此强制规定了每一个expert所负责处理的token数量有一个最大值，这个最大值就叫专家容量，在这里设置为2N/E，相当于平均分配的量。  \n\n这个expert capacity通过GATE(·)给每个expert维护一个计数器 $c_e$ 来监控。如果一个token所选的两个专家当前处理量都已经超过设定的专家容量，那么这个token就不会被当前层的任何expert处理，而是直接通过残差链接透传到下一层。  \n\n（2）分组分配 Local group dispatching  \n\n给所有输入token分成了G组，不同的组并行处理，每个组相应地也把组内专家容量变成2N/EG。  \n\n这样做相当于在前向推理时，把大的batch拆分成小的batch，每个小的batch就是一个group。这样做的好处是通讯的时候（特别是all2all）只需要在每个group内进行就可以了，减少了通讯量。  \n\n而进行反向计算的时候这些group可以合起来一起用，相当于进行了gradient accumulation。  \n\n（3）辅助损失函数 Auxiliary loss  \n\n光设置专家容量并不能使得gating负载均衡，而且会导致大量溢出。参考前面LSTM MoE的工作，这里也定义了一个辅助损失函数，来帮助负载均衡。辅助损失函数设计如下  \n\n$$\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot m_e$$  \n\n$S$ 是token数，$E$ 是专家数，$c_e$ 是分配给第 $e$ 个专家的token数，$m_e$ 是第 $e$ 个expert在 $S$ 个token中获得的平均权重。  \n\n思路是，本来是要算 $\\frac{c_e}S$ 的平方的，但这是离散值不可导，因此把平方中的一个 $\\frac{c_e}S$ 换成了 $m_e$ ， $m_e$ 是第 $e$ 个expert在 $S$ 个token中获得的平均权重。在平均分配的情况下，这个loss达到最小。  \n\n相比前面的负载均衡损失，这个loss的设计就简单许多。  \n\ngating的整个算法如下  \n\n{% asset_img gshard_algo_1.png GShard gating 算法 %}  \n\n（4）随机路由 Random routing  \n\n前面提到，每层会选择最多top-2 expert来激活，就是因为有随机路由的机制。直观来说，就是认为如果top-1专家的权重很高，而第二个专家的权重如果较小，那很有可能只用第一个专家就足够解决问题了。  \n\n随机路由的机制是top-1的专家永远会被激活，而第二个专家如果权重很小，就认为它可以被忽略。具体来说，会以与第二个专家的权重g2成比例的概率激活第二个专家。  \n\n3. 效果  \n\n最后看一下模型在翻译任务上的效果\n\n{% asset_img gshard_perf.png GShard效果 %}  \n\n# Switch Transformer\n\n2022年4月，距离ChatGPT发布还有半年，Google发布了《Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity》（实际上2021年Google就提出Switch Transformer了）。  \n\nSwitch Transformer和GShard一样，是encoder-decoder结构，基于T5开发的，具有1.6T的参数，2048个expert。  \n\n和前面的很多工作一样，Switch Transformer有一个出发点，那就是参数量越大，模型效果越好，并且可以通过稀疏激活来减少总计算量。  \n\n但是相比其他工作，Switch Transformer给出了一个更为具体的描述，那就是模型参数量可以是一个独立于总计算量的，单独的缩放轴。也就是说，在改变参数量的同时，（几乎）不改变训练和推理的计算量，就可以带来效果的提升。因此Switch Transformer关注在“同样的FLOPS/token的计算量”下，如何扩大模型，提升效果。  \n\nSwitch Transformer所做的工作还是比较多的，包括：  \n\n（1）模型结构简化：简化了Transformer上的MoE架构，提出Switch Transformer架构。  \n\n（2）MoE to dense：把训出来的效果较好的MoE模型蒸馏到dense模型，在压缩MoE模型99%的参数的情况下，效果还是比直接训练dense模型好。  \n\n（3）训练和微调技术：  \n- 首次使用bf16成功训练MoE模型  \n- 更适合MoE结构的模型初始化  \n- 增加的专家正则化，改善了稀疏模型的微调和多任务训练  \n\n（4）训练框架：结合数据、模型和专家并行性，训练了超过1T参数的MoE模型。  \n\n（5）多语言：在多语言数据集上训练，发现101种语言效果普遍有提升。  \n\n（6）训练效率：在同样的FLOPS/token的计算量下，Switch Transformer模型收敛速度有数倍的提升。  \n\n## 模型设计  \n\nSwitch Transformer的模型结构如下图，类似GShard，把transformer每层的FFN替换成MoE层  \n\n{% asset_img switch_transformer_structure.png Switch Transformer 模型结构 %}  \n\nSwitch Transformer一个重要的改进点就是简化了gating function的做法（Switch Transformer论文里叫routing）。  \n\n之前的工作大多探索了选择k个expert的做法，而Switch Transformer则直接把gating简化为只选择1个expert，即k=1。这样的MoE层叫做Switch layer。  \n\n这样简化之后，routing的实现更简单，router的计算量小了，也减少了通讯量。  \n\n## 负载均衡  \n\n同GShard一样，Switch Transformer规定了一个专家容量expert capacity，来限制每个expert在一个batch里能处理的最大token数。  \n\n如果一个token被分配到了一个已经满载的expert，就会出现overflow，那这个token在本层就不会被处理，而是直接通过残差链接，透传给下一层。这点也同GShard一样。  \n\n在Switch Transformer，专家容量通过容量系数capacity factor来控制。  \n\n$$\\text{expert capacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of experts}}\\right)\\times\\text{capacity factor}.$$  \n\n一个大的capacity factor意味着每个expert能够处理更多的token，从而减少overflow情况的发生，但是计算量和通讯量的压力也会增大，所以这是一个需要权衡的参数。\n\n下图给出了一个不同capacity factor下的例子  \n\n{% asset_img switch_transformer_diff_expert_capacity.png 不同的expert capacity %}  \n\n那么如何设定expert capacity呢？\n\n如果capacity factor为1的话，只有在完全平均分配的时候，才不会出现overflow的情况。而太大的capacity factor则可能造成算力和存储的浪费。  \n\n首先，实验中发现expert的数量和overflow的数量之间没有什么关系，所以在所有实验中，所有MoE和Switch Transformer模型都用128个专家。  \n\n不同的capacity factor对模型影响如下表。可以看到，大的容量系数相对来说能取得更好的效果（因为更少的overflow），但是相应地，大容量系数的模型处理速度就会慢一些。  \n\n{% asset_img switch_transformer_capacity_effect.png expert capacity的效果 %}  \n\n经验上，低的token丢弃率对模型的scaling很重要，想要训练超大规模的模型，就要解决这个问题。而通过负载均衡损失就可以确保良好的平衡，使得在使用较小容量系数的情况下，overflow尽量少，从而兼顾效果和计算速度。  \n\n关键问题来到负载均衡损失怎么设计。  \n\n给定 $N$ 个expert，和包含 $T$ 个token的batch $\\mathcal{B}$，负载均衡损失是这么计算的 \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$f_{i}$ 表示被分配到第 $i$ 个expert的token数，这个不可导  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$P_i$ 表示整个batch每个token分配给第$i$ 个expert的概率的总和，这个可导  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\n这个损失的设计其实和GShard中的也是一样的。  \n\n在完美平均分配的情况下，$f$ 和 $P$ 这两个向量都是 $1/N$，这个时候负载均衡损失是最小的。  \n\n$\\alpha$ 扫描了1e-5到1e-1，发现设为1e-2，已经足够大保持负载平衡，同时不过分影响模型收敛。  \n\n观察到 $\\sum_{i=1}^N(f_i\\cdot P_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N$，所以负载均衡loss还乘了个 $N$，这样可以保持无论使用多少个expert，在平均分配的情况下，loss都能保持相同的常数。  \n\n## 实验\n\n1. 一些训练的trick  \n\n（1）选择性地使用bf16  \n\n半精度训练会带来一些训练的不稳定。因此选择性地使用bf16，具体来说，routing function内部使用单精度，其他部分使用半精度，这样既不影响通讯，也能提高效果。  \n\n为什么选择在routing提高精度？因为softmax对误差特别敏感，exponential计算会极大放大输入中的rounding error，因此高精度对routing很重要。  \n\n（2）较小的参数初始化  \n\n从截断正态分布中抽取元素来初始化的模型参数，平均值 $\\mu=0$，标准差$\\sigma=\\sqrt{s}/n$，其中s是超参，n是权重张量中的输入单元数量（e.g. fan-in）。  \n\n论文建议将默认的Transformer初始化尺度s=1.0减少10倍。这个方案在实验中既提高了质量又降低了训练不稳定性的可能性。初始化实验对比如下表  \n\n{% asset_img switch_transformer_init.png 初始化对比 %}  \n\n（3）增大dropout  \n\n由于Switch Transformer参数量很大，在微调的时候更容易过拟合，因此一个简单的方法就是增大dropout，效果如下  \n\n{% asset_img switch_transformer_dropout.png dropout效果 %}  \n\n可以看到大的dropout有效果，并且dense层保持0.1，只有expert层增大dropout效果更好。  \n\n2. scaling  \n\n对Switch Transformer结构预训练的scaling做了一些实验。  \n\n（1）Step-Basis  \n\n首先是验证在固定训练step的条件下，增大expert数量带来的提升，如下图所示。  \n\n左边是不同规模的模型在相同step下收敛的结果，可以看到在保持相同计算量的条件下，只通过增大专家数量来提升规模，就有明显的收益。右边则展示训练过程中，不同规模的模型在各个step下的效果。  \n\n{% asset_img switch_transformer_scaling_step.png step scaling %}  \n\n（2）Time-Basis  \n\n虽然Switch Transformer可以保持计算量不变的情况下提升模型规模，但是专家数量的增多会带来额外的通讯成本，所以即使训练的step数相同，实际的训练时间也不同。因此这里要回答的问题是，给定一个固定的训练时长，Switch Transformer是否相比dense模型仍有收益。  \n\n答案是肯定的。下图展示以训练时长为横轴，Switch Transformer和dense模型的效果对比。Switch Transformer收敛到dense模型最终效果的时间只有dense模型的1/7。  \n\n{% asset_img switch_transformer_scaling_time.png time scaling %}  \n\n（3）和更大的dense模型对比\n\n前面Switch Transformer和dense模型的比较，是基于相同计算量的前提。那么Switch Transformer是否具备超越更大规模dense模型的能力？  \n\n下图在Step-Basis和Time-Basis对比了64个专家的Switch Transformer和T5-Large。无论是相同step还是相同时间下，Switch Transformer都有明显优势。  \n\n{% asset_img switch_transformer_scaling_dense.png dense对比 %}  \n\n3. SFT效果对比  \n\n在GLUE和SuperGLUE等下游任务上微调，和dense模型对比。  \n\n对于各个模型，每两百步进行一次eval，选最好的效果，尽量保证公平。结果如下表，大部分任务都有明显的提升。  \n\n{% asset_img switch_transformer_sft_result.png sft对比 %}  \n\n4. 模型蒸馏  \n\n虽然Switch Transformer在相同计算量下效果更好，但是部署几百B甚至T级别的模型，还是不太方便，因此考虑把稀疏模型蒸馏到dense模型上来进行推理。  \n\n论文中给出了几个蒸馏的技巧：  \n- 初始化的时候，把Switch Transformer模型中的非稀疏部分用于初始化dense模型  \n- 蒸馏所用的label，25%来自教师模型，75%来自ground truth，加权求和  \n\n预训练模型的蒸馏效果如下，相比无蒸馏训练的dense模型，把同样计算量的稀疏模型蒸馏到dense模型，dense模型大约能获得Switch Transformer提升部分30%的增益。  \n\n{% asset_img switch_transformer_distill.png 蒸馏 %}  \n\n更进一步，用不同规模的稀疏模型下进行蒸馏，结果如下表，可以实现高达99%的压缩率  \n\n{% asset_img switch_transformer_distill_diff_model.png 蒸馏 %}  \n\n除了预训练模型，微调模型也可以蒸馏，效果如下，在SuperGLUE也有一定的提升  \n\n{% asset_img switch_transformer_distill_sft.png sft蒸馏 %}  \n\n# GLaM\n\n1. 简介\n\n2021年12月Google发表了《GLaM: Efficient Scaling of Language Models with Mixture-of-Experts》，训练出最大参数量为1.2T，每层包含64个专家，每个token激活参数量为96.6B的MoE模型。  \n\n相比Switch Transformer，GLaM的训练数据量要大得多，达到了1.6T token。  \n\n下表是论文中给出的，当时一些大规模模型的对比  \n\n{% asset_img glam_related_model.png glam和相关模型 %}  \n\n虽然模型总参数量比GPT-3（175B）大很多，但是训练成本却比GPT-3低很多，推理速度也更快，而且在多个NLP任务上的效果都超越了GPT-3，如下所示。  \n\n{% asset_img glam_compare_gpt3.png glam和gpt3对比 %}  \n\n{% asset_img glam_compare_gpt3_2.png glam和gpt3对比 %}  \n\n2. 模型设计\n\n模型设计上，和Switch Transformer一样，每两层把一个FFN替换成MoE层。但是和Switch Transformer不同，GLaM用回了每次激活两个expert的方案，模型结构如下图。  \n\n{% asset_img glam_model.png glam模型 %}  \n\n除此之外，模型在结构上海做了一些其他改动：  \n\n（1）位置编码  \n\n使用XLNET的相对位置编码。  \n\n（2）激活函数\n\n> In the non-MoE Transformer feed-forward sub-layers, we replace the first linear projection and the activation function with the Gated Linear Unit，which computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit.  \n\n3. 实验\n\n训练中的一些trick：  \n\n（1）参考《Lingvo: a modular and scalable framework for sequence-to-sequence modeling》，在梯度出现NaN或者Inf的时候就跳过那一步更新。  \n\n（2）如果在BP更新的时候遇到NaN或者Inf，则重新加载更早的checkpoint并跳过有问题的数据来避免NaN或者Inf。  \n\n论文训了一系列模型来探索MoE，这些模型的设置如下表  \n\n{% asset_img glam_family.png glam模型系列 %}  \n\nGLaM和dense模型的评测结果如下  \n\n{% asset_img glam_perf.png glam模型效果 %}  \n\n可以看到GLaM MoE的有效参数效率一致高于dense模型。  \n\n# ST-MoE  \n\n2022年2月，Google发表了《ST-MOE: DESIGNING STABLE AND TRANSFERABLE SPARSE EXPERT MODELS》。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，可以说是MoE的必读论文。  \n\nST-MoE最大模型包含269B总参数量，和与32B dense模型相当的激活计算量。论文中把模型称为称为Stable Transferable Mixture-of-Experts，或者ST-MoE-32B。  \n\n在MoE层的使用上，ST-MoE比Switch Transformer更“节省”一点，每四层才替换1个MoE层。  \n\n论文中主要训了两个规模的ST-MoE模型，分别有4B和269B的总参数量。ST-MoE以及其他用于对比的模型参数如下表  \n\n{% asset_img st_moe_models.png ST-MoE模型及对比模型的参数 %}  \n\n## 稳定性与效果分析  \n\n论文通过对乘性操作、噪音和裁剪这几个内容进行探索，来指导模型的设计。  \n\n1. 乘性操作对模型稳定性和效果的影响  \n\n论文首先研究了乘性操作对模型的训练稳定性和最终效果的影响。  \n\n之前已经有一些工作表明更多的乘法对模型效果有收益。  \n\n> Some architectural improvements involve more multiplications than additions or do not sum many items at once\n\n（1）GELU Gated Linear Units (GEGLU)  \n\n第一个例子是关于激活函数的。GLU是一个对两个输入向量进行component-wise相乘的操作，之后被扩展成GELU-Linear FFN变体，用于替换transformer中的ReLU FFN变体，其计算如下  \n\n$$\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}$$  \n\n这样在一些其他工作里已经被证明了对模型效果有提升。  \n\n（2）RMSNorm  \n\n第二个例子是RMSNorm中的缩放参数，也就是下面公式的 $g$。  \n\n$$y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot g_i$$  \n\nST-MoE针对GEGLU和RMSNorm这两个乘性操作，做了实验，结果如下表。  \n\n{% asset_img st_moe_remove_multiplications.png 移除乘法操作的影响 %}  \n\n发现移除乘性操作可以使模型稳定性更好（训练中发散的情况减少），但是最终效果变差了。  \n\n（3）增加dense层  \n\nST-MoE还验证了在expert层增加更多dense层的效果。结果发现增加更多的乘法交互（增加dense层），可以在带来效果收益的同时，基本不影响推理速度，如下表所示。\n\n{% asset_img st_moe_more_dense_layer.png 更多的dense层 %}  \n\n（4）增加一个bias\n\n在FFN层的第一个矩阵乘法后面增加一个可学习的bias B，分别通过加法和乘法加入  \n\n$$\\text{FFN}_{\\text{GEGLU}}+\\text{Add Bias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2$$  \n\n$$\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot xW_{12})\\odot B]W_2$$  \n\n乘法的收敛速度更快，效果也更好。  \n\n上面这些实验显示，后续在模型效果的探索方向可以往多使用乘性操作去考虑。  \n\n2. noise对模型稳定性和效果的影响  \n\n接下来ST-MoE探索了“噪音可以提升模型稳定性”的假设。  \n\n通过input-jitter，给router的输入logits乘以一个在[1e-2, 1e2]之间的均匀随机变量来添加噪音。  \n\n{% asset_img st_moe_more_add_noise.png 增加noise %}  \n\n结果是增加noise之后，有助于让模型的收敛更加稳定，但是对模型最终效果有负面影响。  \n\n这里论文还提到，小模型上的结果不一定能直接推广到更大的模型上，比如在小模型上稳定的配置，在大模型就可能就不稳定了。因此还是需要在大模型上也进行充分实验。  \n\n3. 限制激活值和梯度值对模型稳定性和效果的影响  \n\n对activation和gradient进行限制是目前广泛应用的提升模型训练稳定性的手段。在反向传播过程中，通过裁剪梯度的范数来缓解梯度爆炸，就是一种常用的限制手段。  \n\n但是在ST-MoE训练269B的大规模模型时，发现裁剪会使得模型收敛的效果很差。  \n\n为了解决这个问题，ST-MoE在训练中引入了router z-loss，形式如下。  \n\n$$L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2$$  \n\n$B$ 是token的数量，$N$ 是专家数，$x\\in\\mathcal{R}^{B\\times N}$ 是router的输入。  \n\nz-loss会对进入router的较大的logits值进行惩罚，以达到尽量减少进入指数函数的较大误差的目的。什么意思呢？后面来解释，先看下使用z-loss的效果。  \n\n{% asset_img st_moe_z_loss_result.png z-loss效果 %}  \n\nST-MoE认为，在模型训练过程中，由于精度不足或者其他问题，会产生很大的值，从而引入误差。而对梯度进行裁剪是在误差发生之后，并且裁剪本身也造成了数据的不连续性，某种程度上，裁剪本身也是一种误差。相反地，z-loss自然地鼓励模型产生较小的对数值，因此可以更精确地建模。  \n\nz-loss乘以一个权重超参 $c_z$ 加入到模型训练的总损失中，如下式所示。  \n\n$$L_{tot}=L_{CE}+c_BL_B+c_zL_Z$$  \n\nST-MoE经过实验，选择了$c_z=0.001$。  \n\n$L_B$ 是 auxiliary load balance loss负载均衡损失，ST-MoE这里使用了和GShard/Switch Transformer所用的相同的损失计算，这里回顾一下：  \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\n$N$ 是专家数， $\\mathcal{B}$是包含 $T$ 个token的batch。$f_{i}$ 表示被分配到第 $i$ 个expert的token数，这个不可导；$P_i$ 表示整个batch每个token分配给第$i$ 个expert的概率的总和，这个可导。  \n\n4. 数据精度对训练效率和训练效果的影响\n\n目前大部分的大模型训练都使用混合精度训练：模型权重以float32格式存储以进行梯度更新，然后在正向和反向传播的矩阵乘法中转换为bfloat16；此外，所有激活值都以bfloat16存储和操作，而allreduce通信可以在bfloat16或float32数值精度中进行。  \n\n对于ST-MoE-32B的训练，allreduce的数值使用半精度可以加速训练，然而这也会使训练变得不稳定，因此ST-MoE保持allreduce的数值精度为float32。  \n\nbfloat16和float32在不同范围的舍入误差如下表所示  \n\n{% asset_img st_moe_round_error.png bf16精度损失 %}  \n\n可以看到，表达的数值越大，舍入误差越大。而z-loss限制了数值大小，也就将误差值限制在比较小的范围。  \n\nMoE模型天生对舍入误差敏感，因为它们由于router的使用而有更多的指数函数，而指数函数会将小的输入误差放大很多，这就加剧舍入误差所导致的训练不稳定。  \n\n另外，ST-MoE有一个策略：只有当排第二的专家的权重大于等于第一的专家的1/5时，token才会被路由到其第二位专家，否则第二个专家就会被忽略。  \n\n因此虽然舍入误差不会改变softmax运算中各个概率的排序，但它确实会影响MoE中第二个专家的激活。  \n\n## 模型设计\n\ndense模型的设计有scaling law进行指导，但是MoE模型的设计比dense模型多出几个要考虑的点： \n \n（1）使用多少个expert  \n\n（2）怎么routing  \n\n（3）专家容量系数怎么定  \n\n（4）硬件的影响  \n\n（这里提到MoE模型的scaling law工作：《Unified scaling laws for routed language models》，可以了解一下）  \n\n1. 使用多少个expert  \n\nST-MoE认为，从以往的经验来看，在总专家数量较少的情况下（如8/16/32），提升专家数量，能有收益。但是在特别稀疏的情况下（如激活专家数量<1%），或者总专家数较大（比如>256）之后，提升专家数量收益就很小了。  \n\n从另一个角度来看，如果一个计算核心使用>1个专家，那么就会出现比较大的加载参数张量的成本，因此建议每个计算核心使用<=1个专家。  \n\n2. routing和capacity factor  \n\n论文做了一系列实验来探索capacity factor的选择，如下表所示  \n\n{% asset_img st_moe_capacity_factor.png capacity factor %}  \n\n从这些实验中得到几个结论：  \n\n（1）训练和推理的capacity factor增大都会有收益  \n\n（2）如果硬件资源足够，推理的capacity facotr可以设得比训练的时候大，会有进一步提升  \n \n（3）激活的expert数量提升会有收益，但是收益随着capacity factor提升而越来越小  \n\n当然，选择capacity factor还要看硬件的特性，如果通讯很快，可以适当增大capacity factor，否则就不能选择太大的。  \n\n下表展示了不同capacity factor对推理速度的影响  \n\n{% asset_img st_moe_capacity_factor_speed.png 不同capacity factor推理速度 %}  \n\n## 实验  \n\n1. ST-MoE效果  \n\nST-MoE-32B在下游任务上和以往最佳结果对比如下表，ST-MoE-32B刷新了超过一半任务的最佳效果  \n\n{% asset_img st_moe_perf.png 不同capacity ST-MoE-32B效果 %}  \n\n2. Expert Specialization  \n\n论文还对各个专家的专业化进行了追踪，发现decoder中几乎没有专业化的迹象，各种类型的token近乎随机分配给不同的专家。而在encoder中则表现出了高度专业化的特征，如下表  \n\n{% asset_img st_moe_encoder_specialization.png encoder专业化 %}  \n\n此外，还发现在多语言的模型的encoder中，专业化的情况并不想原先预想那样，按不同语言划分，而是每个专家都会处理一种语言的一部分token，如下表  \n\n{% asset_img st_moe_multiling_specialization.png 多语言专业化 %}  \n\n# DeepseekMoE\n\n2024年1月，幻方量化开源了DeepseekMoE，是国内首个开源的MoE大模型。幻方还发布了论文《DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models》，给出了一些DeepSeekMoE的细节内容，颇为实在了。  \n\nDeepSeekMoE在其他MoE工作的基础上，进一步给出了2个模型设计的主要思路：  \n\n（1）对expert的粒度进行细分，以提供更多样的expert激活组合；  \n\n（2）对expert的类型进行区分，从所有expert中保留一部分作为shared expert共享专家，这部分专家对所有输入都保持激活。  \n\n这样的做法可以帮助每个expert达到更高程度的专业化(specialization)的水平，更好地学习不同的专业知识。  \n\nDeepSeekMoE先在2B的较小MoE模型上进行了充分的实验，然后把方案应用到16B参数的MoE模型上，并获得了较好的效果。其中DeepSeekMoE-16B不需要量化就可以在40GB显存的设备上运行。  \n\nDeepSeekMoE-2B模型具有和稠密2B模型相当的性能，而DeepSeekMoE-16B则具有和7B稠密模型相当的性能，且计算量仅为稠密模型的40%。  \n\nDeepSeekMoE-16B的参数效率相比稠密模型有明显的优势，如下图所示  \n\n{% asset_img ds_moe_perf.png deepseek moe %}  \n\n并且DeepSeekMoE-2B和16B模型都开源了。  \n\n在前面实验的基础上，幻方还训练了DeepSeekMoE-145B的超大MoE模型，具有和稠密的DeepSeek-67B模型相当的表现，但计算量更小。这个后续也有机会放出来。  \n\n## 模型设计  \n\nMoE，mixture of expert，顾名思义，一个最初始的motivation就是让不同expert学习不同的内容，然后再混合起来。  \n\n比如最上面提到的1991年的工作里，就是让不同的expert学习不同的元音特征，以此提升特征提取的准确率。  \n\n但是当前大部分的MoE架构都会遇到“knowledge hybridity”和“knowledge redundancy”的问题，即知识的杂糅和冗余：  \n\n（1）知识冗余  \n\n有些基础的常识在不同的领域都需要用到，每个expert就都会学一点，这样这些常识就被多个expert重复学习了。  \n\n（2）知识杂糅  \n\n在expert数量不够多的情况下，一个expert就可能要负责学习多个领域的内容。以学习高中知识为例，在只有两个expert的时候，只能一个expert学习理科知识，另一个学习文科知识；当我们有8个expert的时候，不同expert就可以分别学习语文、英语、历史、地理、物理、生物、化学、数学知识。显然后者所学知识的专业化程度更高。  \n\n知识的杂糅和冗余阻碍了专家专业化(expert specialization)的程度，也就阻碍了模型达到MoE结构理论上限性能。  \n\n我们期望每个expert能够学习到non-overlap & foucusd knowledge的知识。  \n\n针对上面的问题，DeepSeekMoE的架构设计有2个主要策略：  \n\n（1）Fine-Grained Expert Segmentation  \n\n参数总量不变的情况下，将expert分成更细的粒度（每个expert更小）。这样可以带来更灵活的激活组合，让每个expert可以有更强的specialization。比如原本是16个expert选择激活2个，那么总的组合数是120种；如果把每个expert缩小为原来的1/4，那在总参数量和激活数量不变的情况下，是64个expert选择激活8个，那么总的排列组合数就是 $\\binom{64}8=4,426,165,368$ ，排列组合数比原来多了很多。   \n\n（2）Shared Expert Isolation  \n\n把部分expert分离出来，保持永远激活。我们期望这部分专家能够学到在多个领域间都通用的common knowledge。这样的策略同样可以使得其他expert能够提高专业化的程度，并且减少不同expert间的知识冗余。还是以学习高中知识为例，数学、物理和化学都需要算术能力，如果让学这三个领域的expert都学习算术技能，就会有冗余；我们可以把通用算术的技能剥离出来，由一个助手专门负责算术任务，相当于给他们发了一个计算器，这样学习数学、物理和化学的expert就能把更多的精力放在专业知识上，也就能达到更好的专业化效果。  \n\n下图展示了在传统MoE结构上增加Fine-Grained Expert Segmentation和Shared Expert Isolation策略的设计  \n\n{% asset_img ds_moe_structure.png deepseek moe 结构 %}  \n\n（expert isolation的思路最早可以追溯到2022年1月发表的《DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale》，这里就不展开了。）  \n\n假设传统的MoE模型每层的expert数量为 $N$，激活expert数为 $K$，DeepSeekMoE使用的细粒度expert大小为原来的 $1/m$，那DeepSeekMoE每层就有 $mN$ 个expert，激活的expert数量为 $mK$个。假设 $T$ 为输入长度，$L$ 为模型层数，$e_i^l$ 表示第 $i$ 个expert，DeepSeekMoE可以公式化为以下表示（忽略了layernorm）  \n\n$$\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}$$  \n\n$$\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{ FFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l$$  \n\n$$g_{i,t}=\\begin{cases}s_{i,t},&s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant j\\leqslant mN\\},mK)\\\\0,&\\text{otherwise,}\\end{cases}$$  \n\n$$s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)$$  \n\n## 负载均衡\n\n如之前工作反复提及的，如果任由MoE模型自主学习gating，可能会遇到两个问题  \n\n（1）routing collapse：专家分配的不均衡，也就是gating倾向于总是选择特定的少量expert，并且这种情况还会自我增强。  \n\n（2）计算效率问题：多设备间，不平衡的负载可能会成为计算效率的瓶颈。  \n\n针对routing collapse的问题，DeepSeekMoE引入一个expert-level balance loss，如下所示\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}& =\\alpha_1\\sum_{i=1}^{N'}f_iP_i\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nf_{i}& =\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i)\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nP_{i}& =\\frac1T\\sum_{t=1}^Ts_{i,t} \n\\end{aligned}$$  \n\n$\\alpha_1$ 叫做expert-level balance factor，是人工设定的超参。  \n\n而 $f_i$ 和 $P_i$ 和Switch Transformer里的设定基本一样。  \n\n在Switch Transformer里， $f_i$ 表示分配到第 $i$ 个expert的token数量。在DeepSeekMoE这里也是一样的含义，只是多乘了一个系数 $N'/K'$ ，其中 $N'=mN-K_s$，$K'=mK-K_s$，$K_s$ 是划分出来的共享expert的数量。这个系数是个常数，可以拿到求和符号外面，这样DeepSeekMoE里的 $f_i$ 就和Switch Transformer里的完全一样了。  \n\n$N'/K'$ 这个系数可以使得在使用不同的数量的expert时，在完美平均分配的情况下，负载均衡loss都是相同的常数。  \n\n$P_i$ 表示所有每个token分配给第 $i$ 个expert的权重的总和，和Switch Transformer里的含义一样。  \n\n注意这里 $f_i$ 是不可导的，$P_i$ 是可导的。  \n\n针对多设备间负载均衡的问题，DeepSeekMoE引入一个device-level balance loss，如下所示\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}& =\\alpha_2\\sum_{i=1}^Df_i'P_i'\n\\end{aligned}$$\n\n$$\\begin{aligned}\nf_i^{\\prime}& =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}$$\n\n$$\\begin{aligned}\nP_{i}^{\\prime}& =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}$$\n\n$\\alpha_2$ 叫做device-level balance factor，是人工设定的超参。  \n\n$\\mathcal{E}_i$ 指第 $i$ 个设备。\n\ndevice-level balance loss形式上和expert-level balance loss一样，只是 $f_i$ 和 $P_i$ 对应的对象从单个expert变成单个设备了。  \n\n当我们的目标是缓解计算瓶颈时，我们不需要强制执行expert间的均匀分配，而只需确保设备之间计算量的平衡。比如我们每层有64个expert，均匀分布在8个设备上，我们只需要每个设备处理的token数平衡即可，在设备内部即使所有token都是同一个expert处理的，依然能满足设备间负载平衡的要求。  \n\n相比expert间严格的负载平衡，只要求设备间的平衡是更松的限制条件，这样缓解了因为过度的负载平衡而损害模型性能的问题。\n\n## 实验\n\n1. 小规模模型验证\n\n为了验证以上策略的有效性，先拿100B token的语料数据在DeepSeekMoE-2B模型做实验。词表也是通过BPE在语料上训练的8k词表，后面训练更大规模模型的时候再扩大词表。\n\nDeepSeekMoE-2B模型参数初始化方差为0.006，使用multi-head attention，前向激活参数量约0.3B，具体参数如下表\n\n{% asset_img ds_model_param.png 模型超参 %}  \n\nrelative expert size指的是DeepSeekMoE所用的细粒度expert的大小和正常FFN层大小的比值。\n\n训练的具体参数设置如下  \n\n<center>\n\n| 属性 | 数值 |\n| :----: | :----: |\n| optimizer | AdamW |\n| adam_beta_1 | 0.9 |\n| adam_beta_2 | 0.95 |\n| adam_weight_decay | 0.1 |\n| warmup schedule | linear |\n| warmup step | 2000 |\n| max lr | 1.08e-3 |\n| dropout | 0 |\n| sequence length | 2k |\n| batch size | 2k |\n| total step | 25,000 |\n\n\n</center>  \n\n其他训练细节：  \n- 所有expert放在单个GPU上，没有使用device-level balance loss  \n- expert-level balance factor设为0.01  \n- 训练到80%的时候，学习率乘以0.316，训练到90%的时候，再乘以0.316  \n\n使用相同的100B训练数据，训了DeepSeekMoE-2B，在包含语言模型和下游任务的benchmark上和其他4个模型做对比：dense，hash layer（也是一种moe，《Hash layers for large sparse models》），Switch Transformer，GShard。效果对比如下\n\n{% asset_img ds_moe_comparison.png deepseek moe 效果%}  \n\n可以得到几个结论：  \n- 更大的模型参数量和稀疏的架构，使得Hash Layer和Switch Transformer和具有同样激活参数的dense模型相比，有明显的优势。  \n- 同样的模型参数下，GSshard比Hash Layer和Switch Transformer有更多激活参数，效果也更好  \n- 同样的模型参数和激活参数下，DeepSeekMoE效果比GShard有明显优势。  \n\n为了进一步探索DeepSeekMoE架构带来的收益，提升了dense模型和GShard模型的激活参数，直到效果和DeepSeekMoE-2B差不多。\n\n结果dense模型和GShard模型需要分别扩大到16倍和1.5倍的参数量，才能达到DeepSeekMoE-2B相近的效果，如下表所示  \n\n{% asset_img ds_moe_upper_bound_2b.png deepseek moe upper bound %}  \n\nDeepSeekMoE的优势在更大规模的情况下，依然成立。训了DeepSeekMoE-13B, 对比参数量提升至1.2和1.5倍的GShard，DeepSeekMoE-13B依然能match，具体如下表  \n\n{% asset_img ds_moe_upper_bound_13b.png deepseek moe upper bound %}  \n\n2. DeepSeekMoE架构消融实验\n\n针对DeepSeekMoE架构的两个主要设计，shared expert和fine-grained expert进行消融实验。使用不同数量的共享专家和不同粒度的expert进行效果对比，结果如下图。\n\n{% asset_img ds_moe_ablation.png deepseek moe upper bound 消融实验 %}  \n\n（1）对比蓝色和橙色，可以看到增加共享专家带来了收益\n\n（2）绿色和红色在橙色的基础上进一步把专家颗粒分得更细，效果进一步提升\n\n（3）共享专家和路由专家的比例：在总共64个expert的情况下，对比了1/2/4个共享专家的情况，结果并没有显著差别，在pile上的loss分别是1.808,1.806,1.811。最终选择了共享专家和激活路由专家1:3（2+6）的比例。\n\n3. expert specialization的分析\n\n通过实验来验证DeepSeekMoE中expert specialization的优化。\n\n（1）前面实验看到DeepSeekMoE-2B和1.5倍参数量的GShard模型效果相当。在这个基础上，通过禁用不同数量的top专家，而只能从次优的专家中选择进行回答。  \n\n实验结果如下\n\n{% asset_img ds_moe_expert_specialization.png 专家专门化 %}  \n\n发现DeepSeekMoE损失更大，说明DeepSeekMoE每个专家的专业化程度更好，必要性更高。  \n\n（2）另外，通过禁用DeepSeekMoE的共享专家，而额外激活一个专家，发现loss也大大提升。这个结果突出了共享专家的关键功能，并表明共享专家捕捉到了与路由专家不共享的基本且重要的知识，使得它无法被路由专家替代。\n\n（3）只激活更少专家，也能和GShard达到相同水平，这一观察结果支持了DeepSeekMoE可以更准确和高效地获取所需知识的观点。  \n\n{% asset_img ds_moe_less_activated_expert.png 激活更少专家 %}  \n\n此外还从零训了一个只用1个共享专家和3个激活专家的2b模型（正常是2个共享专家+6个激活专家），也比GShard好，说明DeepSeekMoE的有效参数效率更高\n\n{% asset_img ds_2b_less_expert.png 2B激活更少专家 %}  \n\n1. DeepSeekMoE-16B  \n\nDeepSeekMoE-16B模型使用了2T数据训练（和LLAMA2-7B对齐）训练，并使用了100k的词表。其他参数如下表所示  \n\n{% asset_img ds_model_param.png 模型超参 %}  \n\n论文中提到，除了第一层以外，其他层都使用了MoE层。  \n\n第一层不使用MoE是因为观察到第一层的负载均衡loss在训练中收敛得特别慢。  \n\nDeepSeekMoE-16B每层有64个专家，其中有2个作为共享专家保持永远激活，加上6个通过gating function选择激活的，每个token共使用8个专家。每个token会激活16.4B中的2.8B参数。  \n\n这里没有把专家的dimension再减小，是因为如果专家太小，计算效率就下降得太厉害。  \n\n训练中使用的其他设置：  \n- lr = 4.2e-4  \n- 训练进行到80%和90%的时候，lr都会缩小到0.316倍  \n- batch size = 4.5k，训练窗口长度是4k，因此每个batch有18M token，2T数据差不多是10.6w步  \n- 使用了pipeline parallelism\n\nexpert level balance loss的系数设得比较小，0.001，因为实验中发现设得再大并不能进一步优化负载平衡，反而会损害模型效果。  \n\nDeepSeekMoE-16B和DeepSeek-7B模型的对比如下  \n\n{% asset_img ds_16b_perf_1.png 和DeepSeek-7B对比 %}  \n\nDeepSeekMoE-16B和LLAMA2-7B模型的对比如下  \n\n{% asset_img ds_16b_perf_2.png 和LLAMA2-7B对比 %}  \n\n5. DeepSeekMoE-145B  \n\n幻方还用245B的token训练了DeepSeekMoE-145B，模型效果上达到DeepSeek-67B的同等水平  \n\n{% asset_img ds_moe_145b.png 145b %}  \n\n# DBRX\n\n2024年3月27日，Databricks开源了DBRX，一个拥有有132B参数，激活参数为36B的MoE模型。\n\n结构上，DBRX使用了RoPE、GLU、GQA，采用了fine-grained expert的设计，每层有16个专家，每个token激活其中4个。相比Mixtral和Grok-1在8个专家中激活2个，DBRX有更多的专家组合方式。  \n\nDBRX训练的上下文长度为32k，并使用了12T文本和代码token进行训练。DBRX在3072个H100上完成预训练，加上post-training、效果评估、red-team优化，整个过程耗费3个月时间。  \n\nDBRX整体效果超过GPT-3.5，与Gemini 1.0 Pro相当，并且具有比较强的代码能力，甚至超过了在代码上专门优化过的模型，如CodeLLaMA-70B，如下图所示。  \n\n{% asset_img dbrx_perf.png DBRX效果 %}  \n\n推理效率效率上，DBRX也领先于其他模型。  \n\n{% asset_img dbrx_infer_efficiency.png 推理效率 %}  \n\n# Qwen1.5-MoE \n\n2024年3月28日，阿里放出了Qwen1.5-MoE-A2.7B，以2.7B的模型参数，达到了Qwen1.5-7B模型的相近效果。  \n\nQwen1.5-MoE-A2.7B参考了DeepSeekMoE和DBRX的工作，采用了fine-grained expert的做法，总共有64个专家，每个token激活8个专家，其中有4个为共享专家。  \n\nQwen1.5-MoE-A2.7B使用Qwen-1.8B进行初始化，并在初始化阶段引入随机性，这样可以显著加快收敛速度，并得到更好的收敛结果。  \n\nQwen1.5-MoE-A2.7B和其他模型效果对比如下  \n\n{% asset_img qwen1.5_moe_perf.png Qwen1.5-MoE-A2.7B效果 %}  \n\n虽然Qwen1.5-MoE-A2.7B总参数量较大，但激活的non-embedding参数量远小于7B模型，如下表所示  \n\n{% asset_img qwen1.5_moe_params.png Qwen1.5-MoE-A2.7B参数量 %}  \n\n实践中，Qwen1.5-MoE-A2.7B相比于Qwen1.5-7B，训练成本降低了75%。  \n\n推理性能上，在A100-80G用vLLM部署Qwen1.5-7B和Qwen1.5-MoE-A2.7B模型进行了性能测试。  \n\n输入/输出token数都设置为1000，输出token数设置为1000，TPS和throughput如下  \n\n{% asset_img qwen1.5_moe_tps.png Qwen1.5-MoE-A2.7B TPS %}  \n\n虽然MoE模型对内存需求更大，但是由于稀疏激活以及共享专家的设计，但是在速度和吞吐量上都比dense模型更好。Qwen1.5-MoE-A2.7B与Qwen1.5-7B相比，速度提高了约1.74倍。  \n\n# Mistral\n\n## Mistral 8x7B\n\n2023年12月11日，Mistral AI开源Mistral-8x7B，每个token激活8个专家中的2个。  \n\nMistral-8x7B支持32k推理窗口和多语言，并且代码能力较好。和LLAM2-70B以及GPT-3.5的对比如下。  \n\n{% asset_img mistral_8_7b_perf.png Mistral 8x7B效果 %}  \n\nMistral-8x7B在大多数任务表现优于LLAM2-70B，且推理速度提高了6倍。  \n\n而和激活参数量相近的LLAM2-13B比，优势更为明显  \n\n{% asset_img mistral_8_7b_active_perf.png Mistral 8x7B同样激活参数量下效果 %}  \n\n## Mistral 8x22B\n\n2024年4月17日，Mistral AI开源Mistral-8x22B模型，一个总参数为141B，激活参数为39B的超大MoE模型。  \n\nMistral-8x22B支持多语言，并且具有较强的数学和代码能力。此外，推理窗口长度也从Mistral-8x7B的32k增加到64k。Mistral-8x22B还具备function call的能力。  \n\n在各个维度的评测结果如下  \n\n{% asset_img mistral_8_22b_reasoning.png Mistral 8x22B reasoning效果 %}  \n\n{% asset_img mistral_8_22b_multiling.png Mistral 8x22B 多语言效果 %}  \n\n{% asset_img mistral_8_22b_code.png Mistral 8x22B 代码与数学效果 %}  \n\n# 小结  \n\n- 现有的工作都表明，MoE模型相比dense模型具有更高的参数效率，即同样的计算量下，MoE模型普遍能有更优的效果  \n- 因此MoE不仅能支持更大规模模型的训练，在较小规模模型上使用MoE架构也有很大收益  \n- 但是相比dense模型，MoE模型的训练也需要考虑更多内容，包括专家数量、激活数量和专家容量的设计，负载均衡的问题，如何在多设备上的并行等，训练难度更大  \n- 结构上，共享专家和细粒度专家目前被验证效果较好  \n- 负载均衡上，GShard和Switch Transformer的负载均衡损失被广泛采用  \n- 推理时需要对底层框架进行优化以适配MoE机制，否则难以发挥MoE的性能优势  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n# Reference  \n【1】Adaptive Mixtures of Local Experts https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf  \n【2】Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer https://arxiv.org/abs/1701.06538  \n【3】GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding https://arxiv.org/abs/2006.16668  \n【4】Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961  \n【5】GLaM: Efficient Scaling of Language Models with Mixture-of-Experts https://arxiv.org/abs/2112.06905  \n【6】ST-MoE: Designing Stable and Transferable Sparse Expert Models https://arxiv.org/abs/2202.08906  \n【7】DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models https://arxiv.org/abs/2401.06066  \n【8】Introducing DBRX: A New State-of-the-Art Open LLM https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm  \n【9】Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters https://qwenlm.github.io/zh/blog/qwen-moe/  \n","source":"_posts/cs/nlp/2024/03/混合专家MoE-基础篇.md","raw":"---\ntitle: MoE模型的前世今生\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 44e38c1b\ndate: 2024-03-30 09:56:05\n---\n\n【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  \n\n***\n\n2024年3、4月这段时间，很多MoE模型扎堆发布，包括Qwen1.5-MoE、DBRX、Jamba和Mistral等。  \n\n下面这个表格列出了部分近期发布的MoE工作  \n\n<center>\n\n| 模型 | 发布时间 | 备注 |\n| :----: | :----: | :----: |\n| GPT4 | 2023年3月 | 23年6月George Hotz爆料GPT4是8×220B模型 |\n| Mistral-8×7B | 2023年12月 | Mistral AI，开源 |\n| LLAMA-MoE | 2023年12月 | github开源项目 |\n| DeepSeek-MoE | 2024年1月 | 幻方量化，国内首个开源MoE模型，有技术报告 |\n| abab6 |2024年1月 | MiniMax，号称千亿MoE，无开源，无细节发布 |\n| 天工2.0 | 2024年2月 | 昆仑万维，无开源，无细节发布 |\n| Step-2 | 2024年3月 | 阶跃星辰，无开源，无细节发布 |\n| MM1 | 2024年3月 | 苹果，多模态MoE，无开源，有技术报告 |\n| Grok-1 | 2024年3月 | X，开源 |\n| Qwen1.5-MoE-A2.7B| 2024年3月 | 阿里巴巴，开源 |\n| DBRX | 2024年3月 | Databricks，开源 |\n| Jamba | 2024年3月 | AI21，开源 |\n| Mistral-8×22B | 2024年4月 | Mistral AI，开源 |\n| WizardLM-2-8×22B | 2024年4月 | 微软，开源 |\n| 天工3.0 | 2024年4月 | 昆仑万维，400BMoE |\n| Arctic | 2024年4月 | Snowflake，480B，Dense-MoE Hybrid，开源 |\n\n</center>  \n\nMoE模型目前风头正劲，就连前不久小米汽车发布会上，雷总也弄了个多模态MoE大模型做汽车智能中控  \n\n{% asset_img xiaomi_moe.jpg 小米汽车多模态MoE模型 %}  \n\n相信今年接下来的这段时间，MoE还会给我们带来更多的大新闻。  \n\n本篇将初步梳理MoE相关的一些经典工作和几个近期发布的中文MoE模型，从背景、思路和效果来了解MoE模型。  \n\n到文章发出的2024年4月为止，个人认为DeepSeek-MoE和Qwen1.5-MoE是中文领域做得比较好的两个工作，赶时间的朋友可以优先关注这两个工作。\n\n# 时间线  \n\n这里先对后面会涉及的MoE相关工作，大致按时间线梳理一下，也列出一些关键信息包括模型结构、模型规模等。  \n\n（很多经典的MoE工作都出自Google）\n\n## 上古时代  \n\n首先是很多MoE相关论文都会引用的，发表在1991年的论文[《Adaptive Mixtures of Local Experts》](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)，这篇文章出自Geoffrey Hinton和Michael I. Jordan两位大神之手。虽然在更早的时候就有MoE相关概念的工作，如原文所提到的，1988年这个概念就有了  \n\n>This idea was first presented by Jacobs and Hinton at the Connectionist Summer School in Pittsburg in 1988.  \n\n但是大部分MoE文章还是认为是这个工作奠定了MoE的基础。  \n\n## RNN时代  \n\n时隔二十多年，Google在2017年1月发布了[《Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer》](https://arxiv.org/abs/1701.06538)，把MoE带进了LSTM，训出了最大137B参数，专家数达到128k的LSTM模型。  \n\n## Transformer时代  \n\n1. 2020年6月，Google发布[《GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding》](https://arxiv.org/abs/2006.16668)，把MoE应用在encoder-decoder结构的transformer模型上，每两层将一个FFN层替换成一个MoE层，训出了模型参数量从12.5B到600B的一系列MoE模型，每层最大专家数也达到2048个。  \n\n2. 2021年1月，Google发布[《Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity》](https://arxiv.org/abs/2101.03961) ，在T5（encoder-decoder结构）的基础上，把FFN层替换成MoE层，并简化了routing策略，训出了最大1.6T参数量的switch transformer。Switch Transformers对scaling、蒸馏等做了很多详细的探索，影响深远，是很重要的一个工作。  \n\n3. 2022年2月，Google发布[《ST-MoE: Designing Stable and Transferable Sparse Expert Models》](https://arxiv.org/abs/2202.08906)，也是一个基于encoder-decoder结构的MoE模型，最大模型有269B的总参数，32B的激活参数。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，个人认为其重要程度相比Switch Transformer都有过之而无不及。  \n\n## GPT时代  \n\n1. 2021年12月，Google发布了GLaM，[《GLaM: Efficient Scaling of Language Models with Mixture-of-Experts》](https://arxiv.org/abs/2112.06905)，训出了最大为1.2T参数量的decoder-only模型。（从encoder-decoder到decoder-only，可以看到Google内部在模型结构方向上也有很多不同的尝试）  \n\n2. 2024年1月，幻方量化发布[《DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models》](https://arxiv.org/abs/2401.06066)，对在23年12月开源的DeepSeekMoE，给出了一些细节。  \n\n3. 2024年，Databricks的DBRX、阿里的Qwen1.5-MoE-A2.7B、Mistral AI的Mistral-8x22B等陆续发布。  \n\n# 奠基工作  \n\nGeoffrey Hinton和Michael I. Jordan的[《Adaptive Mixtures of Local Experts》](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)是大多数MoE论文都会引用的最早工作。  \n\n1. 思路  \n\n这篇文章大致的思路是这样的：对于比较复杂的任务，一般可以拆分为多个子任务。比如要求计算输入文本中有多少个动词和名词，那就可以拆分为“数动词”和“数名词”这两个子任务。  \n\n而一个模型如果要同时学习多个子任务，多个子任务相互之间就会互相影响，模型的学习就会比较缓慢、困难，最终的学习效果也不好。  \n\n因此这篇文章提出了一种由多个分开的子网络组成的监督学习方法。这些分开的网络，在训练过程中，分别学习处理整个训练数据集中的一个子集，也就是一个子任务。这个思路就是现代MoE的思路，每个子网络（也就是一个expert）学习处理一部分内容。  \n\n文章里把这个MoE的方法应用于vowel discrimination task，即元音辨别任务，验证了MoE设计的有效性。元音辨别指的是语音学中区分不同元音的能力，在语音学中，模型需要学习辨别不同的元音因素，以便准确地理解和识别语音输入。通过让多个子模型分别学习分别学习不同元音（a、e、i、o、u）辨别的子任务，最终效果得到了提升。  \n\n2. 模型设计  \n\n下图展示的就是这个MoE的思路：各个expert network和gating network接收同样的输入，每个expert给出各自的处理结果；而gating network输出每个expert的权重，就像一个开关一样，控制着每个expert对当前输入的打开程度，只是这个开关不是离散的，而是stochastic的，给出的不是true和false，而是权重。  \n\n{% asset_img vanilla_moe.png Vanilla MoE %}  \n\n3. 损失函数优化  \n\n实际上，MoE这个idea在这篇文章之前就有了。如论文中所提，Jacobs和Hinton在1988就讨论过。但是之前的工作在loss的设计上，和ensemble更相近，多个expert之间更倾向于合作，每个expert会学习其他expert的residual部分。  \n\n具体来说，对于case $c$，假设第 $d^c$ 是对应的ground truth，第 $i$ 个expert的输出是 $o_{i}^c$，$p_{i}^c$ 是gating network给第 $i$ 个expert分配的权重，那么以前的工作所使用的损失函数 $E^{c}$ 计算如下\n\n$$E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}$$\n\n这样的损失计算方式，是把期望输出和所有expert输出的混合结果进行比较。  \n\n这样做的结果是，在训练过程中，每个expert学习的其实是其他expert的组合结果所剩下的残差。这样的学习目标并不能很好迫使每个expert单独输出好的结果，因此不能得到稀疏的模型。  \n\n从另一个角度来看，这个损失计算把所有专家耦合在了一起。即当一个expert的输出发生了变化，所有expert的组合结果也会变化，其他所有的expert也需要做相应的改动来适应这个变化。因此各个expert之间更加倾向于合作，而不是相互竞争并单独给出好的结果，让gating network输出稀疏的结果。  \n\n虽然可以使用如增加辅助损失函数的做法，迫使模型给出稀疏激活的结果，但是这样相当于增加了很强的先验正则化，对模型最终效果也是有损害的。  \n\n而Hinton和Jordan在这个工作里，提出更简单的做法是对loss计算进行修改，使得各个expert之间的关系从合作变成竞争。  \n\n假设gating network每次随机选择一个expert，损失计算如下  \n\n$$E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$\n\n在这个损失函数中，每个expert的输出结果会单独和期望结果进行对比，这就要求每个expert单独给出完整的结果，而不是仅学习其他expert的残差。  \n\n这样的loss计算具有localization的特性，即如果一个训练case错了，那么会被修改的主要是被gating network选中且出错的expert，以及负责分配权重的gating network，而不会很大地影响其他expert。  \n\n此外，localization还体现在，每个expert只会负责处理输入空间中某个特定子空间的向量，而不是完整的输入空间。  \n\n这样一来，不同的expert之间不会直接相互影响，虽然还是有间接的影响，比如某个expert的输出变了，gating network可能会分配新的权重，但是至少不会改变其他expert error的符号（+，-），即优化的方向。  \n\n最终的结果是，对于给定的输入，这样的系统会倾向于以高权重分配单一一个expert来预测结果（但其他权重还不是真正的0，不是真正的稀疏）。  \n\n4. 实操技巧\n\n上面提出的这个loss计算，理论上没有问题，实际上也能训练，但是为了得到更好的效果，作者把原loss计算作了如下变化：先指数化再求和，最后再取对数，得到了优化loss。看下变化前后的对比  \n\n$$\\text{原loss：}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$  \n\n$$\\text{优化loss：}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}$$  \n\n这样做有什么好处呢？来对比一下原loss函数和优化后的loss函数的求导结果  \n\n$$\\text{原loss导数：}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\n$$\\text{优化loss导数：}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\n相比原loss函数的导数，优化后的loss函数的导数，把当前第 $i$ 个expert的表现，和其他expert联系起来了。这样能够更好地衡量expert $i$ 对当前case的处理结果好坏。特别是在训练初期，gating network的权重是近似平均分配的，那么使用原loss函数的结果是，对当前case效果最好的expert，学习速度是最慢的（因为loss最小）；而优化的loss函数则可以让当前最好的expert的学习速度最快。相当于让“有天赋”的专家在对应的子任务上尽快提高水平。这样就强化了localization的特征，使得各个expert更快拟合到自己擅长的部分，加速训练。  \n\n（BTW，优化后的这个loss导数，和现在的对比学习形式上看起来也很相似）  \n\n这个工作在今天看来不很复杂，但是思路还是很踏实有效的，给MoE奠定了基础。  \n\n# LSTM MoE  \n\nGoogle在2017年1月发布了\n[《OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER》](https://arxiv.org/abs/1701.06538)，把MoE应用到了LSTM上，训出了最大137B的LSTM模型。这样规模的模型哪怕放在7年后的今天，也是巨无霸的存在，需要解决很多工程问题。  \n\n相比1991年的工作，这里做到了真正的稀疏激活，从而可以在实际计算量较少的情况下，训练巨大的模型。  \n\n## 背景\n\n虽然当时Transformer还没出来，大规模模型的竞赛也还不像今天这么激烈，但是在多个领域中（文本、图像、音频），已经有不少工作反复证实了一件事：模型容量越大，能训出来的效果越好，上限越高。但是模型越大，需要的训练数据也就越多，二者共同作用下，就造成了训练开销基本是随着模型增大，以平方关系在增长。  \n\n在这个背景下就出现一些conditional computation，条件计算的工作来解决这个问题。conditional computation就是根据输入，有选择地只激活部分网络模块。那么MoE其实就是一种条件计算的实现。由于不用激活全部参数，训练所需的计算量就大大减小，整体计算成本就不用以平方速度增长。  \n\n虽然理论上计算量的成本下来了，不过实操起来还是会遇到几个问题：  \n\n- 训练的时候，在MoE结构下，每个expert的batch size比整个模型的batch size小了。  \n比如模型的batch size是32，一共有16个expert，那实际上一次迭代平均每个expert只能分到2个训练样本。而batch size对训练效率影响是很大的，大的batch size摊小了参数传输和更新的成本。如果直接增大模型的batch size，又会受显存和通讯效率的限制。  \n- 训练数据量不足。  \n要训大模型就需要大量的数据，让模型参数充分学习。在当时的背景下，大规模的NLP数据是比较缺的。当然如今数据集多了很多，特别是预训练数据，这个问题现在来看没有那么突出了。  \n- 损失函数的设计。  \n如何使用合适的损失函数来训练模型，提升效果，并且使得模型的负载比较均衡，这是一个不容易解决的问题。  \n- 集群通讯问题。  \n一个GPU集群的计算能力可能比设备间网络带宽的总和高出数千倍，因此设备间的通讯很可能成为训练效率的瓶颈。为了计算效率，就要使得设备内计算量和所需的通讯量的比值，达到相应的比例。  \n- GPU计算特点。  \nGPU做数学计算很快，但是并不擅长做branching（if/else），因此MoE的工作基本上都是用gating network来控制参数的激活。这个严格来说不算是新的挑战了，应该说是根据计算设备沿用下来的设计。  \n\n要解决好这些问题，才能训出比较好的模型来。  \n\n## 模型设计\n\n1. 整体结构  \n\n先看下模型结构的设计。  \n\n论文里使用的是两个LSTM层，中间夹着一个MoE层，最上面和最下面分别还有一个embedding层和一个任务输出层，结构如下图所示  \n\n{% asset_img rnn_moe.png LSTM MoE %}  \n\n每个expert是一个简单的feed-forward neural network。一共有n个expert，gating network输出是一个稀疏的n维向量  \n\n$$\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}$$  \n\n$E_{i}(x)$ 是第 $i$ 个expert的输出，$G(x)_{i}$ 是gating network给出的第 $i$ 个expert的权重。  \n\n如果 $G(x)_{i}$ 为0，就不用计算对应的那个expert了，节省了计算。  \n\n如果expert的数量特别多，可以用two-level hierarchical MoE，即使用两层gating network，第一层的gating network先选择一个包含一批expert的分支，每个分支又有一个单独的gating network来选择具体的expert。类似word2vec训练所用的hierarchical softmax。这样做可以节省一些计算。  \n\n2. gating network  \n\n那具体gating network怎么设计呢？  \n\n如果对输入进行线性变换，再简单加上一个softmax，那得到的是一个非稀疏的gating function  \n\n$$\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot W_g)\\end{aligned}$$  \n\n在这个基础上，使用一个topk函数，只保留最大的k个值，其他都设为﹣∞（softmax之后变成0），这样就能只选择部分expert，得到了稀疏性。  \n\n论文提到，虽然理论上这个形式的sparsity（topk）会造成gating function的不连续，不过在实操中暂时没有遇到相关问题。  \n\n在这个基础上，在输入再加上一个Gaussian noise，这个noise的大小由另外一个可学习的参数来控制。整体的计算公式如下  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$KeepTopK(v,k)_i=\\begin{cases}v_i&\\text{if }v_i\\text{ is in the top }k\\text{ elements of }v.\\\\-\\infty&\\text{otherwise.}\\end{cases}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\n其中用来调整noise的非线性函数softplus是个类似ReLU的激活函数，但是更为光滑，函数图像如下  \n\n{% asset_img softplus.png softplus %}  \n\n这里添加噪声的原因和负载均衡有关，下面来分析下负载均衡。  \n\n## 负载均衡  \n\n在MoE模型训练的实验中观察到，如果不对gating network进行干预，任由模型自由学习，那么最终模型会倾向于收敛到“总是选那几个固定的expert”的状态，而其他expert几乎不会被使用。这就是负载不均衡的状态，如果这些专家分布在不同的计算设备上，结果就是有些设备输入排队特别长，而有些设备基本处于闲置状态，这明显不是我们想要的。  \n\n这种负载不均衡的状态有自我加强的属性，因为一旦开始出现部分专家被较多选中激活，这些专家就会得到更充分的训练，从而获得更好的效果，进而又提升被选中激活的概率。  \n\n针对这种情况，之前有一些工作使用hard constraint来缓解，比如当某个expert激活次数达到上限，就把它从候选集合中移除。hard constraint明显会对模型效果有影响。而这篇论文使用的是一种soft constraint。  \n\n具体来说，对于每个expert，定义了一个它在当前这批输入数据里的重要性指标，如以下公式所示  \n\n$$Importance(X)=\\sum_{x\\in X}G(x)$$  \n\n$G(x)$ 是gating network给出的权重，是一个维度等于expert数量的向量。  \n\n基于这个重要性指标，论文定义了一个辅助损失 $L_{importance}$，训练时和模型的交叉熵损失加到一起。$L_{importance}$ 的计算方式如下  \n\n$$L_{importance}(X)=w_{importance}\\cdot CV(Importance(X))^2$$  \n\n其中权重 $w_{importance}$ 是手动设置的超参，实验的推荐值是0.1，CV是coefficient of variation。  \n\ncoefficient of variation离散系数，是概率分布离散程度的一个归一化量度，定义为标准差 $\\sigma$ 和 均值 $\\mu$ 的比值。  \n\n对于MoE来说，确定激活的expert数之后，均值是固定的。如果expert的gating很不平衡，标准差就会很大，离散系数也会很大，使得 $L_{importance}$ 变大。  \n\n但是这里还是有问题，虽然均衡的负载可以推导出 $L_{importance}$ 较小的结论，但是 $L_{importance}$ 较小却不能保证负载均衡。也就是说 $L_{importance}$ 较小只是负载均衡一个必要不充分条件。  \n\n比如一个expert可能以很高的权重被分配到一个样本，而另一个expert可能以不太高的权重被分配到好几个样本。这种情况下对所有输入数据的gating权重进行求和，仍然可能呈现出均匀的表象（离散系数比较小），但这并不符合我们的要求。  \n\n为了解决这个问题，需要额外再加上一个损失 $L_{load}$ 。这里就要用到添加在每个expert输出上的随机噪音了。  \n\n我们想要各个expert的负载均衡，也就是每个专家需要处理的样本数基本一致，但是分配到各个专家的样本数是个离散值，因此没有办法直接用于back propagation，而 $L_{load}$ 就是对各个expert负载的一个平滑评估。  \n\n回想一下前面在设计MoE的时候，定义了 $H(x)$ 为KeepTopK函数的输入  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\n那么这里先定义一个 $kth\\_excluding(H(x),k,i)$，表示在除去 $H(x)$ 中的第 $i$ 个分量之后，排在第 $k$ 大的值。基于这个，再定义 $P(x,i)$ 为：固定其他分量已经选取好的noise，重新给第 $i$ 个分量再添加一次noise，结果比 $kth\\_excluding(H(x),k,i)$ 大的概率，公式如下  \n\n$$\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\\\>kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}$$  \n\n通过这个noise，我们把“第 $i$ 个专家是否处理这个输入”的离散值，变成“第 $i$ 个专家处理这个输入的概率”这样一个平滑的估计，$P(x,i)$ 就表示这个概率。这个概率可以简化写成  \n\n$$\\begin{aligned}P(x,i)&=\\Phi\\Big(\\frac{(x\\cdot W_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot W_{noise})_i)}\\Big)\\end{aligned}$$  \n\n其中 $\\Phi$ 是标准正态分布的CDF。  \n\n接下来就可以把第 $i$ 个expert的负载定义为  \n\n$$\\begin{aligned}Load(X)_i=\\sum_{x\\in X}P(x,i)\\end{aligned}$$  \n\n有了每个expert的负载衡量，就可以和前面第一个负载均衡损失一样，计算新的负载均衡损失了  \n\n$$L_{load}(X)=w_{load}\\cdot CV(Load(X))^2$$  \n\n$w_{load}$ 是手动设置的超参，实验的推荐值是0.1。  \n\n相比前面的 $L_{importance}(X)$，$Load(X)$ 是对负载是否均衡更细粒度的评估。  \n\n论文中提到一个细节，在刚开始训练的时候，希望模型分配的expert尽量均衡，因此把 $W_g$ 和  $W_{noise}$ 都设为0，这样相当于没有信号，也没有噪音。  \n\n最终使用负载均衡之后的效果如下  \n\n{% asset_img rnn_moe_load_function.png 负载平衡效果 %}  \n\n使用这两个负载均衡损失之后，能达到接近完全平均分配的效果。  \n\n## 实验  \n\n1. 解决工程问题  \n\n针对前面提出的一些工程问题，论文给出一些方案  \n\n（1）batch size减小  \n\n由于稀疏激活的原因，每个expert的batch size会变小。假设每次在n个expert中选择k个，模型训练的batch size为b，那么每个expert的batch size就是kb/n。论文通过以下这几种方法来提升每个expert的batch size：  \n- 混合使用数据并行和模型并行。本来在使用数据并行的情况下，每个模型副本是异步处理各自的数据的。而这里做了优化，各个副本的batch是同步处理的，这样就可以把多个模型副本的batch组合起来。对于非MoE部分的参数，依然使用标准的数据并行机制；而对于每个expert，则在整个集群中只保留一个副本。如果模型分布在d个设备上，那每个expert就能得到一个kbd/n的batch size。\n- 对于LSTM模型，在时间步上展开，就能把batch size提升相应的倍数。\n\n（2）集群通讯问题  \n\n另一个挑战就是平衡集群计算量和通讯量的关系。  \n\n对于每个expert来说，主要的通讯就是input和output的传输。而每个专家的主要计算量就是两个全连接层，大小分别为[input_size, hidden_size]和[hidden_size, output_size]。对于GPU来说，计算速度可能是通讯速度的1000倍，那我们就需要把计算量设计得足够大。最简单的做法就是把hidden_size提高，使得每个expert的内部计算量比通讯量大1000倍，以保证通讯不会成为训练的瓶颈。  \n\n2. 模型容量 & 参数效率  \n\n为了验证模型容量提升带来的收益，以及MoE模型的参数效率（即和dense模型同样推理计算量下能达到的效果），训练了包含4/32/256个expert的flat MoE模型，和包含256/1024/4096个expert的hierarchical MoE模型。每个expert大约是1M参数量，对于所有flat模型都是激活4个expert，而对于hierarchical MoE是每层gating激活2个。  \n\n效果如下图。左边的图显示，随着模型容量提升，测试的ppl有明显下降。右边的图将相近模型容量的dense模型和MoE模型的效果放在一起对比，可以看到MoE模型在相同模型容量下，效果更好\n\n{% asset_img rnn_moe_perf.png 效果 %}  \n\n3. 更大的模型  \n\n前面几个模型训练用的数据量不是很大，模型最大也只有4B左右，训练不久就出现diminishing returns。  \n\n为了验证更大数据集 + 更大模型的收益，在100B token的语料上，分别训了包含32, 256, 1024，4096, 16384, 65536, 和131072个expert的MoE模型，最大的模型达到了137B的参数量。  \n\n各个模型对比如下表。整体来看，增加数据和模型容量，是可以继续获得提升的。  \n\n{% asset_img rnn_moe_137b.png 137模型效果 %}  \n\n从这里还可以看出，在专家数量不太多时，提升专家数量效果有提升，但是收益会慢慢减小，甚至会出现专家数量太多，效果反而下降的情况。  \n\n4. Expert Specialization  \n\n按照MoE的设计思路，不同的专家应该学习到不同的子任务，但是实际上是否是这样呢？\n\n论文里把模型中不同的专家分配到token拿出看，发现确实有比较强的specialization效果，不同的专家处理不同的内容，如下所示  \n\n{% asset_img rnn_moe_specilized.png RNN MoE 专门化 %}  \n\n# GShard\n\n1. 简介\n\n2018年，随着Bert的发布，transformer结构彻底火了起来。2020年6月，Google发布《GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding》，把MoE用到了encoder-decoder结构的transformer模型上。MoE开始变成我们现在熟悉的样子了。  \n\nGShard这个工作做了很多的实验，训了很多规模巨大的MoE模型，最大的达到了600B。训练的一系列模型的参数如下表  \n\n{% asset_img gshard_moe_family.png GShard MoE family %}  \n\n在expert数量的设计上，延续上面LSMT MoE工作的思路 -- expert越多，效果越好。（站在24年这个时间节点来看，太多的expert未必适合；但是也不能说这个思路一定错误，毕竟事物的发展是螺旋式的，就像ChatGPT出来之前大多数人都在魔改各种Bert，而GPT已经坐了几年冷板凳了。）  \n\nGShard论文中很大的篇幅在介绍工程实现和优化，这也是MoE模型训练最大的痛点。关于工程框架的内容比较硬核，因此这里不会展开讲太多，而是关注在模型算法层面上。  \n\n2. 模型设计\n\n先来看下模型设计。  \n\nGoogle在那段时间走的是encoder-decoder transfomer的技术路线，因此GShard也是基于encoder-decoder transfomer的模型结构。  \n\nGShard的模型设计是，在encoder和decoder中，每两层把其中一个FFN层替换成MoE层。对于总共有N层的模型，则有N/2个MoE层，如下图  \n\n{% asset_img gshard_model.png GShard模型结构 %}  \n\n每层会选择最多top-2 expert来激活。为什么是最多，后面解释。  \n\nGShard在上面这篇LSTM MoE论文的基础上，改进了gating function和auxiliary loss function。  \n\n从公式来看，MoE层的具体计算如下\n\n$$\\begin{aligned}\n\\mathcal{G}_{s,E}& =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)& =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}& =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s) \n\\end{aligned}$$\n\n其中 $x_s$ 是MoE的输入token，$w_i$ 和 $w_o$ 分别是输入输出的线性变换矩阵。向量$\\mathcal{G}_{s}$ 就是gating function的输出。\n\nGShard在gating function的设计上提出了两个要求：（1）负载均衡（2）高效扩展。  \n\n负载均衡和前面讲的一样，很好理解。而为什么要高效扩展，因为如果要对N个token分别进行E个expert的分配，在N能达到百万甚至千万级别，而E也有几百上千的情况下，就需要一个高效的分布式实现，以免其他计算资源等待gating function。  \n\n为了满足这些要求，gating function提出了以下机制  \n\n（1）专家容量 expert capacity  \n\n为了确保负载平衡，我们不希望有少量expert需要处理很多token，因此强制规定了每一个expert所负责处理的token数量有一个最大值，这个最大值就叫专家容量，在这里设置为2N/E，相当于平均分配的量。  \n\n这个expert capacity通过GATE(·)给每个expert维护一个计数器 $c_e$ 来监控。如果一个token所选的两个专家当前处理量都已经超过设定的专家容量，那么这个token就不会被当前层的任何expert处理，而是直接通过残差链接透传到下一层。  \n\n（2）分组分配 Local group dispatching  \n\n给所有输入token分成了G组，不同的组并行处理，每个组相应地也把组内专家容量变成2N/EG。  \n\n这样做相当于在前向推理时，把大的batch拆分成小的batch，每个小的batch就是一个group。这样做的好处是通讯的时候（特别是all2all）只需要在每个group内进行就可以了，减少了通讯量。  \n\n而进行反向计算的时候这些group可以合起来一起用，相当于进行了gradient accumulation。  \n\n（3）辅助损失函数 Auxiliary loss  \n\n光设置专家容量并不能使得gating负载均衡，而且会导致大量溢出。参考前面LSTM MoE的工作，这里也定义了一个辅助损失函数，来帮助负载均衡。辅助损失函数设计如下  \n\n$$\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot m_e$$  \n\n$S$ 是token数，$E$ 是专家数，$c_e$ 是分配给第 $e$ 个专家的token数，$m_e$ 是第 $e$ 个expert在 $S$ 个token中获得的平均权重。  \n\n思路是，本来是要算 $\\frac{c_e}S$ 的平方的，但这是离散值不可导，因此把平方中的一个 $\\frac{c_e}S$ 换成了 $m_e$ ， $m_e$ 是第 $e$ 个expert在 $S$ 个token中获得的平均权重。在平均分配的情况下，这个loss达到最小。  \n\n相比前面的负载均衡损失，这个loss的设计就简单许多。  \n\ngating的整个算法如下  \n\n{% asset_img gshard_algo_1.png GShard gating 算法 %}  \n\n（4）随机路由 Random routing  \n\n前面提到，每层会选择最多top-2 expert来激活，就是因为有随机路由的机制。直观来说，就是认为如果top-1专家的权重很高，而第二个专家的权重如果较小，那很有可能只用第一个专家就足够解决问题了。  \n\n随机路由的机制是top-1的专家永远会被激活，而第二个专家如果权重很小，就认为它可以被忽略。具体来说，会以与第二个专家的权重g2成比例的概率激活第二个专家。  \n\n3. 效果  \n\n最后看一下模型在翻译任务上的效果\n\n{% asset_img gshard_perf.png GShard效果 %}  \n\n# Switch Transformer\n\n2022年4月，距离ChatGPT发布还有半年，Google发布了《Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity》（实际上2021年Google就提出Switch Transformer了）。  \n\nSwitch Transformer和GShard一样，是encoder-decoder结构，基于T5开发的，具有1.6T的参数，2048个expert。  \n\n和前面的很多工作一样，Switch Transformer有一个出发点，那就是参数量越大，模型效果越好，并且可以通过稀疏激活来减少总计算量。  \n\n但是相比其他工作，Switch Transformer给出了一个更为具体的描述，那就是模型参数量可以是一个独立于总计算量的，单独的缩放轴。也就是说，在改变参数量的同时，（几乎）不改变训练和推理的计算量，就可以带来效果的提升。因此Switch Transformer关注在“同样的FLOPS/token的计算量”下，如何扩大模型，提升效果。  \n\nSwitch Transformer所做的工作还是比较多的，包括：  \n\n（1）模型结构简化：简化了Transformer上的MoE架构，提出Switch Transformer架构。  \n\n（2）MoE to dense：把训出来的效果较好的MoE模型蒸馏到dense模型，在压缩MoE模型99%的参数的情况下，效果还是比直接训练dense模型好。  \n\n（3）训练和微调技术：  \n- 首次使用bf16成功训练MoE模型  \n- 更适合MoE结构的模型初始化  \n- 增加的专家正则化，改善了稀疏模型的微调和多任务训练  \n\n（4）训练框架：结合数据、模型和专家并行性，训练了超过1T参数的MoE模型。  \n\n（5）多语言：在多语言数据集上训练，发现101种语言效果普遍有提升。  \n\n（6）训练效率：在同样的FLOPS/token的计算量下，Switch Transformer模型收敛速度有数倍的提升。  \n\n## 模型设计  \n\nSwitch Transformer的模型结构如下图，类似GShard，把transformer每层的FFN替换成MoE层  \n\n{% asset_img switch_transformer_structure.png Switch Transformer 模型结构 %}  \n\nSwitch Transformer一个重要的改进点就是简化了gating function的做法（Switch Transformer论文里叫routing）。  \n\n之前的工作大多探索了选择k个expert的做法，而Switch Transformer则直接把gating简化为只选择1个expert，即k=1。这样的MoE层叫做Switch layer。  \n\n这样简化之后，routing的实现更简单，router的计算量小了，也减少了通讯量。  \n\n## 负载均衡  \n\n同GShard一样，Switch Transformer规定了一个专家容量expert capacity，来限制每个expert在一个batch里能处理的最大token数。  \n\n如果一个token被分配到了一个已经满载的expert，就会出现overflow，那这个token在本层就不会被处理，而是直接通过残差链接，透传给下一层。这点也同GShard一样。  \n\n在Switch Transformer，专家容量通过容量系数capacity factor来控制。  \n\n$$\\text{expert capacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of experts}}\\right)\\times\\text{capacity factor}.$$  \n\n一个大的capacity factor意味着每个expert能够处理更多的token，从而减少overflow情况的发生，但是计算量和通讯量的压力也会增大，所以这是一个需要权衡的参数。\n\n下图给出了一个不同capacity factor下的例子  \n\n{% asset_img switch_transformer_diff_expert_capacity.png 不同的expert capacity %}  \n\n那么如何设定expert capacity呢？\n\n如果capacity factor为1的话，只有在完全平均分配的时候，才不会出现overflow的情况。而太大的capacity factor则可能造成算力和存储的浪费。  \n\n首先，实验中发现expert的数量和overflow的数量之间没有什么关系，所以在所有实验中，所有MoE和Switch Transformer模型都用128个专家。  \n\n不同的capacity factor对模型影响如下表。可以看到，大的容量系数相对来说能取得更好的效果（因为更少的overflow），但是相应地，大容量系数的模型处理速度就会慢一些。  \n\n{% asset_img switch_transformer_capacity_effect.png expert capacity的效果 %}  \n\n经验上，低的token丢弃率对模型的scaling很重要，想要训练超大规模的模型，就要解决这个问题。而通过负载均衡损失就可以确保良好的平衡，使得在使用较小容量系数的情况下，overflow尽量少，从而兼顾效果和计算速度。  \n\n关键问题来到负载均衡损失怎么设计。  \n\n给定 $N$ 个expert，和包含 $T$ 个token的batch $\\mathcal{B}$，负载均衡损失是这么计算的 \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$f_{i}$ 表示被分配到第 $i$ 个expert的token数，这个不可导  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$P_i$ 表示整个batch每个token分配给第$i$ 个expert的概率的总和，这个可导  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\n这个损失的设计其实和GShard中的也是一样的。  \n\n在完美平均分配的情况下，$f$ 和 $P$ 这两个向量都是 $1/N$，这个时候负载均衡损失是最小的。  \n\n$\\alpha$ 扫描了1e-5到1e-1，发现设为1e-2，已经足够大保持负载平衡，同时不过分影响模型收敛。  \n\n观察到 $\\sum_{i=1}^N(f_i\\cdot P_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N$，所以负载均衡loss还乘了个 $N$，这样可以保持无论使用多少个expert，在平均分配的情况下，loss都能保持相同的常数。  \n\n## 实验\n\n1. 一些训练的trick  \n\n（1）选择性地使用bf16  \n\n半精度训练会带来一些训练的不稳定。因此选择性地使用bf16，具体来说，routing function内部使用单精度，其他部分使用半精度，这样既不影响通讯，也能提高效果。  \n\n为什么选择在routing提高精度？因为softmax对误差特别敏感，exponential计算会极大放大输入中的rounding error，因此高精度对routing很重要。  \n\n（2）较小的参数初始化  \n\n从截断正态分布中抽取元素来初始化的模型参数，平均值 $\\mu=0$，标准差$\\sigma=\\sqrt{s}/n$，其中s是超参，n是权重张量中的输入单元数量（e.g. fan-in）。  \n\n论文建议将默认的Transformer初始化尺度s=1.0减少10倍。这个方案在实验中既提高了质量又降低了训练不稳定性的可能性。初始化实验对比如下表  \n\n{% asset_img switch_transformer_init.png 初始化对比 %}  \n\n（3）增大dropout  \n\n由于Switch Transformer参数量很大，在微调的时候更容易过拟合，因此一个简单的方法就是增大dropout，效果如下  \n\n{% asset_img switch_transformer_dropout.png dropout效果 %}  \n\n可以看到大的dropout有效果，并且dense层保持0.1，只有expert层增大dropout效果更好。  \n\n2. scaling  \n\n对Switch Transformer结构预训练的scaling做了一些实验。  \n\n（1）Step-Basis  \n\n首先是验证在固定训练step的条件下，增大expert数量带来的提升，如下图所示。  \n\n左边是不同规模的模型在相同step下收敛的结果，可以看到在保持相同计算量的条件下，只通过增大专家数量来提升规模，就有明显的收益。右边则展示训练过程中，不同规模的模型在各个step下的效果。  \n\n{% asset_img switch_transformer_scaling_step.png step scaling %}  \n\n（2）Time-Basis  \n\n虽然Switch Transformer可以保持计算量不变的情况下提升模型规模，但是专家数量的增多会带来额外的通讯成本，所以即使训练的step数相同，实际的训练时间也不同。因此这里要回答的问题是，给定一个固定的训练时长，Switch Transformer是否相比dense模型仍有收益。  \n\n答案是肯定的。下图展示以训练时长为横轴，Switch Transformer和dense模型的效果对比。Switch Transformer收敛到dense模型最终效果的时间只有dense模型的1/7。  \n\n{% asset_img switch_transformer_scaling_time.png time scaling %}  \n\n（3）和更大的dense模型对比\n\n前面Switch Transformer和dense模型的比较，是基于相同计算量的前提。那么Switch Transformer是否具备超越更大规模dense模型的能力？  \n\n下图在Step-Basis和Time-Basis对比了64个专家的Switch Transformer和T5-Large。无论是相同step还是相同时间下，Switch Transformer都有明显优势。  \n\n{% asset_img switch_transformer_scaling_dense.png dense对比 %}  \n\n3. SFT效果对比  \n\n在GLUE和SuperGLUE等下游任务上微调，和dense模型对比。  \n\n对于各个模型，每两百步进行一次eval，选最好的效果，尽量保证公平。结果如下表，大部分任务都有明显的提升。  \n\n{% asset_img switch_transformer_sft_result.png sft对比 %}  \n\n4. 模型蒸馏  \n\n虽然Switch Transformer在相同计算量下效果更好，但是部署几百B甚至T级别的模型，还是不太方便，因此考虑把稀疏模型蒸馏到dense模型上来进行推理。  \n\n论文中给出了几个蒸馏的技巧：  \n- 初始化的时候，把Switch Transformer模型中的非稀疏部分用于初始化dense模型  \n- 蒸馏所用的label，25%来自教师模型，75%来自ground truth，加权求和  \n\n预训练模型的蒸馏效果如下，相比无蒸馏训练的dense模型，把同样计算量的稀疏模型蒸馏到dense模型，dense模型大约能获得Switch Transformer提升部分30%的增益。  \n\n{% asset_img switch_transformer_distill.png 蒸馏 %}  \n\n更进一步，用不同规模的稀疏模型下进行蒸馏，结果如下表，可以实现高达99%的压缩率  \n\n{% asset_img switch_transformer_distill_diff_model.png 蒸馏 %}  \n\n除了预训练模型，微调模型也可以蒸馏，效果如下，在SuperGLUE也有一定的提升  \n\n{% asset_img switch_transformer_distill_sft.png sft蒸馏 %}  \n\n# GLaM\n\n1. 简介\n\n2021年12月Google发表了《GLaM: Efficient Scaling of Language Models with Mixture-of-Experts》，训练出最大参数量为1.2T，每层包含64个专家，每个token激活参数量为96.6B的MoE模型。  \n\n相比Switch Transformer，GLaM的训练数据量要大得多，达到了1.6T token。  \n\n下表是论文中给出的，当时一些大规模模型的对比  \n\n{% asset_img glam_related_model.png glam和相关模型 %}  \n\n虽然模型总参数量比GPT-3（175B）大很多，但是训练成本却比GPT-3低很多，推理速度也更快，而且在多个NLP任务上的效果都超越了GPT-3，如下所示。  \n\n{% asset_img glam_compare_gpt3.png glam和gpt3对比 %}  \n\n{% asset_img glam_compare_gpt3_2.png glam和gpt3对比 %}  \n\n2. 模型设计\n\n模型设计上，和Switch Transformer一样，每两层把一个FFN替换成MoE层。但是和Switch Transformer不同，GLaM用回了每次激活两个expert的方案，模型结构如下图。  \n\n{% asset_img glam_model.png glam模型 %}  \n\n除此之外，模型在结构上海做了一些其他改动：  \n\n（1）位置编码  \n\n使用XLNET的相对位置编码。  \n\n（2）激活函数\n\n> In the non-MoE Transformer feed-forward sub-layers, we replace the first linear projection and the activation function with the Gated Linear Unit，which computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit.  \n\n3. 实验\n\n训练中的一些trick：  \n\n（1）参考《Lingvo: a modular and scalable framework for sequence-to-sequence modeling》，在梯度出现NaN或者Inf的时候就跳过那一步更新。  \n\n（2）如果在BP更新的时候遇到NaN或者Inf，则重新加载更早的checkpoint并跳过有问题的数据来避免NaN或者Inf。  \n\n论文训了一系列模型来探索MoE，这些模型的设置如下表  \n\n{% asset_img glam_family.png glam模型系列 %}  \n\nGLaM和dense模型的评测结果如下  \n\n{% asset_img glam_perf.png glam模型效果 %}  \n\n可以看到GLaM MoE的有效参数效率一致高于dense模型。  \n\n# ST-MoE  \n\n2022年2月，Google发表了《ST-MOE: DESIGNING STABLE AND TRANSFERABLE SPARSE EXPERT MODELS》。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，可以说是MoE的必读论文。  \n\nST-MoE最大模型包含269B总参数量，和与32B dense模型相当的激活计算量。论文中把模型称为称为Stable Transferable Mixture-of-Experts，或者ST-MoE-32B。  \n\n在MoE层的使用上，ST-MoE比Switch Transformer更“节省”一点，每四层才替换1个MoE层。  \n\n论文中主要训了两个规模的ST-MoE模型，分别有4B和269B的总参数量。ST-MoE以及其他用于对比的模型参数如下表  \n\n{% asset_img st_moe_models.png ST-MoE模型及对比模型的参数 %}  \n\n## 稳定性与效果分析  \n\n论文通过对乘性操作、噪音和裁剪这几个内容进行探索，来指导模型的设计。  \n\n1. 乘性操作对模型稳定性和效果的影响  \n\n论文首先研究了乘性操作对模型的训练稳定性和最终效果的影响。  \n\n之前已经有一些工作表明更多的乘法对模型效果有收益。  \n\n> Some architectural improvements involve more multiplications than additions or do not sum many items at once\n\n（1）GELU Gated Linear Units (GEGLU)  \n\n第一个例子是关于激活函数的。GLU是一个对两个输入向量进行component-wise相乘的操作，之后被扩展成GELU-Linear FFN变体，用于替换transformer中的ReLU FFN变体，其计算如下  \n\n$$\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}$$  \n\n这样在一些其他工作里已经被证明了对模型效果有提升。  \n\n（2）RMSNorm  \n\n第二个例子是RMSNorm中的缩放参数，也就是下面公式的 $g$。  \n\n$$y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot g_i$$  \n\nST-MoE针对GEGLU和RMSNorm这两个乘性操作，做了实验，结果如下表。  \n\n{% asset_img st_moe_remove_multiplications.png 移除乘法操作的影响 %}  \n\n发现移除乘性操作可以使模型稳定性更好（训练中发散的情况减少），但是最终效果变差了。  \n\n（3）增加dense层  \n\nST-MoE还验证了在expert层增加更多dense层的效果。结果发现增加更多的乘法交互（增加dense层），可以在带来效果收益的同时，基本不影响推理速度，如下表所示。\n\n{% asset_img st_moe_more_dense_layer.png 更多的dense层 %}  \n\n（4）增加一个bias\n\n在FFN层的第一个矩阵乘法后面增加一个可学习的bias B，分别通过加法和乘法加入  \n\n$$\\text{FFN}_{\\text{GEGLU}}+\\text{Add Bias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2$$  \n\n$$\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot xW_{12})\\odot B]W_2$$  \n\n乘法的收敛速度更快，效果也更好。  \n\n上面这些实验显示，后续在模型效果的探索方向可以往多使用乘性操作去考虑。  \n\n2. noise对模型稳定性和效果的影响  \n\n接下来ST-MoE探索了“噪音可以提升模型稳定性”的假设。  \n\n通过input-jitter，给router的输入logits乘以一个在[1e-2, 1e2]之间的均匀随机变量来添加噪音。  \n\n{% asset_img st_moe_more_add_noise.png 增加noise %}  \n\n结果是增加noise之后，有助于让模型的收敛更加稳定，但是对模型最终效果有负面影响。  \n\n这里论文还提到，小模型上的结果不一定能直接推广到更大的模型上，比如在小模型上稳定的配置，在大模型就可能就不稳定了。因此还是需要在大模型上也进行充分实验。  \n\n3. 限制激活值和梯度值对模型稳定性和效果的影响  \n\n对activation和gradient进行限制是目前广泛应用的提升模型训练稳定性的手段。在反向传播过程中，通过裁剪梯度的范数来缓解梯度爆炸，就是一种常用的限制手段。  \n\n但是在ST-MoE训练269B的大规模模型时，发现裁剪会使得模型收敛的效果很差。  \n\n为了解决这个问题，ST-MoE在训练中引入了router z-loss，形式如下。  \n\n$$L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2$$  \n\n$B$ 是token的数量，$N$ 是专家数，$x\\in\\mathcal{R}^{B\\times N}$ 是router的输入。  \n\nz-loss会对进入router的较大的logits值进行惩罚，以达到尽量减少进入指数函数的较大误差的目的。什么意思呢？后面来解释，先看下使用z-loss的效果。  \n\n{% asset_img st_moe_z_loss_result.png z-loss效果 %}  \n\nST-MoE认为，在模型训练过程中，由于精度不足或者其他问题，会产生很大的值，从而引入误差。而对梯度进行裁剪是在误差发生之后，并且裁剪本身也造成了数据的不连续性，某种程度上，裁剪本身也是一种误差。相反地，z-loss自然地鼓励模型产生较小的对数值，因此可以更精确地建模。  \n\nz-loss乘以一个权重超参 $c_z$ 加入到模型训练的总损失中，如下式所示。  \n\n$$L_{tot}=L_{CE}+c_BL_B+c_zL_Z$$  \n\nST-MoE经过实验，选择了$c_z=0.001$。  \n\n$L_B$ 是 auxiliary load balance loss负载均衡损失，ST-MoE这里使用了和GShard/Switch Transformer所用的相同的损失计算，这里回顾一下：  \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\n$N$ 是专家数， $\\mathcal{B}$是包含 $T$ 个token的batch。$f_{i}$ 表示被分配到第 $i$ 个expert的token数，这个不可导；$P_i$ 表示整个batch每个token分配给第$i$ 个expert的概率的总和，这个可导。  \n\n4. 数据精度对训练效率和训练效果的影响\n\n目前大部分的大模型训练都使用混合精度训练：模型权重以float32格式存储以进行梯度更新，然后在正向和反向传播的矩阵乘法中转换为bfloat16；此外，所有激活值都以bfloat16存储和操作，而allreduce通信可以在bfloat16或float32数值精度中进行。  \n\n对于ST-MoE-32B的训练，allreduce的数值使用半精度可以加速训练，然而这也会使训练变得不稳定，因此ST-MoE保持allreduce的数值精度为float32。  \n\nbfloat16和float32在不同范围的舍入误差如下表所示  \n\n{% asset_img st_moe_round_error.png bf16精度损失 %}  \n\n可以看到，表达的数值越大，舍入误差越大。而z-loss限制了数值大小，也就将误差值限制在比较小的范围。  \n\nMoE模型天生对舍入误差敏感，因为它们由于router的使用而有更多的指数函数，而指数函数会将小的输入误差放大很多，这就加剧舍入误差所导致的训练不稳定。  \n\n另外，ST-MoE有一个策略：只有当排第二的专家的权重大于等于第一的专家的1/5时，token才会被路由到其第二位专家，否则第二个专家就会被忽略。  \n\n因此虽然舍入误差不会改变softmax运算中各个概率的排序，但它确实会影响MoE中第二个专家的激活。  \n\n## 模型设计\n\ndense模型的设计有scaling law进行指导，但是MoE模型的设计比dense模型多出几个要考虑的点： \n \n（1）使用多少个expert  \n\n（2）怎么routing  \n\n（3）专家容量系数怎么定  \n\n（4）硬件的影响  \n\n（这里提到MoE模型的scaling law工作：《Unified scaling laws for routed language models》，可以了解一下）  \n\n1. 使用多少个expert  \n\nST-MoE认为，从以往的经验来看，在总专家数量较少的情况下（如8/16/32），提升专家数量，能有收益。但是在特别稀疏的情况下（如激活专家数量<1%），或者总专家数较大（比如>256）之后，提升专家数量收益就很小了。  \n\n从另一个角度来看，如果一个计算核心使用>1个专家，那么就会出现比较大的加载参数张量的成本，因此建议每个计算核心使用<=1个专家。  \n\n2. routing和capacity factor  \n\n论文做了一系列实验来探索capacity factor的选择，如下表所示  \n\n{% asset_img st_moe_capacity_factor.png capacity factor %}  \n\n从这些实验中得到几个结论：  \n\n（1）训练和推理的capacity factor增大都会有收益  \n\n（2）如果硬件资源足够，推理的capacity facotr可以设得比训练的时候大，会有进一步提升  \n \n（3）激活的expert数量提升会有收益，但是收益随着capacity factor提升而越来越小  \n\n当然，选择capacity factor还要看硬件的特性，如果通讯很快，可以适当增大capacity factor，否则就不能选择太大的。  \n\n下表展示了不同capacity factor对推理速度的影响  \n\n{% asset_img st_moe_capacity_factor_speed.png 不同capacity factor推理速度 %}  \n\n## 实验  \n\n1. ST-MoE效果  \n\nST-MoE-32B在下游任务上和以往最佳结果对比如下表，ST-MoE-32B刷新了超过一半任务的最佳效果  \n\n{% asset_img st_moe_perf.png 不同capacity ST-MoE-32B效果 %}  \n\n2. Expert Specialization  \n\n论文还对各个专家的专业化进行了追踪，发现decoder中几乎没有专业化的迹象，各种类型的token近乎随机分配给不同的专家。而在encoder中则表现出了高度专业化的特征，如下表  \n\n{% asset_img st_moe_encoder_specialization.png encoder专业化 %}  \n\n此外，还发现在多语言的模型的encoder中，专业化的情况并不想原先预想那样，按不同语言划分，而是每个专家都会处理一种语言的一部分token，如下表  \n\n{% asset_img st_moe_multiling_specialization.png 多语言专业化 %}  \n\n# DeepseekMoE\n\n2024年1月，幻方量化开源了DeepseekMoE，是国内首个开源的MoE大模型。幻方还发布了论文《DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models》，给出了一些DeepSeekMoE的细节内容，颇为实在了。  \n\nDeepSeekMoE在其他MoE工作的基础上，进一步给出了2个模型设计的主要思路：  \n\n（1）对expert的粒度进行细分，以提供更多样的expert激活组合；  \n\n（2）对expert的类型进行区分，从所有expert中保留一部分作为shared expert共享专家，这部分专家对所有输入都保持激活。  \n\n这样的做法可以帮助每个expert达到更高程度的专业化(specialization)的水平，更好地学习不同的专业知识。  \n\nDeepSeekMoE先在2B的较小MoE模型上进行了充分的实验，然后把方案应用到16B参数的MoE模型上，并获得了较好的效果。其中DeepSeekMoE-16B不需要量化就可以在40GB显存的设备上运行。  \n\nDeepSeekMoE-2B模型具有和稠密2B模型相当的性能，而DeepSeekMoE-16B则具有和7B稠密模型相当的性能，且计算量仅为稠密模型的40%。  \n\nDeepSeekMoE-16B的参数效率相比稠密模型有明显的优势，如下图所示  \n\n{% asset_img ds_moe_perf.png deepseek moe %}  \n\n并且DeepSeekMoE-2B和16B模型都开源了。  \n\n在前面实验的基础上，幻方还训练了DeepSeekMoE-145B的超大MoE模型，具有和稠密的DeepSeek-67B模型相当的表现，但计算量更小。这个后续也有机会放出来。  \n\n## 模型设计  \n\nMoE，mixture of expert，顾名思义，一个最初始的motivation就是让不同expert学习不同的内容，然后再混合起来。  \n\n比如最上面提到的1991年的工作里，就是让不同的expert学习不同的元音特征，以此提升特征提取的准确率。  \n\n但是当前大部分的MoE架构都会遇到“knowledge hybridity”和“knowledge redundancy”的问题，即知识的杂糅和冗余：  \n\n（1）知识冗余  \n\n有些基础的常识在不同的领域都需要用到，每个expert就都会学一点，这样这些常识就被多个expert重复学习了。  \n\n（2）知识杂糅  \n\n在expert数量不够多的情况下，一个expert就可能要负责学习多个领域的内容。以学习高中知识为例，在只有两个expert的时候，只能一个expert学习理科知识，另一个学习文科知识；当我们有8个expert的时候，不同expert就可以分别学习语文、英语、历史、地理、物理、生物、化学、数学知识。显然后者所学知识的专业化程度更高。  \n\n知识的杂糅和冗余阻碍了专家专业化(expert specialization)的程度，也就阻碍了模型达到MoE结构理论上限性能。  \n\n我们期望每个expert能够学习到non-overlap & foucusd knowledge的知识。  \n\n针对上面的问题，DeepSeekMoE的架构设计有2个主要策略：  \n\n（1）Fine-Grained Expert Segmentation  \n\n参数总量不变的情况下，将expert分成更细的粒度（每个expert更小）。这样可以带来更灵活的激活组合，让每个expert可以有更强的specialization。比如原本是16个expert选择激活2个，那么总的组合数是120种；如果把每个expert缩小为原来的1/4，那在总参数量和激活数量不变的情况下，是64个expert选择激活8个，那么总的排列组合数就是 $\\binom{64}8=4,426,165,368$ ，排列组合数比原来多了很多。   \n\n（2）Shared Expert Isolation  \n\n把部分expert分离出来，保持永远激活。我们期望这部分专家能够学到在多个领域间都通用的common knowledge。这样的策略同样可以使得其他expert能够提高专业化的程度，并且减少不同expert间的知识冗余。还是以学习高中知识为例，数学、物理和化学都需要算术能力，如果让学这三个领域的expert都学习算术技能，就会有冗余；我们可以把通用算术的技能剥离出来，由一个助手专门负责算术任务，相当于给他们发了一个计算器，这样学习数学、物理和化学的expert就能把更多的精力放在专业知识上，也就能达到更好的专业化效果。  \n\n下图展示了在传统MoE结构上增加Fine-Grained Expert Segmentation和Shared Expert Isolation策略的设计  \n\n{% asset_img ds_moe_structure.png deepseek moe 结构 %}  \n\n（expert isolation的思路最早可以追溯到2022年1月发表的《DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale》，这里就不展开了。）  \n\n假设传统的MoE模型每层的expert数量为 $N$，激活expert数为 $K$，DeepSeekMoE使用的细粒度expert大小为原来的 $1/m$，那DeepSeekMoE每层就有 $mN$ 个expert，激活的expert数量为 $mK$个。假设 $T$ 为输入长度，$L$ 为模型层数，$e_i^l$ 表示第 $i$ 个expert，DeepSeekMoE可以公式化为以下表示（忽略了layernorm）  \n\n$$\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}$$  \n\n$$\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{ FFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l$$  \n\n$$g_{i,t}=\\begin{cases}s_{i,t},&s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant j\\leqslant mN\\},mK)\\\\0,&\\text{otherwise,}\\end{cases}$$  \n\n$$s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)$$  \n\n## 负载均衡\n\n如之前工作反复提及的，如果任由MoE模型自主学习gating，可能会遇到两个问题  \n\n（1）routing collapse：专家分配的不均衡，也就是gating倾向于总是选择特定的少量expert，并且这种情况还会自我增强。  \n\n（2）计算效率问题：多设备间，不平衡的负载可能会成为计算效率的瓶颈。  \n\n针对routing collapse的问题，DeepSeekMoE引入一个expert-level balance loss，如下所示\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}& =\\alpha_1\\sum_{i=1}^{N'}f_iP_i\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nf_{i}& =\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i)\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nP_{i}& =\\frac1T\\sum_{t=1}^Ts_{i,t} \n\\end{aligned}$$  \n\n$\\alpha_1$ 叫做expert-level balance factor，是人工设定的超参。  \n\n而 $f_i$ 和 $P_i$ 和Switch Transformer里的设定基本一样。  \n\n在Switch Transformer里， $f_i$ 表示分配到第 $i$ 个expert的token数量。在DeepSeekMoE这里也是一样的含义，只是多乘了一个系数 $N'/K'$ ，其中 $N'=mN-K_s$，$K'=mK-K_s$，$K_s$ 是划分出来的共享expert的数量。这个系数是个常数，可以拿到求和符号外面，这样DeepSeekMoE里的 $f_i$ 就和Switch Transformer里的完全一样了。  \n\n$N'/K'$ 这个系数可以使得在使用不同的数量的expert时，在完美平均分配的情况下，负载均衡loss都是相同的常数。  \n\n$P_i$ 表示所有每个token分配给第 $i$ 个expert的权重的总和，和Switch Transformer里的含义一样。  \n\n注意这里 $f_i$ 是不可导的，$P_i$ 是可导的。  \n\n针对多设备间负载均衡的问题，DeepSeekMoE引入一个device-level balance loss，如下所示\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}& =\\alpha_2\\sum_{i=1}^Df_i'P_i'\n\\end{aligned}$$\n\n$$\\begin{aligned}\nf_i^{\\prime}& =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}$$\n\n$$\\begin{aligned}\nP_{i}^{\\prime}& =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}$$\n\n$\\alpha_2$ 叫做device-level balance factor，是人工设定的超参。  \n\n$\\mathcal{E}_i$ 指第 $i$ 个设备。\n\ndevice-level balance loss形式上和expert-level balance loss一样，只是 $f_i$ 和 $P_i$ 对应的对象从单个expert变成单个设备了。  \n\n当我们的目标是缓解计算瓶颈时，我们不需要强制执行expert间的均匀分配，而只需确保设备之间计算量的平衡。比如我们每层有64个expert，均匀分布在8个设备上，我们只需要每个设备处理的token数平衡即可，在设备内部即使所有token都是同一个expert处理的，依然能满足设备间负载平衡的要求。  \n\n相比expert间严格的负载平衡，只要求设备间的平衡是更松的限制条件，这样缓解了因为过度的负载平衡而损害模型性能的问题。\n\n## 实验\n\n1. 小规模模型验证\n\n为了验证以上策略的有效性，先拿100B token的语料数据在DeepSeekMoE-2B模型做实验。词表也是通过BPE在语料上训练的8k词表，后面训练更大规模模型的时候再扩大词表。\n\nDeepSeekMoE-2B模型参数初始化方差为0.006，使用multi-head attention，前向激活参数量约0.3B，具体参数如下表\n\n{% asset_img ds_model_param.png 模型超参 %}  \n\nrelative expert size指的是DeepSeekMoE所用的细粒度expert的大小和正常FFN层大小的比值。\n\n训练的具体参数设置如下  \n\n<center>\n\n| 属性 | 数值 |\n| :----: | :----: |\n| optimizer | AdamW |\n| adam_beta_1 | 0.9 |\n| adam_beta_2 | 0.95 |\n| adam_weight_decay | 0.1 |\n| warmup schedule | linear |\n| warmup step | 2000 |\n| max lr | 1.08e-3 |\n| dropout | 0 |\n| sequence length | 2k |\n| batch size | 2k |\n| total step | 25,000 |\n\n\n</center>  \n\n其他训练细节：  \n- 所有expert放在单个GPU上，没有使用device-level balance loss  \n- expert-level balance factor设为0.01  \n- 训练到80%的时候，学习率乘以0.316，训练到90%的时候，再乘以0.316  \n\n使用相同的100B训练数据，训了DeepSeekMoE-2B，在包含语言模型和下游任务的benchmark上和其他4个模型做对比：dense，hash layer（也是一种moe，《Hash layers for large sparse models》），Switch Transformer，GShard。效果对比如下\n\n{% asset_img ds_moe_comparison.png deepseek moe 效果%}  \n\n可以得到几个结论：  \n- 更大的模型参数量和稀疏的架构，使得Hash Layer和Switch Transformer和具有同样激活参数的dense模型相比，有明显的优势。  \n- 同样的模型参数下，GSshard比Hash Layer和Switch Transformer有更多激活参数，效果也更好  \n- 同样的模型参数和激活参数下，DeepSeekMoE效果比GShard有明显优势。  \n\n为了进一步探索DeepSeekMoE架构带来的收益，提升了dense模型和GShard模型的激活参数，直到效果和DeepSeekMoE-2B差不多。\n\n结果dense模型和GShard模型需要分别扩大到16倍和1.5倍的参数量，才能达到DeepSeekMoE-2B相近的效果，如下表所示  \n\n{% asset_img ds_moe_upper_bound_2b.png deepseek moe upper bound %}  \n\nDeepSeekMoE的优势在更大规模的情况下，依然成立。训了DeepSeekMoE-13B, 对比参数量提升至1.2和1.5倍的GShard，DeepSeekMoE-13B依然能match，具体如下表  \n\n{% asset_img ds_moe_upper_bound_13b.png deepseek moe upper bound %}  \n\n2. DeepSeekMoE架构消融实验\n\n针对DeepSeekMoE架构的两个主要设计，shared expert和fine-grained expert进行消融实验。使用不同数量的共享专家和不同粒度的expert进行效果对比，结果如下图。\n\n{% asset_img ds_moe_ablation.png deepseek moe upper bound 消融实验 %}  \n\n（1）对比蓝色和橙色，可以看到增加共享专家带来了收益\n\n（2）绿色和红色在橙色的基础上进一步把专家颗粒分得更细，效果进一步提升\n\n（3）共享专家和路由专家的比例：在总共64个expert的情况下，对比了1/2/4个共享专家的情况，结果并没有显著差别，在pile上的loss分别是1.808,1.806,1.811。最终选择了共享专家和激活路由专家1:3（2+6）的比例。\n\n3. expert specialization的分析\n\n通过实验来验证DeepSeekMoE中expert specialization的优化。\n\n（1）前面实验看到DeepSeekMoE-2B和1.5倍参数量的GShard模型效果相当。在这个基础上，通过禁用不同数量的top专家，而只能从次优的专家中选择进行回答。  \n\n实验结果如下\n\n{% asset_img ds_moe_expert_specialization.png 专家专门化 %}  \n\n发现DeepSeekMoE损失更大，说明DeepSeekMoE每个专家的专业化程度更好，必要性更高。  \n\n（2）另外，通过禁用DeepSeekMoE的共享专家，而额外激活一个专家，发现loss也大大提升。这个结果突出了共享专家的关键功能，并表明共享专家捕捉到了与路由专家不共享的基本且重要的知识，使得它无法被路由专家替代。\n\n（3）只激活更少专家，也能和GShard达到相同水平，这一观察结果支持了DeepSeekMoE可以更准确和高效地获取所需知识的观点。  \n\n{% asset_img ds_moe_less_activated_expert.png 激活更少专家 %}  \n\n此外还从零训了一个只用1个共享专家和3个激活专家的2b模型（正常是2个共享专家+6个激活专家），也比GShard好，说明DeepSeekMoE的有效参数效率更高\n\n{% asset_img ds_2b_less_expert.png 2B激活更少专家 %}  \n\n1. DeepSeekMoE-16B  \n\nDeepSeekMoE-16B模型使用了2T数据训练（和LLAMA2-7B对齐）训练，并使用了100k的词表。其他参数如下表所示  \n\n{% asset_img ds_model_param.png 模型超参 %}  \n\n论文中提到，除了第一层以外，其他层都使用了MoE层。  \n\n第一层不使用MoE是因为观察到第一层的负载均衡loss在训练中收敛得特别慢。  \n\nDeepSeekMoE-16B每层有64个专家，其中有2个作为共享专家保持永远激活，加上6个通过gating function选择激活的，每个token共使用8个专家。每个token会激活16.4B中的2.8B参数。  \n\n这里没有把专家的dimension再减小，是因为如果专家太小，计算效率就下降得太厉害。  \n\n训练中使用的其他设置：  \n- lr = 4.2e-4  \n- 训练进行到80%和90%的时候，lr都会缩小到0.316倍  \n- batch size = 4.5k，训练窗口长度是4k，因此每个batch有18M token，2T数据差不多是10.6w步  \n- 使用了pipeline parallelism\n\nexpert level balance loss的系数设得比较小，0.001，因为实验中发现设得再大并不能进一步优化负载平衡，反而会损害模型效果。  \n\nDeepSeekMoE-16B和DeepSeek-7B模型的对比如下  \n\n{% asset_img ds_16b_perf_1.png 和DeepSeek-7B对比 %}  \n\nDeepSeekMoE-16B和LLAMA2-7B模型的对比如下  \n\n{% asset_img ds_16b_perf_2.png 和LLAMA2-7B对比 %}  \n\n5. DeepSeekMoE-145B  \n\n幻方还用245B的token训练了DeepSeekMoE-145B，模型效果上达到DeepSeek-67B的同等水平  \n\n{% asset_img ds_moe_145b.png 145b %}  \n\n# DBRX\n\n2024年3月27日，Databricks开源了DBRX，一个拥有有132B参数，激活参数为36B的MoE模型。\n\n结构上，DBRX使用了RoPE、GLU、GQA，采用了fine-grained expert的设计，每层有16个专家，每个token激活其中4个。相比Mixtral和Grok-1在8个专家中激活2个，DBRX有更多的专家组合方式。  \n\nDBRX训练的上下文长度为32k，并使用了12T文本和代码token进行训练。DBRX在3072个H100上完成预训练，加上post-training、效果评估、red-team优化，整个过程耗费3个月时间。  \n\nDBRX整体效果超过GPT-3.5，与Gemini 1.0 Pro相当，并且具有比较强的代码能力，甚至超过了在代码上专门优化过的模型，如CodeLLaMA-70B，如下图所示。  \n\n{% asset_img dbrx_perf.png DBRX效果 %}  \n\n推理效率效率上，DBRX也领先于其他模型。  \n\n{% asset_img dbrx_infer_efficiency.png 推理效率 %}  \n\n# Qwen1.5-MoE \n\n2024年3月28日，阿里放出了Qwen1.5-MoE-A2.7B，以2.7B的模型参数，达到了Qwen1.5-7B模型的相近效果。  \n\nQwen1.5-MoE-A2.7B参考了DeepSeekMoE和DBRX的工作，采用了fine-grained expert的做法，总共有64个专家，每个token激活8个专家，其中有4个为共享专家。  \n\nQwen1.5-MoE-A2.7B使用Qwen-1.8B进行初始化，并在初始化阶段引入随机性，这样可以显著加快收敛速度，并得到更好的收敛结果。  \n\nQwen1.5-MoE-A2.7B和其他模型效果对比如下  \n\n{% asset_img qwen1.5_moe_perf.png Qwen1.5-MoE-A2.7B效果 %}  \n\n虽然Qwen1.5-MoE-A2.7B总参数量较大，但激活的non-embedding参数量远小于7B模型，如下表所示  \n\n{% asset_img qwen1.5_moe_params.png Qwen1.5-MoE-A2.7B参数量 %}  \n\n实践中，Qwen1.5-MoE-A2.7B相比于Qwen1.5-7B，训练成本降低了75%。  \n\n推理性能上，在A100-80G用vLLM部署Qwen1.5-7B和Qwen1.5-MoE-A2.7B模型进行了性能测试。  \n\n输入/输出token数都设置为1000，输出token数设置为1000，TPS和throughput如下  \n\n{% asset_img qwen1.5_moe_tps.png Qwen1.5-MoE-A2.7B TPS %}  \n\n虽然MoE模型对内存需求更大，但是由于稀疏激活以及共享专家的设计，但是在速度和吞吐量上都比dense模型更好。Qwen1.5-MoE-A2.7B与Qwen1.5-7B相比，速度提高了约1.74倍。  \n\n# Mistral\n\n## Mistral 8x7B\n\n2023年12月11日，Mistral AI开源Mistral-8x7B，每个token激活8个专家中的2个。  \n\nMistral-8x7B支持32k推理窗口和多语言，并且代码能力较好。和LLAM2-70B以及GPT-3.5的对比如下。  \n\n{% asset_img mistral_8_7b_perf.png Mistral 8x7B效果 %}  \n\nMistral-8x7B在大多数任务表现优于LLAM2-70B，且推理速度提高了6倍。  \n\n而和激活参数量相近的LLAM2-13B比，优势更为明显  \n\n{% asset_img mistral_8_7b_active_perf.png Mistral 8x7B同样激活参数量下效果 %}  \n\n## Mistral 8x22B\n\n2024年4月17日，Mistral AI开源Mistral-8x22B模型，一个总参数为141B，激活参数为39B的超大MoE模型。  \n\nMistral-8x22B支持多语言，并且具有较强的数学和代码能力。此外，推理窗口长度也从Mistral-8x7B的32k增加到64k。Mistral-8x22B还具备function call的能力。  \n\n在各个维度的评测结果如下  \n\n{% asset_img mistral_8_22b_reasoning.png Mistral 8x22B reasoning效果 %}  \n\n{% asset_img mistral_8_22b_multiling.png Mistral 8x22B 多语言效果 %}  \n\n{% asset_img mistral_8_22b_code.png Mistral 8x22B 代码与数学效果 %}  \n\n# 小结  \n\n- 现有的工作都表明，MoE模型相比dense模型具有更高的参数效率，即同样的计算量下，MoE模型普遍能有更优的效果  \n- 因此MoE不仅能支持更大规模模型的训练，在较小规模模型上使用MoE架构也有很大收益  \n- 但是相比dense模型，MoE模型的训练也需要考虑更多内容，包括专家数量、激活数量和专家容量的设计，负载均衡的问题，如何在多设备上的并行等，训练难度更大  \n- 结构上，共享专家和细粒度专家目前被验证效果较好  \n- 负载均衡上，GShard和Switch Transformer的负载均衡损失被广泛采用  \n- 推理时需要对底层框架进行优化以适配MoE机制，否则难以发挥MoE的性能优势  \n\n***\n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n【往期文章】\n\n[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  \n[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  \n[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  \n[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  \n[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  \n[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  \n[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  \n[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  \n[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n# Reference  \n【1】Adaptive Mixtures of Local Experts https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf  \n【2】Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer https://arxiv.org/abs/1701.06538  \n【3】GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding https://arxiv.org/abs/2006.16668  \n【4】Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961  \n【5】GLaM: Efficient Scaling of Language Models with Mixture-of-Experts https://arxiv.org/abs/2112.06905  \n【6】ST-MoE: Designing Stable and Transferable Sparse Expert Models https://arxiv.org/abs/2202.08906  \n【7】DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models https://arxiv.org/abs/2401.06066  \n【8】Introducing DBRX: A New State-of-the-Art Open LLM https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm  \n【9】Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters https://qwenlm.github.io/zh/blog/qwen-moe/  \n","slug":"cs/nlp/2024/03/混合专家MoE-基础篇","published":1,"updated":"2024-05-10T06:51:36.425Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4ka9009s314k3xq4e1sm","content":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>2024年3、4月这段时间，很多MoE模型扎堆发布，包括Qwen1.5-MoE、DBRX、Jamba和Mistral等。</p>\n<p>下面这个表格列出了部分近期发布的MoE工作</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\">模型</th>\n<th style=\"text-align: center;\">发布时间</th>\n<th style=\"text-align: center;\">备注</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">GPT4</td>\n<td style=\"text-align: center;\">2023年3月</td>\n<td style=\"text-align: center;\">23年6月George\nHotz爆料GPT4是8×220B模型</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Mistral-8×7B</td>\n<td style=\"text-align: center;\">2023年12月</td>\n<td style=\"text-align: center;\">Mistral AI，开源</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">LLAMA-MoE</td>\n<td style=\"text-align: center;\">2023年12月</td>\n<td style=\"text-align: center;\">github开源项目</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">DeepSeek-MoE</td>\n<td style=\"text-align: center;\">2024年1月</td>\n<td style=\"text-align: center;\">幻方量化，国内首个开源MoE模型，有技术报告</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">abab6</td>\n<td style=\"text-align: center;\">2024年1月</td>\n<td style=\"text-align: center;\">MiniMax，号称千亿MoE，无开源，无细节发布</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">天工2.0</td>\n<td style=\"text-align: center;\">2024年2月</td>\n<td style=\"text-align: center;\">昆仑万维，无开源，无细节发布</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Step-2</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">阶跃星辰，无开源，无细节发布</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">MM1</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">苹果，多模态MoE，无开源，有技术报告</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Grok-1</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">X，开源</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Qwen1.5-MoE-A2.7B</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">阿里巴巴，开源</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">DBRX</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">Databricks，开源</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Jamba</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">AI21，开源</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Mistral-8×22B</td>\n<td style=\"text-align: center;\">2024年4月</td>\n<td style=\"text-align: center;\">Mistral AI，开源</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">WizardLM-2-8×22B</td>\n<td style=\"text-align: center;\">2024年4月</td>\n<td style=\"text-align: center;\">微软，开源</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">天工3.0</td>\n<td style=\"text-align: center;\">2024年4月</td>\n<td style=\"text-align: center;\">昆仑万维，400BMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Arctic</td>\n<td style=\"text-align: center;\">2024年4月</td>\n<td style=\"text-align: center;\">Snowflake，480B，Dense-MoE\nHybrid，开源</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>MoE模型目前风头正劲，就连前不久小米汽车发布会上，雷总也弄了个多模态MoE大模型做汽车智能中控</p>\n<img src=\"/44e38c1b/xiaomi_moe.jpg\" class title=\"小米汽车多模态MoE模型\">\n<p>相信今年接下来的这段时间，MoE还会给我们带来更多的大新闻。</p>\n<p>本篇将初步梳理MoE相关的一些经典工作和几个近期发布的中文MoE模型，从背景、思路和效果来了解MoE模型。</p>\n<p>到文章发出的2024年4月为止，个人认为DeepSeek-MoE和Qwen1.5-MoE是中文领域做得比较好的两个工作，赶时间的朋友可以优先关注这两个工作。</p>\n<h1 id=\"时间线\">时间线</h1>\n<p>这里先对后面会涉及的MoE相关工作，大致按时间线梳理一下，也列出一些关键信息包括模型结构、模型规模等。</p>\n<p>（很多经典的MoE工作都出自Google）</p>\n<h2 id=\"上古时代\">上古时代</h2>\n<p>首先是很多MoE相关论文都会引用的，发表在1991年的论文<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">《Adaptive\nMixtures of Local Experts》</a>，这篇文章出自Geoffrey Hinton和Michael I.\nJordan两位大神之手。虽然在更早的时候就有MoE相关概念的工作，如原文所提到的，1988年这个概念就有了</p>\n<blockquote>\n<p>This idea was first presented by Jacobs and Hinton at the\nConnectionist Summer School in Pittsburg in 1988.</p>\n</blockquote>\n<p>但是大部分MoE文章还是认为是这个工作奠定了MoE的基础。</p>\n<h2 id=\"rnn时代\">RNN时代</h2>\n<p>时隔二十多年，Google在2017年1月发布了<a href=\"https://arxiv.org/abs/1701.06538\">《Outrageously Large Neural\nNetworks: The Sparsely-Gated Mixture-of-Experts\nLayer》</a>，把MoE带进了LSTM，训出了最大137B参数，专家数达到128k的LSTM模型。</p>\n<h2 id=\"transformer时代\">Transformer时代</h2>\n<ol type=\"1\">\n<li><p>2020年6月，Google发布<a href=\"https://arxiv.org/abs/2006.16668\">《GShard: Scaling Giant Models\nwith Conditional Computation and Automatic\nSharding》</a>，把MoE应用在encoder-decoder结构的transformer模型上，每两层将一个FFN层替换成一个MoE层，训出了模型参数量从12.5B到600B的一系列MoE模型，每层最大专家数也达到2048个。</p></li>\n<li><p>2021年1月，Google发布<a href=\"https://arxiv.org/abs/2101.03961\">《Switch Transformers: Scaling\nto Trillion Parameter Models with Simple and Efficient Sparsity》</a>\n，在T5（encoder-decoder结构）的基础上，把FFN层替换成MoE层，并简化了routing策略，训出了最大1.6T参数量的switch\ntransformer。Switch\nTransformers对scaling、蒸馏等做了很多详细的探索，影响深远，是很重要的一个工作。</p></li>\n<li><p>2022年2月，Google发布<a href=\"https://arxiv.org/abs/2202.08906\">《ST-MoE: Designing Stable and\nTransferable Sparse Expert\nModels》</a>，也是一个基于encoder-decoder结构的MoE模型，最大模型有269B的总参数，32B的激活参数。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，个人认为其重要程度相比Switch\nTransformer都有过之而无不及。</p></li>\n</ol>\n<h2 id=\"gpt时代\">GPT时代</h2>\n<ol type=\"1\">\n<li><p>2021年12月，Google发布了GLaM，<a href=\"https://arxiv.org/abs/2112.06905\">《GLaM: Efficient Scaling of\nLanguage Models with\nMixture-of-Experts》</a>，训出了最大为1.2T参数量的decoder-only模型。（从encoder-decoder到decoder-only，可以看到Google内部在模型结构方向上也有很多不同的尝试）</p></li>\n<li><p>2024年1月，幻方量化发布<a href=\"https://arxiv.org/abs/2401.06066\">《DeepSeekMoE: Towards Ultimate\nExpert Specialization in Mixture-of-Experts Language\nModels》</a>，对在23年12月开源的DeepSeekMoE，给出了一些细节。</p></li>\n<li><p>2024年，Databricks的DBRX、阿里的Qwen1.5-MoE-A2.7B、Mistral\nAI的Mistral-8x22B等陆续发布。</p></li>\n</ol>\n<h1 id=\"奠基工作\">奠基工作</h1>\n<p>Geoffrey Hinton和Michael I. Jordan的<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">《Adaptive\nMixtures of Local Experts》</a>是大多数MoE论文都会引用的最早工作。</p>\n<ol type=\"1\">\n<li>思路</li>\n</ol>\n<p>这篇文章大致的思路是这样的：对于比较复杂的任务，一般可以拆分为多个子任务。比如要求计算输入文本中有多少个动词和名词，那就可以拆分为“数动词”和“数名词”这两个子任务。</p>\n<p>而一个模型如果要同时学习多个子任务，多个子任务相互之间就会互相影响，模型的学习就会比较缓慢、困难，最终的学习效果也不好。</p>\n<p>因此这篇文章提出了一种由多个分开的子网络组成的监督学习方法。这些分开的网络，在训练过程中，分别学习处理整个训练数据集中的一个子集，也就是一个子任务。这个思路就是现代MoE的思路，每个子网络（也就是一个expert）学习处理一部分内容。</p>\n<p>文章里把这个MoE的方法应用于vowel discrimination\ntask，即元音辨别任务，验证了MoE设计的有效性。元音辨别指的是语音学中区分不同元音的能力，在语音学中，模型需要学习辨别不同的元音因素，以便准确地理解和识别语音输入。通过让多个子模型分别学习分别学习不同元音（a、e、i、o、u）辨别的子任务，最终效果得到了提升。</p>\n<ol start=\"2\" type=\"1\">\n<li>模型设计</li>\n</ol>\n<p>下图展示的就是这个MoE的思路：各个expert network和gating\nnetwork接收同样的输入，每个expert给出各自的处理结果；而gating\nnetwork输出每个expert的权重，就像一个开关一样，控制着每个expert对当前输入的打开程度，只是这个开关不是离散的，而是stochastic的，给出的不是true和false，而是权重。</p>\n<img src=\"/44e38c1b/vanilla_moe.png\" class title=\"Vanilla MoE\">\n<ol start=\"3\" type=\"1\">\n<li>损失函数优化</li>\n</ol>\n<p>实际上，MoE这个idea在这篇文章之前就有了。如论文中所提，Jacobs和Hinton在1988就讨论过。但是之前的工作在loss的设计上，和ensemble更相近，多个expert之间更倾向于合作，每个expert会学习其他expert的residual部分。</p>\n<p>具体来说，对于case <span class=\"math inline\">\\(c\\)</span>，假设第\n<span class=\"math inline\">\\(d^c\\)</span> 是对应的ground truth，第 <span class=\"math inline\">\\(i\\)</span> 个expert的输出是 <span class=\"math inline\">\\(o_{i}^c\\)</span>，<span class=\"math inline\">\\(p_{i}^c\\)</span> 是gating network给第 <span class=\"math inline\">\\(i\\)</span>\n个expert分配的权重，那么以前的工作所使用的损失函数 <span class=\"math inline\">\\(E^{c}\\)</span> 计算如下</p>\n<p><span class=\"math display\">\\[E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>这样的损失计算方式，是把期望输出和所有expert输出的混合结果进行比较。</p>\n<p>这样做的结果是，在训练过程中，每个expert学习的其实是其他expert的组合结果所剩下的残差。这样的学习目标并不能很好迫使每个expert单独输出好的结果，因此不能得到稀疏的模型。</p>\n<p>从另一个角度来看，这个损失计算把所有专家耦合在了一起。即当一个expert的输出发生了变化，所有expert的组合结果也会变化，其他所有的expert也需要做相应的改动来适应这个变化。因此各个expert之间更加倾向于合作，而不是相互竞争并单独给出好的结果，让gating\nnetwork输出稀疏的结果。</p>\n<p>虽然可以使用如增加辅助损失函数的做法，迫使模型给出稀疏激活的结果，但是这样相当于增加了很强的先验正则化，对模型最终效果也是有损害的。</p>\n<p>而Hinton和Jordan在这个工作里，提出更简单的做法是对loss计算进行修改，使得各个expert之间的关系从合作变成竞争。</p>\n<p>假设gating network每次随机选择一个expert，损失计算如下</p>\n<p><span class=\"math display\">\\[E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>在这个损失函数中，每个expert的输出结果会单独和期望结果进行对比，这就要求每个expert单独给出完整的结果，而不是仅学习其他expert的残差。</p>\n<p>这样的loss计算具有localization的特性，即如果一个训练case错了，那么会被修改的主要是被gating\nnetwork选中且出错的expert，以及负责分配权重的gating\nnetwork，而不会很大地影响其他expert。</p>\n<p>此外，localization还体现在，每个expert只会负责处理输入空间中某个特定子空间的向量，而不是完整的输入空间。</p>\n<p>这样一来，不同的expert之间不会直接相互影响，虽然还是有间接的影响，比如某个expert的输出变了，gating\nnetwork可能会分配新的权重，但是至少不会改变其他expert\nerror的符号（+，-），即优化的方向。</p>\n<p>最终的结果是，对于给定的输入，这样的系统会倾向于以高权重分配单一一个expert来预测结果（但其他权重还不是真正的0，不是真正的稀疏）。</p>\n<ol start=\"4\" type=\"1\">\n<li>实操技巧</li>\n</ol>\n<p>上面提出的这个loss计算，理论上没有问题，实际上也能训练，但是为了得到更好的效果，作者把原loss计算作了如下变化：先指数化再求和，最后再取对数，得到了优化loss。看下变化前后的对比</p>\n<p><span class=\"math display\">\\[\\text{原loss：}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p><span class=\"math display\">\\[\\text{优化loss：}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}\\]</span></p>\n<p>这样做有什么好处呢？来对比一下原loss函数和优化后的loss函数的求导结果</p>\n<p><span class=\"math display\">\\[\\text{原loss导数：}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p><span class=\"math display\">\\[\\text{优化loss导数：}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p>相比原loss函数的导数，优化后的loss函数的导数，把当前第 <span class=\"math inline\">\\(i\\)</span>\n个expert的表现，和其他expert联系起来了。这样能够更好地衡量expert <span class=\"math inline\">\\(i\\)</span>\n对当前case的处理结果好坏。特别是在训练初期，gating\nnetwork的权重是近似平均分配的，那么使用原loss函数的结果是，对当前case效果最好的expert，学习速度是最慢的（因为loss最小）；而优化的loss函数则可以让当前最好的expert的学习速度最快。相当于让“有天赋”的专家在对应的子任务上尽快提高水平。这样就强化了localization的特征，使得各个expert更快拟合到自己擅长的部分，加速训练。</p>\n<p>（BTW，优化后的这个loss导数，和现在的对比学习形式上看起来也很相似）</p>\n<p>这个工作在今天看来不很复杂，但是思路还是很踏实有效的，给MoE奠定了基础。</p>\n<h1 id=\"lstm-moe\">LSTM MoE</h1>\n<p>Google在2017年1月发布了 <a href=\"https://arxiv.org/abs/1701.06538\">《OUTRAGEOUSLY LARGE NEURAL\nNETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS\nLAYER》</a>，把MoE应用到了LSTM上，训出了最大137B的LSTM模型。这样规模的模型哪怕放在7年后的今天，也是巨无霸的存在，需要解决很多工程问题。</p>\n<p>相比1991年的工作，这里做到了真正的稀疏激活，从而可以在实际计算量较少的情况下，训练巨大的模型。</p>\n<h2 id=\"背景\">背景</h2>\n<p>虽然当时Transformer还没出来，大规模模型的竞赛也还不像今天这么激烈，但是在多个领域中（文本、图像、音频），已经有不少工作反复证实了一件事：模型容量越大，能训出来的效果越好，上限越高。但是模型越大，需要的训练数据也就越多，二者共同作用下，就造成了训练开销基本是随着模型增大，以平方关系在增长。</p>\n<p>在这个背景下就出现一些conditional\ncomputation，条件计算的工作来解决这个问题。conditional\ncomputation就是根据输入，有选择地只激活部分网络模块。那么MoE其实就是一种条件计算的实现。由于不用激活全部参数，训练所需的计算量就大大减小，整体计算成本就不用以平方速度增长。</p>\n<p>虽然理论上计算量的成本下来了，不过实操起来还是会遇到几个问题：</p>\n<ul>\n<li>训练的时候，在MoE结构下，每个expert的batch size比整个模型的batch\nsize小了。<br>\n比如模型的batch\nsize是32，一共有16个expert，那实际上一次迭代平均每个expert只能分到2个训练样本。而batch\nsize对训练效率影响是很大的，大的batch\nsize摊小了参数传输和更新的成本。如果直接增大模型的batch\nsize，又会受显存和通讯效率的限制。<br>\n</li>\n<li>训练数据量不足。<br>\n要训大模型就需要大量的数据，让模型参数充分学习。在当时的背景下，大规模的NLP数据是比较缺的。当然如今数据集多了很多，特别是预训练数据，这个问题现在来看没有那么突出了。<br>\n</li>\n<li>损失函数的设计。<br>\n如何使用合适的损失函数来训练模型，提升效果，并且使得模型的负载比较均衡，这是一个不容易解决的问题。<br>\n</li>\n<li>集群通讯问题。<br>\n一个GPU集群的计算能力可能比设备间网络带宽的总和高出数千倍，因此设备间的通讯很可能成为训练效率的瓶颈。为了计算效率，就要使得设备内计算量和所需的通讯量的比值，达到相应的比例。<br>\n</li>\n<li>GPU计算特点。<br>\nGPU做数学计算很快，但是并不擅长做branching（if/else），因此MoE的工作基本上都是用gating\nnetwork来控制参数的激活。这个严格来说不算是新的挑战了，应该说是根据计算设备沿用下来的设计。</li>\n</ul>\n<p>要解决好这些问题，才能训出比较好的模型来。</p>\n<h2 id=\"模型设计\">模型设计</h2>\n<ol type=\"1\">\n<li>整体结构</li>\n</ol>\n<p>先看下模型结构的设计。</p>\n<p>论文里使用的是两个LSTM层，中间夹着一个MoE层，最上面和最下面分别还有一个embedding层和一个任务输出层，结构如下图所示</p>\n<img src=\"/44e38c1b/rnn_moe.png\" class title=\"LSTM MoE\">\n<p>每个expert是一个简单的feed-forward neural\nnetwork。一共有n个expert，gating network输出是一个稀疏的n维向量</p>\n<p><span class=\"math display\">\\[\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(E_{i}(x)\\)</span> 是第 <span class=\"math inline\">\\(i\\)</span> 个expert的输出，<span class=\"math inline\">\\(G(x)_{i}\\)</span> 是gating network给出的第 <span class=\"math inline\">\\(i\\)</span> 个expert的权重。</p>\n<p>如果 <span class=\"math inline\">\\(G(x)_{i}\\)</span>\n为0，就不用计算对应的那个expert了，节省了计算。</p>\n<p>如果expert的数量特别多，可以用two-level hierarchical\nMoE，即使用两层gating network，第一层的gating\nnetwork先选择一个包含一批expert的分支，每个分支又有一个单独的gating\nnetwork来选择具体的expert。类似word2vec训练所用的hierarchical\nsoftmax。这样做可以节省一些计算。</p>\n<ol start=\"2\" type=\"1\">\n<li>gating network</li>\n</ol>\n<p>那具体gating network怎么设计呢？</p>\n<p>如果对输入进行线性变换，再简单加上一个softmax，那得到的是一个非稀疏的gating\nfunction</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot\nW_g)\\end{aligned}\\]</span></p>\n<p>在这个基础上，使用一个topk函数，只保留最大的k个值，其他都设为﹣∞（softmax之后变成0），这样就能只选择部分expert，得到了稀疏性。</p>\n<p>论文提到，虽然理论上这个形式的sparsity（topk）会造成gating\nfunction的不连续，不过在实操中暂时没有遇到相关问题。</p>\n<p>在这个基础上，在输入再加上一个Gaussian\nnoise，这个noise的大小由另外一个可学习的参数来控制。整体的计算公式如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[KeepTopK(v,k)_i=\\begin{cases}v_i&amp;\\text{if\n}v_i\\text{ is in the top }k\\text{ elements of\n}v.\\\\-\\infty&amp;\\text{otherwise.}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p>其中用来调整noise的非线性函数softplus是个类似ReLU的激活函数，但是更为光滑，函数图像如下</p>\n<img src=\"/44e38c1b/softplus.png\" class title=\"softplus\">\n<p>这里添加噪声的原因和负载均衡有关，下面来分析下负载均衡。</p>\n<h2 id=\"负载均衡\">负载均衡</h2>\n<p>在MoE模型训练的实验中观察到，如果不对gating\nnetwork进行干预，任由模型自由学习，那么最终模型会倾向于收敛到“总是选那几个固定的expert”的状态，而其他expert几乎不会被使用。这就是负载不均衡的状态，如果这些专家分布在不同的计算设备上，结果就是有些设备输入排队特别长，而有些设备基本处于闲置状态，这明显不是我们想要的。</p>\n<p>这种负载不均衡的状态有自我加强的属性，因为一旦开始出现部分专家被较多选中激活，这些专家就会得到更充分的训练，从而获得更好的效果，进而又提升被选中激活的概率。</p>\n<p>针对这种情况，之前有一些工作使用hard\nconstraint来缓解，比如当某个expert激活次数达到上限，就把它从候选集合中移除。hard\nconstraint明显会对模型效果有影响。而这篇论文使用的是一种soft\nconstraint。</p>\n<p>具体来说，对于每个expert，定义了一个它在当前这批输入数据里的重要性指标，如以下公式所示</p>\n<p><span class=\"math display\">\\[Importance(X)=\\sum_{x\\in\nX}G(x)\\]</span></p>\n<p><span class=\"math inline\">\\(G(x)\\)</span> 是gating\nnetwork给出的权重，是一个维度等于expert数量的向量。</p>\n<p>基于这个重要性指标，论文定义了一个辅助损失 <span class=\"math inline\">\\(L_{importance}\\)</span>，训练时和模型的交叉熵损失加到一起。<span class=\"math inline\">\\(L_{importance}\\)</span> 的计算方式如下</p>\n<p><span class=\"math display\">\\[L_{importance}(X)=w_{importance}\\cdot\nCV(Importance(X))^2\\]</span></p>\n<p>其中权重 <span class=\"math inline\">\\(w_{importance}\\)</span>\n是手动设置的超参，实验的推荐值是0.1，CV是coefficient of variation。</p>\n<p>coefficient of\nvariation离散系数，是概率分布离散程度的一个归一化量度，定义为标准差\n<span class=\"math inline\">\\(\\sigma\\)</span> 和 均值 <span class=\"math inline\">\\(\\mu\\)</span> 的比值。</p>\n<p>对于MoE来说，确定激活的expert数之后，均值是固定的。如果expert的gating很不平衡，标准差就会很大，离散系数也会很大，使得\n<span class=\"math inline\">\\(L_{importance}\\)</span> 变大。</p>\n<p>但是这里还是有问题，虽然均衡的负载可以推导出 <span class=\"math inline\">\\(L_{importance}\\)</span> 较小的结论，但是 <span class=\"math inline\">\\(L_{importance}\\)</span>\n较小却不能保证负载均衡。也就是说 <span class=\"math inline\">\\(L_{importance}\\)</span>\n较小只是负载均衡一个必要不充分条件。</p>\n<p>比如一个expert可能以很高的权重被分配到一个样本，而另一个expert可能以不太高的权重被分配到好几个样本。这种情况下对所有输入数据的gating权重进行求和，仍然可能呈现出均匀的表象（离散系数比较小），但这并不符合我们的要求。</p>\n<p>为了解决这个问题，需要额外再加上一个损失 <span class=\"math inline\">\\(L_{load}\\)</span>\n。这里就要用到添加在每个expert输出上的随机噪音了。</p>\n<p>我们想要各个expert的负载均衡，也就是每个专家需要处理的样本数基本一致，但是分配到各个专家的样本数是个离散值，因此没有办法直接用于back\npropagation，而 <span class=\"math inline\">\\(L_{load}\\)</span>\n就是对各个expert负载的一个平滑评估。</p>\n<p>回想一下前面在设计MoE的时候，定义了 <span class=\"math inline\">\\(H(x)\\)</span> 为KeepTopK函数的输入</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p>那么这里先定义一个 <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>，表示在除去\n<span class=\"math inline\">\\(H(x)\\)</span> 中的第 <span class=\"math inline\">\\(i\\)</span> 个分量之后，排在第 <span class=\"math inline\">\\(k\\)</span> 大的值。基于这个，再定义 <span class=\"math inline\">\\(P(x,i)\\)</span>\n为：固定其他分量已经选取好的noise，重新给第 <span class=\"math inline\">\\(i\\)</span> 个分量再添加一次noise，结果比 <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n大的概率，公式如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\\\&gt;kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}\\]</span></p>\n<p>通过这个noise，我们把“第 <span class=\"math inline\">\\(i\\)</span>\n个专家是否处理这个输入”的离散值，变成“第 <span class=\"math inline\">\\(i\\)</span>\n个专家处理这个输入的概率”这样一个平滑的估计，<span class=\"math inline\">\\(P(x,i)\\)</span>\n就表示这个概率。这个概率可以简化写成</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)&amp;=\\Phi\\Big(\\frac{(x\\cdot\nW_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot\nW_{noise})_i)}\\Big)\\end{aligned}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\Phi\\)</span>\n是标准正态分布的CDF。</p>\n<p>接下来就可以把第 <span class=\"math inline\">\\(i\\)</span>\n个expert的负载定义为</p>\n<p><span class=\"math display\">\\[\\begin{aligned}Load(X)_i=\\sum_{x\\in\nX}P(x,i)\\end{aligned}\\]</span></p>\n<p>有了每个expert的负载衡量，就可以和前面第一个负载均衡损失一样，计算新的负载均衡损失了</p>\n<p><span class=\"math display\">\\[L_{load}(X)=w_{load}\\cdot\nCV(Load(X))^2\\]</span></p>\n<p><span class=\"math inline\">\\(w_{load}\\)</span>\n是手动设置的超参，实验的推荐值是0.1。</p>\n<p>相比前面的 <span class=\"math inline\">\\(L_{importance}(X)\\)</span>，<span class=\"math inline\">\\(Load(X)\\)</span>\n是对负载是否均衡更细粒度的评估。</p>\n<p>论文中提到一个细节，在刚开始训练的时候，希望模型分配的expert尽量均衡，因此把\n<span class=\"math inline\">\\(W_g\\)</span> 和 <span class=\"math inline\">\\(W_{noise}\\)</span>\n都设为0，这样相当于没有信号，也没有噪音。</p>\n<p>最终使用负载均衡之后的效果如下</p>\n<img src=\"/44e38c1b/rnn_moe_load_function.png\" class title=\"负载平衡效果\">\n<p>使用这两个负载均衡损失之后，能达到接近完全平均分配的效果。</p>\n<h2 id=\"实验\">实验</h2>\n<ol type=\"1\">\n<li>解决工程问题</li>\n</ol>\n<p>针对前面提出的一些工程问题，论文给出一些方案</p>\n<p>（1）batch size减小</p>\n<p>由于稀疏激活的原因，每个expert的batch\nsize会变小。假设每次在n个expert中选择k个，模型训练的batch\nsize为b，那么每个expert的batch\nsize就是kb/n。论文通过以下这几种方法来提升每个expert的batch size：<br>\n-\n混合使用数据并行和模型并行。本来在使用数据并行的情况下，每个模型副本是异步处理各自的数据的。而这里做了优化，各个副本的batch是同步处理的，这样就可以把多个模型副本的batch组合起来。对于非MoE部分的参数，依然使用标准的数据并行机制；而对于每个expert，则在整个集群中只保留一个副本。如果模型分布在d个设备上，那每个expert就能得到一个kbd/n的batch\nsize。 - 对于LSTM模型，在时间步上展开，就能把batch\nsize提升相应的倍数。</p>\n<p>（2）集群通讯问题</p>\n<p>另一个挑战就是平衡集群计算量和通讯量的关系。</p>\n<p>对于每个expert来说，主要的通讯就是input和output的传输。而每个专家的主要计算量就是两个全连接层，大小分别为[input_size,\nhidden_size]和[hidden_size,\noutput_size]。对于GPU来说，计算速度可能是通讯速度的1000倍，那我们就需要把计算量设计得足够大。最简单的做法就是把hidden_size提高，使得每个expert的内部计算量比通讯量大1000倍，以保证通讯不会成为训练的瓶颈。</p>\n<ol start=\"2\" type=\"1\">\n<li>模型容量 &amp; 参数效率</li>\n</ol>\n<p>为了验证模型容量提升带来的收益，以及MoE模型的参数效率（即和dense模型同样推理计算量下能达到的效果），训练了包含4/32/256个expert的flat\nMoE模型，和包含256/1024/4096个expert的hierarchical\nMoE模型。每个expert大约是1M参数量，对于所有flat模型都是激活4个expert，而对于hierarchical\nMoE是每层gating激活2个。</p>\n<p>效果如下图。左边的图显示，随着模型容量提升，测试的ppl有明显下降。右边的图将相近模型容量的dense模型和MoE模型的效果放在一起对比，可以看到MoE模型在相同模型容量下，效果更好</p>\n<img src=\"/44e38c1b/rnn_moe_perf.png\" class title=\"效果\">\n<ol start=\"3\" type=\"1\">\n<li>更大的模型</li>\n</ol>\n<p>前面几个模型训练用的数据量不是很大，模型最大也只有4B左右，训练不久就出现diminishing\nreturns。</p>\n<p>为了验证更大数据集 + 更大模型的收益，在100B\ntoken的语料上，分别训了包含32, 256, 1024，4096, 16384, 65536,\n和131072个expert的MoE模型，最大的模型达到了137B的参数量。</p>\n<p>各个模型对比如下表。整体来看，增加数据和模型容量，是可以继续获得提升的。</p>\n<img src=\"/44e38c1b/rnn_moe_137b.png\" class title=\"137模型效果\">\n<p>从这里还可以看出，在专家数量不太多时，提升专家数量效果有提升，但是收益会慢慢减小，甚至会出现专家数量太多，效果反而下降的情况。</p>\n<ol start=\"4\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>按照MoE的设计思路，不同的专家应该学习到不同的子任务，但是实际上是否是这样呢？</p>\n<p>论文里把模型中不同的专家分配到token拿出看，发现确实有比较强的specialization效果，不同的专家处理不同的内容，如下所示</p>\n<img src=\"/44e38c1b/rnn_moe_specilized.png\" class title=\"RNN MoE 专门化\">\n<h1 id=\"gshard\">GShard</h1>\n<ol type=\"1\">\n<li>简介</li>\n</ol>\n<p>2018年，随着Bert的发布，transformer结构彻底火了起来。2020年6月，Google发布《GShard:\nScaling Giant Models with Conditional Computation and Automatic\nSharding》，把MoE用到了encoder-decoder结构的transformer模型上。MoE开始变成我们现在熟悉的样子了。</p>\n<p>GShard这个工作做了很多的实验，训了很多规模巨大的MoE模型，最大的达到了600B。训练的一系列模型的参数如下表</p>\n<img src=\"/44e38c1b/gshard_moe_family.png\" class title=\"GShard MoE family\">\n<p>在expert数量的设计上，延续上面LSMT MoE工作的思路 --\nexpert越多，效果越好。（站在24年这个时间节点来看，太多的expert未必适合；但是也不能说这个思路一定错误，毕竟事物的发展是螺旋式的，就像ChatGPT出来之前大多数人都在魔改各种Bert，而GPT已经坐了几年冷板凳了。）</p>\n<p>GShard论文中很大的篇幅在介绍工程实现和优化，这也是MoE模型训练最大的痛点。关于工程框架的内容比较硬核，因此这里不会展开讲太多，而是关注在模型算法层面上。</p>\n<ol start=\"2\" type=\"1\">\n<li>模型设计</li>\n</ol>\n<p>先来看下模型设计。</p>\n<p>Google在那段时间走的是encoder-decoder\ntransfomer的技术路线，因此GShard也是基于encoder-decoder\ntransfomer的模型结构。</p>\n<p>GShard的模型设计是，在encoder和decoder中，每两层把其中一个FFN层替换成MoE层。对于总共有N层的模型，则有N/2个MoE层，如下图</p>\n<img src=\"/44e38c1b/gshard_model.png\" class title=\"GShard模型结构\">\n<p>每层会选择最多top-2 expert来激活。为什么是最多，后面解释。</p>\n<p>GShard在上面这篇LSTM MoE论文的基础上，改进了gating\nfunction和auxiliary loss function。</p>\n<p>从公式来看，MoE层的具体计算如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{G}_{s,E}&amp; =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)&amp; =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}&amp; =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s)\n\\end{aligned}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(x_s\\)</span> 是MoE的输入token，<span class=\"math inline\">\\(w_i\\)</span> 和 <span class=\"math inline\">\\(w_o\\)</span>\n分别是输入输出的线性变换矩阵。向量<span class=\"math inline\">\\(\\mathcal{G}_{s}\\)</span> 就是gating\nfunction的输出。</p>\n<p>GShard在gating\nfunction的设计上提出了两个要求：（1）负载均衡（2）高效扩展。</p>\n<p>负载均衡和前面讲的一样，很好理解。而为什么要高效扩展，因为如果要对N个token分别进行E个expert的分配，在N能达到百万甚至千万级别，而E也有几百上千的情况下，就需要一个高效的分布式实现，以免其他计算资源等待gating\nfunction。</p>\n<p>为了满足这些要求，gating function提出了以下机制</p>\n<p>（1）专家容量 expert capacity</p>\n<p>为了确保负载平衡，我们不希望有少量expert需要处理很多token，因此强制规定了每一个expert所负责处理的token数量有一个最大值，这个最大值就叫专家容量，在这里设置为2N/E，相当于平均分配的量。</p>\n<p>这个expert capacity通过GATE(·)给每个expert维护一个计数器 <span class=\"math inline\">\\(c_e\\)</span>\n来监控。如果一个token所选的两个专家当前处理量都已经超过设定的专家容量，那么这个token就不会被当前层的任何expert处理，而是直接通过残差链接透传到下一层。</p>\n<p>（2）分组分配 Local group dispatching</p>\n<p>给所有输入token分成了G组，不同的组并行处理，每个组相应地也把组内专家容量变成2N/EG。</p>\n<p>这样做相当于在前向推理时，把大的batch拆分成小的batch，每个小的batch就是一个group。这样做的好处是通讯的时候（特别是all2all）只需要在每个group内进行就可以了，减少了通讯量。</p>\n<p>而进行反向计算的时候这些group可以合起来一起用，相当于进行了gradient\naccumulation。</p>\n<p>（3）辅助损失函数 Auxiliary loss</p>\n<p>光设置专家容量并不能使得gating负载均衡，而且会导致大量溢出。参考前面LSTM\nMoE的工作，这里也定义了一个辅助损失函数，来帮助负载均衡。辅助损失函数设计如下</p>\n<p><span class=\"math display\">\\[\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot\nm_e\\]</span></p>\n<p><span class=\"math inline\">\\(S\\)</span> 是token数，<span class=\"math inline\">\\(E\\)</span> 是专家数，<span class=\"math inline\">\\(c_e\\)</span> 是分配给第 <span class=\"math inline\">\\(e\\)</span> 个专家的token数，<span class=\"math inline\">\\(m_e\\)</span> 是第 <span class=\"math inline\">\\(e\\)</span> 个expert在 <span class=\"math inline\">\\(S\\)</span> 个token中获得的平均权重。</p>\n<p>思路是，本来是要算 <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>\n的平方的，但这是离散值不可导，因此把平方中的一个 <span class=\"math inline\">\\(\\frac{c_e}S\\)</span> 换成了 <span class=\"math inline\">\\(m_e\\)</span> ， <span class=\"math inline\">\\(m_e\\)</span> 是第 <span class=\"math inline\">\\(e\\)</span> 个expert在 <span class=\"math inline\">\\(S\\)</span>\n个token中获得的平均权重。在平均分配的情况下，这个loss达到最小。</p>\n<p>相比前面的负载均衡损失，这个loss的设计就简单许多。</p>\n<p>gating的整个算法如下</p>\n<img src=\"/44e38c1b/gshard_algo_1.png\" class title=\"GShard gating 算法\">\n<p>（4）随机路由 Random routing</p>\n<p>前面提到，每层会选择最多top-2\nexpert来激活，就是因为有随机路由的机制。直观来说，就是认为如果top-1专家的权重很高，而第二个专家的权重如果较小，那很有可能只用第一个专家就足够解决问题了。</p>\n<p>随机路由的机制是top-1的专家永远会被激活，而第二个专家如果权重很小，就认为它可以被忽略。具体来说，会以与第二个专家的权重g2成比例的概率激活第二个专家。</p>\n<ol start=\"3\" type=\"1\">\n<li>效果</li>\n</ol>\n<p>最后看一下模型在翻译任务上的效果</p>\n<img src=\"/44e38c1b/gshard_perf.png\" class title=\"GShard效果\">\n<h1 id=\"switch-transformer\">Switch Transformer</h1>\n<p>2022年4月，距离ChatGPT发布还有半年，Google发布了《Switch\nTransformers: Scaling to Trillion Parameter Models with Simple and\nEfficient Sparsity》（实际上2021年Google就提出Switch\nTransformer了）。</p>\n<p>Switch\nTransformer和GShard一样，是encoder-decoder结构，基于T5开发的，具有1.6T的参数，2048个expert。</p>\n<p>和前面的很多工作一样，Switch\nTransformer有一个出发点，那就是参数量越大，模型效果越好，并且可以通过稀疏激活来减少总计算量。</p>\n<p>但是相比其他工作，Switch\nTransformer给出了一个更为具体的描述，那就是模型参数量可以是一个独立于总计算量的，单独的缩放轴。也就是说，在改变参数量的同时，（几乎）不改变训练和推理的计算量，就可以带来效果的提升。因此Switch\nTransformer关注在“同样的FLOPS/token的计算量”下，如何扩大模型，提升效果。</p>\n<p>Switch Transformer所做的工作还是比较多的，包括：</p>\n<p>（1）模型结构简化：简化了Transformer上的MoE架构，提出Switch\nTransformer架构。</p>\n<p>（2）MoE to\ndense：把训出来的效果较好的MoE模型蒸馏到dense模型，在压缩MoE模型99%的参数的情况下，效果还是比直接训练dense模型好。</p>\n<p>（3）训练和微调技术：<br>\n- 首次使用bf16成功训练MoE模型<br>\n- 更适合MoE结构的模型初始化<br>\n- 增加的专家正则化，改善了稀疏模型的微调和多任务训练</p>\n<p>（4）训练框架：结合数据、模型和专家并行性，训练了超过1T参数的MoE模型。</p>\n<p>（5）多语言：在多语言数据集上训练，发现101种语言效果普遍有提升。</p>\n<p>（6）训练效率：在同样的FLOPS/token的计算量下，Switch\nTransformer模型收敛速度有数倍的提升。</p>\n<h2 id=\"模型设计-1\">模型设计</h2>\n<p>Switch\nTransformer的模型结构如下图，类似GShard，把transformer每层的FFN替换成MoE层</p>\n<img src=\"/44e38c1b/switch_transformer_structure.png\" class title=\"Switch Transformer 模型结构\">\n<p>Switch Transformer一个重要的改进点就是简化了gating\nfunction的做法（Switch Transformer论文里叫routing）。</p>\n<p>之前的工作大多探索了选择k个expert的做法，而Switch\nTransformer则直接把gating简化为只选择1个expert，即k=1。这样的MoE层叫做Switch\nlayer。</p>\n<p>这样简化之后，routing的实现更简单，router的计算量小了，也减少了通讯量。</p>\n<h2 id=\"负载均衡-1\">负载均衡</h2>\n<p>同GShard一样，Switch Transformer规定了一个专家容量expert\ncapacity，来限制每个expert在一个batch里能处理的最大token数。</p>\n<p>如果一个token被分配到了一个已经满载的expert，就会出现overflow，那这个token在本层就不会被处理，而是直接通过残差链接，透传给下一层。这点也同GShard一样。</p>\n<p>在Switch Transformer，专家容量通过容量系数capacity factor来控制。</p>\n<p><span class=\"math display\">\\[\\text{expert\ncapacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of\nexperts}}\\right)\\times\\text{capacity factor}.\\]</span></p>\n<p>一个大的capacity\nfactor意味着每个expert能够处理更多的token，从而减少overflow情况的发生，但是计算量和通讯量的压力也会增大，所以这是一个需要权衡的参数。</p>\n<p>下图给出了一个不同capacity factor下的例子</p>\n<img src=\"/44e38c1b/switch_transformer_diff_expert_capacity.png\" class title=\"不同的expert capacity\">\n<p>那么如何设定expert capacity呢？</p>\n<p>如果capacity\nfactor为1的话，只有在完全平均分配的时候，才不会出现overflow的情况。而太大的capacity\nfactor则可能造成算力和存储的浪费。</p>\n<p>首先，实验中发现expert的数量和overflow的数量之间没有什么关系，所以在所有实验中，所有MoE和Switch\nTransformer模型都用128个专家。</p>\n<p>不同的capacity\nfactor对模型影响如下表。可以看到，大的容量系数相对来说能取得更好的效果（因为更少的overflow），但是相应地，大容量系数的模型处理速度就会慢一些。</p>\n<img src=\"/44e38c1b/switch_transformer_capacity_effect.png\" class title=\"expert capacity的效果\">\n<p>经验上，低的token丢弃率对模型的scaling很重要，想要训练超大规模的模型，就要解决这个问题。而通过负载均衡损失就可以确保良好的平衡，使得在使用较小容量系数的情况下，overflow尽量少，从而兼顾效果和计算速度。</p>\n<p>关键问题来到负载均衡损失怎么设计。</p>\n<p>给定 <span class=\"math inline\">\\(N\\)</span> 个expert，和包含 <span class=\"math inline\">\\(T\\)</span> 个token的batch <span class=\"math inline\">\\(\\mathcal{B}\\)</span>，负载均衡损失是这么计算的</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(f_{i}\\)</span> 表示被分配到第 <span class=\"math inline\">\\(i\\)</span> 个expert的token数，这个不可导</p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(P_i\\)</span>\n表示整个batch每个token分配给第<span class=\"math inline\">\\(i\\)</span>\n个expert的概率的总和，这个可导</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p>这个损失的设计其实和GShard中的也是一样的。</p>\n<p>在完美平均分配的情况下，<span class=\"math inline\">\\(f\\)</span> 和\n<span class=\"math inline\">\\(P\\)</span> 这两个向量都是 <span class=\"math inline\">\\(1/N\\)</span>，这个时候负载均衡损失是最小的。</p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span>\n扫描了1e-5到1e-1，发现设为1e-2，已经足够大保持负载平衡，同时不过分影响模型收敛。</p>\n<p>观察到 <span class=\"math inline\">\\(\\sum_{i=1}^N(f_i\\cdot\nP_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N\\)</span>，所以负载均衡loss还乘了个\n<span class=\"math inline\">\\(N\\)</span>，这样可以保持无论使用多少个expert，在平均分配的情况下，loss都能保持相同的常数。</p>\n<h2 id=\"实验-1\">实验</h2>\n<ol type=\"1\">\n<li>一些训练的trick</li>\n</ol>\n<p>（1）选择性地使用bf16</p>\n<p>半精度训练会带来一些训练的不稳定。因此选择性地使用bf16，具体来说，routing\nfunction内部使用单精度，其他部分使用半精度，这样既不影响通讯，也能提高效果。</p>\n<p>为什么选择在routing提高精度？因为softmax对误差特别敏感，exponential计算会极大放大输入中的rounding\nerror，因此高精度对routing很重要。</p>\n<p>（2）较小的参数初始化</p>\n<p>从截断正态分布中抽取元素来初始化的模型参数，平均值 <span class=\"math inline\">\\(\\mu=0\\)</span>，标准差<span class=\"math inline\">\\(\\sigma=\\sqrt{s}/n\\)</span>，其中s是超参，n是权重张量中的输入单元数量（e.g.\nfan-in）。</p>\n<p>论文建议将默认的Transformer初始化尺度s=1.0减少10倍。这个方案在实验中既提高了质量又降低了训练不稳定性的可能性。初始化实验对比如下表</p>\n<img src=\"/44e38c1b/switch_transformer_init.png\" class title=\"初始化对比\">\n<p>（3）增大dropout</p>\n<p>由于Switch\nTransformer参数量很大，在微调的时候更容易过拟合，因此一个简单的方法就是增大dropout，效果如下</p>\n<img src=\"/44e38c1b/switch_transformer_dropout.png\" class title=\"dropout效果\">\n<p>可以看到大的dropout有效果，并且dense层保持0.1，只有expert层增大dropout效果更好。</p>\n<ol start=\"2\" type=\"1\">\n<li>scaling</li>\n</ol>\n<p>对Switch Transformer结构预训练的scaling做了一些实验。</p>\n<p>（1）Step-Basis</p>\n<p>首先是验证在固定训练step的条件下，增大expert数量带来的提升，如下图所示。</p>\n<p>左边是不同规模的模型在相同step下收敛的结果，可以看到在保持相同计算量的条件下，只通过增大专家数量来提升规模，就有明显的收益。右边则展示训练过程中，不同规模的模型在各个step下的效果。</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_step.png\" class title=\"step scaling\">\n<p>（2）Time-Basis</p>\n<p>虽然Switch\nTransformer可以保持计算量不变的情况下提升模型规模，但是专家数量的增多会带来额外的通讯成本，所以即使训练的step数相同，实际的训练时间也不同。因此这里要回答的问题是，给定一个固定的训练时长，Switch\nTransformer是否相比dense模型仍有收益。</p>\n<p>答案是肯定的。下图展示以训练时长为横轴，Switch\nTransformer和dense模型的效果对比。Switch\nTransformer收敛到dense模型最终效果的时间只有dense模型的1/7。</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_time.png\" class title=\"time scaling\">\n<p>（3）和更大的dense模型对比</p>\n<p>前面Switch\nTransformer和dense模型的比较，是基于相同计算量的前提。那么Switch\nTransformer是否具备超越更大规模dense模型的能力？</p>\n<p>下图在Step-Basis和Time-Basis对比了64个专家的Switch\nTransformer和T5-Large。无论是相同step还是相同时间下，Switch\nTransformer都有明显优势。</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_dense.png\" class title=\"dense对比\">\n<ol start=\"3\" type=\"1\">\n<li>SFT效果对比</li>\n</ol>\n<p>在GLUE和SuperGLUE等下游任务上微调，和dense模型对比。</p>\n<p>对于各个模型，每两百步进行一次eval，选最好的效果，尽量保证公平。结果如下表，大部分任务都有明显的提升。</p>\n<img src=\"/44e38c1b/switch_transformer_sft_result.png\" class title=\"sft对比\">\n<ol start=\"4\" type=\"1\">\n<li>模型蒸馏</li>\n</ol>\n<p>虽然Switch\nTransformer在相同计算量下效果更好，但是部署几百B甚至T级别的模型，还是不太方便，因此考虑把稀疏模型蒸馏到dense模型上来进行推理。</p>\n<p>论文中给出了几个蒸馏的技巧：<br>\n- 初始化的时候，把Switch\nTransformer模型中的非稀疏部分用于初始化dense模型<br>\n- 蒸馏所用的label，25%来自教师模型，75%来自ground truth，加权求和</p>\n<p>预训练模型的蒸馏效果如下，相比无蒸馏训练的dense模型，把同样计算量的稀疏模型蒸馏到dense模型，dense模型大约能获得Switch\nTransformer提升部分30%的增益。</p>\n<img src=\"/44e38c1b/switch_transformer_distill.png\" class title=\"蒸馏\">\n<p>更进一步，用不同规模的稀疏模型下进行蒸馏，结果如下表，可以实现高达99%的压缩率</p>\n<img src=\"/44e38c1b/switch_transformer_distill_diff_model.png\" class title=\"蒸馏\">\n<p>除了预训练模型，微调模型也可以蒸馏，效果如下，在SuperGLUE也有一定的提升</p>\n<img src=\"/44e38c1b/switch_transformer_distill_sft.png\" class title=\"sft蒸馏\">\n<h1 id=\"glam\">GLaM</h1>\n<ol type=\"1\">\n<li>简介</li>\n</ol>\n<p>2021年12月Google发表了《GLaM: Efficient Scaling of Language Models\nwith\nMixture-of-Experts》，训练出最大参数量为1.2T，每层包含64个专家，每个token激活参数量为96.6B的MoE模型。</p>\n<p>相比Switch Transformer，GLaM的训练数据量要大得多，达到了1.6T\ntoken。</p>\n<p>下表是论文中给出的，当时一些大规模模型的对比</p>\n<img src=\"/44e38c1b/glam_related_model.png\" class title=\"glam和相关模型\">\n<p>虽然模型总参数量比GPT-3（175B）大很多，但是训练成本却比GPT-3低很多，推理速度也更快，而且在多个NLP任务上的效果都超越了GPT-3，如下所示。</p>\n<img src=\"/44e38c1b/glam_compare_gpt3.png\" class title=\"glam和gpt3对比\">\n<img src=\"/44e38c1b/glam_compare_gpt3_2.png\" class title=\"glam和gpt3对比\">\n<ol start=\"2\" type=\"1\">\n<li>模型设计</li>\n</ol>\n<p>模型设计上，和Switch\nTransformer一样，每两层把一个FFN替换成MoE层。但是和Switch\nTransformer不同，GLaM用回了每次激活两个expert的方案，模型结构如下图。</p>\n<img src=\"/44e38c1b/glam_model.png\" class title=\"glam模型\">\n<p>除此之外，模型在结构上海做了一些其他改动：</p>\n<p>（1）位置编码</p>\n<p>使用XLNET的相对位置编码。</p>\n<p>（2）激活函数</p>\n<blockquote>\n<p>In the non-MoE Transformer feed-forward sub-layers, we replace the\nfirst linear projection and the activation function with the Gated\nLinear Unit，which computes the component-wise product of two linear\ntransformation of the input, followed by a Gaussian Error Linear\nUnit.</p>\n</blockquote>\n<ol start=\"3\" type=\"1\">\n<li>实验</li>\n</ol>\n<p>训练中的一些trick：</p>\n<p>（1）参考《Lingvo: a modular and scalable framework for\nsequence-to-sequence\nmodeling》，在梯度出现NaN或者Inf的时候就跳过那一步更新。</p>\n<p>（2）如果在BP更新的时候遇到NaN或者Inf，则重新加载更早的checkpoint并跳过有问题的数据来避免NaN或者Inf。</p>\n<p>论文训了一系列模型来探索MoE，这些模型的设置如下表</p>\n<img src=\"/44e38c1b/glam_family.png\" class title=\"glam模型系列\">\n<p>GLaM和dense模型的评测结果如下</p>\n<img src=\"/44e38c1b/glam_perf.png\" class title=\"glam模型效果\">\n<p>可以看到GLaM MoE的有效参数效率一致高于dense模型。</p>\n<h1 id=\"st-moe\">ST-MoE</h1>\n<p>2022年2月，Google发表了《ST-MOE: DESIGNING STABLE AND TRANSFERABLE\nSPARSE EXPERT\nMODELS》。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，可以说是MoE的必读论文。</p>\n<p>ST-MoE最大模型包含269B总参数量，和与32B\ndense模型相当的激活计算量。论文中把模型称为称为Stable Transferable\nMixture-of-Experts，或者ST-MoE-32B。</p>\n<p>在MoE层的使用上，ST-MoE比Switch\nTransformer更“节省”一点，每四层才替换1个MoE层。</p>\n<p>论文中主要训了两个规模的ST-MoE模型，分别有4B和269B的总参数量。ST-MoE以及其他用于对比的模型参数如下表</p>\n<img src=\"/44e38c1b/st_moe_models.png\" class title=\"ST-MoE模型及对比模型的参数\">\n<h2 id=\"稳定性与效果分析\">稳定性与效果分析</h2>\n<p>论文通过对乘性操作、噪音和裁剪这几个内容进行探索，来指导模型的设计。</p>\n<ol type=\"1\">\n<li>乘性操作对模型稳定性和效果的影响</li>\n</ol>\n<p>论文首先研究了乘性操作对模型的训练稳定性和最终效果的影响。</p>\n<p>之前已经有一些工作表明更多的乘法对模型效果有收益。</p>\n<blockquote>\n<p>Some architectural improvements involve more multiplications than\nadditions or do not sum many items at once</p>\n</blockquote>\n<p>（1）GELU Gated Linear Units (GEGLU)</p>\n<p>第一个例子是关于激活函数的。GLU是一个对两个输入向量进行component-wise相乘的操作，之后被扩展成GELU-Linear\nFFN变体，用于替换transformer中的ReLU FFN变体，其计算如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}\\]</span></p>\n<p>这样在一些其他工作里已经被证明了对模型效果有提升。</p>\n<p>（2）RMSNorm</p>\n<p>第二个例子是RMSNorm中的缩放参数，也就是下面公式的 <span class=\"math inline\">\\(g\\)</span>。</p>\n<p><span class=\"math display\">\\[y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot\ng_i\\]</span></p>\n<p>ST-MoE针对GEGLU和RMSNorm这两个乘性操作，做了实验，结果如下表。</p>\n<img src=\"/44e38c1b/st_moe_remove_multiplications.png\" class title=\"移除乘法操作的影响\">\n<p>发现移除乘性操作可以使模型稳定性更好（训练中发散的情况减少），但是最终效果变差了。</p>\n<p>（3）增加dense层</p>\n<p>ST-MoE还验证了在expert层增加更多dense层的效果。结果发现增加更多的乘法交互（增加dense层），可以在带来效果收益的同时，基本不影响推理速度，如下表所示。</p>\n<img src=\"/44e38c1b/st_moe_more_dense_layer.png\" class title=\"更多的dense层\">\n<p>（4）增加一个bias</p>\n<p>在FFN层的第一个矩阵乘法后面增加一个可学习的bias\nB，分别通过加法和乘法加入</p>\n<p><span class=\"math display\">\\[\\text{FFN}_{\\text{GEGLU}}+\\text{Add\nBias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2\\]</span></p>\n<p><span class=\"math display\">\\[\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot\nxW_{12})\\odot B]W_2\\]</span></p>\n<p>乘法的收敛速度更快，效果也更好。</p>\n<p>上面这些实验显示，后续在模型效果的探索方向可以往多使用乘性操作去考虑。</p>\n<ol start=\"2\" type=\"1\">\n<li>noise对模型稳定性和效果的影响</li>\n</ol>\n<p>接下来ST-MoE探索了“噪音可以提升模型稳定性”的假设。</p>\n<p>通过input-jitter，给router的输入logits乘以一个在[1e-2,\n1e2]之间的均匀随机变量来添加噪音。</p>\n<img src=\"/44e38c1b/st_moe_more_add_noise.png\" class title=\"增加noise\">\n<p>结果是增加noise之后，有助于让模型的收敛更加稳定，但是对模型最终效果有负面影响。</p>\n<p>这里论文还提到，小模型上的结果不一定能直接推广到更大的模型上，比如在小模型上稳定的配置，在大模型就可能就不稳定了。因此还是需要在大模型上也进行充分实验。</p>\n<ol start=\"3\" type=\"1\">\n<li>限制激活值和梯度值对模型稳定性和效果的影响</li>\n</ol>\n<p>对activation和gradient进行限制是目前广泛应用的提升模型训练稳定性的手段。在反向传播过程中，通过裁剪梯度的范数来缓解梯度爆炸，就是一种常用的限制手段。</p>\n<p>但是在ST-MoE训练269B的大规模模型时，发现裁剪会使得模型收敛的效果很差。</p>\n<p>为了解决这个问题，ST-MoE在训练中引入了router z-loss，形式如下。</p>\n<p><span class=\"math display\">\\[L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2\\]</span></p>\n<p><span class=\"math inline\">\\(B\\)</span> 是token的数量，<span class=\"math inline\">\\(N\\)</span> 是专家数，<span class=\"math inline\">\\(x\\in\\mathcal{R}^{B\\times N}\\)</span>\n是router的输入。</p>\n<p>z-loss会对进入router的较大的logits值进行惩罚，以达到尽量减少进入指数函数的较大误差的目的。什么意思呢？后面来解释，先看下使用z-loss的效果。</p>\n<img src=\"/44e38c1b/st_moe_z_loss_result.png\" class title=\"z-loss效果\">\n<p>ST-MoE认为，在模型训练过程中，由于精度不足或者其他问题，会产生很大的值，从而引入误差。而对梯度进行裁剪是在误差发生之后，并且裁剪本身也造成了数据的不连续性，某种程度上，裁剪本身也是一种误差。相反地，z-loss自然地鼓励模型产生较小的对数值，因此可以更精确地建模。</p>\n<p>z-loss乘以一个权重超参 <span class=\"math inline\">\\(c_z\\)</span>\n加入到模型训练的总损失中，如下式所示。</p>\n<p><span class=\"math display\">\\[L_{tot}=L_{CE}+c_BL_B+c_zL_Z\\]</span></p>\n<p>ST-MoE经过实验，选择了<span class=\"math inline\">\\(c_z=0.001\\)</span>。</p>\n<p><span class=\"math inline\">\\(L_B\\)</span> 是 auxiliary load balance\nloss负载均衡损失，ST-MoE这里使用了和GShard/Switch\nTransformer所用的相同的损失计算，这里回顾一下：</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(N\\)</span> 是专家数， <span class=\"math inline\">\\(\\mathcal{B}\\)</span>是包含 <span class=\"math inline\">\\(T\\)</span> 个token的batch。<span class=\"math inline\">\\(f_{i}\\)</span> 表示被分配到第 <span class=\"math inline\">\\(i\\)</span> 个expert的token数，这个不可导；<span class=\"math inline\">\\(P_i\\)</span> 表示整个batch每个token分配给第<span class=\"math inline\">\\(i\\)</span> 个expert的概率的总和，这个可导。</p>\n<ol start=\"4\" type=\"1\">\n<li>数据精度对训练效率和训练效果的影响</li>\n</ol>\n<p>目前大部分的大模型训练都使用混合精度训练：模型权重以float32格式存储以进行梯度更新，然后在正向和反向传播的矩阵乘法中转换为bfloat16；此外，所有激活值都以bfloat16存储和操作，而allreduce通信可以在bfloat16或float32数值精度中进行。</p>\n<p>对于ST-MoE-32B的训练，allreduce的数值使用半精度可以加速训练，然而这也会使训练变得不稳定，因此ST-MoE保持allreduce的数值精度为float32。</p>\n<p>bfloat16和float32在不同范围的舍入误差如下表所示</p>\n<img src=\"/44e38c1b/st_moe_round_error.png\" class title=\"bf16精度损失\">\n<p>可以看到，表达的数值越大，舍入误差越大。而z-loss限制了数值大小，也就将误差值限制在比较小的范围。</p>\n<p>MoE模型天生对舍入误差敏感，因为它们由于router的使用而有更多的指数函数，而指数函数会将小的输入误差放大很多，这就加剧舍入误差所导致的训练不稳定。</p>\n<p>另外，ST-MoE有一个策略：只有当排第二的专家的权重大于等于第一的专家的1/5时，token才会被路由到其第二位专家，否则第二个专家就会被忽略。</p>\n<p>因此虽然舍入误差不会改变softmax运算中各个概率的排序，但它确实会影响MoE中第二个专家的激活。</p>\n<h2 id=\"模型设计-2\">模型设计</h2>\n<p>dense模型的设计有scaling\nlaw进行指导，但是MoE模型的设计比dense模型多出几个要考虑的点：</p>\n<p>（1）使用多少个expert</p>\n<p>（2）怎么routing</p>\n<p>（3）专家容量系数怎么定</p>\n<p>（4）硬件的影响</p>\n<p>（这里提到MoE模型的scaling law工作：《Unified scaling laws for routed\nlanguage models》，可以了解一下）</p>\n<ol type=\"1\">\n<li>使用多少个expert</li>\n</ol>\n<p>ST-MoE认为，从以往的经验来看，在总专家数量较少的情况下（如8/16/32），提升专家数量，能有收益。但是在特别稀疏的情况下（如激活专家数量&lt;1%），或者总专家数较大（比如&gt;256）之后，提升专家数量收益就很小了。</p>\n<p>从另一个角度来看，如果一个计算核心使用&gt;1个专家，那么就会出现比较大的加载参数张量的成本，因此建议每个计算核心使用&lt;=1个专家。</p>\n<ol start=\"2\" type=\"1\">\n<li>routing和capacity factor</li>\n</ol>\n<p>论文做了一系列实验来探索capacity factor的选择，如下表所示</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor.png\" class title=\"capacity factor\">\n<p>从这些实验中得到几个结论：</p>\n<p>（1）训练和推理的capacity factor增大都会有收益</p>\n<p>（2）如果硬件资源足够，推理的capacity\nfacotr可以设得比训练的时候大，会有进一步提升</p>\n<p>（3）激活的expert数量提升会有收益，但是收益随着capacity\nfactor提升而越来越小</p>\n<p>当然，选择capacity\nfactor还要看硬件的特性，如果通讯很快，可以适当增大capacity\nfactor，否则就不能选择太大的。</p>\n<p>下表展示了不同capacity factor对推理速度的影响</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor_speed.png\" class title=\"不同capacity factor推理速度\">\n<h2 id=\"实验-2\">实验</h2>\n<ol type=\"1\">\n<li>ST-MoE效果</li>\n</ol>\n<p>ST-MoE-32B在下游任务上和以往最佳结果对比如下表，ST-MoE-32B刷新了超过一半任务的最佳效果</p>\n<img src=\"/44e38c1b/st_moe_perf.png\" class title=\"不同capacity ST-MoE-32B效果\">\n<ol start=\"2\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>论文还对各个专家的专业化进行了追踪，发现decoder中几乎没有专业化的迹象，各种类型的token近乎随机分配给不同的专家。而在encoder中则表现出了高度专业化的特征，如下表</p>\n<img src=\"/44e38c1b/st_moe_encoder_specialization.png\" class title=\"encoder专业化\">\n<p>此外，还发现在多语言的模型的encoder中，专业化的情况并不想原先预想那样，按不同语言划分，而是每个专家都会处理一种语言的一部分token，如下表</p>\n<img src=\"/44e38c1b/st_moe_multiling_specialization.png\" class title=\"多语言专业化\">\n<h1 id=\"deepseekmoe\">DeepseekMoE</h1>\n<p>2024年1月，幻方量化开源了DeepseekMoE，是国内首个开源的MoE大模型。幻方还发布了论文《DeepSeekMoE:\nTowards Ultimate Expert Specialization in Mixture-of-Experts Language\nModels》，给出了一些DeepSeekMoE的细节内容，颇为实在了。</p>\n<p>DeepSeekMoE在其他MoE工作的基础上，进一步给出了2个模型设计的主要思路：</p>\n<p>（1）对expert的粒度进行细分，以提供更多样的expert激活组合；</p>\n<p>（2）对expert的类型进行区分，从所有expert中保留一部分作为shared\nexpert共享专家，这部分专家对所有输入都保持激活。</p>\n<p>这样的做法可以帮助每个expert达到更高程度的专业化(specialization)的水平，更好地学习不同的专业知识。</p>\n<p>DeepSeekMoE先在2B的较小MoE模型上进行了充分的实验，然后把方案应用到16B参数的MoE模型上，并获得了较好的效果。其中DeepSeekMoE-16B不需要量化就可以在40GB显存的设备上运行。</p>\n<p>DeepSeekMoE-2B模型具有和稠密2B模型相当的性能，而DeepSeekMoE-16B则具有和7B稠密模型相当的性能，且计算量仅为稠密模型的40%。</p>\n<p>DeepSeekMoE-16B的参数效率相比稠密模型有明显的优势，如下图所示</p>\n<img src=\"/44e38c1b/ds_moe_perf.png\" class title=\"deepseek moe\">\n<p>并且DeepSeekMoE-2B和16B模型都开源了。</p>\n<p>在前面实验的基础上，幻方还训练了DeepSeekMoE-145B的超大MoE模型，具有和稠密的DeepSeek-67B模型相当的表现，但计算量更小。这个后续也有机会放出来。</p>\n<h2 id=\"模型设计-3\">模型设计</h2>\n<p>MoE，mixture of\nexpert，顾名思义，一个最初始的motivation就是让不同expert学习不同的内容，然后再混合起来。</p>\n<p>比如最上面提到的1991年的工作里，就是让不同的expert学习不同的元音特征，以此提升特征提取的准确率。</p>\n<p>但是当前大部分的MoE架构都会遇到“knowledge hybridity”和“knowledge\nredundancy”的问题，即知识的杂糅和冗余：</p>\n<p>（1）知识冗余</p>\n<p>有些基础的常识在不同的领域都需要用到，每个expert就都会学一点，这样这些常识就被多个expert重复学习了。</p>\n<p>（2）知识杂糅</p>\n<p>在expert数量不够多的情况下，一个expert就可能要负责学习多个领域的内容。以学习高中知识为例，在只有两个expert的时候，只能一个expert学习理科知识，另一个学习文科知识；当我们有8个expert的时候，不同expert就可以分别学习语文、英语、历史、地理、物理、生物、化学、数学知识。显然后者所学知识的专业化程度更高。</p>\n<p>知识的杂糅和冗余阻碍了专家专业化(expert\nspecialization)的程度，也就阻碍了模型达到MoE结构理论上限性能。</p>\n<p>我们期望每个expert能够学习到non-overlap &amp; foucusd\nknowledge的知识。</p>\n<p>针对上面的问题，DeepSeekMoE的架构设计有2个主要策略：</p>\n<p>（1）Fine-Grained Expert Segmentation</p>\n<p>参数总量不变的情况下，将expert分成更细的粒度（每个expert更小）。这样可以带来更灵活的激活组合，让每个expert可以有更强的specialization。比如原本是16个expert选择激活2个，那么总的组合数是120种；如果把每个expert缩小为原来的1/4，那在总参数量和激活数量不变的情况下，是64个expert选择激活8个，那么总的排列组合数就是\n<span class=\"math inline\">\\(\\binom{64}8=4,426,165,368\\)</span>\n，排列组合数比原来多了很多。</p>\n<p>（2）Shared Expert Isolation</p>\n<p>把部分expert分离出来，保持永远激活。我们期望这部分专家能够学到在多个领域间都通用的common\nknowledge。这样的策略同样可以使得其他expert能够提高专业化的程度，并且减少不同expert间的知识冗余。还是以学习高中知识为例，数学、物理和化学都需要算术能力，如果让学这三个领域的expert都学习算术技能，就会有冗余；我们可以把通用算术的技能剥离出来，由一个助手专门负责算术任务，相当于给他们发了一个计算器，这样学习数学、物理和化学的expert就能把更多的精力放在专业知识上，也就能达到更好的专业化效果。</p>\n<p>下图展示了在传统MoE结构上增加Fine-Grained Expert Segmentation和Shared\nExpert Isolation策略的设计</p>\n<img src=\"/44e38c1b/ds_moe_structure.png\" class title=\"deepseek moe 结构\">\n<p>（expert isolation的思路最早可以追溯到2022年1月发表的《DeepSpeed-MoE:\nAdvancing Mixture-of-Experts Inference and Training to Power\nNext-Generation AI Scale》，这里就不展开了。）</p>\n<p>假设传统的MoE模型每层的expert数量为 <span class=\"math inline\">\\(N\\)</span>，激活expert数为 <span class=\"math inline\">\\(K\\)</span>，DeepSeekMoE使用的细粒度expert大小为原来的\n<span class=\"math inline\">\\(1/m\\)</span>，那DeepSeekMoE每层就有 <span class=\"math inline\">\\(mN\\)</span> 个expert，激活的expert数量为 <span class=\"math inline\">\\(mK\\)</span>个。假设 <span class=\"math inline\">\\(T\\)</span> 为输入长度，<span class=\"math inline\">\\(L\\)</span> 为模型层数，<span class=\"math inline\">\\(e_i^l\\)</span> 表示第 <span class=\"math inline\">\\(i\\)</span>\n个expert，DeepSeekMoE可以公式化为以下表示（忽略了layernorm）</p>\n<p><span class=\"math display\">\\[\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{\nFFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l\\]</span></p>\n<p><span class=\"math display\">\\[g_{i,t}=\\begin{cases}s_{i,t},&amp;s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant\nj\\leqslant mN\\},mK)\\\\0,&amp;\\text{otherwise,}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)\\]</span></p>\n<h2 id=\"负载均衡-2\">负载均衡</h2>\n<p>如之前工作反复提及的，如果任由MoE模型自主学习gating，可能会遇到两个问题</p>\n<p>（1）routing\ncollapse：专家分配的不均衡，也就是gating倾向于总是选择特定的少量expert，并且这种情况还会自我增强。</p>\n<p>（2）计算效率问题：多设备间，不平衡的负载可能会成为计算效率的瓶颈。</p>\n<p>针对routing collapse的问题，DeepSeekMoE引入一个expert-level balance\nloss，如下所示</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}&amp; =\\alpha_1\\sum_{i=1}^{N&#39;}f_iP_i\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_{i}&amp;\n=\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token\n}t\\text{ selects Expert }i)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}&amp; =\\frac1T\\sum_{t=1}^Ts_{i,t}\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_1\\)</span> 叫做expert-level\nbalance factor，是人工设定的超参。</p>\n<p>而 <span class=\"math inline\">\\(f_i\\)</span> 和 <span class=\"math inline\">\\(P_i\\)</span> 和Switch\nTransformer里的设定基本一样。</p>\n<p>在Switch Transformer里， <span class=\"math inline\">\\(f_i\\)</span>\n表示分配到第 <span class=\"math inline\">\\(i\\)</span>\n个expert的token数量。在DeepSeekMoE这里也是一样的含义，只是多乘了一个系数\n<span class=\"math inline\">\\(N&#39;/K&#39;\\)</span> ，其中 <span class=\"math inline\">\\(N&#39;=mN-K_s\\)</span>，<span class=\"math inline\">\\(K&#39;=mK-K_s\\)</span>，<span class=\"math inline\">\\(K_s\\)</span>\n是划分出来的共享expert的数量。这个系数是个常数，可以拿到求和符号外面，这样DeepSeekMoE里的\n<span class=\"math inline\">\\(f_i\\)</span> 就和Switch\nTransformer里的完全一样了。</p>\n<p><span class=\"math inline\">\\(N&#39;/K&#39;\\)</span>\n这个系数可以使得在使用不同的数量的expert时，在完美平均分配的情况下，负载均衡loss都是相同的常数。</p>\n<p><span class=\"math inline\">\\(P_i\\)</span> 表示所有每个token分配给第\n<span class=\"math inline\">\\(i\\)</span> 个expert的权重的总和，和Switch\nTransformer里的含义一样。</p>\n<p>注意这里 <span class=\"math inline\">\\(f_i\\)</span> 是不可导的，<span class=\"math inline\">\\(P_i\\)</span> 是可导的。</p>\n<p>针对多设备间负载均衡的问题，DeepSeekMoE引入一个device-level balance\nloss，如下所示</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}&amp; =\\alpha_2\\sum_{i=1}^Df_i&#39;P_i&#39;\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_i^{\\prime}&amp; =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}^{\\prime}&amp; =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_2\\)</span> 叫做device-level\nbalance factor，是人工设定的超参。</p>\n<p><span class=\"math inline\">\\(\\mathcal{E}_i\\)</span> 指第 <span class=\"math inline\">\\(i\\)</span> 个设备。</p>\n<p>device-level balance loss形式上和expert-level balance loss一样，只是\n<span class=\"math inline\">\\(f_i\\)</span> 和 <span class=\"math inline\">\\(P_i\\)</span>\n对应的对象从单个expert变成单个设备了。</p>\n<p>当我们的目标是缓解计算瓶颈时，我们不需要强制执行expert间的均匀分配，而只需确保设备之间计算量的平衡。比如我们每层有64个expert，均匀分布在8个设备上，我们只需要每个设备处理的token数平衡即可，在设备内部即使所有token都是同一个expert处理的，依然能满足设备间负载平衡的要求。</p>\n<p>相比expert间严格的负载平衡，只要求设备间的平衡是更松的限制条件，这样缓解了因为过度的负载平衡而损害模型性能的问题。</p>\n<h2 id=\"实验-3\">实验</h2>\n<ol type=\"1\">\n<li>小规模模型验证</li>\n</ol>\n<p>为了验证以上策略的有效性，先拿100B\ntoken的语料数据在DeepSeekMoE-2B模型做实验。词表也是通过BPE在语料上训练的8k词表，后面训练更大规模模型的时候再扩大词表。</p>\n<p>DeepSeekMoE-2B模型参数初始化方差为0.006，使用multi-head\nattention，前向激活参数量约0.3B，具体参数如下表</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"模型超参\">\n<p>relative expert\nsize指的是DeepSeekMoE所用的细粒度expert的大小和正常FFN层大小的比值。</p>\n<p>训练的具体参数设置如下</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\">属性</th>\n<th style=\"text-align: center;\">数值</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">optimizer</td>\n<td style=\"text-align: center;\">AdamW</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_beta_1</td>\n<td style=\"text-align: center;\">0.9</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">adam_beta_2</td>\n<td style=\"text-align: center;\">0.95</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_weight_decay</td>\n<td style=\"text-align: center;\">0.1</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">warmup schedule</td>\n<td style=\"text-align: center;\">linear</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">warmup step</td>\n<td style=\"text-align: center;\">2000</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">max lr</td>\n<td style=\"text-align: center;\">1.08e-3</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">dropout</td>\n<td style=\"text-align: center;\">0</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">sequence length</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">batch size</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">total step</td>\n<td style=\"text-align: center;\">25,000</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>其他训练细节：<br>\n- 所有expert放在单个GPU上，没有使用device-level balance loss<br>\n- expert-level balance factor设为0.01<br>\n- 训练到80%的时候，学习率乘以0.316，训练到90%的时候，再乘以0.316</p>\n<p>使用相同的100B训练数据，训了DeepSeekMoE-2B，在包含语言模型和下游任务的benchmark上和其他4个模型做对比：dense，hash\nlayer（也是一种moe，《Hash layers for large sparse models》），Switch\nTransformer，GShard。效果对比如下</p>\n<img src=\"/44e38c1b/ds_moe_comparison.png\" class title=\"deepseek moe 效果\">\n<p>可以得到几个结论：<br>\n- 更大的模型参数量和稀疏的架构，使得Hash Layer和Switch\nTransformer和具有同样激活参数的dense模型相比，有明显的优势。<br>\n- 同样的模型参数下，GSshard比Hash Layer和Switch\nTransformer有更多激活参数，效果也更好<br>\n- 同样的模型参数和激活参数下，DeepSeekMoE效果比GShard有明显优势。</p>\n<p>为了进一步探索DeepSeekMoE架构带来的收益，提升了dense模型和GShard模型的激活参数，直到效果和DeepSeekMoE-2B差不多。</p>\n<p>结果dense模型和GShard模型需要分别扩大到16倍和1.5倍的参数量，才能达到DeepSeekMoE-2B相近的效果，如下表所示</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_2b.png\" class title=\"deepseek moe upper bound\">\n<p>DeepSeekMoE的优势在更大规模的情况下，依然成立。训了DeepSeekMoE-13B,\n对比参数量提升至1.2和1.5倍的GShard，DeepSeekMoE-13B依然能match，具体如下表</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_13b.png\" class title=\"deepseek moe upper bound\">\n<ol start=\"2\" type=\"1\">\n<li>DeepSeekMoE架构消融实验</li>\n</ol>\n<p>针对DeepSeekMoE架构的两个主要设计，shared expert和fine-grained\nexpert进行消融实验。使用不同数量的共享专家和不同粒度的expert进行效果对比，结果如下图。</p>\n<img src=\"/44e38c1b/ds_moe_ablation.png\" class title=\"deepseek moe upper bound 消融实验\">\n<p>（1）对比蓝色和橙色，可以看到增加共享专家带来了收益</p>\n<p>（2）绿色和红色在橙色的基础上进一步把专家颗粒分得更细，效果进一步提升</p>\n<p>（3）共享专家和路由专家的比例：在总共64个expert的情况下，对比了1/2/4个共享专家的情况，结果并没有显著差别，在pile上的loss分别是1.808,1.806,1.811。最终选择了共享专家和激活路由专家1:3（2+6）的比例。</p>\n<ol start=\"3\" type=\"1\">\n<li>expert specialization的分析</li>\n</ol>\n<p>通过实验来验证DeepSeekMoE中expert specialization的优化。</p>\n<p>（1）前面实验看到DeepSeekMoE-2B和1.5倍参数量的GShard模型效果相当。在这个基础上，通过禁用不同数量的top专家，而只能从次优的专家中选择进行回答。</p>\n<p>实验结果如下</p>\n<img src=\"/44e38c1b/ds_moe_expert_specialization.png\" class title=\"专家专门化\">\n<p>发现DeepSeekMoE损失更大，说明DeepSeekMoE每个专家的专业化程度更好，必要性更高。</p>\n<p>（2）另外，通过禁用DeepSeekMoE的共享专家，而额外激活一个专家，发现loss也大大提升。这个结果突出了共享专家的关键功能，并表明共享专家捕捉到了与路由专家不共享的基本且重要的知识，使得它无法被路由专家替代。</p>\n<p>（3）只激活更少专家，也能和GShard达到相同水平，这一观察结果支持了DeepSeekMoE可以更准确和高效地获取所需知识的观点。</p>\n<img src=\"/44e38c1b/ds_moe_less_activated_expert.png\" class title=\"激活更少专家\">\n<p>此外还从零训了一个只用1个共享专家和3个激活专家的2b模型（正常是2个共享专家+6个激活专家），也比GShard好，说明DeepSeekMoE的有效参数效率更高</p>\n<img src=\"/44e38c1b/ds_2b_less_expert.png\" class title=\"2B激活更少专家\">\n<ol type=\"1\">\n<li>DeepSeekMoE-16B</li>\n</ol>\n<p>DeepSeekMoE-16B模型使用了2T数据训练（和LLAMA2-7B对齐）训练，并使用了100k的词表。其他参数如下表所示</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"模型超参\">\n<p>论文中提到，除了第一层以外，其他层都使用了MoE层。</p>\n<p>第一层不使用MoE是因为观察到第一层的负载均衡loss在训练中收敛得特别慢。</p>\n<p>DeepSeekMoE-16B每层有64个专家，其中有2个作为共享专家保持永远激活，加上6个通过gating\nfunction选择激活的，每个token共使用8个专家。每个token会激活16.4B中的2.8B参数。</p>\n<p>这里没有把专家的dimension再减小，是因为如果专家太小，计算效率就下降得太厉害。</p>\n<p>训练中使用的其他设置：<br>\n- lr = 4.2e-4<br>\n- 训练进行到80%和90%的时候，lr都会缩小到0.316倍<br>\n- batch size = 4.5k，训练窗口长度是4k，因此每个batch有18M\ntoken，2T数据差不多是10.6w步<br>\n- 使用了pipeline parallelism</p>\n<p>expert level balance\nloss的系数设得比较小，0.001，因为实验中发现设得再大并不能进一步优化负载平衡，反而会损害模型效果。</p>\n<p>DeepSeekMoE-16B和DeepSeek-7B模型的对比如下</p>\n<img src=\"/44e38c1b/ds_16b_perf_1.png\" class title=\"和DeepSeek-7B对比\">\n<p>DeepSeekMoE-16B和LLAMA2-7B模型的对比如下</p>\n<img src=\"/44e38c1b/ds_16b_perf_2.png\" class title=\"和LLAMA2-7B对比\">\n<ol start=\"5\" type=\"1\">\n<li>DeepSeekMoE-145B</li>\n</ol>\n<p>幻方还用245B的token训练了DeepSeekMoE-145B，模型效果上达到DeepSeek-67B的同等水平</p>\n<img src=\"/44e38c1b/ds_moe_145b.png\" class title=\"145b\">\n<h1 id=\"dbrx\">DBRX</h1>\n<p>2024年3月27日，Databricks开源了DBRX，一个拥有有132B参数，激活参数为36B的MoE模型。</p>\n<p>结构上，DBRX使用了RoPE、GLU、GQA，采用了fine-grained\nexpert的设计，每层有16个专家，每个token激活其中4个。相比Mixtral和Grok-1在8个专家中激活2个，DBRX有更多的专家组合方式。</p>\n<p>DBRX训练的上下文长度为32k，并使用了12T文本和代码token进行训练。DBRX在3072个H100上完成预训练，加上post-training、效果评估、red-team优化，整个过程耗费3个月时间。</p>\n<p>DBRX整体效果超过GPT-3.5，与Gemini 1.0\nPro相当，并且具有比较强的代码能力，甚至超过了在代码上专门优化过的模型，如CodeLLaMA-70B，如下图所示。</p>\n<img src=\"/44e38c1b/dbrx_perf.png\" class title=\"DBRX效果\">\n<p>推理效率效率上，DBRX也领先于其他模型。</p>\n<img src=\"/44e38c1b/dbrx_infer_efficiency.png\" class title=\"推理效率\">\n<h1 id=\"qwen1.5-moe\">Qwen1.5-MoE</h1>\n<p>2024年3月28日，阿里放出了Qwen1.5-MoE-A2.7B，以2.7B的模型参数，达到了Qwen1.5-7B模型的相近效果。</p>\n<p>Qwen1.5-MoE-A2.7B参考了DeepSeekMoE和DBRX的工作，采用了fine-grained\nexpert的做法，总共有64个专家，每个token激活8个专家，其中有4个为共享专家。</p>\n<p>Qwen1.5-MoE-A2.7B使用Qwen-1.8B进行初始化，并在初始化阶段引入随机性，这样可以显著加快收敛速度，并得到更好的收敛结果。</p>\n<p>Qwen1.5-MoE-A2.7B和其他模型效果对比如下</p>\n<img src=\"/44e38c1b/qwen1.5_moe_perf.png\" class title=\"Qwen1.5-MoE-A2.7B效果\">\n<p>虽然Qwen1.5-MoE-A2.7B总参数量较大，但激活的non-embedding参数量远小于7B模型，如下表所示</p>\n<img src=\"/44e38c1b/qwen1.5_moe_params.png\" class title=\"Qwen1.5-MoE-A2.7B参数量\">\n<p>实践中，Qwen1.5-MoE-A2.7B相比于Qwen1.5-7B，训练成本降低了75%。</p>\n<p>推理性能上，在A100-80G用vLLM部署Qwen1.5-7B和Qwen1.5-MoE-A2.7B模型进行了性能测试。</p>\n<p>输入/输出token数都设置为1000，输出token数设置为1000，TPS和throughput如下</p>\n<img src=\"/44e38c1b/qwen1.5_moe_tps.png\" class title=\"Qwen1.5-MoE-A2.7B TPS\">\n<p>虽然MoE模型对内存需求更大，但是由于稀疏激活以及共享专家的设计，但是在速度和吞吐量上都比dense模型更好。Qwen1.5-MoE-A2.7B与Qwen1.5-7B相比，速度提高了约1.74倍。</p>\n<h1 id=\"mistral\">Mistral</h1>\n<h2 id=\"mistral-8x7b\">Mistral 8x7B</h2>\n<p>2023年12月11日，Mistral\nAI开源Mistral-8x7B，每个token激活8个专家中的2个。</p>\n<p>Mistral-8x7B支持32k推理窗口和多语言，并且代码能力较好。和LLAM2-70B以及GPT-3.5的对比如下。</p>\n<img src=\"/44e38c1b/mistral_8_7b_perf.png\" class title=\"Mistral 8x7B效果\">\n<p>Mistral-8x7B在大多数任务表现优于LLAM2-70B，且推理速度提高了6倍。</p>\n<p>而和激活参数量相近的LLAM2-13B比，优势更为明显</p>\n<img src=\"/44e38c1b/mistral_8_7b_active_perf.png\" class title=\"Mistral 8x7B同样激活参数量下效果\">\n<h2 id=\"mistral-8x22b\">Mistral 8x22B</h2>\n<p>2024年4月17日，Mistral\nAI开源Mistral-8x22B模型，一个总参数为141B，激活参数为39B的超大MoE模型。</p>\n<p>Mistral-8x22B支持多语言，并且具有较强的数学和代码能力。此外，推理窗口长度也从Mistral-8x7B的32k增加到64k。Mistral-8x22B还具备function\ncall的能力。</p>\n<p>在各个维度的评测结果如下</p>\n<img src=\"/44e38c1b/mistral_8_22b_reasoning.png\" class title=\"Mistral 8x22B reasoning效果\">\n<img src=\"/44e38c1b/mistral_8_22b_multiling.png\" class title=\"Mistral 8x22B 多语言效果\">\n<img src=\"/44e38c1b/mistral_8_22b_code.png\" class title=\"Mistral 8x22B 代码与数学效果\">\n<h1 id=\"小结\">小结</h1>\n<ul>\n<li>现有的工作都表明，MoE模型相比dense模型具有更高的参数效率，即同样的计算量下，MoE模型普遍能有更优的效果<br>\n</li>\n<li>因此MoE不仅能支持更大规模模型的训练，在较小规模模型上使用MoE架构也有很大收益<br>\n</li>\n<li>但是相比dense模型，MoE模型的训练也需要考虑更多内容，包括专家数量、激活数量和专家容量的设计，负载均衡的问题，如何在多设备上的并行等，训练难度更大<br>\n</li>\n<li>结构上，共享专家和细粒度专家目前被验证效果较好<br>\n</li>\n<li>负载均衡上，GShard和Switch Transformer的负载均衡损失被广泛采用<br>\n</li>\n<li>推理时需要对底层框架进行优化以适配MoE机制，否则难以发挥MoE的性能优势</li>\n</ul>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Adaptive Mixtures of Local Experts\nhttps://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf<br>\n【2】Outrageously Large Neural Networks: The Sparsely-Gated\nMixture-of-Experts Layer https://arxiv.org/abs/1701.06538<br>\n【3】GShard: Scaling Giant Models with Conditional Computation and\nAutomatic Sharding https://arxiv.org/abs/2006.16668<br>\n【4】Switch Transformers: Scaling to Trillion Parameter Models with\nSimple and Efficient Sparsity https://arxiv.org/abs/2101.03961<br>\n【5】GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nhttps://arxiv.org/abs/2112.06905<br>\n【6】ST-MoE: Designing Stable and Transferable Sparse Expert Models\nhttps://arxiv.org/abs/2202.08906<br>\n【7】DeepSeekMoE: Towards Ultimate Expert Specialization in\nMixture-of-Experts Language Models\nhttps://arxiv.org/abs/2401.06066<br>\n【8】Introducing DBRX: A New State-of-the-Art Open LLM\nhttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm<br>\n【9】Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated\nParameters https://qwenlm.github.io/zh/blog/qwen-moe/</p>\n","length":33074,"excerpt":"","more":"<p>【本文已在同名 微信公众号 / 知乎 / <a href=\"http://www.linsight.cn/\">个人博客linsight.cn</a> 上线】</p>\n<hr>\n<p>2024年3、4月这段时间，很多MoE模型扎堆发布，包括Qwen1.5-MoE、DBRX、Jamba和Mistral等。</p>\n<p>下面这个表格列出了部分近期发布的MoE工作</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\">模型</th>\n<th style=\"text-align: center;\">发布时间</th>\n<th style=\"text-align: center;\">备注</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">GPT4</td>\n<td style=\"text-align: center;\">2023年3月</td>\n<td style=\"text-align: center;\">23年6月George\nHotz爆料GPT4是8×220B模型</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Mistral-8×7B</td>\n<td style=\"text-align: center;\">2023年12月</td>\n<td style=\"text-align: center;\">Mistral AI，开源</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">LLAMA-MoE</td>\n<td style=\"text-align: center;\">2023年12月</td>\n<td style=\"text-align: center;\">github开源项目</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">DeepSeek-MoE</td>\n<td style=\"text-align: center;\">2024年1月</td>\n<td style=\"text-align: center;\">幻方量化，国内首个开源MoE模型，有技术报告</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">abab6</td>\n<td style=\"text-align: center;\">2024年1月</td>\n<td style=\"text-align: center;\">MiniMax，号称千亿MoE，无开源，无细节发布</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">天工2.0</td>\n<td style=\"text-align: center;\">2024年2月</td>\n<td style=\"text-align: center;\">昆仑万维，无开源，无细节发布</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Step-2</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">阶跃星辰，无开源，无细节发布</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">MM1</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">苹果，多模态MoE，无开源，有技术报告</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Grok-1</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">X，开源</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Qwen1.5-MoE-A2.7B</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">阿里巴巴，开源</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">DBRX</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">Databricks，开源</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Jamba</td>\n<td style=\"text-align: center;\">2024年3月</td>\n<td style=\"text-align: center;\">AI21，开源</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Mistral-8×22B</td>\n<td style=\"text-align: center;\">2024年4月</td>\n<td style=\"text-align: center;\">Mistral AI，开源</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">WizardLM-2-8×22B</td>\n<td style=\"text-align: center;\">2024年4月</td>\n<td style=\"text-align: center;\">微软，开源</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">天工3.0</td>\n<td style=\"text-align: center;\">2024年4月</td>\n<td style=\"text-align: center;\">昆仑万维，400BMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Arctic</td>\n<td style=\"text-align: center;\">2024年4月</td>\n<td style=\"text-align: center;\">Snowflake，480B，Dense-MoE\nHybrid，开源</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>MoE模型目前风头正劲，就连前不久小米汽车发布会上，雷总也弄了个多模态MoE大模型做汽车智能中控</p>\n<img src=\"/44e38c1b/xiaomi_moe.jpg\" class title=\"小米汽车多模态MoE模型\">\n<p>相信今年接下来的这段时间，MoE还会给我们带来更多的大新闻。</p>\n<p>本篇将初步梳理MoE相关的一些经典工作和几个近期发布的中文MoE模型，从背景、思路和效果来了解MoE模型。</p>\n<p>到文章发出的2024年4月为止，个人认为DeepSeek-MoE和Qwen1.5-MoE是中文领域做得比较好的两个工作，赶时间的朋友可以优先关注这两个工作。</p>\n<h1 id=\"时间线\">时间线</h1>\n<p>这里先对后面会涉及的MoE相关工作，大致按时间线梳理一下，也列出一些关键信息包括模型结构、模型规模等。</p>\n<p>（很多经典的MoE工作都出自Google）</p>\n<h2 id=\"上古时代\">上古时代</h2>\n<p>首先是很多MoE相关论文都会引用的，发表在1991年的论文<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">《Adaptive\nMixtures of Local Experts》</a>，这篇文章出自Geoffrey Hinton和Michael I.\nJordan两位大神之手。虽然在更早的时候就有MoE相关概念的工作，如原文所提到的，1988年这个概念就有了</p>\n<blockquote>\n<p>This idea was first presented by Jacobs and Hinton at the\nConnectionist Summer School in Pittsburg in 1988.</p>\n</blockquote>\n<p>但是大部分MoE文章还是认为是这个工作奠定了MoE的基础。</p>\n<h2 id=\"rnn时代\">RNN时代</h2>\n<p>时隔二十多年，Google在2017年1月发布了<a href=\"https://arxiv.org/abs/1701.06538\">《Outrageously Large Neural\nNetworks: The Sparsely-Gated Mixture-of-Experts\nLayer》</a>，把MoE带进了LSTM，训出了最大137B参数，专家数达到128k的LSTM模型。</p>\n<h2 id=\"transformer时代\">Transformer时代</h2>\n<ol type=\"1\">\n<li><p>2020年6月，Google发布<a href=\"https://arxiv.org/abs/2006.16668\">《GShard: Scaling Giant Models\nwith Conditional Computation and Automatic\nSharding》</a>，把MoE应用在encoder-decoder结构的transformer模型上，每两层将一个FFN层替换成一个MoE层，训出了模型参数量从12.5B到600B的一系列MoE模型，每层最大专家数也达到2048个。</p></li>\n<li><p>2021年1月，Google发布<a href=\"https://arxiv.org/abs/2101.03961\">《Switch Transformers: Scaling\nto Trillion Parameter Models with Simple and Efficient Sparsity》</a>\n，在T5（encoder-decoder结构）的基础上，把FFN层替换成MoE层，并简化了routing策略，训出了最大1.6T参数量的switch\ntransformer。Switch\nTransformers对scaling、蒸馏等做了很多详细的探索，影响深远，是很重要的一个工作。</p></li>\n<li><p>2022年2月，Google发布<a href=\"https://arxiv.org/abs/2202.08906\">《ST-MoE: Designing Stable and\nTransferable Sparse Expert\nModels》</a>，也是一个基于encoder-decoder结构的MoE模型，最大模型有269B的总参数，32B的激活参数。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，个人认为其重要程度相比Switch\nTransformer都有过之而无不及。</p></li>\n</ol>\n<h2 id=\"gpt时代\">GPT时代</h2>\n<ol type=\"1\">\n<li><p>2021年12月，Google发布了GLaM，<a href=\"https://arxiv.org/abs/2112.06905\">《GLaM: Efficient Scaling of\nLanguage Models with\nMixture-of-Experts》</a>，训出了最大为1.2T参数量的decoder-only模型。（从encoder-decoder到decoder-only，可以看到Google内部在模型结构方向上也有很多不同的尝试）</p></li>\n<li><p>2024年1月，幻方量化发布<a href=\"https://arxiv.org/abs/2401.06066\">《DeepSeekMoE: Towards Ultimate\nExpert Specialization in Mixture-of-Experts Language\nModels》</a>，对在23年12月开源的DeepSeekMoE，给出了一些细节。</p></li>\n<li><p>2024年，Databricks的DBRX、阿里的Qwen1.5-MoE-A2.7B、Mistral\nAI的Mistral-8x22B等陆续发布。</p></li>\n</ol>\n<h1 id=\"奠基工作\">奠基工作</h1>\n<p>Geoffrey Hinton和Michael I. Jordan的<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">《Adaptive\nMixtures of Local Experts》</a>是大多数MoE论文都会引用的最早工作。</p>\n<ol type=\"1\">\n<li>思路</li>\n</ol>\n<p>这篇文章大致的思路是这样的：对于比较复杂的任务，一般可以拆分为多个子任务。比如要求计算输入文本中有多少个动词和名词，那就可以拆分为“数动词”和“数名词”这两个子任务。</p>\n<p>而一个模型如果要同时学习多个子任务，多个子任务相互之间就会互相影响，模型的学习就会比较缓慢、困难，最终的学习效果也不好。</p>\n<p>因此这篇文章提出了一种由多个分开的子网络组成的监督学习方法。这些分开的网络，在训练过程中，分别学习处理整个训练数据集中的一个子集，也就是一个子任务。这个思路就是现代MoE的思路，每个子网络（也就是一个expert）学习处理一部分内容。</p>\n<p>文章里把这个MoE的方法应用于vowel discrimination\ntask，即元音辨别任务，验证了MoE设计的有效性。元音辨别指的是语音学中区分不同元音的能力，在语音学中，模型需要学习辨别不同的元音因素，以便准确地理解和识别语音输入。通过让多个子模型分别学习分别学习不同元音（a、e、i、o、u）辨别的子任务，最终效果得到了提升。</p>\n<ol start=\"2\" type=\"1\">\n<li>模型设计</li>\n</ol>\n<p>下图展示的就是这个MoE的思路：各个expert network和gating\nnetwork接收同样的输入，每个expert给出各自的处理结果；而gating\nnetwork输出每个expert的权重，就像一个开关一样，控制着每个expert对当前输入的打开程度，只是这个开关不是离散的，而是stochastic的，给出的不是true和false，而是权重。</p>\n<img src=\"/44e38c1b/vanilla_moe.png\" class title=\"Vanilla MoE\">\n<ol start=\"3\" type=\"1\">\n<li>损失函数优化</li>\n</ol>\n<p>实际上，MoE这个idea在这篇文章之前就有了。如论文中所提，Jacobs和Hinton在1988就讨论过。但是之前的工作在loss的设计上，和ensemble更相近，多个expert之间更倾向于合作，每个expert会学习其他expert的residual部分。</p>\n<p>具体来说，对于case <span class=\"math inline\">\\(c\\)</span>，假设第\n<span class=\"math inline\">\\(d^c\\)</span> 是对应的ground truth，第 <span class=\"math inline\">\\(i\\)</span> 个expert的输出是 <span class=\"math inline\">\\(o_{i}^c\\)</span>，<span class=\"math inline\">\\(p_{i}^c\\)</span> 是gating network给第 <span class=\"math inline\">\\(i\\)</span>\n个expert分配的权重，那么以前的工作所使用的损失函数 <span class=\"math inline\">\\(E^{c}\\)</span> 计算如下</p>\n<p><span class=\"math display\">\\[E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>这样的损失计算方式，是把期望输出和所有expert输出的混合结果进行比较。</p>\n<p>这样做的结果是，在训练过程中，每个expert学习的其实是其他expert的组合结果所剩下的残差。这样的学习目标并不能很好迫使每个expert单独输出好的结果，因此不能得到稀疏的模型。</p>\n<p>从另一个角度来看，这个损失计算把所有专家耦合在了一起。即当一个expert的输出发生了变化，所有expert的组合结果也会变化，其他所有的expert也需要做相应的改动来适应这个变化。因此各个expert之间更加倾向于合作，而不是相互竞争并单独给出好的结果，让gating\nnetwork输出稀疏的结果。</p>\n<p>虽然可以使用如增加辅助损失函数的做法，迫使模型给出稀疏激活的结果，但是这样相当于增加了很强的先验正则化，对模型最终效果也是有损害的。</p>\n<p>而Hinton和Jordan在这个工作里，提出更简单的做法是对loss计算进行修改，使得各个expert之间的关系从合作变成竞争。</p>\n<p>假设gating network每次随机选择一个expert，损失计算如下</p>\n<p><span class=\"math display\">\\[E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>在这个损失函数中，每个expert的输出结果会单独和期望结果进行对比，这就要求每个expert单独给出完整的结果，而不是仅学习其他expert的残差。</p>\n<p>这样的loss计算具有localization的特性，即如果一个训练case错了，那么会被修改的主要是被gating\nnetwork选中且出错的expert，以及负责分配权重的gating\nnetwork，而不会很大地影响其他expert。</p>\n<p>此外，localization还体现在，每个expert只会负责处理输入空间中某个特定子空间的向量，而不是完整的输入空间。</p>\n<p>这样一来，不同的expert之间不会直接相互影响，虽然还是有间接的影响，比如某个expert的输出变了，gating\nnetwork可能会分配新的权重，但是至少不会改变其他expert\nerror的符号（+，-），即优化的方向。</p>\n<p>最终的结果是，对于给定的输入，这样的系统会倾向于以高权重分配单一一个expert来预测结果（但其他权重还不是真正的0，不是真正的稀疏）。</p>\n<ol start=\"4\" type=\"1\">\n<li>实操技巧</li>\n</ol>\n<p>上面提出的这个loss计算，理论上没有问题，实际上也能训练，但是为了得到更好的效果，作者把原loss计算作了如下变化：先指数化再求和，最后再取对数，得到了优化loss。看下变化前后的对比</p>\n<p><span class=\"math display\">\\[\\text{原loss：}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p><span class=\"math display\">\\[\\text{优化loss：}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}\\]</span></p>\n<p>这样做有什么好处呢？来对比一下原loss函数和优化后的loss函数的求导结果</p>\n<p><span class=\"math display\">\\[\\text{原loss导数：}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p><span class=\"math display\">\\[\\text{优化loss导数：}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p>相比原loss函数的导数，优化后的loss函数的导数，把当前第 <span class=\"math inline\">\\(i\\)</span>\n个expert的表现，和其他expert联系起来了。这样能够更好地衡量expert <span class=\"math inline\">\\(i\\)</span>\n对当前case的处理结果好坏。特别是在训练初期，gating\nnetwork的权重是近似平均分配的，那么使用原loss函数的结果是，对当前case效果最好的expert，学习速度是最慢的（因为loss最小）；而优化的loss函数则可以让当前最好的expert的学习速度最快。相当于让“有天赋”的专家在对应的子任务上尽快提高水平。这样就强化了localization的特征，使得各个expert更快拟合到自己擅长的部分，加速训练。</p>\n<p>（BTW，优化后的这个loss导数，和现在的对比学习形式上看起来也很相似）</p>\n<p>这个工作在今天看来不很复杂，但是思路还是很踏实有效的，给MoE奠定了基础。</p>\n<h1 id=\"lstm-moe\">LSTM MoE</h1>\n<p>Google在2017年1月发布了 <a href=\"https://arxiv.org/abs/1701.06538\">《OUTRAGEOUSLY LARGE NEURAL\nNETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS\nLAYER》</a>，把MoE应用到了LSTM上，训出了最大137B的LSTM模型。这样规模的模型哪怕放在7年后的今天，也是巨无霸的存在，需要解决很多工程问题。</p>\n<p>相比1991年的工作，这里做到了真正的稀疏激活，从而可以在实际计算量较少的情况下，训练巨大的模型。</p>\n<h2 id=\"背景\">背景</h2>\n<p>虽然当时Transformer还没出来，大规模模型的竞赛也还不像今天这么激烈，但是在多个领域中（文本、图像、音频），已经有不少工作反复证实了一件事：模型容量越大，能训出来的效果越好，上限越高。但是模型越大，需要的训练数据也就越多，二者共同作用下，就造成了训练开销基本是随着模型增大，以平方关系在增长。</p>\n<p>在这个背景下就出现一些conditional\ncomputation，条件计算的工作来解决这个问题。conditional\ncomputation就是根据输入，有选择地只激活部分网络模块。那么MoE其实就是一种条件计算的实现。由于不用激活全部参数，训练所需的计算量就大大减小，整体计算成本就不用以平方速度增长。</p>\n<p>虽然理论上计算量的成本下来了，不过实操起来还是会遇到几个问题：</p>\n<ul>\n<li>训练的时候，在MoE结构下，每个expert的batch size比整个模型的batch\nsize小了。<br>\n比如模型的batch\nsize是32，一共有16个expert，那实际上一次迭代平均每个expert只能分到2个训练样本。而batch\nsize对训练效率影响是很大的，大的batch\nsize摊小了参数传输和更新的成本。如果直接增大模型的batch\nsize，又会受显存和通讯效率的限制。<br>\n</li>\n<li>训练数据量不足。<br>\n要训大模型就需要大量的数据，让模型参数充分学习。在当时的背景下，大规模的NLP数据是比较缺的。当然如今数据集多了很多，特别是预训练数据，这个问题现在来看没有那么突出了。<br>\n</li>\n<li>损失函数的设计。<br>\n如何使用合适的损失函数来训练模型，提升效果，并且使得模型的负载比较均衡，这是一个不容易解决的问题。<br>\n</li>\n<li>集群通讯问题。<br>\n一个GPU集群的计算能力可能比设备间网络带宽的总和高出数千倍，因此设备间的通讯很可能成为训练效率的瓶颈。为了计算效率，就要使得设备内计算量和所需的通讯量的比值，达到相应的比例。<br>\n</li>\n<li>GPU计算特点。<br>\nGPU做数学计算很快，但是并不擅长做branching（if/else），因此MoE的工作基本上都是用gating\nnetwork来控制参数的激活。这个严格来说不算是新的挑战了，应该说是根据计算设备沿用下来的设计。</li>\n</ul>\n<p>要解决好这些问题，才能训出比较好的模型来。</p>\n<h2 id=\"模型设计\">模型设计</h2>\n<ol type=\"1\">\n<li>整体结构</li>\n</ol>\n<p>先看下模型结构的设计。</p>\n<p>论文里使用的是两个LSTM层，中间夹着一个MoE层，最上面和最下面分别还有一个embedding层和一个任务输出层，结构如下图所示</p>\n<img src=\"/44e38c1b/rnn_moe.png\" class title=\"LSTM MoE\">\n<p>每个expert是一个简单的feed-forward neural\nnetwork。一共有n个expert，gating network输出是一个稀疏的n维向量</p>\n<p><span class=\"math display\">\\[\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(E_{i}(x)\\)</span> 是第 <span class=\"math inline\">\\(i\\)</span> 个expert的输出，<span class=\"math inline\">\\(G(x)_{i}\\)</span> 是gating network给出的第 <span class=\"math inline\">\\(i\\)</span> 个expert的权重。</p>\n<p>如果 <span class=\"math inline\">\\(G(x)_{i}\\)</span>\n为0，就不用计算对应的那个expert了，节省了计算。</p>\n<p>如果expert的数量特别多，可以用two-level hierarchical\nMoE，即使用两层gating network，第一层的gating\nnetwork先选择一个包含一批expert的分支，每个分支又有一个单独的gating\nnetwork来选择具体的expert。类似word2vec训练所用的hierarchical\nsoftmax。这样做可以节省一些计算。</p>\n<ol start=\"2\" type=\"1\">\n<li>gating network</li>\n</ol>\n<p>那具体gating network怎么设计呢？</p>\n<p>如果对输入进行线性变换，再简单加上一个softmax，那得到的是一个非稀疏的gating\nfunction</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot\nW_g)\\end{aligned}\\]</span></p>\n<p>在这个基础上，使用一个topk函数，只保留最大的k个值，其他都设为﹣∞（softmax之后变成0），这样就能只选择部分expert，得到了稀疏性。</p>\n<p>论文提到，虽然理论上这个形式的sparsity（topk）会造成gating\nfunction的不连续，不过在实操中暂时没有遇到相关问题。</p>\n<p>在这个基础上，在输入再加上一个Gaussian\nnoise，这个noise的大小由另外一个可学习的参数来控制。整体的计算公式如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[KeepTopK(v,k)_i=\\begin{cases}v_i&amp;\\text{if\n}v_i\\text{ is in the top }k\\text{ elements of\n}v.\\\\-\\infty&amp;\\text{otherwise.}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p>其中用来调整noise的非线性函数softplus是个类似ReLU的激活函数，但是更为光滑，函数图像如下</p>\n<img src=\"/44e38c1b/softplus.png\" class title=\"softplus\">\n<p>这里添加噪声的原因和负载均衡有关，下面来分析下负载均衡。</p>\n<h2 id=\"负载均衡\">负载均衡</h2>\n<p>在MoE模型训练的实验中观察到，如果不对gating\nnetwork进行干预，任由模型自由学习，那么最终模型会倾向于收敛到“总是选那几个固定的expert”的状态，而其他expert几乎不会被使用。这就是负载不均衡的状态，如果这些专家分布在不同的计算设备上，结果就是有些设备输入排队特别长，而有些设备基本处于闲置状态，这明显不是我们想要的。</p>\n<p>这种负载不均衡的状态有自我加强的属性，因为一旦开始出现部分专家被较多选中激活，这些专家就会得到更充分的训练，从而获得更好的效果，进而又提升被选中激活的概率。</p>\n<p>针对这种情况，之前有一些工作使用hard\nconstraint来缓解，比如当某个expert激活次数达到上限，就把它从候选集合中移除。hard\nconstraint明显会对模型效果有影响。而这篇论文使用的是一种soft\nconstraint。</p>\n<p>具体来说，对于每个expert，定义了一个它在当前这批输入数据里的重要性指标，如以下公式所示</p>\n<p><span class=\"math display\">\\[Importance(X)=\\sum_{x\\in\nX}G(x)\\]</span></p>\n<p><span class=\"math inline\">\\(G(x)\\)</span> 是gating\nnetwork给出的权重，是一个维度等于expert数量的向量。</p>\n<p>基于这个重要性指标，论文定义了一个辅助损失 <span class=\"math inline\">\\(L_{importance}\\)</span>，训练时和模型的交叉熵损失加到一起。<span class=\"math inline\">\\(L_{importance}\\)</span> 的计算方式如下</p>\n<p><span class=\"math display\">\\[L_{importance}(X)=w_{importance}\\cdot\nCV(Importance(X))^2\\]</span></p>\n<p>其中权重 <span class=\"math inline\">\\(w_{importance}\\)</span>\n是手动设置的超参，实验的推荐值是0.1，CV是coefficient of variation。</p>\n<p>coefficient of\nvariation离散系数，是概率分布离散程度的一个归一化量度，定义为标准差\n<span class=\"math inline\">\\(\\sigma\\)</span> 和 均值 <span class=\"math inline\">\\(\\mu\\)</span> 的比值。</p>\n<p>对于MoE来说，确定激活的expert数之后，均值是固定的。如果expert的gating很不平衡，标准差就会很大，离散系数也会很大，使得\n<span class=\"math inline\">\\(L_{importance}\\)</span> 变大。</p>\n<p>但是这里还是有问题，虽然均衡的负载可以推导出 <span class=\"math inline\">\\(L_{importance}\\)</span> 较小的结论，但是 <span class=\"math inline\">\\(L_{importance}\\)</span>\n较小却不能保证负载均衡。也就是说 <span class=\"math inline\">\\(L_{importance}\\)</span>\n较小只是负载均衡一个必要不充分条件。</p>\n<p>比如一个expert可能以很高的权重被分配到一个样本，而另一个expert可能以不太高的权重被分配到好几个样本。这种情况下对所有输入数据的gating权重进行求和，仍然可能呈现出均匀的表象（离散系数比较小），但这并不符合我们的要求。</p>\n<p>为了解决这个问题，需要额外再加上一个损失 <span class=\"math inline\">\\(L_{load}\\)</span>\n。这里就要用到添加在每个expert输出上的随机噪音了。</p>\n<p>我们想要各个expert的负载均衡，也就是每个专家需要处理的样本数基本一致，但是分配到各个专家的样本数是个离散值，因此没有办法直接用于back\npropagation，而 <span class=\"math inline\">\\(L_{load}\\)</span>\n就是对各个expert负载的一个平滑评估。</p>\n<p>回想一下前面在设计MoE的时候，定义了 <span class=\"math inline\">\\(H(x)\\)</span> 为KeepTopK函数的输入</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p>那么这里先定义一个 <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>，表示在除去\n<span class=\"math inline\">\\(H(x)\\)</span> 中的第 <span class=\"math inline\">\\(i\\)</span> 个分量之后，排在第 <span class=\"math inline\">\\(k\\)</span> 大的值。基于这个，再定义 <span class=\"math inline\">\\(P(x,i)\\)</span>\n为：固定其他分量已经选取好的noise，重新给第 <span class=\"math inline\">\\(i\\)</span> 个分量再添加一次noise，结果比 <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n大的概率，公式如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\\\&gt;kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}\\]</span></p>\n<p>通过这个noise，我们把“第 <span class=\"math inline\">\\(i\\)</span>\n个专家是否处理这个输入”的离散值，变成“第 <span class=\"math inline\">\\(i\\)</span>\n个专家处理这个输入的概率”这样一个平滑的估计，<span class=\"math inline\">\\(P(x,i)\\)</span>\n就表示这个概率。这个概率可以简化写成</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)&amp;=\\Phi\\Big(\\frac{(x\\cdot\nW_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot\nW_{noise})_i)}\\Big)\\end{aligned}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\Phi\\)</span>\n是标准正态分布的CDF。</p>\n<p>接下来就可以把第 <span class=\"math inline\">\\(i\\)</span>\n个expert的负载定义为</p>\n<p><span class=\"math display\">\\[\\begin{aligned}Load(X)_i=\\sum_{x\\in\nX}P(x,i)\\end{aligned}\\]</span></p>\n<p>有了每个expert的负载衡量，就可以和前面第一个负载均衡损失一样，计算新的负载均衡损失了</p>\n<p><span class=\"math display\">\\[L_{load}(X)=w_{load}\\cdot\nCV(Load(X))^2\\]</span></p>\n<p><span class=\"math inline\">\\(w_{load}\\)</span>\n是手动设置的超参，实验的推荐值是0.1。</p>\n<p>相比前面的 <span class=\"math inline\">\\(L_{importance}(X)\\)</span>，<span class=\"math inline\">\\(Load(X)\\)</span>\n是对负载是否均衡更细粒度的评估。</p>\n<p>论文中提到一个细节，在刚开始训练的时候，希望模型分配的expert尽量均衡，因此把\n<span class=\"math inline\">\\(W_g\\)</span> 和 <span class=\"math inline\">\\(W_{noise}\\)</span>\n都设为0，这样相当于没有信号，也没有噪音。</p>\n<p>最终使用负载均衡之后的效果如下</p>\n<img src=\"/44e38c1b/rnn_moe_load_function.png\" class title=\"负载平衡效果\">\n<p>使用这两个负载均衡损失之后，能达到接近完全平均分配的效果。</p>\n<h2 id=\"实验\">实验</h2>\n<ol type=\"1\">\n<li>解决工程问题</li>\n</ol>\n<p>针对前面提出的一些工程问题，论文给出一些方案</p>\n<p>（1）batch size减小</p>\n<p>由于稀疏激活的原因，每个expert的batch\nsize会变小。假设每次在n个expert中选择k个，模型训练的batch\nsize为b，那么每个expert的batch\nsize就是kb/n。论文通过以下这几种方法来提升每个expert的batch size：<br>\n-\n混合使用数据并行和模型并行。本来在使用数据并行的情况下，每个模型副本是异步处理各自的数据的。而这里做了优化，各个副本的batch是同步处理的，这样就可以把多个模型副本的batch组合起来。对于非MoE部分的参数，依然使用标准的数据并行机制；而对于每个expert，则在整个集群中只保留一个副本。如果模型分布在d个设备上，那每个expert就能得到一个kbd/n的batch\nsize。 - 对于LSTM模型，在时间步上展开，就能把batch\nsize提升相应的倍数。</p>\n<p>（2）集群通讯问题</p>\n<p>另一个挑战就是平衡集群计算量和通讯量的关系。</p>\n<p>对于每个expert来说，主要的通讯就是input和output的传输。而每个专家的主要计算量就是两个全连接层，大小分别为[input_size,\nhidden_size]和[hidden_size,\noutput_size]。对于GPU来说，计算速度可能是通讯速度的1000倍，那我们就需要把计算量设计得足够大。最简单的做法就是把hidden_size提高，使得每个expert的内部计算量比通讯量大1000倍，以保证通讯不会成为训练的瓶颈。</p>\n<ol start=\"2\" type=\"1\">\n<li>模型容量 &amp; 参数效率</li>\n</ol>\n<p>为了验证模型容量提升带来的收益，以及MoE模型的参数效率（即和dense模型同样推理计算量下能达到的效果），训练了包含4/32/256个expert的flat\nMoE模型，和包含256/1024/4096个expert的hierarchical\nMoE模型。每个expert大约是1M参数量，对于所有flat模型都是激活4个expert，而对于hierarchical\nMoE是每层gating激活2个。</p>\n<p>效果如下图。左边的图显示，随着模型容量提升，测试的ppl有明显下降。右边的图将相近模型容量的dense模型和MoE模型的效果放在一起对比，可以看到MoE模型在相同模型容量下，效果更好</p>\n<img src=\"/44e38c1b/rnn_moe_perf.png\" class title=\"效果\">\n<ol start=\"3\" type=\"1\">\n<li>更大的模型</li>\n</ol>\n<p>前面几个模型训练用的数据量不是很大，模型最大也只有4B左右，训练不久就出现diminishing\nreturns。</p>\n<p>为了验证更大数据集 + 更大模型的收益，在100B\ntoken的语料上，分别训了包含32, 256, 1024，4096, 16384, 65536,\n和131072个expert的MoE模型，最大的模型达到了137B的参数量。</p>\n<p>各个模型对比如下表。整体来看，增加数据和模型容量，是可以继续获得提升的。</p>\n<img src=\"/44e38c1b/rnn_moe_137b.png\" class title=\"137模型效果\">\n<p>从这里还可以看出，在专家数量不太多时，提升专家数量效果有提升，但是收益会慢慢减小，甚至会出现专家数量太多，效果反而下降的情况。</p>\n<ol start=\"4\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>按照MoE的设计思路，不同的专家应该学习到不同的子任务，但是实际上是否是这样呢？</p>\n<p>论文里把模型中不同的专家分配到token拿出看，发现确实有比较强的specialization效果，不同的专家处理不同的内容，如下所示</p>\n<img src=\"/44e38c1b/rnn_moe_specilized.png\" class title=\"RNN MoE 专门化\">\n<h1 id=\"gshard\">GShard</h1>\n<ol type=\"1\">\n<li>简介</li>\n</ol>\n<p>2018年，随着Bert的发布，transformer结构彻底火了起来。2020年6月，Google发布《GShard:\nScaling Giant Models with Conditional Computation and Automatic\nSharding》，把MoE用到了encoder-decoder结构的transformer模型上。MoE开始变成我们现在熟悉的样子了。</p>\n<p>GShard这个工作做了很多的实验，训了很多规模巨大的MoE模型，最大的达到了600B。训练的一系列模型的参数如下表</p>\n<img src=\"/44e38c1b/gshard_moe_family.png\" class title=\"GShard MoE family\">\n<p>在expert数量的设计上，延续上面LSMT MoE工作的思路 --\nexpert越多，效果越好。（站在24年这个时间节点来看，太多的expert未必适合；但是也不能说这个思路一定错误，毕竟事物的发展是螺旋式的，就像ChatGPT出来之前大多数人都在魔改各种Bert，而GPT已经坐了几年冷板凳了。）</p>\n<p>GShard论文中很大的篇幅在介绍工程实现和优化，这也是MoE模型训练最大的痛点。关于工程框架的内容比较硬核，因此这里不会展开讲太多，而是关注在模型算法层面上。</p>\n<ol start=\"2\" type=\"1\">\n<li>模型设计</li>\n</ol>\n<p>先来看下模型设计。</p>\n<p>Google在那段时间走的是encoder-decoder\ntransfomer的技术路线，因此GShard也是基于encoder-decoder\ntransfomer的模型结构。</p>\n<p>GShard的模型设计是，在encoder和decoder中，每两层把其中一个FFN层替换成MoE层。对于总共有N层的模型，则有N/2个MoE层，如下图</p>\n<img src=\"/44e38c1b/gshard_model.png\" class title=\"GShard模型结构\">\n<p>每层会选择最多top-2 expert来激活。为什么是最多，后面解释。</p>\n<p>GShard在上面这篇LSTM MoE论文的基础上，改进了gating\nfunction和auxiliary loss function。</p>\n<p>从公式来看，MoE层的具体计算如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{G}_{s,E}&amp; =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)&amp; =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}&amp; =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s)\n\\end{aligned}\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(x_s\\)</span> 是MoE的输入token，<span class=\"math inline\">\\(w_i\\)</span> 和 <span class=\"math inline\">\\(w_o\\)</span>\n分别是输入输出的线性变换矩阵。向量<span class=\"math inline\">\\(\\mathcal{G}_{s}\\)</span> 就是gating\nfunction的输出。</p>\n<p>GShard在gating\nfunction的设计上提出了两个要求：（1）负载均衡（2）高效扩展。</p>\n<p>负载均衡和前面讲的一样，很好理解。而为什么要高效扩展，因为如果要对N个token分别进行E个expert的分配，在N能达到百万甚至千万级别，而E也有几百上千的情况下，就需要一个高效的分布式实现，以免其他计算资源等待gating\nfunction。</p>\n<p>为了满足这些要求，gating function提出了以下机制</p>\n<p>（1）专家容量 expert capacity</p>\n<p>为了确保负载平衡，我们不希望有少量expert需要处理很多token，因此强制规定了每一个expert所负责处理的token数量有一个最大值，这个最大值就叫专家容量，在这里设置为2N/E，相当于平均分配的量。</p>\n<p>这个expert capacity通过GATE(·)给每个expert维护一个计数器 <span class=\"math inline\">\\(c_e\\)</span>\n来监控。如果一个token所选的两个专家当前处理量都已经超过设定的专家容量，那么这个token就不会被当前层的任何expert处理，而是直接通过残差链接透传到下一层。</p>\n<p>（2）分组分配 Local group dispatching</p>\n<p>给所有输入token分成了G组，不同的组并行处理，每个组相应地也把组内专家容量变成2N/EG。</p>\n<p>这样做相当于在前向推理时，把大的batch拆分成小的batch，每个小的batch就是一个group。这样做的好处是通讯的时候（特别是all2all）只需要在每个group内进行就可以了，减少了通讯量。</p>\n<p>而进行反向计算的时候这些group可以合起来一起用，相当于进行了gradient\naccumulation。</p>\n<p>（3）辅助损失函数 Auxiliary loss</p>\n<p>光设置专家容量并不能使得gating负载均衡，而且会导致大量溢出。参考前面LSTM\nMoE的工作，这里也定义了一个辅助损失函数，来帮助负载均衡。辅助损失函数设计如下</p>\n<p><span class=\"math display\">\\[\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot\nm_e\\]</span></p>\n<p><span class=\"math inline\">\\(S\\)</span> 是token数，<span class=\"math inline\">\\(E\\)</span> 是专家数，<span class=\"math inline\">\\(c_e\\)</span> 是分配给第 <span class=\"math inline\">\\(e\\)</span> 个专家的token数，<span class=\"math inline\">\\(m_e\\)</span> 是第 <span class=\"math inline\">\\(e\\)</span> 个expert在 <span class=\"math inline\">\\(S\\)</span> 个token中获得的平均权重。</p>\n<p>思路是，本来是要算 <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>\n的平方的，但这是离散值不可导，因此把平方中的一个 <span class=\"math inline\">\\(\\frac{c_e}S\\)</span> 换成了 <span class=\"math inline\">\\(m_e\\)</span> ， <span class=\"math inline\">\\(m_e\\)</span> 是第 <span class=\"math inline\">\\(e\\)</span> 个expert在 <span class=\"math inline\">\\(S\\)</span>\n个token中获得的平均权重。在平均分配的情况下，这个loss达到最小。</p>\n<p>相比前面的负载均衡损失，这个loss的设计就简单许多。</p>\n<p>gating的整个算法如下</p>\n<img src=\"/44e38c1b/gshard_algo_1.png\" class title=\"GShard gating 算法\">\n<p>（4）随机路由 Random routing</p>\n<p>前面提到，每层会选择最多top-2\nexpert来激活，就是因为有随机路由的机制。直观来说，就是认为如果top-1专家的权重很高，而第二个专家的权重如果较小，那很有可能只用第一个专家就足够解决问题了。</p>\n<p>随机路由的机制是top-1的专家永远会被激活，而第二个专家如果权重很小，就认为它可以被忽略。具体来说，会以与第二个专家的权重g2成比例的概率激活第二个专家。</p>\n<ol start=\"3\" type=\"1\">\n<li>效果</li>\n</ol>\n<p>最后看一下模型在翻译任务上的效果</p>\n<img src=\"/44e38c1b/gshard_perf.png\" class title=\"GShard效果\">\n<h1 id=\"switch-transformer\">Switch Transformer</h1>\n<p>2022年4月，距离ChatGPT发布还有半年，Google发布了《Switch\nTransformers: Scaling to Trillion Parameter Models with Simple and\nEfficient Sparsity》（实际上2021年Google就提出Switch\nTransformer了）。</p>\n<p>Switch\nTransformer和GShard一样，是encoder-decoder结构，基于T5开发的，具有1.6T的参数，2048个expert。</p>\n<p>和前面的很多工作一样，Switch\nTransformer有一个出发点，那就是参数量越大，模型效果越好，并且可以通过稀疏激活来减少总计算量。</p>\n<p>但是相比其他工作，Switch\nTransformer给出了一个更为具体的描述，那就是模型参数量可以是一个独立于总计算量的，单独的缩放轴。也就是说，在改变参数量的同时，（几乎）不改变训练和推理的计算量，就可以带来效果的提升。因此Switch\nTransformer关注在“同样的FLOPS/token的计算量”下，如何扩大模型，提升效果。</p>\n<p>Switch Transformer所做的工作还是比较多的，包括：</p>\n<p>（1）模型结构简化：简化了Transformer上的MoE架构，提出Switch\nTransformer架构。</p>\n<p>（2）MoE to\ndense：把训出来的效果较好的MoE模型蒸馏到dense模型，在压缩MoE模型99%的参数的情况下，效果还是比直接训练dense模型好。</p>\n<p>（3）训练和微调技术：<br>\n- 首次使用bf16成功训练MoE模型<br>\n- 更适合MoE结构的模型初始化<br>\n- 增加的专家正则化，改善了稀疏模型的微调和多任务训练</p>\n<p>（4）训练框架：结合数据、模型和专家并行性，训练了超过1T参数的MoE模型。</p>\n<p>（5）多语言：在多语言数据集上训练，发现101种语言效果普遍有提升。</p>\n<p>（6）训练效率：在同样的FLOPS/token的计算量下，Switch\nTransformer模型收敛速度有数倍的提升。</p>\n<h2 id=\"模型设计-1\">模型设计</h2>\n<p>Switch\nTransformer的模型结构如下图，类似GShard，把transformer每层的FFN替换成MoE层</p>\n<img src=\"/44e38c1b/switch_transformer_structure.png\" class title=\"Switch Transformer 模型结构\">\n<p>Switch Transformer一个重要的改进点就是简化了gating\nfunction的做法（Switch Transformer论文里叫routing）。</p>\n<p>之前的工作大多探索了选择k个expert的做法，而Switch\nTransformer则直接把gating简化为只选择1个expert，即k=1。这样的MoE层叫做Switch\nlayer。</p>\n<p>这样简化之后，routing的实现更简单，router的计算量小了，也减少了通讯量。</p>\n<h2 id=\"负载均衡-1\">负载均衡</h2>\n<p>同GShard一样，Switch Transformer规定了一个专家容量expert\ncapacity，来限制每个expert在一个batch里能处理的最大token数。</p>\n<p>如果一个token被分配到了一个已经满载的expert，就会出现overflow，那这个token在本层就不会被处理，而是直接通过残差链接，透传给下一层。这点也同GShard一样。</p>\n<p>在Switch Transformer，专家容量通过容量系数capacity factor来控制。</p>\n<p><span class=\"math display\">\\[\\text{expert\ncapacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of\nexperts}}\\right)\\times\\text{capacity factor}.\\]</span></p>\n<p>一个大的capacity\nfactor意味着每个expert能够处理更多的token，从而减少overflow情况的发生，但是计算量和通讯量的压力也会增大，所以这是一个需要权衡的参数。</p>\n<p>下图给出了一个不同capacity factor下的例子</p>\n<img src=\"/44e38c1b/switch_transformer_diff_expert_capacity.png\" class title=\"不同的expert capacity\">\n<p>那么如何设定expert capacity呢？</p>\n<p>如果capacity\nfactor为1的话，只有在完全平均分配的时候，才不会出现overflow的情况。而太大的capacity\nfactor则可能造成算力和存储的浪费。</p>\n<p>首先，实验中发现expert的数量和overflow的数量之间没有什么关系，所以在所有实验中，所有MoE和Switch\nTransformer模型都用128个专家。</p>\n<p>不同的capacity\nfactor对模型影响如下表。可以看到，大的容量系数相对来说能取得更好的效果（因为更少的overflow），但是相应地，大容量系数的模型处理速度就会慢一些。</p>\n<img src=\"/44e38c1b/switch_transformer_capacity_effect.png\" class title=\"expert capacity的效果\">\n<p>经验上，低的token丢弃率对模型的scaling很重要，想要训练超大规模的模型，就要解决这个问题。而通过负载均衡损失就可以确保良好的平衡，使得在使用较小容量系数的情况下，overflow尽量少，从而兼顾效果和计算速度。</p>\n<p>关键问题来到负载均衡损失怎么设计。</p>\n<p>给定 <span class=\"math inline\">\\(N\\)</span> 个expert，和包含 <span class=\"math inline\">\\(T\\)</span> 个token的batch <span class=\"math inline\">\\(\\mathcal{B}\\)</span>，负载均衡损失是这么计算的</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(f_{i}\\)</span> 表示被分配到第 <span class=\"math inline\">\\(i\\)</span> 个expert的token数，这个不可导</p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(P_i\\)</span>\n表示整个batch每个token分配给第<span class=\"math inline\">\\(i\\)</span>\n个expert的概率的总和，这个可导</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p>这个损失的设计其实和GShard中的也是一样的。</p>\n<p>在完美平均分配的情况下，<span class=\"math inline\">\\(f\\)</span> 和\n<span class=\"math inline\">\\(P\\)</span> 这两个向量都是 <span class=\"math inline\">\\(1/N\\)</span>，这个时候负载均衡损失是最小的。</p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span>\n扫描了1e-5到1e-1，发现设为1e-2，已经足够大保持负载平衡，同时不过分影响模型收敛。</p>\n<p>观察到 <span class=\"math inline\">\\(\\sum_{i=1}^N(f_i\\cdot\nP_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N\\)</span>，所以负载均衡loss还乘了个\n<span class=\"math inline\">\\(N\\)</span>，这样可以保持无论使用多少个expert，在平均分配的情况下，loss都能保持相同的常数。</p>\n<h2 id=\"实验-1\">实验</h2>\n<ol type=\"1\">\n<li>一些训练的trick</li>\n</ol>\n<p>（1）选择性地使用bf16</p>\n<p>半精度训练会带来一些训练的不稳定。因此选择性地使用bf16，具体来说，routing\nfunction内部使用单精度，其他部分使用半精度，这样既不影响通讯，也能提高效果。</p>\n<p>为什么选择在routing提高精度？因为softmax对误差特别敏感，exponential计算会极大放大输入中的rounding\nerror，因此高精度对routing很重要。</p>\n<p>（2）较小的参数初始化</p>\n<p>从截断正态分布中抽取元素来初始化的模型参数，平均值 <span class=\"math inline\">\\(\\mu=0\\)</span>，标准差<span class=\"math inline\">\\(\\sigma=\\sqrt{s}/n\\)</span>，其中s是超参，n是权重张量中的输入单元数量（e.g.\nfan-in）。</p>\n<p>论文建议将默认的Transformer初始化尺度s=1.0减少10倍。这个方案在实验中既提高了质量又降低了训练不稳定性的可能性。初始化实验对比如下表</p>\n<img src=\"/44e38c1b/switch_transformer_init.png\" class title=\"初始化对比\">\n<p>（3）增大dropout</p>\n<p>由于Switch\nTransformer参数量很大，在微调的时候更容易过拟合，因此一个简单的方法就是增大dropout，效果如下</p>\n<img src=\"/44e38c1b/switch_transformer_dropout.png\" class title=\"dropout效果\">\n<p>可以看到大的dropout有效果，并且dense层保持0.1，只有expert层增大dropout效果更好。</p>\n<ol start=\"2\" type=\"1\">\n<li>scaling</li>\n</ol>\n<p>对Switch Transformer结构预训练的scaling做了一些实验。</p>\n<p>（1）Step-Basis</p>\n<p>首先是验证在固定训练step的条件下，增大expert数量带来的提升，如下图所示。</p>\n<p>左边是不同规模的模型在相同step下收敛的结果，可以看到在保持相同计算量的条件下，只通过增大专家数量来提升规模，就有明显的收益。右边则展示训练过程中，不同规模的模型在各个step下的效果。</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_step.png\" class title=\"step scaling\">\n<p>（2）Time-Basis</p>\n<p>虽然Switch\nTransformer可以保持计算量不变的情况下提升模型规模，但是专家数量的增多会带来额外的通讯成本，所以即使训练的step数相同，实际的训练时间也不同。因此这里要回答的问题是，给定一个固定的训练时长，Switch\nTransformer是否相比dense模型仍有收益。</p>\n<p>答案是肯定的。下图展示以训练时长为横轴，Switch\nTransformer和dense模型的效果对比。Switch\nTransformer收敛到dense模型最终效果的时间只有dense模型的1/7。</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_time.png\" class title=\"time scaling\">\n<p>（3）和更大的dense模型对比</p>\n<p>前面Switch\nTransformer和dense模型的比较，是基于相同计算量的前提。那么Switch\nTransformer是否具备超越更大规模dense模型的能力？</p>\n<p>下图在Step-Basis和Time-Basis对比了64个专家的Switch\nTransformer和T5-Large。无论是相同step还是相同时间下，Switch\nTransformer都有明显优势。</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_dense.png\" class title=\"dense对比\">\n<ol start=\"3\" type=\"1\">\n<li>SFT效果对比</li>\n</ol>\n<p>在GLUE和SuperGLUE等下游任务上微调，和dense模型对比。</p>\n<p>对于各个模型，每两百步进行一次eval，选最好的效果，尽量保证公平。结果如下表，大部分任务都有明显的提升。</p>\n<img src=\"/44e38c1b/switch_transformer_sft_result.png\" class title=\"sft对比\">\n<ol start=\"4\" type=\"1\">\n<li>模型蒸馏</li>\n</ol>\n<p>虽然Switch\nTransformer在相同计算量下效果更好，但是部署几百B甚至T级别的模型，还是不太方便，因此考虑把稀疏模型蒸馏到dense模型上来进行推理。</p>\n<p>论文中给出了几个蒸馏的技巧：<br>\n- 初始化的时候，把Switch\nTransformer模型中的非稀疏部分用于初始化dense模型<br>\n- 蒸馏所用的label，25%来自教师模型，75%来自ground truth，加权求和</p>\n<p>预训练模型的蒸馏效果如下，相比无蒸馏训练的dense模型，把同样计算量的稀疏模型蒸馏到dense模型，dense模型大约能获得Switch\nTransformer提升部分30%的增益。</p>\n<img src=\"/44e38c1b/switch_transformer_distill.png\" class title=\"蒸馏\">\n<p>更进一步，用不同规模的稀疏模型下进行蒸馏，结果如下表，可以实现高达99%的压缩率</p>\n<img src=\"/44e38c1b/switch_transformer_distill_diff_model.png\" class title=\"蒸馏\">\n<p>除了预训练模型，微调模型也可以蒸馏，效果如下，在SuperGLUE也有一定的提升</p>\n<img src=\"/44e38c1b/switch_transformer_distill_sft.png\" class title=\"sft蒸馏\">\n<h1 id=\"glam\">GLaM</h1>\n<ol type=\"1\">\n<li>简介</li>\n</ol>\n<p>2021年12月Google发表了《GLaM: Efficient Scaling of Language Models\nwith\nMixture-of-Experts》，训练出最大参数量为1.2T，每层包含64个专家，每个token激活参数量为96.6B的MoE模型。</p>\n<p>相比Switch Transformer，GLaM的训练数据量要大得多，达到了1.6T\ntoken。</p>\n<p>下表是论文中给出的，当时一些大规模模型的对比</p>\n<img src=\"/44e38c1b/glam_related_model.png\" class title=\"glam和相关模型\">\n<p>虽然模型总参数量比GPT-3（175B）大很多，但是训练成本却比GPT-3低很多，推理速度也更快，而且在多个NLP任务上的效果都超越了GPT-3，如下所示。</p>\n<img src=\"/44e38c1b/glam_compare_gpt3.png\" class title=\"glam和gpt3对比\">\n<img src=\"/44e38c1b/glam_compare_gpt3_2.png\" class title=\"glam和gpt3对比\">\n<ol start=\"2\" type=\"1\">\n<li>模型设计</li>\n</ol>\n<p>模型设计上，和Switch\nTransformer一样，每两层把一个FFN替换成MoE层。但是和Switch\nTransformer不同，GLaM用回了每次激活两个expert的方案，模型结构如下图。</p>\n<img src=\"/44e38c1b/glam_model.png\" class title=\"glam模型\">\n<p>除此之外，模型在结构上海做了一些其他改动：</p>\n<p>（1）位置编码</p>\n<p>使用XLNET的相对位置编码。</p>\n<p>（2）激活函数</p>\n<blockquote>\n<p>In the non-MoE Transformer feed-forward sub-layers, we replace the\nfirst linear projection and the activation function with the Gated\nLinear Unit，which computes the component-wise product of two linear\ntransformation of the input, followed by a Gaussian Error Linear\nUnit.</p>\n</blockquote>\n<ol start=\"3\" type=\"1\">\n<li>实验</li>\n</ol>\n<p>训练中的一些trick：</p>\n<p>（1）参考《Lingvo: a modular and scalable framework for\nsequence-to-sequence\nmodeling》，在梯度出现NaN或者Inf的时候就跳过那一步更新。</p>\n<p>（2）如果在BP更新的时候遇到NaN或者Inf，则重新加载更早的checkpoint并跳过有问题的数据来避免NaN或者Inf。</p>\n<p>论文训了一系列模型来探索MoE，这些模型的设置如下表</p>\n<img src=\"/44e38c1b/glam_family.png\" class title=\"glam模型系列\">\n<p>GLaM和dense模型的评测结果如下</p>\n<img src=\"/44e38c1b/glam_perf.png\" class title=\"glam模型效果\">\n<p>可以看到GLaM MoE的有效参数效率一致高于dense模型。</p>\n<h1 id=\"st-moe\">ST-MoE</h1>\n<p>2022年2月，Google发表了《ST-MOE: DESIGNING STABLE AND TRANSFERABLE\nSPARSE EXPERT\nMODELS》。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，可以说是MoE的必读论文。</p>\n<p>ST-MoE最大模型包含269B总参数量，和与32B\ndense模型相当的激活计算量。论文中把模型称为称为Stable Transferable\nMixture-of-Experts，或者ST-MoE-32B。</p>\n<p>在MoE层的使用上，ST-MoE比Switch\nTransformer更“节省”一点，每四层才替换1个MoE层。</p>\n<p>论文中主要训了两个规模的ST-MoE模型，分别有4B和269B的总参数量。ST-MoE以及其他用于对比的模型参数如下表</p>\n<img src=\"/44e38c1b/st_moe_models.png\" class title=\"ST-MoE模型及对比模型的参数\">\n<h2 id=\"稳定性与效果分析\">稳定性与效果分析</h2>\n<p>论文通过对乘性操作、噪音和裁剪这几个内容进行探索，来指导模型的设计。</p>\n<ol type=\"1\">\n<li>乘性操作对模型稳定性和效果的影响</li>\n</ol>\n<p>论文首先研究了乘性操作对模型的训练稳定性和最终效果的影响。</p>\n<p>之前已经有一些工作表明更多的乘法对模型效果有收益。</p>\n<blockquote>\n<p>Some architectural improvements involve more multiplications than\nadditions or do not sum many items at once</p>\n</blockquote>\n<p>（1）GELU Gated Linear Units (GEGLU)</p>\n<p>第一个例子是关于激活函数的。GLU是一个对两个输入向量进行component-wise相乘的操作，之后被扩展成GELU-Linear\nFFN变体，用于替换transformer中的ReLU FFN变体，其计算如下</p>\n<p><span class=\"math display\">\\[\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}\\]</span></p>\n<p>这样在一些其他工作里已经被证明了对模型效果有提升。</p>\n<p>（2）RMSNorm</p>\n<p>第二个例子是RMSNorm中的缩放参数，也就是下面公式的 <span class=\"math inline\">\\(g\\)</span>。</p>\n<p><span class=\"math display\">\\[y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot\ng_i\\]</span></p>\n<p>ST-MoE针对GEGLU和RMSNorm这两个乘性操作，做了实验，结果如下表。</p>\n<img src=\"/44e38c1b/st_moe_remove_multiplications.png\" class title=\"移除乘法操作的影响\">\n<p>发现移除乘性操作可以使模型稳定性更好（训练中发散的情况减少），但是最终效果变差了。</p>\n<p>（3）增加dense层</p>\n<p>ST-MoE还验证了在expert层增加更多dense层的效果。结果发现增加更多的乘法交互（增加dense层），可以在带来效果收益的同时，基本不影响推理速度，如下表所示。</p>\n<img src=\"/44e38c1b/st_moe_more_dense_layer.png\" class title=\"更多的dense层\">\n<p>（4）增加一个bias</p>\n<p>在FFN层的第一个矩阵乘法后面增加一个可学习的bias\nB，分别通过加法和乘法加入</p>\n<p><span class=\"math display\">\\[\\text{FFN}_{\\text{GEGLU}}+\\text{Add\nBias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2\\]</span></p>\n<p><span class=\"math display\">\\[\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot\nxW_{12})\\odot B]W_2\\]</span></p>\n<p>乘法的收敛速度更快，效果也更好。</p>\n<p>上面这些实验显示，后续在模型效果的探索方向可以往多使用乘性操作去考虑。</p>\n<ol start=\"2\" type=\"1\">\n<li>noise对模型稳定性和效果的影响</li>\n</ol>\n<p>接下来ST-MoE探索了“噪音可以提升模型稳定性”的假设。</p>\n<p>通过input-jitter，给router的输入logits乘以一个在[1e-2,\n1e2]之间的均匀随机变量来添加噪音。</p>\n<img src=\"/44e38c1b/st_moe_more_add_noise.png\" class title=\"增加noise\">\n<p>结果是增加noise之后，有助于让模型的收敛更加稳定，但是对模型最终效果有负面影响。</p>\n<p>这里论文还提到，小模型上的结果不一定能直接推广到更大的模型上，比如在小模型上稳定的配置，在大模型就可能就不稳定了。因此还是需要在大模型上也进行充分实验。</p>\n<ol start=\"3\" type=\"1\">\n<li>限制激活值和梯度值对模型稳定性和效果的影响</li>\n</ol>\n<p>对activation和gradient进行限制是目前广泛应用的提升模型训练稳定性的手段。在反向传播过程中，通过裁剪梯度的范数来缓解梯度爆炸，就是一种常用的限制手段。</p>\n<p>但是在ST-MoE训练269B的大规模模型时，发现裁剪会使得模型收敛的效果很差。</p>\n<p>为了解决这个问题，ST-MoE在训练中引入了router z-loss，形式如下。</p>\n<p><span class=\"math display\">\\[L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2\\]</span></p>\n<p><span class=\"math inline\">\\(B\\)</span> 是token的数量，<span class=\"math inline\">\\(N\\)</span> 是专家数，<span class=\"math inline\">\\(x\\in\\mathcal{R}^{B\\times N}\\)</span>\n是router的输入。</p>\n<p>z-loss会对进入router的较大的logits值进行惩罚，以达到尽量减少进入指数函数的较大误差的目的。什么意思呢？后面来解释，先看下使用z-loss的效果。</p>\n<img src=\"/44e38c1b/st_moe_z_loss_result.png\" class title=\"z-loss效果\">\n<p>ST-MoE认为，在模型训练过程中，由于精度不足或者其他问题，会产生很大的值，从而引入误差。而对梯度进行裁剪是在误差发生之后，并且裁剪本身也造成了数据的不连续性，某种程度上，裁剪本身也是一种误差。相反地，z-loss自然地鼓励模型产生较小的对数值，因此可以更精确地建模。</p>\n<p>z-loss乘以一个权重超参 <span class=\"math inline\">\\(c_z\\)</span>\n加入到模型训练的总损失中，如下式所示。</p>\n<p><span class=\"math display\">\\[L_{tot}=L_{CE}+c_BL_B+c_zL_Z\\]</span></p>\n<p>ST-MoE经过实验，选择了<span class=\"math inline\">\\(c_z=0.001\\)</span>。</p>\n<p><span class=\"math inline\">\\(L_B\\)</span> 是 auxiliary load balance\nloss负载均衡损失，ST-MoE这里使用了和GShard/Switch\nTransformer所用的相同的损失计算，这里回顾一下：</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(N\\)</span> 是专家数， <span class=\"math inline\">\\(\\mathcal{B}\\)</span>是包含 <span class=\"math inline\">\\(T\\)</span> 个token的batch。<span class=\"math inline\">\\(f_{i}\\)</span> 表示被分配到第 <span class=\"math inline\">\\(i\\)</span> 个expert的token数，这个不可导；<span class=\"math inline\">\\(P_i\\)</span> 表示整个batch每个token分配给第<span class=\"math inline\">\\(i\\)</span> 个expert的概率的总和，这个可导。</p>\n<ol start=\"4\" type=\"1\">\n<li>数据精度对训练效率和训练效果的影响</li>\n</ol>\n<p>目前大部分的大模型训练都使用混合精度训练：模型权重以float32格式存储以进行梯度更新，然后在正向和反向传播的矩阵乘法中转换为bfloat16；此外，所有激活值都以bfloat16存储和操作，而allreduce通信可以在bfloat16或float32数值精度中进行。</p>\n<p>对于ST-MoE-32B的训练，allreduce的数值使用半精度可以加速训练，然而这也会使训练变得不稳定，因此ST-MoE保持allreduce的数值精度为float32。</p>\n<p>bfloat16和float32在不同范围的舍入误差如下表所示</p>\n<img src=\"/44e38c1b/st_moe_round_error.png\" class title=\"bf16精度损失\">\n<p>可以看到，表达的数值越大，舍入误差越大。而z-loss限制了数值大小，也就将误差值限制在比较小的范围。</p>\n<p>MoE模型天生对舍入误差敏感，因为它们由于router的使用而有更多的指数函数，而指数函数会将小的输入误差放大很多，这就加剧舍入误差所导致的训练不稳定。</p>\n<p>另外，ST-MoE有一个策略：只有当排第二的专家的权重大于等于第一的专家的1/5时，token才会被路由到其第二位专家，否则第二个专家就会被忽略。</p>\n<p>因此虽然舍入误差不会改变softmax运算中各个概率的排序，但它确实会影响MoE中第二个专家的激活。</p>\n<h2 id=\"模型设计-2\">模型设计</h2>\n<p>dense模型的设计有scaling\nlaw进行指导，但是MoE模型的设计比dense模型多出几个要考虑的点：</p>\n<p>（1）使用多少个expert</p>\n<p>（2）怎么routing</p>\n<p>（3）专家容量系数怎么定</p>\n<p>（4）硬件的影响</p>\n<p>（这里提到MoE模型的scaling law工作：《Unified scaling laws for routed\nlanguage models》，可以了解一下）</p>\n<ol type=\"1\">\n<li>使用多少个expert</li>\n</ol>\n<p>ST-MoE认为，从以往的经验来看，在总专家数量较少的情况下（如8/16/32），提升专家数量，能有收益。但是在特别稀疏的情况下（如激活专家数量&lt;1%），或者总专家数较大（比如&gt;256）之后，提升专家数量收益就很小了。</p>\n<p>从另一个角度来看，如果一个计算核心使用&gt;1个专家，那么就会出现比较大的加载参数张量的成本，因此建议每个计算核心使用&lt;=1个专家。</p>\n<ol start=\"2\" type=\"1\">\n<li>routing和capacity factor</li>\n</ol>\n<p>论文做了一系列实验来探索capacity factor的选择，如下表所示</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor.png\" class title=\"capacity factor\">\n<p>从这些实验中得到几个结论：</p>\n<p>（1）训练和推理的capacity factor增大都会有收益</p>\n<p>（2）如果硬件资源足够，推理的capacity\nfacotr可以设得比训练的时候大，会有进一步提升</p>\n<p>（3）激活的expert数量提升会有收益，但是收益随着capacity\nfactor提升而越来越小</p>\n<p>当然，选择capacity\nfactor还要看硬件的特性，如果通讯很快，可以适当增大capacity\nfactor，否则就不能选择太大的。</p>\n<p>下表展示了不同capacity factor对推理速度的影响</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor_speed.png\" class title=\"不同capacity factor推理速度\">\n<h2 id=\"实验-2\">实验</h2>\n<ol type=\"1\">\n<li>ST-MoE效果</li>\n</ol>\n<p>ST-MoE-32B在下游任务上和以往最佳结果对比如下表，ST-MoE-32B刷新了超过一半任务的最佳效果</p>\n<img src=\"/44e38c1b/st_moe_perf.png\" class title=\"不同capacity ST-MoE-32B效果\">\n<ol start=\"2\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>论文还对各个专家的专业化进行了追踪，发现decoder中几乎没有专业化的迹象，各种类型的token近乎随机分配给不同的专家。而在encoder中则表现出了高度专业化的特征，如下表</p>\n<img src=\"/44e38c1b/st_moe_encoder_specialization.png\" class title=\"encoder专业化\">\n<p>此外，还发现在多语言的模型的encoder中，专业化的情况并不想原先预想那样，按不同语言划分，而是每个专家都会处理一种语言的一部分token，如下表</p>\n<img src=\"/44e38c1b/st_moe_multiling_specialization.png\" class title=\"多语言专业化\">\n<h1 id=\"deepseekmoe\">DeepseekMoE</h1>\n<p>2024年1月，幻方量化开源了DeepseekMoE，是国内首个开源的MoE大模型。幻方还发布了论文《DeepSeekMoE:\nTowards Ultimate Expert Specialization in Mixture-of-Experts Language\nModels》，给出了一些DeepSeekMoE的细节内容，颇为实在了。</p>\n<p>DeepSeekMoE在其他MoE工作的基础上，进一步给出了2个模型设计的主要思路：</p>\n<p>（1）对expert的粒度进行细分，以提供更多样的expert激活组合；</p>\n<p>（2）对expert的类型进行区分，从所有expert中保留一部分作为shared\nexpert共享专家，这部分专家对所有输入都保持激活。</p>\n<p>这样的做法可以帮助每个expert达到更高程度的专业化(specialization)的水平，更好地学习不同的专业知识。</p>\n<p>DeepSeekMoE先在2B的较小MoE模型上进行了充分的实验，然后把方案应用到16B参数的MoE模型上，并获得了较好的效果。其中DeepSeekMoE-16B不需要量化就可以在40GB显存的设备上运行。</p>\n<p>DeepSeekMoE-2B模型具有和稠密2B模型相当的性能，而DeepSeekMoE-16B则具有和7B稠密模型相当的性能，且计算量仅为稠密模型的40%。</p>\n<p>DeepSeekMoE-16B的参数效率相比稠密模型有明显的优势，如下图所示</p>\n<img src=\"/44e38c1b/ds_moe_perf.png\" class title=\"deepseek moe\">\n<p>并且DeepSeekMoE-2B和16B模型都开源了。</p>\n<p>在前面实验的基础上，幻方还训练了DeepSeekMoE-145B的超大MoE模型，具有和稠密的DeepSeek-67B模型相当的表现，但计算量更小。这个后续也有机会放出来。</p>\n<h2 id=\"模型设计-3\">模型设计</h2>\n<p>MoE，mixture of\nexpert，顾名思义，一个最初始的motivation就是让不同expert学习不同的内容，然后再混合起来。</p>\n<p>比如最上面提到的1991年的工作里，就是让不同的expert学习不同的元音特征，以此提升特征提取的准确率。</p>\n<p>但是当前大部分的MoE架构都会遇到“knowledge hybridity”和“knowledge\nredundancy”的问题，即知识的杂糅和冗余：</p>\n<p>（1）知识冗余</p>\n<p>有些基础的常识在不同的领域都需要用到，每个expert就都会学一点，这样这些常识就被多个expert重复学习了。</p>\n<p>（2）知识杂糅</p>\n<p>在expert数量不够多的情况下，一个expert就可能要负责学习多个领域的内容。以学习高中知识为例，在只有两个expert的时候，只能一个expert学习理科知识，另一个学习文科知识；当我们有8个expert的时候，不同expert就可以分别学习语文、英语、历史、地理、物理、生物、化学、数学知识。显然后者所学知识的专业化程度更高。</p>\n<p>知识的杂糅和冗余阻碍了专家专业化(expert\nspecialization)的程度，也就阻碍了模型达到MoE结构理论上限性能。</p>\n<p>我们期望每个expert能够学习到non-overlap &amp; foucusd\nknowledge的知识。</p>\n<p>针对上面的问题，DeepSeekMoE的架构设计有2个主要策略：</p>\n<p>（1）Fine-Grained Expert Segmentation</p>\n<p>参数总量不变的情况下，将expert分成更细的粒度（每个expert更小）。这样可以带来更灵活的激活组合，让每个expert可以有更强的specialization。比如原本是16个expert选择激活2个，那么总的组合数是120种；如果把每个expert缩小为原来的1/4，那在总参数量和激活数量不变的情况下，是64个expert选择激活8个，那么总的排列组合数就是\n<span class=\"math inline\">\\(\\binom{64}8=4,426,165,368\\)</span>\n，排列组合数比原来多了很多。</p>\n<p>（2）Shared Expert Isolation</p>\n<p>把部分expert分离出来，保持永远激活。我们期望这部分专家能够学到在多个领域间都通用的common\nknowledge。这样的策略同样可以使得其他expert能够提高专业化的程度，并且减少不同expert间的知识冗余。还是以学习高中知识为例，数学、物理和化学都需要算术能力，如果让学这三个领域的expert都学习算术技能，就会有冗余；我们可以把通用算术的技能剥离出来，由一个助手专门负责算术任务，相当于给他们发了一个计算器，这样学习数学、物理和化学的expert就能把更多的精力放在专业知识上，也就能达到更好的专业化效果。</p>\n<p>下图展示了在传统MoE结构上增加Fine-Grained Expert Segmentation和Shared\nExpert Isolation策略的设计</p>\n<img src=\"/44e38c1b/ds_moe_structure.png\" class title=\"deepseek moe 结构\">\n<p>（expert isolation的思路最早可以追溯到2022年1月发表的《DeepSpeed-MoE:\nAdvancing Mixture-of-Experts Inference and Training to Power\nNext-Generation AI Scale》，这里就不展开了。）</p>\n<p>假设传统的MoE模型每层的expert数量为 <span class=\"math inline\">\\(N\\)</span>，激活expert数为 <span class=\"math inline\">\\(K\\)</span>，DeepSeekMoE使用的细粒度expert大小为原来的\n<span class=\"math inline\">\\(1/m\\)</span>，那DeepSeekMoE每层就有 <span class=\"math inline\">\\(mN\\)</span> 个expert，激活的expert数量为 <span class=\"math inline\">\\(mK\\)</span>个。假设 <span class=\"math inline\">\\(T\\)</span> 为输入长度，<span class=\"math inline\">\\(L\\)</span> 为模型层数，<span class=\"math inline\">\\(e_i^l\\)</span> 表示第 <span class=\"math inline\">\\(i\\)</span>\n个expert，DeepSeekMoE可以公式化为以下表示（忽略了layernorm）</p>\n<p><span class=\"math display\">\\[\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{\nFFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l\\]</span></p>\n<p><span class=\"math display\">\\[g_{i,t}=\\begin{cases}s_{i,t},&amp;s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant\nj\\leqslant mN\\},mK)\\\\0,&amp;\\text{otherwise,}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)\\]</span></p>\n<h2 id=\"负载均衡-2\">负载均衡</h2>\n<p>如之前工作反复提及的，如果任由MoE模型自主学习gating，可能会遇到两个问题</p>\n<p>（1）routing\ncollapse：专家分配的不均衡，也就是gating倾向于总是选择特定的少量expert，并且这种情况还会自我增强。</p>\n<p>（2）计算效率问题：多设备间，不平衡的负载可能会成为计算效率的瓶颈。</p>\n<p>针对routing collapse的问题，DeepSeekMoE引入一个expert-level balance\nloss，如下所示</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}&amp; =\\alpha_1\\sum_{i=1}^{N&#39;}f_iP_i\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_{i}&amp;\n=\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token\n}t\\text{ selects Expert }i)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}&amp; =\\frac1T\\sum_{t=1}^Ts_{i,t}\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_1\\)</span> 叫做expert-level\nbalance factor，是人工设定的超参。</p>\n<p>而 <span class=\"math inline\">\\(f_i\\)</span> 和 <span class=\"math inline\">\\(P_i\\)</span> 和Switch\nTransformer里的设定基本一样。</p>\n<p>在Switch Transformer里， <span class=\"math inline\">\\(f_i\\)</span>\n表示分配到第 <span class=\"math inline\">\\(i\\)</span>\n个expert的token数量。在DeepSeekMoE这里也是一样的含义，只是多乘了一个系数\n<span class=\"math inline\">\\(N&#39;/K&#39;\\)</span> ，其中 <span class=\"math inline\">\\(N&#39;=mN-K_s\\)</span>，<span class=\"math inline\">\\(K&#39;=mK-K_s\\)</span>，<span class=\"math inline\">\\(K_s\\)</span>\n是划分出来的共享expert的数量。这个系数是个常数，可以拿到求和符号外面，这样DeepSeekMoE里的\n<span class=\"math inline\">\\(f_i\\)</span> 就和Switch\nTransformer里的完全一样了。</p>\n<p><span class=\"math inline\">\\(N&#39;/K&#39;\\)</span>\n这个系数可以使得在使用不同的数量的expert时，在完美平均分配的情况下，负载均衡loss都是相同的常数。</p>\n<p><span class=\"math inline\">\\(P_i\\)</span> 表示所有每个token分配给第\n<span class=\"math inline\">\\(i\\)</span> 个expert的权重的总和，和Switch\nTransformer里的含义一样。</p>\n<p>注意这里 <span class=\"math inline\">\\(f_i\\)</span> 是不可导的，<span class=\"math inline\">\\(P_i\\)</span> 是可导的。</p>\n<p>针对多设备间负载均衡的问题，DeepSeekMoE引入一个device-level balance\nloss，如下所示</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}&amp; =\\alpha_2\\sum_{i=1}^Df_i&#39;P_i&#39;\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_i^{\\prime}&amp; =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}^{\\prime}&amp; =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_2\\)</span> 叫做device-level\nbalance factor，是人工设定的超参。</p>\n<p><span class=\"math inline\">\\(\\mathcal{E}_i\\)</span> 指第 <span class=\"math inline\">\\(i\\)</span> 个设备。</p>\n<p>device-level balance loss形式上和expert-level balance loss一样，只是\n<span class=\"math inline\">\\(f_i\\)</span> 和 <span class=\"math inline\">\\(P_i\\)</span>\n对应的对象从单个expert变成单个设备了。</p>\n<p>当我们的目标是缓解计算瓶颈时，我们不需要强制执行expert间的均匀分配，而只需确保设备之间计算量的平衡。比如我们每层有64个expert，均匀分布在8个设备上，我们只需要每个设备处理的token数平衡即可，在设备内部即使所有token都是同一个expert处理的，依然能满足设备间负载平衡的要求。</p>\n<p>相比expert间严格的负载平衡，只要求设备间的平衡是更松的限制条件，这样缓解了因为过度的负载平衡而损害模型性能的问题。</p>\n<h2 id=\"实验-3\">实验</h2>\n<ol type=\"1\">\n<li>小规模模型验证</li>\n</ol>\n<p>为了验证以上策略的有效性，先拿100B\ntoken的语料数据在DeepSeekMoE-2B模型做实验。词表也是通过BPE在语料上训练的8k词表，后面训练更大规模模型的时候再扩大词表。</p>\n<p>DeepSeekMoE-2B模型参数初始化方差为0.006，使用multi-head\nattention，前向激活参数量约0.3B，具体参数如下表</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"模型超参\">\n<p>relative expert\nsize指的是DeepSeekMoE所用的细粒度expert的大小和正常FFN层大小的比值。</p>\n<p>训练的具体参数设置如下</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\">属性</th>\n<th style=\"text-align: center;\">数值</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">optimizer</td>\n<td style=\"text-align: center;\">AdamW</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_beta_1</td>\n<td style=\"text-align: center;\">0.9</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">adam_beta_2</td>\n<td style=\"text-align: center;\">0.95</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_weight_decay</td>\n<td style=\"text-align: center;\">0.1</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">warmup schedule</td>\n<td style=\"text-align: center;\">linear</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">warmup step</td>\n<td style=\"text-align: center;\">2000</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">max lr</td>\n<td style=\"text-align: center;\">1.08e-3</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">dropout</td>\n<td style=\"text-align: center;\">0</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">sequence length</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">batch size</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">total step</td>\n<td style=\"text-align: center;\">25,000</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>其他训练细节：<br>\n- 所有expert放在单个GPU上，没有使用device-level balance loss<br>\n- expert-level balance factor设为0.01<br>\n- 训练到80%的时候，学习率乘以0.316，训练到90%的时候，再乘以0.316</p>\n<p>使用相同的100B训练数据，训了DeepSeekMoE-2B，在包含语言模型和下游任务的benchmark上和其他4个模型做对比：dense，hash\nlayer（也是一种moe，《Hash layers for large sparse models》），Switch\nTransformer，GShard。效果对比如下</p>\n<img src=\"/44e38c1b/ds_moe_comparison.png\" class title=\"deepseek moe 效果\">\n<p>可以得到几个结论：<br>\n- 更大的模型参数量和稀疏的架构，使得Hash Layer和Switch\nTransformer和具有同样激活参数的dense模型相比，有明显的优势。<br>\n- 同样的模型参数下，GSshard比Hash Layer和Switch\nTransformer有更多激活参数，效果也更好<br>\n- 同样的模型参数和激活参数下，DeepSeekMoE效果比GShard有明显优势。</p>\n<p>为了进一步探索DeepSeekMoE架构带来的收益，提升了dense模型和GShard模型的激活参数，直到效果和DeepSeekMoE-2B差不多。</p>\n<p>结果dense模型和GShard模型需要分别扩大到16倍和1.5倍的参数量，才能达到DeepSeekMoE-2B相近的效果，如下表所示</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_2b.png\" class title=\"deepseek moe upper bound\">\n<p>DeepSeekMoE的优势在更大规模的情况下，依然成立。训了DeepSeekMoE-13B,\n对比参数量提升至1.2和1.5倍的GShard，DeepSeekMoE-13B依然能match，具体如下表</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_13b.png\" class title=\"deepseek moe upper bound\">\n<ol start=\"2\" type=\"1\">\n<li>DeepSeekMoE架构消融实验</li>\n</ol>\n<p>针对DeepSeekMoE架构的两个主要设计，shared expert和fine-grained\nexpert进行消融实验。使用不同数量的共享专家和不同粒度的expert进行效果对比，结果如下图。</p>\n<img src=\"/44e38c1b/ds_moe_ablation.png\" class title=\"deepseek moe upper bound 消融实验\">\n<p>（1）对比蓝色和橙色，可以看到增加共享专家带来了收益</p>\n<p>（2）绿色和红色在橙色的基础上进一步把专家颗粒分得更细，效果进一步提升</p>\n<p>（3）共享专家和路由专家的比例：在总共64个expert的情况下，对比了1/2/4个共享专家的情况，结果并没有显著差别，在pile上的loss分别是1.808,1.806,1.811。最终选择了共享专家和激活路由专家1:3（2+6）的比例。</p>\n<ol start=\"3\" type=\"1\">\n<li>expert specialization的分析</li>\n</ol>\n<p>通过实验来验证DeepSeekMoE中expert specialization的优化。</p>\n<p>（1）前面实验看到DeepSeekMoE-2B和1.5倍参数量的GShard模型效果相当。在这个基础上，通过禁用不同数量的top专家，而只能从次优的专家中选择进行回答。</p>\n<p>实验结果如下</p>\n<img src=\"/44e38c1b/ds_moe_expert_specialization.png\" class title=\"专家专门化\">\n<p>发现DeepSeekMoE损失更大，说明DeepSeekMoE每个专家的专业化程度更好，必要性更高。</p>\n<p>（2）另外，通过禁用DeepSeekMoE的共享专家，而额外激活一个专家，发现loss也大大提升。这个结果突出了共享专家的关键功能，并表明共享专家捕捉到了与路由专家不共享的基本且重要的知识，使得它无法被路由专家替代。</p>\n<p>（3）只激活更少专家，也能和GShard达到相同水平，这一观察结果支持了DeepSeekMoE可以更准确和高效地获取所需知识的观点。</p>\n<img src=\"/44e38c1b/ds_moe_less_activated_expert.png\" class title=\"激活更少专家\">\n<p>此外还从零训了一个只用1个共享专家和3个激活专家的2b模型（正常是2个共享专家+6个激活专家），也比GShard好，说明DeepSeekMoE的有效参数效率更高</p>\n<img src=\"/44e38c1b/ds_2b_less_expert.png\" class title=\"2B激活更少专家\">\n<ol type=\"1\">\n<li>DeepSeekMoE-16B</li>\n</ol>\n<p>DeepSeekMoE-16B模型使用了2T数据训练（和LLAMA2-7B对齐）训练，并使用了100k的词表。其他参数如下表所示</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"模型超参\">\n<p>论文中提到，除了第一层以外，其他层都使用了MoE层。</p>\n<p>第一层不使用MoE是因为观察到第一层的负载均衡loss在训练中收敛得特别慢。</p>\n<p>DeepSeekMoE-16B每层有64个专家，其中有2个作为共享专家保持永远激活，加上6个通过gating\nfunction选择激活的，每个token共使用8个专家。每个token会激活16.4B中的2.8B参数。</p>\n<p>这里没有把专家的dimension再减小，是因为如果专家太小，计算效率就下降得太厉害。</p>\n<p>训练中使用的其他设置：<br>\n- lr = 4.2e-4<br>\n- 训练进行到80%和90%的时候，lr都会缩小到0.316倍<br>\n- batch size = 4.5k，训练窗口长度是4k，因此每个batch有18M\ntoken，2T数据差不多是10.6w步<br>\n- 使用了pipeline parallelism</p>\n<p>expert level balance\nloss的系数设得比较小，0.001，因为实验中发现设得再大并不能进一步优化负载平衡，反而会损害模型效果。</p>\n<p>DeepSeekMoE-16B和DeepSeek-7B模型的对比如下</p>\n<img src=\"/44e38c1b/ds_16b_perf_1.png\" class title=\"和DeepSeek-7B对比\">\n<p>DeepSeekMoE-16B和LLAMA2-7B模型的对比如下</p>\n<img src=\"/44e38c1b/ds_16b_perf_2.png\" class title=\"和LLAMA2-7B对比\">\n<ol start=\"5\" type=\"1\">\n<li>DeepSeekMoE-145B</li>\n</ol>\n<p>幻方还用245B的token训练了DeepSeekMoE-145B，模型效果上达到DeepSeek-67B的同等水平</p>\n<img src=\"/44e38c1b/ds_moe_145b.png\" class title=\"145b\">\n<h1 id=\"dbrx\">DBRX</h1>\n<p>2024年3月27日，Databricks开源了DBRX，一个拥有有132B参数，激活参数为36B的MoE模型。</p>\n<p>结构上，DBRX使用了RoPE、GLU、GQA，采用了fine-grained\nexpert的设计，每层有16个专家，每个token激活其中4个。相比Mixtral和Grok-1在8个专家中激活2个，DBRX有更多的专家组合方式。</p>\n<p>DBRX训练的上下文长度为32k，并使用了12T文本和代码token进行训练。DBRX在3072个H100上完成预训练，加上post-training、效果评估、red-team优化，整个过程耗费3个月时间。</p>\n<p>DBRX整体效果超过GPT-3.5，与Gemini 1.0\nPro相当，并且具有比较强的代码能力，甚至超过了在代码上专门优化过的模型，如CodeLLaMA-70B，如下图所示。</p>\n<img src=\"/44e38c1b/dbrx_perf.png\" class title=\"DBRX效果\">\n<p>推理效率效率上，DBRX也领先于其他模型。</p>\n<img src=\"/44e38c1b/dbrx_infer_efficiency.png\" class title=\"推理效率\">\n<h1 id=\"qwen1.5-moe\">Qwen1.5-MoE</h1>\n<p>2024年3月28日，阿里放出了Qwen1.5-MoE-A2.7B，以2.7B的模型参数，达到了Qwen1.5-7B模型的相近效果。</p>\n<p>Qwen1.5-MoE-A2.7B参考了DeepSeekMoE和DBRX的工作，采用了fine-grained\nexpert的做法，总共有64个专家，每个token激活8个专家，其中有4个为共享专家。</p>\n<p>Qwen1.5-MoE-A2.7B使用Qwen-1.8B进行初始化，并在初始化阶段引入随机性，这样可以显著加快收敛速度，并得到更好的收敛结果。</p>\n<p>Qwen1.5-MoE-A2.7B和其他模型效果对比如下</p>\n<img src=\"/44e38c1b/qwen1.5_moe_perf.png\" class title=\"Qwen1.5-MoE-A2.7B效果\">\n<p>虽然Qwen1.5-MoE-A2.7B总参数量较大，但激活的non-embedding参数量远小于7B模型，如下表所示</p>\n<img src=\"/44e38c1b/qwen1.5_moe_params.png\" class title=\"Qwen1.5-MoE-A2.7B参数量\">\n<p>实践中，Qwen1.5-MoE-A2.7B相比于Qwen1.5-7B，训练成本降低了75%。</p>\n<p>推理性能上，在A100-80G用vLLM部署Qwen1.5-7B和Qwen1.5-MoE-A2.7B模型进行了性能测试。</p>\n<p>输入/输出token数都设置为1000，输出token数设置为1000，TPS和throughput如下</p>\n<img src=\"/44e38c1b/qwen1.5_moe_tps.png\" class title=\"Qwen1.5-MoE-A2.7B TPS\">\n<p>虽然MoE模型对内存需求更大，但是由于稀疏激活以及共享专家的设计，但是在速度和吞吐量上都比dense模型更好。Qwen1.5-MoE-A2.7B与Qwen1.5-7B相比，速度提高了约1.74倍。</p>\n<h1 id=\"mistral\">Mistral</h1>\n<h2 id=\"mistral-8x7b\">Mistral 8x7B</h2>\n<p>2023年12月11日，Mistral\nAI开源Mistral-8x7B，每个token激活8个专家中的2个。</p>\n<p>Mistral-8x7B支持32k推理窗口和多语言，并且代码能力较好。和LLAM2-70B以及GPT-3.5的对比如下。</p>\n<img src=\"/44e38c1b/mistral_8_7b_perf.png\" class title=\"Mistral 8x7B效果\">\n<p>Mistral-8x7B在大多数任务表现优于LLAM2-70B，且推理速度提高了6倍。</p>\n<p>而和激活参数量相近的LLAM2-13B比，优势更为明显</p>\n<img src=\"/44e38c1b/mistral_8_7b_active_perf.png\" class title=\"Mistral 8x7B同样激活参数量下效果\">\n<h2 id=\"mistral-8x22b\">Mistral 8x22B</h2>\n<p>2024年4月17日，Mistral\nAI开源Mistral-8x22B模型，一个总参数为141B，激活参数为39B的超大MoE模型。</p>\n<p>Mistral-8x22B支持多语言，并且具有较强的数学和代码能力。此外，推理窗口长度也从Mistral-8x7B的32k增加到64k。Mistral-8x22B还具备function\ncall的能力。</p>\n<p>在各个维度的评测结果如下</p>\n<img src=\"/44e38c1b/mistral_8_22b_reasoning.png\" class title=\"Mistral 8x22B reasoning效果\">\n<img src=\"/44e38c1b/mistral_8_22b_multiling.png\" class title=\"Mistral 8x22B 多语言效果\">\n<img src=\"/44e38c1b/mistral_8_22b_code.png\" class title=\"Mistral 8x22B 代码与数学效果\">\n<h1 id=\"小结\">小结</h1>\n<ul>\n<li>现有的工作都表明，MoE模型相比dense模型具有更高的参数效率，即同样的计算量下，MoE模型普遍能有更优的效果<br>\n</li>\n<li>因此MoE不仅能支持更大规模模型的训练，在较小规模模型上使用MoE架构也有很大收益<br>\n</li>\n<li>但是相比dense模型，MoE模型的训练也需要考虑更多内容，包括专家数量、激活数量和专家容量的设计，负载均衡的问题，如何在多设备上的并行等，训练难度更大<br>\n</li>\n<li>结构上，共享专家和细粒度专家目前被验证效果较好<br>\n</li>\n<li>负载均衡上，GShard和Switch Transformer的负载均衡损失被广泛采用<br>\n</li>\n<li>推理时需要对底层框架进行优化以适配MoE机制，否则难以发挥MoE的性能优势</li>\n</ul>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p>【往期文章】</p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE模型的前世今生</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM长上下文的问题</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\">解锁大模型长上下文能力</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">理解Attention:从起源到MHA,MQA和GQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi技术报告-划重点看细节</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformer中normalization的二三事</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">从代码实现看normalization-到底做了什么</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">稀疏注意力计算:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">理解LLM位置编码:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">大模型算法题(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">大模型算法题(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Adaptive Mixtures of Local Experts\nhttps://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf<br>\n【2】Outrageously Large Neural Networks: The Sparsely-Gated\nMixture-of-Experts Layer https://arxiv.org/abs/1701.06538<br>\n【3】GShard: Scaling Giant Models with Conditional Computation and\nAutomatic Sharding https://arxiv.org/abs/2006.16668<br>\n【4】Switch Transformers: Scaling to Trillion Parameter Models with\nSimple and Efficient Sparsity https://arxiv.org/abs/2101.03961<br>\n【5】GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nhttps://arxiv.org/abs/2112.06905<br>\n【6】ST-MoE: Designing Stable and Transferable Sparse Expert Models\nhttps://arxiv.org/abs/2202.08906<br>\n【7】DeepSeekMoE: Towards Ultimate Expert Specialization in\nMixture-of-Experts Language Models\nhttps://arxiv.org/abs/2401.06066<br>\n【8】Introducing DBRX: A New State-of-the-Art Open LLM\nhttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm<br>\n【9】Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated\nParameters https://qwenlm.github.io/zh/blog/qwen-moe/</p>\n"},{"title":"大规模对话模型：ChatGPT下的银牌选手们","abbrlink":"14e576c","date":"2023-03-17T02:36:53.000Z","_content":"\n{% asset_img 1.png page_1 %}  \n\n{% asset_img 2.png page_2 %}  \n\n{% asset_img 3.png page_3 %}  \n\n{% asset_img 4.png page_4 %}  \n\n{% asset_img 5.png page_5 %}  \n\n{% asset_img 6.png page_6 %}  \n\n{% asset_img 7.png page_7 %}  \n\n{% asset_img 8.png page_8 %}  \n\n{% asset_img 9.png page_9 %}  \n\n{% asset_img 10.png page_10 %}  \n\n{% asset_img 11.png page_11 %}  \n\n{% asset_img 12.png page_12 %}  \n\n{% asset_img 13.png page_13 %}  \n\n{% asset_img 14.png page_14 %}  \n\n{% asset_img 15.png page_15 %}  \n\n{% asset_img 16.png page_16 %}  \n\n{% asset_img 17.png page_17 %}  \n\n{% asset_img 18.png page_18 %}  \n\n{% asset_img 19.png page_19 %}  \n\n{% asset_img 20.png page_20 %}  \n\n{% asset_img 21.png page_21 %}  \n\n{% asset_img 22.png page_22 %}  \n\n{% asset_img 23.png page_23 %}  \n\n{% asset_img 24.png page_24 %}  \n\n{% asset_img 25.png page_25 %}  \n\n{% asset_img 26.png page_26 %}  \n\n{% asset_img 27.png page_27 %}  \n\n{% asset_img 28.png page_28 %}  ","source":"_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们.md","raw":"---\ntitle: 大规模对话模型：ChatGPT下的银牌选手们\ntags:\n  - NLP\n  - LLM\n  - ChatGPT\n  - Sparrow\n  - LaMDA\n  - GopherCite\n  - WebGPT\n  - InstructGPT\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 14e576c\ndate: 2023-03-17 10:36:53\n---\n\n{% asset_img 1.png page_1 %}  \n\n{% asset_img 2.png page_2 %}  \n\n{% asset_img 3.png page_3 %}  \n\n{% asset_img 4.png page_4 %}  \n\n{% asset_img 5.png page_5 %}  \n\n{% asset_img 6.png page_6 %}  \n\n{% asset_img 7.png page_7 %}  \n\n{% asset_img 8.png page_8 %}  \n\n{% asset_img 9.png page_9 %}  \n\n{% asset_img 10.png page_10 %}  \n\n{% asset_img 11.png page_11 %}  \n\n{% asset_img 12.png page_12 %}  \n\n{% asset_img 13.png page_13 %}  \n\n{% asset_img 14.png page_14 %}  \n\n{% asset_img 15.png page_15 %}  \n\n{% asset_img 16.png page_16 %}  \n\n{% asset_img 17.png page_17 %}  \n\n{% asset_img 18.png page_18 %}  \n\n{% asset_img 19.png page_19 %}  \n\n{% asset_img 20.png page_20 %}  \n\n{% asset_img 21.png page_21 %}  \n\n{% asset_img 22.png page_22 %}  \n\n{% asset_img 23.png page_23 %}  \n\n{% asset_img 24.png page_24 %}  \n\n{% asset_img 25.png page_25 %}  \n\n{% asset_img 26.png page_26 %}  \n\n{% asset_img 27.png page_27 %}  \n\n{% asset_img 28.png page_28 %}  ","slug":"cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们","published":1,"updated":"2024-03-17T03:28:29.054Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4ka9009u314keeth33n8","content":"<img src=\"/14e576c/1.png\" class title=\"page_1\">\n<img src=\"/14e576c/2.png\" class title=\"page_2\">\n<img src=\"/14e576c/3.png\" class title=\"page_3\">\n<img src=\"/14e576c/4.png\" class title=\"page_4\">\n<img src=\"/14e576c/5.png\" class title=\"page_5\">\n<img src=\"/14e576c/6.png\" class title=\"page_6\">\n<img src=\"/14e576c/7.png\" class title=\"page_7\">\n<img src=\"/14e576c/8.png\" class title=\"page_8\">\n<img src=\"/14e576c/9.png\" class title=\"page_9\">\n<img src=\"/14e576c/10.png\" class title=\"page_10\">\n<img src=\"/14e576c/11.png\" class title=\"page_11\">\n<img src=\"/14e576c/12.png\" class title=\"page_12\">\n<img src=\"/14e576c/13.png\" class title=\"page_13\">\n<img src=\"/14e576c/14.png\" class title=\"page_14\">\n<img src=\"/14e576c/15.png\" class title=\"page_15\">\n<img src=\"/14e576c/16.png\" class title=\"page_16\">\n<img src=\"/14e576c/17.png\" class title=\"page_17\">\n<img src=\"/14e576c/18.png\" class title=\"page_18\">\n<img src=\"/14e576c/19.png\" class title=\"page_19\">\n<img src=\"/14e576c/20.png\" class title=\"page_20\">\n<img src=\"/14e576c/21.png\" class title=\"page_21\">\n<img src=\"/14e576c/22.png\" class title=\"page_22\">\n<img src=\"/14e576c/23.png\" class title=\"page_23\">\n<img src=\"/14e576c/24.png\" class title=\"page_24\">\n<img src=\"/14e576c/25.png\" class title=\"page_25\">\n<img src=\"/14e576c/26.png\" class title=\"page_26\">\n<img src=\"/14e576c/27.png\" class title=\"page_27\">\n<img src=\"/14e576c/28.png\" class title=\"page_28\">\n","length":0,"excerpt":"","more":"<img src=\"/14e576c/1.png\" class title=\"page_1\">\n<img src=\"/14e576c/2.png\" class title=\"page_2\">\n<img src=\"/14e576c/3.png\" class title=\"page_3\">\n<img src=\"/14e576c/4.png\" class title=\"page_4\">\n<img src=\"/14e576c/5.png\" class title=\"page_5\">\n<img src=\"/14e576c/6.png\" class title=\"page_6\">\n<img src=\"/14e576c/7.png\" class title=\"page_7\">\n<img src=\"/14e576c/8.png\" class title=\"page_8\">\n<img src=\"/14e576c/9.png\" class title=\"page_9\">\n<img src=\"/14e576c/10.png\" class title=\"page_10\">\n<img src=\"/14e576c/11.png\" class title=\"page_11\">\n<img src=\"/14e576c/12.png\" class title=\"page_12\">\n<img src=\"/14e576c/13.png\" class title=\"page_13\">\n<img src=\"/14e576c/14.png\" class title=\"page_14\">\n<img src=\"/14e576c/15.png\" class title=\"page_15\">\n<img src=\"/14e576c/16.png\" class title=\"page_16\">\n<img src=\"/14e576c/17.png\" class title=\"page_17\">\n<img src=\"/14e576c/18.png\" class title=\"page_18\">\n<img src=\"/14e576c/19.png\" class title=\"page_19\">\n<img src=\"/14e576c/20.png\" class title=\"page_20\">\n<img src=\"/14e576c/21.png\" class title=\"page_21\">\n<img src=\"/14e576c/22.png\" class title=\"page_22\">\n<img src=\"/14e576c/23.png\" class title=\"page_23\">\n<img src=\"/14e576c/24.png\" class title=\"page_24\">\n<img src=\"/14e576c/25.png\" class title=\"page_25\">\n<img src=\"/14e576c/26.png\" class title=\"page_26\">\n<img src=\"/14e576c/27.png\" class title=\"page_27\">\n<img src=\"/14e576c/28.png\" class title=\"page_28\">\n"},{"title":"理解Attention:从起源到MHA,MQA和GQA","abbrlink":"3dc22f96","date":"2024-03-05T10:49:38.000Z","_content":"\n【本文已在同名微信公众号/知乎/个人博客同步上线】  \n\nAttention模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head Attention）、MQA（Multi-Query Attention）和GQA（Grouped-Query Attention）这一路线的思路和做法被很多主流模型所采用，因此简单地梳理一些这几个变体的思路和做法，以及会涉及到的KV Cache相关内容。思路比较直白，但也有一些细节和原理值得思考。  \n\n当然针对Attention优化，也有很多其他优秀的方案和思路，如线性注意力、FlashAttention和Sliding Window Attention等，这些在后续再开篇梳理。  \n\n（应一些朋友的要求，会增加一些直观基础的内容，以及LLM应用的案例。也欢迎大家提出更多建议。）\n\n# 关于Attention：从RNN到Attention\n\n了解一个概念的诞生和演进，有助于我们更深入去理解它。我们先简单回顾下attention从起源到最初的实现。\n\n（熟悉attention的朋友可以跳过这一节）\n\n## 从RNN说起\n\n> Memory is attention through time. ~ Alex Graves 2020\n\n注意力机制最初起源是为了解决序列问题。回想在还没有Transformer的上一世代，使用RNN的Seq2Seq是这样的\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n（图来自[AI Summer](https://theaisummer.com/attention/)）  \n\n每个RNN cell接收两个输入，输出一个hidden state。比如在翻译任务中，RNN encoder把所有输入迭代地编码成context向量 $z$ ，然后由RNN decoder基于 $z$ 迭代地解码。一般来说，这里decoder的第一个输入是一个特殊token，如[start]，表示解码开始。  \n\n这样会有一个问题， $z$ 能编码的长度显然有限，而且由于模型结构问题，会更加关注靠近尾部的输入。这样如果关键信息出现在开头，就容易被忽略。  \n\n并且时间步骤上的传播由于有多次迭代相乘，梯度很容易就过小，导致梯度消失问题。  \n\n当然我们有LSMT和GRU等变体来增强长距离记忆的能力，也缓解了梯度问题，但这些方案还是没有产生质变的能力。  \n\n回到问题的核心，我们想要 $z$ 能够编码所有前面的内容，但是显然， $z$ 的生成方式天然会让它更容易注意到靠后的内容，而容易忽略靠前的输入。  \n\n一个直觉的想法就是，我们需要想个办法跳过 $z$ ，和前面的每个输入建立直接的联系。我们希望模型能够有机会学习到去“注意”关键的输入，不管这个输入是在前面还是后面。  \n\n实际上神经网络天生就具有“注意力”的天赋。  \n\n比如在CNN分类中，如果我们画出分类层前的heatmap，会是如下图这个样子  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\n可以看到，值比较高的地方是在猫的鼻子胡子嘴巴区域，次之是身上和头上的花纹。直观来说，就是模型主要通过脸部的特征和身上的花纹，来识别出这是一只猫。这就是CNN学习到的注意力，这样的特征是神经网络implicitly学到的。\n\n回归到Seq2Seq，我们怎么来实现注意力，并且让这种implicit的机制变得explicit：单独抽离出来并具备一定可控制性？  \n\n回想翻译场景，在RNN中，每一个时间步骤 $i$ 都会产生一个隐向量，$h_i$ 向量，我们把这些 $h_i$ 保存起来，在最后要生成新的输出的时候，我们让模型回头看一下之前的这每一个 $h_i$ ，再决定要生成什么内容。相比原来只利用最后一个hidden state，现在我们可以访问之前所有的中间状态，如果发现前面有关键信息，就可以直接用上了，而不用担心输入太长而被覆盖了。  \n\n那么问题又来了，我们怎么知道前面某一个中间状态对于当前的生成来说是否重要？如果我们不知道怎么定义是否重要，那我们就把这个问题交给模型自己解决好了 -- 通过网络参数来学习识别某个输入状态是否重要，学习是否要“注意”到它，要给予多少的“注意力”。  \n\n具体来说，我们定义在解码第 $i$ 个输出是，decoder当前隐状态 $y_{i-1}$ 和encoder的所有隐状态 $\\mathbf{h}$ 之间的一个score计算\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n其中\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n注意力网络通过 $\\mathbf{y}_{i-1}$ 和 $h_j$ 来计算一个值 $e_{ij}$，这里的注意力网络可以设计各种操作，比如对输入进行拼接再通过fc层进行计算等。  \n \n 这里 $e_{ij}$ 是一个标量，但它还不是一个可用的权重值，还需要通过一个函数，把attention net对各个encoder hidden state的输出值转成一个分布：softmax。  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\n最后通过加权计算，获得最终输入给decoder的隐变量。  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\n可以看到，这里的attention net的任务就是找到decoder上一个hidden state和encoder hidden state之间的“相关”关系，使得模型能够将更多的注意力放在对应的输入信息上。\n\n实际上，上面这种attention的计算方式并不是唯一的，attention的计算方式有许多种  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\n这些attention的一般形式可以写作 $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$ 。这里的 $s$ 就是decoder的hidden state（也就是前文的 $y$ ），$h$ 就是encoder的hidden state。  \n\n（当然从结果上看，是scaled dot-product attention经受住了历史的考验，成为了主流。）  \n\n## Transformer的attention\n\n从RNN attention到transformer attention，所做的事情就如论文题目所说：《Attention Is All You Need》，彻底抛弃RNN的在time step上的迭代计算，完全拥抱attention机制，只用最简单粗暴的方式同步计算出每个输入的hidden state，其他的就交给attention来解决。  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\n这里还是保留有encoder和decoder的结构，encoder中的attention都是self-attention，decoder则除了self-attention还有cross-attention。  \n\ntransformer结构下，attention的一般形式可以写作 $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$，这里有 $Q=W_{Q}Y，K=W_{K}X，V=W_{V}X$ 。对于cross-attention， $X$ 是encoder的hidden states，$Y$ 是decoder的hidden states，而对于self-attention，则有 $X=Y$。  \n\n具体到我们熟悉的scaled dot-product attention，使用softmax计算，有\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\n到这里，终于见到我们熟悉的attention计算。  \n\n用一张很直观的图来展示整个计算\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\n这里的「query」，「key」和「value」的名称也暗示了整个attention计算的思路。  \n\n类比到一个数据库查询+预测的例子。  \n\n假设我们现在有一个“文章-阅读量”数据库，记录了每篇文章在发布30天内的阅读量。每篇文章就是一个key，对应的阅读量就是value。  \n\n我们现在有一篇将要发布的文章，想要预测这篇文章在30天内的阅读量，那我们就把这篇新的文章，作为query，去和数据库里的文章（key）做一个相关性计算，取最相关的5篇文章。  \n\n假设top5篇文章的相关性分别是 $[8,4,4,2,2]$ ，对应阅读量是 $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$ 。 \n \n那我们把相关性得分归一化成和为1的概率值 $[0.4,0.2,0.2,0.1,0.1]$ ，那我们就可以预测新文章30天内的阅读量是 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ 。\n\n这个例子中，我们计算相关性就相当于transformer attention中的 $QK^T$ ，归一化就是softmax，然后通过加权求和取得最后的阅读量/特征向量。  \n\n对于self-attention， $Q、K、V$ 都来自输入 $X$，sequence自己计算自己每个token的之间的相关性。而对于cross-attention，decoder中的输出sequence就是上面这个例子中的“将要发布的文章”，通过把这篇新的文章和数据库中的文章做相关计算，我们得到了新的预测结果。  \n\n对于self-attention，由于 $Q、K、V$ 都来自输入 $X$ ，在计算 $QK^T$ 时，模型很容易关注到自身的位置上，也就是 $QK^T$ 对角线上的激活值会明显比较大。这样的情况其实不是很好，因为这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。后面讲的MHA对这个问题会有一些缓解作用。  \n\n顺着这样的思路梳理下来，会发现attention的大思路还是很好理解的。而计算上，怎么去获得更好的效果，就是接下来要分析的几个内容，MHA，MQA和GQA所关注的。\n\n代码上，实现也很容易，直接看[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)的代码\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## 关于scaling\n\nBTW，为什么计算中 $QK^T$ 之后还要除以 $\\sqrt{d}$ ？  \n\n简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。  \n\n{% asset_img softmax.png softmax %}  \n\n苏剑林的[博客](https://spaces.ac.cn/archives/8620)中也有详细分析，并提到如果不对attention值进行scaling，也可以通过在参数初始化是将方差除以一个 $\\sqrt{d}$ ，同样可以起到预防softmax饱和的效果。类似地，通过normalization也可以做到类似的效果。不过实现上在attention里做scaling还是比较稳定高效的。  \n\n# MHA\n\n只要理解了attention计算的细节，MHA（multi-head attention）其实就很好明白。\n\nMHA在2017年就随着《Attention Is All You Need》一起提出，主要干的就是一个事：把原来一个attention计算，拆成多个小份的attention，并行计算，分别得出结果，最后再合回原来的维度。  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\n假设原来模型的hidden size是 $d$ ，在MHA中，会把投影后的 $Q、K、V$ 在hidden state的维度上切成 $head_{num}$ 份，每个头的维度是 $d_{head}$ 。这 $head_{num}$ 组小 $Q、K、V$ 分别独立地进行attention计算，之后把得到的  $head_{num}$ 份维度 $d_{head}$ 的输出concat起来。  \n\n直接看这个amazing的图，很直观  \n\n{% asset_img multihead_attention.png MHA %}  \n\n操作是这么个操作，多头注意力相比单头有什么好处呢？  \n\n《Attention Is All You Need》文章中给出的说法是\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\n我们希望多个头能够在训练中学会注意到不同的内容。例如在翻译任务里，一些attention head可以关注语法特征，另一些attention head可以关注单词特性。这样模型就可以从不同角度来分析和理解输入信息，获得更好的效果了。  \n\n这有点类似CNN中，不同的卷积核来学习不同的信息。比如一个 $3\\times3\\times128$ 的卷积，有128个 $3\\times3$ 参数组，假设我们的输入是一个灰度图，其中一组 $3\\times3$ 的参数是这样的  \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n那么这是一个检测纵向边界的卷积，而另外一组参数长这样  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n这是一个检测横向边界的卷积。  \n\n这128组 $3\\times3$ 就是128个不同特征的检测器，就同MHA中多个头一样，从不同的子空间学到不同的内容，最后再放到一起融合使用。  \n\n但是这是我们expect模型能做到的事情，实际情况是否真的是这样？  \n\n知乎上这篇[文章](https://zhuanlan.zhihu.com/p/626820422)里对此做了一些实验和分析。简单来说就是（1）每个头确实学到东西有所不同，但大部分头之间的差异没有我们想的那么大（比如一个学句法，一个学词义这样明显的区分）（2）多个头的情况下，确实有少部分头可以比较好地捕捉到各种文本信息，而不会过分关注自身位置，一定程度缓解了上文提到的计算 $QK^T$ 之后对角线元素过大的问题。  \n\n我们可以把MHA的多个attention计算视为多个独立的小模型，那么最终整体的attention计算相当于把来自多个小模型的结果进行了融合，这样效果比较好也是比较符合直觉的。\n\n另外还有一个问题是，使用几个头比较好呢？  \n\n实际上这个问题比较难有确定性的答案，首先可以确定的是头的数量不是越多约好（毕竟头的数量多了，各个子空间小了，子空间能表达的内容就少了），具体多少要视模型规模，任务而定。另外[《Are Sixteen Heads Really Better than One?》](https://arxiv.org/pdf/1905.10650.pdf)中也指出MHA并不总是优于单头的情况。  \n\n目前可以看到的趋势是，模型越大（也就是hidden size越大），头数的增多越能带来平均效果上的收益（或者说允许注意力头增大而不影响子空间的学习能力）。目前LLM主流的头数视乎模型结构和规模，大致有12、16、24、48、96这样一些主流设置。这里面又有比较多的方向和工作，在此暂时不展开，挖个坑，以后专门开一篇讲。  \n\n最后看一下[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)中的MHA代码实现  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n（[transformers](https://github.com/huggingface/transformers)中的写法就更为成熟一点，不过里面兼容了比较多的功能，代码太长就不放上来了）\n\n# 解码中的KV Cache\n\n在讲MQA和GQA之前，还需要了解一点背景，那就是解码的计算问题，以及KV Cache的方案。  \n\n无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。  \n\n也就是，解码的时候，先根据当前输入 $\\text{input}_{i-1}$ ，生成下一个 $\\text{token}_{i}$ ，然后把新生成的 $\\text{token}_{i}$ 拼接在 $\\text{input}_{i-1}$ 后面，获得新的输入 $\\text{input}_{i}$ ，再用 $\\text{input}_{i}$ 生成 $\\text{token}_{i+1}$ ，依此迭代，直到生成结束。  \n\n比如我们输入“窗前明月光下一句是”，那么模型每次生成一个token，输入输出会是这样（方便起见，默认每个token都是一个字符）  \n\n```\nstep0: 输入=[BOS]窗前明月光下一句是；输出=疑\nstep1: 输入=[BOS]窗前明月光下一句是疑；输出=是\nstep2: 输入=[BOS]窗前明月光下一句是疑是；输出=地\nstep3: 输入=[BOS]窗前明月光下一句是疑是地；输出=上\nstep4: 输入=[BOS]窗前明月光下一句是疑是地上；输出=霜\nstep5: 输入=[BOS]窗前明月光下一句是疑是地上霜；输出=[EOS]\n```\n\n（其中[BOS]和[EOS]分别是起始符号和终止符号）  \n\n仔细想一下，我们在生成“疑”字的时候，用的是输入序列中“是”字的最后一层hidden state，通过最后的分类头预测出来的。以此类推，后面每生成一个字，使用的都是输入序列中最后一个字的输出。 \n\n我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个token）。那么下一个step的计算应该也包含了上一个step的计算。\n\n从公式上来看是这样的：\n\n回想一下我们attention的计算  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\n注意对于decoder的时候，由于mask attention的存在，每个输入只能看到自己和前面的内容，而看不到后面的内容\n\n假设我们当前输入的长度是3，预测第4个字，那每层attention所做的计算有\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n预测完第4个字，放到输入里，继续预测第5个字，每层attention所做的计算有\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n可以看到，在预测第5个字时，只有最后一步引入了新的计算，而 $o_{0}$ 到 $o_{2}$ 的计算和前面是完全重复的。  \n\n但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。  \n\n也就是说中间有很多我们用不到的计算，这样就造成了浪费。  \n\n而且随着生成的结果越来越多，输入的长度也越来越长。上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写高考作文，那可能就有800个step或者更多。这个情况下，step0被算了800次，step1被算了799次...\n\n有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？  \n\n答案就是KV Cache。利用缓存空间，把需要重复利用的中间计算结果存下来，减少重复计算。  \n\n而 $k$ 和 $v$ 就是要缓存的对象。  \n\n想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要踏踏实实地计算一遍。然后把 $k$ 、 $v$ 值缓存起来。\n\n则有\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center>↓</center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache的下标 $l$ 表示模型层数。\n\n在进行第二次预测，也就是预测第5个字的时候，在第 $l$ 层的时候，由于前面我们缓存了<u>**每层**</u>的 $k$ 、 $v$ 值，那本层就只需要算新的 $o_{3}$ ，而不用算 $o_{0}、o_{1}、o_{2}$ 。  \n\n因为第 $l$ 层的 $o_{0}、o_{1}、o_{2}$ 本来会经过FNN层之后进到 $l+1$ 层，再经过新的投影变换，成为 $l+1$ 层的 $k$ 、 $v$ 值，但是 $l+1$ 层的 $k$ 、 $v$ 值我们已经缓存过了！  \n\n然后我们把本次新增算出来的 $k$ 、 $v$ 值也存入缓存。\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center>↓</center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\n这样就节省了attention和FFN的很多重复计算。  \n\ntransformers中，生成的时候传入use_cache=True就会开启KV Cache。  \n\n也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # 过去所存的值\n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # 把当前新的key加入\n            value = torch.cat((past_value, value), dim=-2)  # 把当前新的value加入\n\n        if use_cache is True:\n            present = (key, value)  # 输出用于保存\n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\n总的来说，KV Cache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask attention的存在，使得前面的token可以不用关注后面的token）  \n\n但是，用了KV Cache之后也不是立刻万事大吉。  \n\n我们简单算一下，对于输入长度为 $s$ ，层数为 $L$ ，hidden size为 $d$ 的模型，需要缓存的参数量为  \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n如果使用的是半精度浮点数，那么总共所需的空间就是\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\n以Llama2 7B为例，有 $L=32$ ， $L=4096$ ，那么每个token所需的缓存空间就是524,288 bytes，约52K，当 $s=1024$ 时，则需要536,870,912 bytes，超过500M的空间。  \n\n这里考虑的还只是batch size=1的情况，如果batch size增大，这个值更是很容易就超过1G。  \n\n（MHA相比单头的情况，相当于只是把 $q、k、v$ 切成多份并行计算了，对于实际需要缓存的大小没有影响）\n\n看下现在主流的科学计算卡配置\n\n{% asset_img gpu_cache.png gpu cache %}  \n\n强如H100也只有50M的L2 Cache（L1 Cache的大小更是可以忽略不计），大概只能支持Llama2 7B总共100个token左右的输入。  \n\n想想我们现在用的LLM动辄34B/70B的规模，长度更是以千为基础单位，这样明显是不够用的。\n\n那么超出L2 Cache的部分只能走到显存中去了，但是HBM速度比L2 Cache慢多了。  \n\n{% asset_img sram_dram.png 储存空间与速度 %}  \n\n看来还需要进一步优化。  \n\n要保证模型的推理加速，要么增大Cache的大小，而且是需要一到两个数量级的增强，那这个只能靠黄老板了。  \n\n要么就是减少需要缓存的量。  \n\n# MQA\n\nMQA就是来减少缓存所需要的量的。\n\nGoogle在2019年就在《Fast Transformer Decoding: One Write-Head is All You Need》提出了MQA，不过那时候主要到的人不多，那是大家主要还是关注在用Bert把榜刷出新高上。  \n\nMQA的做法其实很简单。在MHA中，输入分别经过 $W_{Q}、W_{K}、W_{V}$ 的变换之后，都切成了n份（n=头数），维度也从 $d_{model}$ 降到了 $d_{head}$ ，分别进行attention计算再拼接。而MQA这里，在线性变换之后，只对 $Q$ 进行切分（和MHA一样），而 $K、V$ 则直接在线性变换的时候把维度降到了 $d_{head}$ （而不是切分变小），然后这n个Query头分别和同一份 $K、V$ 进行attention计算，之后把结果拼接起来。  \n\n简单来说，就是MHA中，每个注意力头的 $K、V$ 是不一样的，而MQA这里，每个注意力头的 $K、V$ 是一样的，值是共享的。而其他步骤都和MHA一样。  \n\n{% asset_img MQA.webp MQA %}  \n\n这样一来，需要缓存的 $K、V$ 值一下就从所有头变成一个头的量。  \n\n比如在Llama2 7B中用的是32个头，那用MQA后，1024个token需要缓存的量就变成1/32，536,870,912 bytes / 32 = 16,777,216 bytes，差不多是16M，这就能全塞进缓存中了。\n\n（实现上，就是改一下线性变换矩阵，然后把 $K、V$ 的处理从切分变成复制，就不再赘述。）  \n\n当然，由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点，但是并不多，且相比其他修改hidden size或者head num的做法效果都好。  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\n既然MQA对效果有点影响，MHA缓存又存不下，那GQA（Grouped-Query Attention）就提出了一个折中的办法，既能减少MQA效果的损失，又相比MHA需要更少的缓存。  \n\n（文章：《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》，2023年）\n\nGQA里， $Q$ 还是按原来MHA/MQA的做法不变。只使用一套共享的 $K、V$ 不是效果不好吗，那就还是多弄几套。但是不要太多，数量还是比 $Q$ 的头数少一些。这样相当于把 $Q$ 的多个头给分了group，同一个group内的 $Q$ 共享同一套 $K、V$ ，不同group的 $Q$ 所用的 $K、V$ 不同。  \n\nMHA可以认为是 $K、V$ 头数最大时的GQA，而MQA可以任务是 $K、V$ 头数最少时的GQA。  \n\n看论文里的图就很直观\n\n{% asset_img GQA.png GQA %}  \n\n效果怎么样呢？  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n看表中2/3/4行对比，GQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。文中提到，这里的MQA和GQA都是通过average pooling从MHA初始化而来，然后进行了少量的训练得到的。如果我们想要把之前用MHA训练的模型改造成GQA，也可以通过这样的方法，增加少量训练来实现。当然如果从一开始就加上，从零开始训练，也是没有问题的。  \n\nLlama2用的就是GQA，在tech report中也做了MHA、MQA、GQA的效果对比，可以看到效果确实很不错。  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n# 小结  \n\nMHA、MQA、GQA的实现其实并不复杂，效果也很好，理解上并没有太多困难。但是想要真正理解它们的出发点，还是需要深入每一个细节，去了解当时要解决的事什么问题。\n\n目前来看GQA是LLM比较好的方案，但未来肯定还会有针对不同方向的进一步优化方案，计算效率、推理速度、显存消耗这些方向都值得我们继续去探索优化。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n【1】The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n【2】Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n【3】Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n【4】https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n【5】GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n【6】How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n【7】A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n【8】https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n【9】浅谈Transformer的初始化、参数化与标准化 https://spaces.ac.cn/archives/8620  \n【10】https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n【11】https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n【12】Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n【13】This post is all you need（上卷）——层层剥开Transformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n【14】The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n【15】Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","source":"_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA.md","raw":"---\ntitle: '理解Attention:从起源到MHA,MQA和GQA'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - KV Cache\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 3dc22f96\ndate: 2024-03-05 18:49:38\n---\n\n【本文已在同名微信公众号/知乎/个人博客同步上线】  \n\nAttention模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head Attention）、MQA（Multi-Query Attention）和GQA（Grouped-Query Attention）这一路线的思路和做法被很多主流模型所采用，因此简单地梳理一些这几个变体的思路和做法，以及会涉及到的KV Cache相关内容。思路比较直白，但也有一些细节和原理值得思考。  \n\n当然针对Attention优化，也有很多其他优秀的方案和思路，如线性注意力、FlashAttention和Sliding Window Attention等，这些在后续再开篇梳理。  \n\n（应一些朋友的要求，会增加一些直观基础的内容，以及LLM应用的案例。也欢迎大家提出更多建议。）\n\n# 关于Attention：从RNN到Attention\n\n了解一个概念的诞生和演进，有助于我们更深入去理解它。我们先简单回顾下attention从起源到最初的实现。\n\n（熟悉attention的朋友可以跳过这一节）\n\n## 从RNN说起\n\n> Memory is attention through time. ~ Alex Graves 2020\n\n注意力机制最初起源是为了解决序列问题。回想在还没有Transformer的上一世代，使用RNN的Seq2Seq是这样的\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n（图来自[AI Summer](https://theaisummer.com/attention/)）  \n\n每个RNN cell接收两个输入，输出一个hidden state。比如在翻译任务中，RNN encoder把所有输入迭代地编码成context向量 $z$ ，然后由RNN decoder基于 $z$ 迭代地解码。一般来说，这里decoder的第一个输入是一个特殊token，如[start]，表示解码开始。  \n\n这样会有一个问题， $z$ 能编码的长度显然有限，而且由于模型结构问题，会更加关注靠近尾部的输入。这样如果关键信息出现在开头，就容易被忽略。  \n\n并且时间步骤上的传播由于有多次迭代相乘，梯度很容易就过小，导致梯度消失问题。  \n\n当然我们有LSMT和GRU等变体来增强长距离记忆的能力，也缓解了梯度问题，但这些方案还是没有产生质变的能力。  \n\n回到问题的核心，我们想要 $z$ 能够编码所有前面的内容，但是显然， $z$ 的生成方式天然会让它更容易注意到靠后的内容，而容易忽略靠前的输入。  \n\n一个直觉的想法就是，我们需要想个办法跳过 $z$ ，和前面的每个输入建立直接的联系。我们希望模型能够有机会学习到去“注意”关键的输入，不管这个输入是在前面还是后面。  \n\n实际上神经网络天生就具有“注意力”的天赋。  \n\n比如在CNN分类中，如果我们画出分类层前的heatmap，会是如下图这个样子  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\n可以看到，值比较高的地方是在猫的鼻子胡子嘴巴区域，次之是身上和头上的花纹。直观来说，就是模型主要通过脸部的特征和身上的花纹，来识别出这是一只猫。这就是CNN学习到的注意力，这样的特征是神经网络implicitly学到的。\n\n回归到Seq2Seq，我们怎么来实现注意力，并且让这种implicit的机制变得explicit：单独抽离出来并具备一定可控制性？  \n\n回想翻译场景，在RNN中，每一个时间步骤 $i$ 都会产生一个隐向量，$h_i$ 向量，我们把这些 $h_i$ 保存起来，在最后要生成新的输出的时候，我们让模型回头看一下之前的这每一个 $h_i$ ，再决定要生成什么内容。相比原来只利用最后一个hidden state，现在我们可以访问之前所有的中间状态，如果发现前面有关键信息，就可以直接用上了，而不用担心输入太长而被覆盖了。  \n\n那么问题又来了，我们怎么知道前面某一个中间状态对于当前的生成来说是否重要？如果我们不知道怎么定义是否重要，那我们就把这个问题交给模型自己解决好了 -- 通过网络参数来学习识别某个输入状态是否重要，学习是否要“注意”到它，要给予多少的“注意力”。  \n\n具体来说，我们定义在解码第 $i$ 个输出是，decoder当前隐状态 $y_{i-1}$ 和encoder的所有隐状态 $\\mathbf{h}$ 之间的一个score计算\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n其中\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n注意力网络通过 $\\mathbf{y}_{i-1}$ 和 $h_j$ 来计算一个值 $e_{ij}$，这里的注意力网络可以设计各种操作，比如对输入进行拼接再通过fc层进行计算等。  \n \n 这里 $e_{ij}$ 是一个标量，但它还不是一个可用的权重值，还需要通过一个函数，把attention net对各个encoder hidden state的输出值转成一个分布：softmax。  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\n最后通过加权计算，获得最终输入给decoder的隐变量。  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\n可以看到，这里的attention net的任务就是找到decoder上一个hidden state和encoder hidden state之间的“相关”关系，使得模型能够将更多的注意力放在对应的输入信息上。\n\n实际上，上面这种attention的计算方式并不是唯一的，attention的计算方式有许多种  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\n这些attention的一般形式可以写作 $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$ 。这里的 $s$ 就是decoder的hidden state（也就是前文的 $y$ ），$h$ 就是encoder的hidden state。  \n\n（当然从结果上看，是scaled dot-product attention经受住了历史的考验，成为了主流。）  \n\n## Transformer的attention\n\n从RNN attention到transformer attention，所做的事情就如论文题目所说：《Attention Is All You Need》，彻底抛弃RNN的在time step上的迭代计算，完全拥抱attention机制，只用最简单粗暴的方式同步计算出每个输入的hidden state，其他的就交给attention来解决。  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\n这里还是保留有encoder和decoder的结构，encoder中的attention都是self-attention，decoder则除了self-attention还有cross-attention。  \n\ntransformer结构下，attention的一般形式可以写作 $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$，这里有 $Q=W_{Q}Y，K=W_{K}X，V=W_{V}X$ 。对于cross-attention， $X$ 是encoder的hidden states，$Y$ 是decoder的hidden states，而对于self-attention，则有 $X=Y$。  \n\n具体到我们熟悉的scaled dot-product attention，使用softmax计算，有\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\n到这里，终于见到我们熟悉的attention计算。  \n\n用一张很直观的图来展示整个计算\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\n这里的「query」，「key」和「value」的名称也暗示了整个attention计算的思路。  \n\n类比到一个数据库查询+预测的例子。  \n\n假设我们现在有一个“文章-阅读量”数据库，记录了每篇文章在发布30天内的阅读量。每篇文章就是一个key，对应的阅读量就是value。  \n\n我们现在有一篇将要发布的文章，想要预测这篇文章在30天内的阅读量，那我们就把这篇新的文章，作为query，去和数据库里的文章（key）做一个相关性计算，取最相关的5篇文章。  \n\n假设top5篇文章的相关性分别是 $[8,4,4,2,2]$ ，对应阅读量是 $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$ 。 \n \n那我们把相关性得分归一化成和为1的概率值 $[0.4,0.2,0.2,0.1,0.1]$ ，那我们就可以预测新文章30天内的阅读量是 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ 。\n\n这个例子中，我们计算相关性就相当于transformer attention中的 $QK^T$ ，归一化就是softmax，然后通过加权求和取得最后的阅读量/特征向量。  \n\n对于self-attention， $Q、K、V$ 都来自输入 $X$，sequence自己计算自己每个token的之间的相关性。而对于cross-attention，decoder中的输出sequence就是上面这个例子中的“将要发布的文章”，通过把这篇新的文章和数据库中的文章做相关计算，我们得到了新的预测结果。  \n\n对于self-attention，由于 $Q、K、V$ 都来自输入 $X$ ，在计算 $QK^T$ 时，模型很容易关注到自身的位置上，也就是 $QK^T$ 对角线上的激活值会明显比较大。这样的情况其实不是很好，因为这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。后面讲的MHA对这个问题会有一些缓解作用。  \n\n顺着这样的思路梳理下来，会发现attention的大思路还是很好理解的。而计算上，怎么去获得更好的效果，就是接下来要分析的几个内容，MHA，MQA和GQA所关注的。\n\n代码上，实现也很容易，直接看[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)的代码\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## 关于scaling\n\nBTW，为什么计算中 $QK^T$ 之后还要除以 $\\sqrt{d}$ ？  \n\n简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。  \n\n{% asset_img softmax.png softmax %}  \n\n苏剑林的[博客](https://spaces.ac.cn/archives/8620)中也有详细分析，并提到如果不对attention值进行scaling，也可以通过在参数初始化是将方差除以一个 $\\sqrt{d}$ ，同样可以起到预防softmax饱和的效果。类似地，通过normalization也可以做到类似的效果。不过实现上在attention里做scaling还是比较稳定高效的。  \n\n# MHA\n\n只要理解了attention计算的细节，MHA（multi-head attention）其实就很好明白。\n\nMHA在2017年就随着《Attention Is All You Need》一起提出，主要干的就是一个事：把原来一个attention计算，拆成多个小份的attention，并行计算，分别得出结果，最后再合回原来的维度。  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\n假设原来模型的hidden size是 $d$ ，在MHA中，会把投影后的 $Q、K、V$ 在hidden state的维度上切成 $head_{num}$ 份，每个头的维度是 $d_{head}$ 。这 $head_{num}$ 组小 $Q、K、V$ 分别独立地进行attention计算，之后把得到的  $head_{num}$ 份维度 $d_{head}$ 的输出concat起来。  \n\n直接看这个amazing的图，很直观  \n\n{% asset_img multihead_attention.png MHA %}  \n\n操作是这么个操作，多头注意力相比单头有什么好处呢？  \n\n《Attention Is All You Need》文章中给出的说法是\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\n我们希望多个头能够在训练中学会注意到不同的内容。例如在翻译任务里，一些attention head可以关注语法特征，另一些attention head可以关注单词特性。这样模型就可以从不同角度来分析和理解输入信息，获得更好的效果了。  \n\n这有点类似CNN中，不同的卷积核来学习不同的信息。比如一个 $3\\times3\\times128$ 的卷积，有128个 $3\\times3$ 参数组，假设我们的输入是一个灰度图，其中一组 $3\\times3$ 的参数是这样的  \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n那么这是一个检测纵向边界的卷积，而另外一组参数长这样  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n这是一个检测横向边界的卷积。  \n\n这128组 $3\\times3$ 就是128个不同特征的检测器，就同MHA中多个头一样，从不同的子空间学到不同的内容，最后再放到一起融合使用。  \n\n但是这是我们expect模型能做到的事情，实际情况是否真的是这样？  \n\n知乎上这篇[文章](https://zhuanlan.zhihu.com/p/626820422)里对此做了一些实验和分析。简单来说就是（1）每个头确实学到东西有所不同，但大部分头之间的差异没有我们想的那么大（比如一个学句法，一个学词义这样明显的区分）（2）多个头的情况下，确实有少部分头可以比较好地捕捉到各种文本信息，而不会过分关注自身位置，一定程度缓解了上文提到的计算 $QK^T$ 之后对角线元素过大的问题。  \n\n我们可以把MHA的多个attention计算视为多个独立的小模型，那么最终整体的attention计算相当于把来自多个小模型的结果进行了融合，这样效果比较好也是比较符合直觉的。\n\n另外还有一个问题是，使用几个头比较好呢？  \n\n实际上这个问题比较难有确定性的答案，首先可以确定的是头的数量不是越多约好（毕竟头的数量多了，各个子空间小了，子空间能表达的内容就少了），具体多少要视模型规模，任务而定。另外[《Are Sixteen Heads Really Better than One?》](https://arxiv.org/pdf/1905.10650.pdf)中也指出MHA并不总是优于单头的情况。  \n\n目前可以看到的趋势是，模型越大（也就是hidden size越大），头数的增多越能带来平均效果上的收益（或者说允许注意力头增大而不影响子空间的学习能力）。目前LLM主流的头数视乎模型结构和规模，大致有12、16、24、48、96这样一些主流设置。这里面又有比较多的方向和工作，在此暂时不展开，挖个坑，以后专门开一篇讲。  \n\n最后看一下[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)中的MHA代码实现  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n（[transformers](https://github.com/huggingface/transformers)中的写法就更为成熟一点，不过里面兼容了比较多的功能，代码太长就不放上来了）\n\n# 解码中的KV Cache\n\n在讲MQA和GQA之前，还需要了解一点背景，那就是解码的计算问题，以及KV Cache的方案。  \n\n无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。  \n\n也就是，解码的时候，先根据当前输入 $\\text{input}_{i-1}$ ，生成下一个 $\\text{token}_{i}$ ，然后把新生成的 $\\text{token}_{i}$ 拼接在 $\\text{input}_{i-1}$ 后面，获得新的输入 $\\text{input}_{i}$ ，再用 $\\text{input}_{i}$ 生成 $\\text{token}_{i+1}$ ，依此迭代，直到生成结束。  \n\n比如我们输入“窗前明月光下一句是”，那么模型每次生成一个token，输入输出会是这样（方便起见，默认每个token都是一个字符）  \n\n```\nstep0: 输入=[BOS]窗前明月光下一句是；输出=疑\nstep1: 输入=[BOS]窗前明月光下一句是疑；输出=是\nstep2: 输入=[BOS]窗前明月光下一句是疑是；输出=地\nstep3: 输入=[BOS]窗前明月光下一句是疑是地；输出=上\nstep4: 输入=[BOS]窗前明月光下一句是疑是地上；输出=霜\nstep5: 输入=[BOS]窗前明月光下一句是疑是地上霜；输出=[EOS]\n```\n\n（其中[BOS]和[EOS]分别是起始符号和终止符号）  \n\n仔细想一下，我们在生成“疑”字的时候，用的是输入序列中“是”字的最后一层hidden state，通过最后的分类头预测出来的。以此类推，后面每生成一个字，使用的都是输入序列中最后一个字的输出。 \n\n我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个token）。那么下一个step的计算应该也包含了上一个step的计算。\n\n从公式上来看是这样的：\n\n回想一下我们attention的计算  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\n注意对于decoder的时候，由于mask attention的存在，每个输入只能看到自己和前面的内容，而看不到后面的内容\n\n假设我们当前输入的长度是3，预测第4个字，那每层attention所做的计算有\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n预测完第4个字，放到输入里，继续预测第5个字，每层attention所做的计算有\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n可以看到，在预测第5个字时，只有最后一步引入了新的计算，而 $o_{0}$ 到 $o_{2}$ 的计算和前面是完全重复的。  \n\n但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。  \n\n也就是说中间有很多我们用不到的计算，这样就造成了浪费。  \n\n而且随着生成的结果越来越多，输入的长度也越来越长。上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写高考作文，那可能就有800个step或者更多。这个情况下，step0被算了800次，step1被算了799次...\n\n有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？  \n\n答案就是KV Cache。利用缓存空间，把需要重复利用的中间计算结果存下来，减少重复计算。  \n\n而 $k$ 和 $v$ 就是要缓存的对象。  \n\n想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要踏踏实实地计算一遍。然后把 $k$ 、 $v$ 值缓存起来。\n\n则有\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center>↓</center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache的下标 $l$ 表示模型层数。\n\n在进行第二次预测，也就是预测第5个字的时候，在第 $l$ 层的时候，由于前面我们缓存了<u>**每层**</u>的 $k$ 、 $v$ 值，那本层就只需要算新的 $o_{3}$ ，而不用算 $o_{0}、o_{1}、o_{2}$ 。  \n\n因为第 $l$ 层的 $o_{0}、o_{1}、o_{2}$ 本来会经过FNN层之后进到 $l+1$ 层，再经过新的投影变换，成为 $l+1$ 层的 $k$ 、 $v$ 值，但是 $l+1$ 层的 $k$ 、 $v$ 值我们已经缓存过了！  \n\n然后我们把本次新增算出来的 $k$ 、 $v$ 值也存入缓存。\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center>↓</center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\n这样就节省了attention和FFN的很多重复计算。  \n\ntransformers中，生成的时候传入use_cache=True就会开启KV Cache。  \n\n也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # 过去所存的值\n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # 把当前新的key加入\n            value = torch.cat((past_value, value), dim=-2)  # 把当前新的value加入\n\n        if use_cache is True:\n            present = (key, value)  # 输出用于保存\n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\n总的来说，KV Cache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask attention的存在，使得前面的token可以不用关注后面的token）  \n\n但是，用了KV Cache之后也不是立刻万事大吉。  \n\n我们简单算一下，对于输入长度为 $s$ ，层数为 $L$ ，hidden size为 $d$ 的模型，需要缓存的参数量为  \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n如果使用的是半精度浮点数，那么总共所需的空间就是\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\n以Llama2 7B为例，有 $L=32$ ， $L=4096$ ，那么每个token所需的缓存空间就是524,288 bytes，约52K，当 $s=1024$ 时，则需要536,870,912 bytes，超过500M的空间。  \n\n这里考虑的还只是batch size=1的情况，如果batch size增大，这个值更是很容易就超过1G。  \n\n（MHA相比单头的情况，相当于只是把 $q、k、v$ 切成多份并行计算了，对于实际需要缓存的大小没有影响）\n\n看下现在主流的科学计算卡配置\n\n{% asset_img gpu_cache.png gpu cache %}  \n\n强如H100也只有50M的L2 Cache（L1 Cache的大小更是可以忽略不计），大概只能支持Llama2 7B总共100个token左右的输入。  \n\n想想我们现在用的LLM动辄34B/70B的规模，长度更是以千为基础单位，这样明显是不够用的。\n\n那么超出L2 Cache的部分只能走到显存中去了，但是HBM速度比L2 Cache慢多了。  \n\n{% asset_img sram_dram.png 储存空间与速度 %}  \n\n看来还需要进一步优化。  \n\n要保证模型的推理加速，要么增大Cache的大小，而且是需要一到两个数量级的增强，那这个只能靠黄老板了。  \n\n要么就是减少需要缓存的量。  \n\n# MQA\n\nMQA就是来减少缓存所需要的量的。\n\nGoogle在2019年就在《Fast Transformer Decoding: One Write-Head is All You Need》提出了MQA，不过那时候主要到的人不多，那是大家主要还是关注在用Bert把榜刷出新高上。  \n\nMQA的做法其实很简单。在MHA中，输入分别经过 $W_{Q}、W_{K}、W_{V}$ 的变换之后，都切成了n份（n=头数），维度也从 $d_{model}$ 降到了 $d_{head}$ ，分别进行attention计算再拼接。而MQA这里，在线性变换之后，只对 $Q$ 进行切分（和MHA一样），而 $K、V$ 则直接在线性变换的时候把维度降到了 $d_{head}$ （而不是切分变小），然后这n个Query头分别和同一份 $K、V$ 进行attention计算，之后把结果拼接起来。  \n\n简单来说，就是MHA中，每个注意力头的 $K、V$ 是不一样的，而MQA这里，每个注意力头的 $K、V$ 是一样的，值是共享的。而其他步骤都和MHA一样。  \n\n{% asset_img MQA.webp MQA %}  \n\n这样一来，需要缓存的 $K、V$ 值一下就从所有头变成一个头的量。  \n\n比如在Llama2 7B中用的是32个头，那用MQA后，1024个token需要缓存的量就变成1/32，536,870,912 bytes / 32 = 16,777,216 bytes，差不多是16M，这就能全塞进缓存中了。\n\n（实现上，就是改一下线性变换矩阵，然后把 $K、V$ 的处理从切分变成复制，就不再赘述。）  \n\n当然，由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点，但是并不多，且相比其他修改hidden size或者head num的做法效果都好。  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\n既然MQA对效果有点影响，MHA缓存又存不下，那GQA（Grouped-Query Attention）就提出了一个折中的办法，既能减少MQA效果的损失，又相比MHA需要更少的缓存。  \n\n（文章：《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》，2023年）\n\nGQA里， $Q$ 还是按原来MHA/MQA的做法不变。只使用一套共享的 $K、V$ 不是效果不好吗，那就还是多弄几套。但是不要太多，数量还是比 $Q$ 的头数少一些。这样相当于把 $Q$ 的多个头给分了group，同一个group内的 $Q$ 共享同一套 $K、V$ ，不同group的 $Q$ 所用的 $K、V$ 不同。  \n\nMHA可以认为是 $K、V$ 头数最大时的GQA，而MQA可以任务是 $K、V$ 头数最少时的GQA。  \n\n看论文里的图就很直观\n\n{% asset_img GQA.png GQA %}  \n\n效果怎么样呢？  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n看表中2/3/4行对比，GQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。文中提到，这里的MQA和GQA都是通过average pooling从MHA初始化而来，然后进行了少量的训练得到的。如果我们想要把之前用MHA训练的模型改造成GQA，也可以通过这样的方法，增加少量训练来实现。当然如果从一开始就加上，从零开始训练，也是没有问题的。  \n\nLlama2用的就是GQA，在tech report中也做了MHA、MQA、GQA的效果对比，可以看到效果确实很不错。  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n# 小结  \n\nMHA、MQA、GQA的实现其实并不复杂，效果也很好，理解上并没有太多困难。但是想要真正理解它们的出发点，还是需要深入每一个细节，去了解当时要解决的事什么问题。\n\n目前来看GQA是LLM比较好的方案，但未来肯定还会有针对不同方向的进一步优化方案，计算效率、推理速度、显存消耗这些方向都值得我们继续去探索优化。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n【1】The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n【2】Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n【3】Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n【4】https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n【5】GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n【6】How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n【7】A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n【8】https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n【9】浅谈Transformer的初始化、参数化与标准化 https://spaces.ac.cn/archives/8620  \n【10】https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n【11】https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n【12】Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n【13】This post is all you need（上卷）——层层剥开Transformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n【14】The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n【15】Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","slug":"cs/nlp/2024/03/理解Attention-MHA-MQA和GQA","published":1,"updated":"2024-04-27T15:23:22.122Z","comments":1,"layout":"post","photos":[],"_id":"clxvu4ka9009w314k19n78cog","content":"<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>\n<p>Attention模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head\nAttention）、MQA（Multi-Query Attention）和GQA（Grouped-Query\nAttention）这一路线的思路和做法被很多主流模型所采用，因此简单地梳理一些这几个变体的思路和做法，以及会涉及到的KV\nCache相关内容。思路比较直白，但也有一些细节和原理值得思考。</p>\n<p>当然针对Attention优化，也有很多其他优秀的方案和思路，如线性注意力、FlashAttention和Sliding\nWindow Attention等，这些在后续再开篇梳理。</p>\n<p>（应一些朋友的要求，会增加一些直观基础的内容，以及LLM应用的案例。也欢迎大家提出更多建议。）</p>\n<h1 id=\"关于attention从rnn到attention\">关于Attention：从RNN到Attention</h1>\n<p>了解一个概念的诞生和演进，有助于我们更深入去理解它。我们先简单回顾下attention从起源到最初的实现。</p>\n<p>（熟悉attention的朋友可以跳过这一节）</p>\n<h2 id=\"从rnn说起\">从RNN说起</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>注意力机制最初起源是为了解决序列问题。回想在还没有Transformer的上一世代，使用RNN的Seq2Seq是这样的</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p>（图来自<a href=\"https://theaisummer.com/attention/\">AI\nSummer</a>）</p>\n<p>每个RNN cell接收两个输入，输出一个hidden state。比如在翻译任务中，RNN\nencoder把所有输入迭代地编码成context向量 <span class=\"math inline\">\\(z\\)</span> ，然后由RNN decoder基于 <span class=\"math inline\">\\(z\\)</span>\n迭代地解码。一般来说，这里decoder的第一个输入是一个特殊token，如[start]，表示解码开始。</p>\n<p>这样会有一个问题， <span class=\"math inline\">\\(z\\)</span>\n能编码的长度显然有限，而且由于模型结构问题，会更加关注靠近尾部的输入。这样如果关键信息出现在开头，就容易被忽略。</p>\n<p>并且时间步骤上的传播由于有多次迭代相乘，梯度很容易就过小，导致梯度消失问题。</p>\n<p>当然我们有LSMT和GRU等变体来增强长距离记忆的能力，也缓解了梯度问题，但这些方案还是没有产生质变的能力。</p>\n<p>回到问题的核心，我们想要 <span class=\"math inline\">\\(z\\)</span>\n能够编码所有前面的内容，但是显然， <span class=\"math inline\">\\(z\\)</span>\n的生成方式天然会让它更容易注意到靠后的内容，而容易忽略靠前的输入。</p>\n<p>一个直觉的想法就是，我们需要想个办法跳过 <span class=\"math inline\">\\(z\\)</span>\n，和前面的每个输入建立直接的联系。我们希望模型能够有机会学习到去“注意”关键的输入，不管这个输入是在前面还是后面。</p>\n<p>实际上神经网络天生就具有“注意力”的天赋。</p>\n<p>比如在CNN分类中，如果我们画出分类层前的heatmap，会是如下图这个样子</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>可以看到，值比较高的地方是在猫的鼻子胡子嘴巴区域，次之是身上和头上的花纹。直观来说，就是模型主要通过脸部的特征和身上的花纹，来识别出这是一只猫。这就是CNN学习到的注意力，这样的特征是神经网络implicitly学到的。</p>\n<p>回归到Seq2Seq，我们怎么来实现注意力，并且让这种implicit的机制变得explicit：单独抽离出来并具备一定可控制性？</p>\n<p>回想翻译场景，在RNN中，每一个时间步骤 <span class=\"math inline\">\\(i\\)</span> 都会产生一个隐向量，<span class=\"math inline\">\\(h_i\\)</span> 向量，我们把这些 <span class=\"math inline\">\\(h_i\\)</span>\n保存起来，在最后要生成新的输出的时候，我们让模型回头看一下之前的这每一个\n<span class=\"math inline\">\\(h_i\\)</span>\n，再决定要生成什么内容。相比原来只利用最后一个hidden\nstate，现在我们可以访问之前所有的中间状态，如果发现前面有关键信息，就可以直接用上了，而不用担心输入太长而被覆盖了。</p>\n<p>那么问题又来了，我们怎么知道前面某一个中间状态对于当前的生成来说是否重要？如果我们不知道怎么定义是否重要，那我们就把这个问题交给模型自己解决好了\n--\n通过网络参数来学习识别某个输入状态是否重要，学习是否要“注意”到它，要给予多少的“注意力”。</p>\n<p>具体来说，我们定义在解码第 <span class=\"math inline\">\\(i\\)</span>\n个输出是，decoder当前隐状态 <span class=\"math inline\">\\(y_{i-1}\\)</span>\n和encoder的所有隐状态 <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\n之间的一个score计算</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p>其中</p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p>注意力网络通过 <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n和 <span class=\"math inline\">\\(h_j\\)</span> 来计算一个值 <span class=\"math inline\">\\(e_{ij}\\)</span>，这里的注意力网络可以设计各种操作，比如对输入进行拼接再通过fc层进行计算等。</p>\n<p>这里 <span class=\"math inline\">\\(e_{ij}\\)</span>\n是一个标量，但它还不是一个可用的权重值，还需要通过一个函数，把attention\nnet对各个encoder hidden state的输出值转成一个分布：softmax。</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>最后通过加权计算，获得最终输入给decoder的隐变量。</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>可以看到，这里的attention net的任务就是找到decoder上一个hidden\nstate和encoder hidden\nstate之间的“相关”关系，使得模型能够将更多的注意力放在对应的输入信息上。</p>\n<p>实际上，上面这种attention的计算方式并不是唯一的，attention的计算方式有许多种</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>这些attention的一般形式可以写作 <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span> 。这里的 <span class=\"math inline\">\\(s\\)</span>\n就是decoder的hidden state（也就是前文的 <span class=\"math inline\">\\(y\\)</span> ），<span class=\"math inline\">\\(h\\)</span> 就是encoder的hidden state。</p>\n<p>（当然从结果上看，是scaled dot-product\nattention经受住了历史的考验，成为了主流。）</p>\n<h2 id=\"transformer的attention\">Transformer的attention</h2>\n<p>从RNN attention到transformer\nattention，所做的事情就如论文题目所说：《Attention Is All You\nNeed》，彻底抛弃RNN的在time\nstep上的迭代计算，完全拥抱attention机制，只用最简单粗暴的方式同步计算出每个输入的hidden\nstate，其他的就交给attention来解决。</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>这里还是保留有encoder和decoder的结构，encoder中的attention都是self-attention，decoder则除了self-attention还有cross-attention。</p>\n<p>transformer结构下，attention的一般形式可以写作 <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>，这里有\n<span class=\"math inline\">\\(Q=W_{Q}Y，K=W_{K}X，V=W_{V}X\\)</span>\n。对于cross-attention， <span class=\"math inline\">\\(X\\)</span>\n是encoder的hidden states，<span class=\"math inline\">\\(Y\\)</span>\n是decoder的hidden states，而对于self-attention，则有 <span class=\"math inline\">\\(X=Y\\)</span>。</p>\n<p>具体到我们熟悉的scaled dot-product attention，使用softmax计算，有</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>到这里，终于见到我们熟悉的attention计算。</p>\n<p>用一张很直观的图来展示整个计算</p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>这里的「query」，「key」和「value」的名称也暗示了整个attention计算的思路。</p>\n<p>类比到一个数据库查询+预测的例子。</p>\n<p>假设我们现在有一个“文章-阅读量”数据库，记录了每篇文章在发布30天内的阅读量。每篇文章就是一个key，对应的阅读量就是value。</p>\n<p>我们现在有一篇将要发布的文章，想要预测这篇文章在30天内的阅读量，那我们就把这篇新的文章，作为query，去和数据库里的文章（key）做一个相关性计算，取最相关的5篇文章。</p>\n<p>假设top5篇文章的相关性分别是 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span> ，对应阅读量是 <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n。</p>\n<p>那我们把相关性得分归一化成和为1的概率值 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n，那我们就可以预测新文章30天内的阅读量是 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n。</p>\n<p>这个例子中，我们计算相关性就相当于transformer attention中的 <span class=\"math inline\">\\(QK^T\\)</span>\n，归一化就是softmax，然后通过加权求和取得最后的阅读量/特征向量。</p>\n<p>对于self-attention， <span class=\"math inline\">\\(Q、K、V\\)</span>\n都来自输入 <span class=\"math inline\">\\(X\\)</span>，sequence自己计算自己每个token的之间的相关性。而对于cross-attention，decoder中的输出sequence就是上面这个例子中的“将要发布的文章”，通过把这篇新的文章和数据库中的文章做相关计算，我们得到了新的预测结果。</p>\n<p>对于self-attention，由于 <span class=\"math inline\">\\(Q、K、V\\)</span>\n都来自输入 <span class=\"math inline\">\\(X\\)</span> ，在计算 <span class=\"math inline\">\\(QK^T\\)</span>\n时，模型很容易关注到自身的位置上，也就是 <span class=\"math inline\">\\(QK^T\\)</span>\n对角线上的激活值会明显比较大。这样的情况其实不是很好，因为这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。后面讲的MHA对这个问题会有一些缓解作用。</p>\n<p>顺着这样的思路梳理下来，会发现attention的大思路还是很好理解的。而计算上，怎么去获得更好的效果，就是接下来要分析的几个内容，MHA，MQA和GQA所关注的。</p>\n<p>代码上，实现也很容易，直接看<a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a>的代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"关于scaling\">关于scaling</h2>\n<p>BTW，为什么计算中 <span class=\"math inline\">\\(QK^T\\)</span>\n之后还要除以 <span class=\"math inline\">\\(\\sqrt{d}\\)</span> ？</p>\n<p>简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p>苏剑林的<a href=\"https://spaces.ac.cn/archives/8620\">博客</a>中也有详细分析，并提到如果不对attention值进行scaling，也可以通过在参数初始化是将方差除以一个\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\n，同样可以起到预防softmax饱和的效果。类似地，通过normalization也可以做到类似的效果。不过实现上在attention里做scaling还是比较稳定高效的。</p>\n<h1 id=\"mha\">MHA</h1>\n<p>只要理解了attention计算的细节，MHA（multi-head\nattention）其实就很好明白。</p>\n<p>MHA在2017年就随着《Attention Is All You\nNeed》一起提出，主要干的就是一个事：把原来一个attention计算，拆成多个小份的attention，并行计算，分别得出结果，最后再合回原来的维度。</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>假设原来模型的hidden size是 <span class=\"math inline\">\\(d\\)</span>\n，在MHA中，会把投影后的 <span class=\"math inline\">\\(Q、K、V\\)</span>\n在hidden state的维度上切成 <span class=\"math inline\">\\(head_{num}\\)</span> 份，每个头的维度是 <span class=\"math inline\">\\(d_{head}\\)</span> 。这 <span class=\"math inline\">\\(head_{num}\\)</span> 组小 <span class=\"math inline\">\\(Q、K、V\\)</span>\n分别独立地进行attention计算，之后把得到的 <span class=\"math inline\">\\(head_{num}\\)</span> 份维度 <span class=\"math inline\">\\(d_{head}\\)</span> 的输出concat起来。</p>\n<p>直接看这个amazing的图，很直观</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p>操作是这么个操作，多头注意力相比单头有什么好处呢？</p>\n<p>《Attention Is All You Need》文章中给出的说法是</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>我们希望多个头能够在训练中学会注意到不同的内容。例如在翻译任务里，一些attention\nhead可以关注语法特征，另一些attention\nhead可以关注单词特性。这样模型就可以从不同角度来分析和理解输入信息，获得更好的效果了。</p>\n<p>这有点类似CNN中，不同的卷积核来学习不同的信息。比如一个 <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 的卷积，有128个 <span class=\"math inline\">\\(3\\times3\\)</span>\n参数组，假设我们的输入是一个灰度图，其中一组 <span class=\"math inline\">\\(3\\times3\\)</span> 的参数是这样的</p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p>那么这是一个检测纵向边界的卷积，而另外一组参数长这样</p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p>这是一个检测横向边界的卷积。</p>\n<p>这128组 <span class=\"math inline\">\\(3\\times3\\)</span>\n就是128个不同特征的检测器，就同MHA中多个头一样，从不同的子空间学到不同的内容，最后再放到一起融合使用。</p>\n<p>但是这是我们expect模型能做到的事情，实际情况是否真的是这样？</p>\n<p>知乎上这篇<a href=\"https://zhuanlan.zhihu.com/p/626820422\">文章</a>里对此做了一些实验和分析。简单来说就是（1）每个头确实学到东西有所不同，但大部分头之间的差异没有我们想的那么大（比如一个学句法，一个学词义这样明显的区分）（2）多个头的情况下，确实有少部分头可以比较好地捕捉到各种文本信息，而不会过分关注自身位置，一定程度缓解了上文提到的计算\n<span class=\"math inline\">\\(QK^T\\)</span> 之后对角线元素过大的问题。</p>\n<p>我们可以把MHA的多个attention计算视为多个独立的小模型，那么最终整体的attention计算相当于把来自多个小模型的结果进行了融合，这样效果比较好也是比较符合直觉的。</p>\n<p>另外还有一个问题是，使用几个头比较好呢？</p>\n<p>实际上这个问题比较难有确定性的答案，首先可以确定的是头的数量不是越多约好（毕竟头的数量多了，各个子空间小了，子空间能表达的内容就少了），具体多少要视模型规模，任务而定。另外<a href=\"https://arxiv.org/pdf/1905.10650.pdf\">《Are Sixteen Heads Really\nBetter than One?》</a>中也指出MHA并不总是优于单头的情况。</p>\n<p>目前可以看到的趋势是，模型越大（也就是hidden\nsize越大），头数的增多越能带来平均效果上的收益（或者说允许注意力头增大而不影响子空间的学习能力）。目前LLM主流的头数视乎模型结构和规模，大致有12、16、24、48、96这样一些主流设置。这里面又有比较多的方向和工作，在此暂时不展开，挖个坑，以后专门开一篇讲。</p>\n<p>最后看一下<a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>中的MHA代码实现</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p>（<a href=\"https://github.com/huggingface/transformers\">transformers</a>中的写法就更为成熟一点，不过里面兼容了比较多的功能，代码太长就不放上来了）</p>\n<h1 id=\"解码中的kv-cache\">解码中的KV Cache</h1>\n<p>在讲MQA和GQA之前，还需要了解一点背景，那就是解码的计算问题，以及KV\nCache的方案。</p>\n<p>无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。</p>\n<p>也就是，解码的时候，先根据当前输入 <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> ，生成下一个 <span class=\"math inline\">\\(\\text{token}_{i}\\)</span> ，然后把新生成的 <span class=\"math inline\">\\(\\text{token}_{i}\\)</span> 拼接在 <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> 后面，获得新的输入\n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span> ，再用 <span class=\"math inline\">\\(\\text{input}_{i}\\)</span> 生成 <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n，依此迭代，直到生成结束。</p>\n<p>比如我们输入“窗前明月光下一句是”，那么模型每次生成一个token，输入输出会是这样（方便起见，默认每个token都是一个字符）</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: 输入=[BOS]窗前明月光下一句是；输出=疑</span><br><span class=\"line\">step1: 输入=[BOS]窗前明月光下一句是疑；输出=是</span><br><span class=\"line\">step2: 输入=[BOS]窗前明月光下一句是疑是；输出=地</span><br><span class=\"line\">step3: 输入=[BOS]窗前明月光下一句是疑是地；输出=上</span><br><span class=\"line\">step4: 输入=[BOS]窗前明月光下一句是疑是地上；输出=霜</span><br><span class=\"line\">step5: 输入=[BOS]窗前明月光下一句是疑是地上霜；输出=[EOS]</span><br></pre></td></tr></table></figure>\n<p>（其中[BOS]和[EOS]分别是起始符号和终止符号）</p>\n<p>仔细想一下，我们在生成“疑”字的时候，用的是输入序列中“是”字的最后一层hidden\nstate，通过最后的分类头预测出来的。以此类推，后面每生成一个字，使用的都是输入序列中最后一个字的输出。</p>\n<p>我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个token）。那么下一个step的计算应该也包含了上一个step的计算。</p>\n<p>从公式上来看是这样的：</p>\n<p>回想一下我们attention的计算</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>注意对于decoder的时候，由于mask\nattention的存在，每个输入只能看到自己和前面的内容，而看不到后面的内容</p>\n<p>假设我们当前输入的长度是3，预测第4个字，那每层attention所做的计算有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>预测完第4个字，放到输入里，继续预测第5个字，每层attention所做的计算有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>可以看到，在预测第5个字时，只有最后一步引入了新的计算，而 <span class=\"math inline\">\\(o_{0}\\)</span> 到 <span class=\"math inline\">\\(o_{2}\\)</span> 的计算和前面是完全重复的。</p>\n<p>但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。</p>\n<p>也就是说中间有很多我们用不到的计算，这样就造成了浪费。</p>\n<p>而且随着生成的结果越来越多，输入的长度也越来越长。上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写高考作文，那可能就有800个step或者更多。这个情况下，step0被算了800次，step1被算了799次...</p>\n<p>有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？</p>\n<p>答案就是KV\nCache。利用缓存空间，把需要重复利用的中间计算结果存下来，减少重复计算。</p>\n<p>而 <span class=\"math inline\">\\(k\\)</span> 和 <span class=\"math inline\">\\(v\\)</span> 就是要缓存的对象。</p>\n<p>想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要踏踏实实地计算一遍。然后把\n<span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值缓存起来。</p>\n<p>则有</p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n↓\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache的下标 <span class=\"math inline\">\\(l\\)</span>\n表示模型层数。</p>\n<p>在进行第二次预测，也就是预测第5个字的时候，在第 <span class=\"math inline\">\\(l\\)</span>\n层的时候，由于前面我们缓存了<u><strong>每层</strong></u>的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值，那本层就只需要算新的 <span class=\"math inline\">\\(o_{3}\\)</span> ，而不用算 <span class=\"math inline\">\\(o_{0}、o_{1}、o_{2}\\)</span> 。</p>\n<p>因为第 <span class=\"math inline\">\\(l\\)</span> 层的 <span class=\"math inline\">\\(o_{0}、o_{1}、o_{2}\\)</span>\n本来会经过FNN层之后进到 <span class=\"math inline\">\\(l+1\\)</span>\n层，再经过新的投影变换，成为 <span class=\"math inline\">\\(l+1\\)</span>\n层的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值，但是 <span class=\"math inline\">\\(l+1\\)</span> 层的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值我们已经缓存过了！</p>\n<p>然后我们把本次新增算出来的 <span class=\"math inline\">\\(k\\)</span> 、\n<span class=\"math inline\">\\(v\\)</span> 值也存入缓存。</p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n↓\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>这样就节省了attention和FFN的很多重复计算。</p>\n<p>transformers中，生成的时候传入use_cache=True就会开启KV Cache。</p>\n<p>也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 过去所存的值</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># 把当前新的key加入</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># 把当前新的value加入</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># 输出用于保存</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>总的来说，KV\nCache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask\nattention的存在，使得前面的token可以不用关注后面的token）</p>\n<p>但是，用了KV Cache之后也不是立刻万事大吉。</p>\n<p>我们简单算一下，对于输入长度为 <span class=\"math inline\">\\(s\\)</span>\n，层数为 <span class=\"math inline\">\\(L\\)</span> ，hidden size为 <span class=\"math inline\">\\(d\\)</span> 的模型，需要缓存的参数量为</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p>如果使用的是半精度浮点数，那么总共所需的空间就是</p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>以Llama2 7B为例，有 <span class=\"math inline\">\\(L=32\\)</span> ，\n<span class=\"math inline\">\\(L=4096\\)</span>\n，那么每个token所需的缓存空间就是524,288 bytes，约52K，当 <span class=\"math inline\">\\(s=1024\\)</span> 时，则需要536,870,912\nbytes，超过500M的空间。</p>\n<p>这里考虑的还只是batch size=1的情况，如果batch\nsize增大，这个值更是很容易就超过1G。</p>\n<p>（MHA相比单头的情况，相当于只是把 <span class=\"math inline\">\\(q、k、v\\)</span>\n切成多份并行计算了，对于实际需要缓存的大小没有影响）</p>\n<p>看下现在主流的科学计算卡配置</p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>强如H100也只有50M的L2 Cache（L1\nCache的大小更是可以忽略不计），大概只能支持Llama2\n7B总共100个token左右的输入。</p>\n<p>想想我们现在用的LLM动辄34B/70B的规模，长度更是以千为基础单位，这样明显是不够用的。</p>\n<p>那么超出L2 Cache的部分只能走到显存中去了，但是HBM速度比L2\nCache慢多了。</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"储存空间与速度\">\n<p>看来还需要进一步优化。</p>\n<p>要保证模型的推理加速，要么增大Cache的大小，而且是需要一到两个数量级的增强，那这个只能靠黄老板了。</p>\n<p>要么就是减少需要缓存的量。</p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA就是来减少缓存所需要的量的。</p>\n<p>Google在2019年就在《Fast Transformer Decoding: One Write-Head is All\nYou\nNeed》提出了MQA，不过那时候主要到的人不多，那是大家主要还是关注在用Bert把榜刷出新高上。</p>\n<p>MQA的做法其实很简单。在MHA中，输入分别经过 <span class=\"math inline\">\\(W_{Q}、W_{K}、W_{V}\\)</span>\n的变换之后，都切成了n份（n=头数），维度也从 <span class=\"math inline\">\\(d_{model}\\)</span> 降到了 <span class=\"math inline\">\\(d_{head}\\)</span>\n，分别进行attention计算再拼接。而MQA这里，在线性变换之后，只对 <span class=\"math inline\">\\(Q\\)</span> 进行切分（和MHA一样），而 <span class=\"math inline\">\\(K、V\\)</span> 则直接在线性变换的时候把维度降到了\n<span class=\"math inline\">\\(d_{head}\\)</span>\n（而不是切分变小），然后这n个Query头分别和同一份 <span class=\"math inline\">\\(K、V\\)</span>\n进行attention计算，之后把结果拼接起来。</p>\n<p>简单来说，就是MHA中，每个注意力头的 <span class=\"math inline\">\\(K、V\\)</span>\n是不一样的，而MQA这里，每个注意力头的 <span class=\"math inline\">\\(K、V\\)</span>\n是一样的，值是共享的。而其他步骤都和MHA一样。</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p>这样一来，需要缓存的 <span class=\"math inline\">\\(K、V\\)</span>\n值一下就从所有头变成一个头的量。</p>\n<p>比如在Llama2\n7B中用的是32个头，那用MQA后，1024个token需要缓存的量就变成1/32，536,870,912\nbytes / 32 = 16,777,216 bytes，差不多是16M，这就能全塞进缓存中了。</p>\n<p>（实现上，就是改一下线性变换矩阵，然后把 <span class=\"math inline\">\\(K、V\\)</span>\n的处理从切分变成复制，就不再赘述。）</p>\n<p>当然，由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点，但是并不多，且相比其他修改hidden\nsize或者head num的做法效果都好。</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>既然MQA对效果有点影响，MHA缓存又存不下，那GQA（Grouped-Query\nAttention）就提出了一个折中的办法，既能减少MQA效果的损失，又相比MHA需要更少的缓存。</p>\n<p>（文章：《GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints》，2023年）</p>\n<p>GQA里， <span class=\"math inline\">\\(Q\\)</span>\n还是按原来MHA/MQA的做法不变。只使用一套共享的 <span class=\"math inline\">\\(K、V\\)</span>\n不是效果不好吗，那就还是多弄几套。但是不要太多，数量还是比 <span class=\"math inline\">\\(Q\\)</span> 的头数少一些。这样相当于把 <span class=\"math inline\">\\(Q\\)</span> 的多个头给分了group，同一个group内的\n<span class=\"math inline\">\\(Q\\)</span> 共享同一套 <span class=\"math inline\">\\(K、V\\)</span> ，不同group的 <span class=\"math inline\">\\(Q\\)</span> 所用的 <span class=\"math inline\">\\(K、V\\)</span> 不同。</p>\n<p>MHA可以认为是 <span class=\"math inline\">\\(K、V\\)</span>\n头数最大时的GQA，而MQA可以任务是 <span class=\"math inline\">\\(K、V\\)</span> 头数最少时的GQA。</p>\n<p>看论文里的图就很直观</p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p>效果怎么样呢？</p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>看表中2/3/4行对比，GQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。文中提到，这里的MQA和GQA都是通过average\npooling从MHA初始化而来，然后进行了少量的训练得到的。如果我们想要把之前用MHA训练的模型改造成GQA，也可以通过这样的方法，增加少量训练来实现。当然如果从一开始就加上，从零开始训练，也是没有问题的。</p>\n<p>Llama2用的就是GQA，在tech\nreport中也做了MHA、MQA、GQA的效果对比，可以看到效果确实很不错。</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"小结\">小结</h1>\n<p>MHA、MQA、GQA的实现其实并不复杂，效果也很好，理解上并没有太多困难。但是想要真正理解它们的出发点，还是需要深入每一个细节，去了解当时要解决的事什么问题。</p>\n<p>目前来看GQA是LLM比较好的方案，但未来肯定还会有针对不同方向的进一步优化方案，计算效率、推理速度、显存消耗这些方向都值得我们继续去探索优化。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n【2】Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n【3】Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n【4】https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n【5】GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n【6】How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n【7】A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n【8】https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n【9】浅谈Transformer的初始化、参数化与标准化\nhttps://spaces.ac.cn/archives/8620<br>\n【10】https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n【11】https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n【12】Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n【13】This post is all you need（上卷）——层层剥开Transformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n【14】The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n【15】Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n","length":16733,"excerpt":"","more":"<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>\n<p>Attention模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head\nAttention）、MQA（Multi-Query Attention）和GQA（Grouped-Query\nAttention）这一路线的思路和做法被很多主流模型所采用，因此简单地梳理一些这几个变体的思路和做法，以及会涉及到的KV\nCache相关内容。思路比较直白，但也有一些细节和原理值得思考。</p>\n<p>当然针对Attention优化，也有很多其他优秀的方案和思路，如线性注意力、FlashAttention和Sliding\nWindow Attention等，这些在后续再开篇梳理。</p>\n<p>（应一些朋友的要求，会增加一些直观基础的内容，以及LLM应用的案例。也欢迎大家提出更多建议。）</p>\n<h1 id=\"关于attention从rnn到attention\">关于Attention：从RNN到Attention</h1>\n<p>了解一个概念的诞生和演进，有助于我们更深入去理解它。我们先简单回顾下attention从起源到最初的实现。</p>\n<p>（熟悉attention的朋友可以跳过这一节）</p>\n<h2 id=\"从rnn说起\">从RNN说起</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>注意力机制最初起源是为了解决序列问题。回想在还没有Transformer的上一世代，使用RNN的Seq2Seq是这样的</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p>（图来自<a href=\"https://theaisummer.com/attention/\">AI\nSummer</a>）</p>\n<p>每个RNN cell接收两个输入，输出一个hidden state。比如在翻译任务中，RNN\nencoder把所有输入迭代地编码成context向量 <span class=\"math inline\">\\(z\\)</span> ，然后由RNN decoder基于 <span class=\"math inline\">\\(z\\)</span>\n迭代地解码。一般来说，这里decoder的第一个输入是一个特殊token，如[start]，表示解码开始。</p>\n<p>这样会有一个问题， <span class=\"math inline\">\\(z\\)</span>\n能编码的长度显然有限，而且由于模型结构问题，会更加关注靠近尾部的输入。这样如果关键信息出现在开头，就容易被忽略。</p>\n<p>并且时间步骤上的传播由于有多次迭代相乘，梯度很容易就过小，导致梯度消失问题。</p>\n<p>当然我们有LSMT和GRU等变体来增强长距离记忆的能力，也缓解了梯度问题，但这些方案还是没有产生质变的能力。</p>\n<p>回到问题的核心，我们想要 <span class=\"math inline\">\\(z\\)</span>\n能够编码所有前面的内容，但是显然， <span class=\"math inline\">\\(z\\)</span>\n的生成方式天然会让它更容易注意到靠后的内容，而容易忽略靠前的输入。</p>\n<p>一个直觉的想法就是，我们需要想个办法跳过 <span class=\"math inline\">\\(z\\)</span>\n，和前面的每个输入建立直接的联系。我们希望模型能够有机会学习到去“注意”关键的输入，不管这个输入是在前面还是后面。</p>\n<p>实际上神经网络天生就具有“注意力”的天赋。</p>\n<p>比如在CNN分类中，如果我们画出分类层前的heatmap，会是如下图这个样子</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>可以看到，值比较高的地方是在猫的鼻子胡子嘴巴区域，次之是身上和头上的花纹。直观来说，就是模型主要通过脸部的特征和身上的花纹，来识别出这是一只猫。这就是CNN学习到的注意力，这样的特征是神经网络implicitly学到的。</p>\n<p>回归到Seq2Seq，我们怎么来实现注意力，并且让这种implicit的机制变得explicit：单独抽离出来并具备一定可控制性？</p>\n<p>回想翻译场景，在RNN中，每一个时间步骤 <span class=\"math inline\">\\(i\\)</span> 都会产生一个隐向量，<span class=\"math inline\">\\(h_i\\)</span> 向量，我们把这些 <span class=\"math inline\">\\(h_i\\)</span>\n保存起来，在最后要生成新的输出的时候，我们让模型回头看一下之前的这每一个\n<span class=\"math inline\">\\(h_i\\)</span>\n，再决定要生成什么内容。相比原来只利用最后一个hidden\nstate，现在我们可以访问之前所有的中间状态，如果发现前面有关键信息，就可以直接用上了，而不用担心输入太长而被覆盖了。</p>\n<p>那么问题又来了，我们怎么知道前面某一个中间状态对于当前的生成来说是否重要？如果我们不知道怎么定义是否重要，那我们就把这个问题交给模型自己解决好了\n--\n通过网络参数来学习识别某个输入状态是否重要，学习是否要“注意”到它，要给予多少的“注意力”。</p>\n<p>具体来说，我们定义在解码第 <span class=\"math inline\">\\(i\\)</span>\n个输出是，decoder当前隐状态 <span class=\"math inline\">\\(y_{i-1}\\)</span>\n和encoder的所有隐状态 <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\n之间的一个score计算</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p>其中</p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p>注意力网络通过 <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n和 <span class=\"math inline\">\\(h_j\\)</span> 来计算一个值 <span class=\"math inline\">\\(e_{ij}\\)</span>，这里的注意力网络可以设计各种操作，比如对输入进行拼接再通过fc层进行计算等。</p>\n<p>这里 <span class=\"math inline\">\\(e_{ij}\\)</span>\n是一个标量，但它还不是一个可用的权重值，还需要通过一个函数，把attention\nnet对各个encoder hidden state的输出值转成一个分布：softmax。</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>最后通过加权计算，获得最终输入给decoder的隐变量。</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>可以看到，这里的attention net的任务就是找到decoder上一个hidden\nstate和encoder hidden\nstate之间的“相关”关系，使得模型能够将更多的注意力放在对应的输入信息上。</p>\n<p>实际上，上面这种attention的计算方式并不是唯一的，attention的计算方式有许多种</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>这些attention的一般形式可以写作 <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span> 。这里的 <span class=\"math inline\">\\(s\\)</span>\n就是decoder的hidden state（也就是前文的 <span class=\"math inline\">\\(y\\)</span> ），<span class=\"math inline\">\\(h\\)</span> 就是encoder的hidden state。</p>\n<p>（当然从结果上看，是scaled dot-product\nattention经受住了历史的考验，成为了主流。）</p>\n<h2 id=\"transformer的attention\">Transformer的attention</h2>\n<p>从RNN attention到transformer\nattention，所做的事情就如论文题目所说：《Attention Is All You\nNeed》，彻底抛弃RNN的在time\nstep上的迭代计算，完全拥抱attention机制，只用最简单粗暴的方式同步计算出每个输入的hidden\nstate，其他的就交给attention来解决。</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>这里还是保留有encoder和decoder的结构，encoder中的attention都是self-attention，decoder则除了self-attention还有cross-attention。</p>\n<p>transformer结构下，attention的一般形式可以写作 <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>，这里有\n<span class=\"math inline\">\\(Q=W_{Q}Y，K=W_{K}X，V=W_{V}X\\)</span>\n。对于cross-attention， <span class=\"math inline\">\\(X\\)</span>\n是encoder的hidden states，<span class=\"math inline\">\\(Y\\)</span>\n是decoder的hidden states，而对于self-attention，则有 <span class=\"math inline\">\\(X=Y\\)</span>。</p>\n<p>具体到我们熟悉的scaled dot-product attention，使用softmax计算，有</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>到这里，终于见到我们熟悉的attention计算。</p>\n<p>用一张很直观的图来展示整个计算</p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>这里的「query」，「key」和「value」的名称也暗示了整个attention计算的思路。</p>\n<p>类比到一个数据库查询+预测的例子。</p>\n<p>假设我们现在有一个“文章-阅读量”数据库，记录了每篇文章在发布30天内的阅读量。每篇文章就是一个key，对应的阅读量就是value。</p>\n<p>我们现在有一篇将要发布的文章，想要预测这篇文章在30天内的阅读量，那我们就把这篇新的文章，作为query，去和数据库里的文章（key）做一个相关性计算，取最相关的5篇文章。</p>\n<p>假设top5篇文章的相关性分别是 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span> ，对应阅读量是 <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n。</p>\n<p>那我们把相关性得分归一化成和为1的概率值 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n，那我们就可以预测新文章30天内的阅读量是 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n。</p>\n<p>这个例子中，我们计算相关性就相当于transformer attention中的 <span class=\"math inline\">\\(QK^T\\)</span>\n，归一化就是softmax，然后通过加权求和取得最后的阅读量/特征向量。</p>\n<p>对于self-attention， <span class=\"math inline\">\\(Q、K、V\\)</span>\n都来自输入 <span class=\"math inline\">\\(X\\)</span>，sequence自己计算自己每个token的之间的相关性。而对于cross-attention，decoder中的输出sequence就是上面这个例子中的“将要发布的文章”，通过把这篇新的文章和数据库中的文章做相关计算，我们得到了新的预测结果。</p>\n<p>对于self-attention，由于 <span class=\"math inline\">\\(Q、K、V\\)</span>\n都来自输入 <span class=\"math inline\">\\(X\\)</span> ，在计算 <span class=\"math inline\">\\(QK^T\\)</span>\n时，模型很容易关注到自身的位置上，也就是 <span class=\"math inline\">\\(QK^T\\)</span>\n对角线上的激活值会明显比较大。这样的情况其实不是很好，因为这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。后面讲的MHA对这个问题会有一些缓解作用。</p>\n<p>顺着这样的思路梳理下来，会发现attention的大思路还是很好理解的。而计算上，怎么去获得更好的效果，就是接下来要分析的几个内容，MHA，MQA和GQA所关注的。</p>\n<p>代码上，实现也很容易，直接看<a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a>的代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"关于scaling\">关于scaling</h2>\n<p>BTW，为什么计算中 <span class=\"math inline\">\\(QK^T\\)</span>\n之后还要除以 <span class=\"math inline\">\\(\\sqrt{d}\\)</span> ？</p>\n<p>简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p>苏剑林的<a href=\"https://spaces.ac.cn/archives/8620\">博客</a>中也有详细分析，并提到如果不对attention值进行scaling，也可以通过在参数初始化是将方差除以一个\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\n，同样可以起到预防softmax饱和的效果。类似地，通过normalization也可以做到类似的效果。不过实现上在attention里做scaling还是比较稳定高效的。</p>\n<h1 id=\"mha\">MHA</h1>\n<p>只要理解了attention计算的细节，MHA（multi-head\nattention）其实就很好明白。</p>\n<p>MHA在2017年就随着《Attention Is All You\nNeed》一起提出，主要干的就是一个事：把原来一个attention计算，拆成多个小份的attention，并行计算，分别得出结果，最后再合回原来的维度。</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>假设原来模型的hidden size是 <span class=\"math inline\">\\(d\\)</span>\n，在MHA中，会把投影后的 <span class=\"math inline\">\\(Q、K、V\\)</span>\n在hidden state的维度上切成 <span class=\"math inline\">\\(head_{num}\\)</span> 份，每个头的维度是 <span class=\"math inline\">\\(d_{head}\\)</span> 。这 <span class=\"math inline\">\\(head_{num}\\)</span> 组小 <span class=\"math inline\">\\(Q、K、V\\)</span>\n分别独立地进行attention计算，之后把得到的 <span class=\"math inline\">\\(head_{num}\\)</span> 份维度 <span class=\"math inline\">\\(d_{head}\\)</span> 的输出concat起来。</p>\n<p>直接看这个amazing的图，很直观</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p>操作是这么个操作，多头注意力相比单头有什么好处呢？</p>\n<p>《Attention Is All You Need》文章中给出的说法是</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>我们希望多个头能够在训练中学会注意到不同的内容。例如在翻译任务里，一些attention\nhead可以关注语法特征，另一些attention\nhead可以关注单词特性。这样模型就可以从不同角度来分析和理解输入信息，获得更好的效果了。</p>\n<p>这有点类似CNN中，不同的卷积核来学习不同的信息。比如一个 <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 的卷积，有128个 <span class=\"math inline\">\\(3\\times3\\)</span>\n参数组，假设我们的输入是一个灰度图，其中一组 <span class=\"math inline\">\\(3\\times3\\)</span> 的参数是这样的</p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p>那么这是一个检测纵向边界的卷积，而另外一组参数长这样</p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p>这是一个检测横向边界的卷积。</p>\n<p>这128组 <span class=\"math inline\">\\(3\\times3\\)</span>\n就是128个不同特征的检测器，就同MHA中多个头一样，从不同的子空间学到不同的内容，最后再放到一起融合使用。</p>\n<p>但是这是我们expect模型能做到的事情，实际情况是否真的是这样？</p>\n<p>知乎上这篇<a href=\"https://zhuanlan.zhihu.com/p/626820422\">文章</a>里对此做了一些实验和分析。简单来说就是（1）每个头确实学到东西有所不同，但大部分头之间的差异没有我们想的那么大（比如一个学句法，一个学词义这样明显的区分）（2）多个头的情况下，确实有少部分头可以比较好地捕捉到各种文本信息，而不会过分关注自身位置，一定程度缓解了上文提到的计算\n<span class=\"math inline\">\\(QK^T\\)</span> 之后对角线元素过大的问题。</p>\n<p>我们可以把MHA的多个attention计算视为多个独立的小模型，那么最终整体的attention计算相当于把来自多个小模型的结果进行了融合，这样效果比较好也是比较符合直觉的。</p>\n<p>另外还有一个问题是，使用几个头比较好呢？</p>\n<p>实际上这个问题比较难有确定性的答案，首先可以确定的是头的数量不是越多约好（毕竟头的数量多了，各个子空间小了，子空间能表达的内容就少了），具体多少要视模型规模，任务而定。另外<a href=\"https://arxiv.org/pdf/1905.10650.pdf\">《Are Sixteen Heads Really\nBetter than One?》</a>中也指出MHA并不总是优于单头的情况。</p>\n<p>目前可以看到的趋势是，模型越大（也就是hidden\nsize越大），头数的增多越能带来平均效果上的收益（或者说允许注意力头增大而不影响子空间的学习能力）。目前LLM主流的头数视乎模型结构和规模，大致有12、16、24、48、96这样一些主流设置。这里面又有比较多的方向和工作，在此暂时不展开，挖个坑，以后专门开一篇讲。</p>\n<p>最后看一下<a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>中的MHA代码实现</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p>（<a href=\"https://github.com/huggingface/transformers\">transformers</a>中的写法就更为成熟一点，不过里面兼容了比较多的功能，代码太长就不放上来了）</p>\n<h1 id=\"解码中的kv-cache\">解码中的KV Cache</h1>\n<p>在讲MQA和GQA之前，还需要了解一点背景，那就是解码的计算问题，以及KV\nCache的方案。</p>\n<p>无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。</p>\n<p>也就是，解码的时候，先根据当前输入 <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> ，生成下一个 <span class=\"math inline\">\\(\\text{token}_{i}\\)</span> ，然后把新生成的 <span class=\"math inline\">\\(\\text{token}_{i}\\)</span> 拼接在 <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> 后面，获得新的输入\n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span> ，再用 <span class=\"math inline\">\\(\\text{input}_{i}\\)</span> 生成 <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n，依此迭代，直到生成结束。</p>\n<p>比如我们输入“窗前明月光下一句是”，那么模型每次生成一个token，输入输出会是这样（方便起见，默认每个token都是一个字符）</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: 输入=[BOS]窗前明月光下一句是；输出=疑</span><br><span class=\"line\">step1: 输入=[BOS]窗前明月光下一句是疑；输出=是</span><br><span class=\"line\">step2: 输入=[BOS]窗前明月光下一句是疑是；输出=地</span><br><span class=\"line\">step3: 输入=[BOS]窗前明月光下一句是疑是地；输出=上</span><br><span class=\"line\">step4: 输入=[BOS]窗前明月光下一句是疑是地上；输出=霜</span><br><span class=\"line\">step5: 输入=[BOS]窗前明月光下一句是疑是地上霜；输出=[EOS]</span><br></pre></td></tr></table></figure>\n<p>（其中[BOS]和[EOS]分别是起始符号和终止符号）</p>\n<p>仔细想一下，我们在生成“疑”字的时候，用的是输入序列中“是”字的最后一层hidden\nstate，通过最后的分类头预测出来的。以此类推，后面每生成一个字，使用的都是输入序列中最后一个字的输出。</p>\n<p>我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个token）。那么下一个step的计算应该也包含了上一个step的计算。</p>\n<p>从公式上来看是这样的：</p>\n<p>回想一下我们attention的计算</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>注意对于decoder的时候，由于mask\nattention的存在，每个输入只能看到自己和前面的内容，而看不到后面的内容</p>\n<p>假设我们当前输入的长度是3，预测第4个字，那每层attention所做的计算有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>预测完第4个字，放到输入里，继续预测第5个字，每层attention所做的计算有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>可以看到，在预测第5个字时，只有最后一步引入了新的计算，而 <span class=\"math inline\">\\(o_{0}\\)</span> 到 <span class=\"math inline\">\\(o_{2}\\)</span> 的计算和前面是完全重复的。</p>\n<p>但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。</p>\n<p>也就是说中间有很多我们用不到的计算，这样就造成了浪费。</p>\n<p>而且随着生成的结果越来越多，输入的长度也越来越长。上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写高考作文，那可能就有800个step或者更多。这个情况下，step0被算了800次，step1被算了799次...</p>\n<p>有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？</p>\n<p>答案就是KV\nCache。利用缓存空间，把需要重复利用的中间计算结果存下来，减少重复计算。</p>\n<p>而 <span class=\"math inline\">\\(k\\)</span> 和 <span class=\"math inline\">\\(v\\)</span> 就是要缓存的对象。</p>\n<p>想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要踏踏实实地计算一遍。然后把\n<span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值缓存起来。</p>\n<p>则有</p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n↓\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache的下标 <span class=\"math inline\">\\(l\\)</span>\n表示模型层数。</p>\n<p>在进行第二次预测，也就是预测第5个字的时候，在第 <span class=\"math inline\">\\(l\\)</span>\n层的时候，由于前面我们缓存了<u><strong>每层</strong></u>的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值，那本层就只需要算新的 <span class=\"math inline\">\\(o_{3}\\)</span> ，而不用算 <span class=\"math inline\">\\(o_{0}、o_{1}、o_{2}\\)</span> 。</p>\n<p>因为第 <span class=\"math inline\">\\(l\\)</span> 层的 <span class=\"math inline\">\\(o_{0}、o_{1}、o_{2}\\)</span>\n本来会经过FNN层之后进到 <span class=\"math inline\">\\(l+1\\)</span>\n层，再经过新的投影变换，成为 <span class=\"math inline\">\\(l+1\\)</span>\n层的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值，但是 <span class=\"math inline\">\\(l+1\\)</span> 层的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值我们已经缓存过了！</p>\n<p>然后我们把本次新增算出来的 <span class=\"math inline\">\\(k\\)</span> 、\n<span class=\"math inline\">\\(v\\)</span> 值也存入缓存。</p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n↓\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>这样就节省了attention和FFN的很多重复计算。</p>\n<p>transformers中，生成的时候传入use_cache=True就会开启KV Cache。</p>\n<p>也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 过去所存的值</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># 把当前新的key加入</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># 把当前新的value加入</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># 输出用于保存</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>总的来说，KV\nCache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask\nattention的存在，使得前面的token可以不用关注后面的token）</p>\n<p>但是，用了KV Cache之后也不是立刻万事大吉。</p>\n<p>我们简单算一下，对于输入长度为 <span class=\"math inline\">\\(s\\)</span>\n，层数为 <span class=\"math inline\">\\(L\\)</span> ，hidden size为 <span class=\"math inline\">\\(d\\)</span> 的模型，需要缓存的参数量为</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p>如果使用的是半精度浮点数，那么总共所需的空间就是</p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>以Llama2 7B为例，有 <span class=\"math inline\">\\(L=32\\)</span> ，\n<span class=\"math inline\">\\(L=4096\\)</span>\n，那么每个token所需的缓存空间就是524,288 bytes，约52K，当 <span class=\"math inline\">\\(s=1024\\)</span> 时，则需要536,870,912\nbytes，超过500M的空间。</p>\n<p>这里考虑的还只是batch size=1的情况，如果batch\nsize增大，这个值更是很容易就超过1G。</p>\n<p>（MHA相比单头的情况，相当于只是把 <span class=\"math inline\">\\(q、k、v\\)</span>\n切成多份并行计算了，对于实际需要缓存的大小没有影响）</p>\n<p>看下现在主流的科学计算卡配置</p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>强如H100也只有50M的L2 Cache（L1\nCache的大小更是可以忽略不计），大概只能支持Llama2\n7B总共100个token左右的输入。</p>\n<p>想想我们现在用的LLM动辄34B/70B的规模，长度更是以千为基础单位，这样明显是不够用的。</p>\n<p>那么超出L2 Cache的部分只能走到显存中去了，但是HBM速度比L2\nCache慢多了。</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"储存空间与速度\">\n<p>看来还需要进一步优化。</p>\n<p>要保证模型的推理加速，要么增大Cache的大小，而且是需要一到两个数量级的增强，那这个只能靠黄老板了。</p>\n<p>要么就是减少需要缓存的量。</p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA就是来减少缓存所需要的量的。</p>\n<p>Google在2019年就在《Fast Transformer Decoding: One Write-Head is All\nYou\nNeed》提出了MQA，不过那时候主要到的人不多，那是大家主要还是关注在用Bert把榜刷出新高上。</p>\n<p>MQA的做法其实很简单。在MHA中，输入分别经过 <span class=\"math inline\">\\(W_{Q}、W_{K}、W_{V}\\)</span>\n的变换之后，都切成了n份（n=头数），维度也从 <span class=\"math inline\">\\(d_{model}\\)</span> 降到了 <span class=\"math inline\">\\(d_{head}\\)</span>\n，分别进行attention计算再拼接。而MQA这里，在线性变换之后，只对 <span class=\"math inline\">\\(Q\\)</span> 进行切分（和MHA一样），而 <span class=\"math inline\">\\(K、V\\)</span> 则直接在线性变换的时候把维度降到了\n<span class=\"math inline\">\\(d_{head}\\)</span>\n（而不是切分变小），然后这n个Query头分别和同一份 <span class=\"math inline\">\\(K、V\\)</span>\n进行attention计算，之后把结果拼接起来。</p>\n<p>简单来说，就是MHA中，每个注意力头的 <span class=\"math inline\">\\(K、V\\)</span>\n是不一样的，而MQA这里，每个注意力头的 <span class=\"math inline\">\\(K、V\\)</span>\n是一样的，值是共享的。而其他步骤都和MHA一样。</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p>这样一来，需要缓存的 <span class=\"math inline\">\\(K、V\\)</span>\n值一下就从所有头变成一个头的量。</p>\n<p>比如在Llama2\n7B中用的是32个头，那用MQA后，1024个token需要缓存的量就变成1/32，536,870,912\nbytes / 32 = 16,777,216 bytes，差不多是16M，这就能全塞进缓存中了。</p>\n<p>（实现上，就是改一下线性变换矩阵，然后把 <span class=\"math inline\">\\(K、V\\)</span>\n的处理从切分变成复制，就不再赘述。）</p>\n<p>当然，由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点，但是并不多，且相比其他修改hidden\nsize或者head num的做法效果都好。</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>既然MQA对效果有点影响，MHA缓存又存不下，那GQA（Grouped-Query\nAttention）就提出了一个折中的办法，既能减少MQA效果的损失，又相比MHA需要更少的缓存。</p>\n<p>（文章：《GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints》，2023年）</p>\n<p>GQA里， <span class=\"math inline\">\\(Q\\)</span>\n还是按原来MHA/MQA的做法不变。只使用一套共享的 <span class=\"math inline\">\\(K、V\\)</span>\n不是效果不好吗，那就还是多弄几套。但是不要太多，数量还是比 <span class=\"math inline\">\\(Q\\)</span> 的头数少一些。这样相当于把 <span class=\"math inline\">\\(Q\\)</span> 的多个头给分了group，同一个group内的\n<span class=\"math inline\">\\(Q\\)</span> 共享同一套 <span class=\"math inline\">\\(K、V\\)</span> ，不同group的 <span class=\"math inline\">\\(Q\\)</span> 所用的 <span class=\"math inline\">\\(K、V\\)</span> 不同。</p>\n<p>MHA可以认为是 <span class=\"math inline\">\\(K、V\\)</span>\n头数最大时的GQA，而MQA可以任务是 <span class=\"math inline\">\\(K、V\\)</span> 头数最少时的GQA。</p>\n<p>看论文里的图就很直观</p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p>效果怎么样呢？</p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>看表中2/3/4行对比，GQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。文中提到，这里的MQA和GQA都是通过average\npooling从MHA初始化而来，然后进行了少量的训练得到的。如果我们想要把之前用MHA训练的模型改造成GQA，也可以通过这样的方法，增加少量训练来实现。当然如果从一开始就加上，从零开始训练，也是没有问题的。</p>\n<p>Llama2用的就是GQA，在tech\nreport中也做了MHA、MQA、GQA的效果对比，可以看到效果确实很不错。</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"小结\">小结</h1>\n<p>MHA、MQA、GQA的实现其实并不复杂，效果也很好，理解上并没有太多困难。但是想要真正理解它们的出发点，还是需要深入每一个细节，去了解当时要解决的事什么问题。</p>\n<p>目前来看GQA是LLM比较好的方案，但未来肯定还会有针对不同方向的进一步优化方案，计算效率、推理速度、显存消耗这些方向都值得我们继续去探索优化。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n【2】Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n【3】Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n【4】https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n【5】GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n【6】How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n【7】A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n【8】https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n【9】浅谈Transformer的初始化、参数化与标准化\nhttps://spaces.ac.cn/archives/8620<br>\n【10】https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n【11】https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n【12】Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n【13】This post is all you need（上卷）——层层剥开Transformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n【14】The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n【15】Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n"}],"PostAsset":[{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/complex_number.png","post":"clxvu4k9q0001314k8wto8dcn","slug":"complex_number.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/remote_attenuation.png","post":"clxvu4k9q0001314k8wto8dcn","slug":"remote_attenuation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/rope.png","post":"clxvu4k9q0001314k8wto8dcn","slug":"rope.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/大模型算法题-4/transformer.png","post":"clxvu4k9t0009314k1qrlaw74","slug":"transformer.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/128k_result.png","post":"clxvu4k9v000c314k2b930rha","slug":"128k_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/2_stage.png","post":"clxvu4k9v000c314k2b930rha","slug":"2_stage.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/batch_size.png","post":"clxvu4k9v000c314k2b930rha","slug":"batch_size.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/batch_size_2.png","post":"clxvu4k9v000c314k2b930rha","slug":"batch_size_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/cos_loss.png","post":"clxvu4k9v000c314k2b930rha","slug":"cos_loss.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/cos_lr.png","post":"clxvu4k9v000c314k2b930rha","slug":"cos_lr.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/data.png","post":"clxvu4k9v000c314k2b930rha","slug":"data.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/eval.png","post":"clxvu4k9v000c314k2b930rha","slug":"eval.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/exp_model.png","post":"clxvu4k9v000c314k2b930rha","slug":"exp_model.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/layers.png","post":"clxvu4k9v000c314k2b930rha","slug":"layers.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/learning_rate.png","post":"clxvu4k9v000c314k2b930rha","slug":"learning_rate.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/moe_result.png","post":"clxvu4k9v000c314k2b930rha","slug":"moe_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/param_search.png","post":"clxvu4k9v000c314k2b930rha","slug":"param_search.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/param_search_2.png","post":"clxvu4k9v000c314k2b930rha","slug":"param_search_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/scaling_law.png","post":"clxvu4k9v000c314k2b930rha","slug":"scaling_law.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/tokenizer.png","post":"clxvu4k9v000c314k2b930rha","slug":"tokenizer.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/train_loss.png","post":"clxvu4k9v000c314k2b930rha","slug":"train_loss.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_exp1.png","post":"clxvu4k9v000c314k2b930rha","slug":"wsd_exp1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_exp2.png","post":"clxvu4k9v000c314k2b930rha","slug":"wsd_exp2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_update.png","post":"clxvu4k9v000c314k2b930rha","slug":"wsd_update.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/从实现看normalization-到底干了什么/bn_and_ln.png","post":"clxvu4k9t0007314k6r0fefgg","slug":"bn_and_ln.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/从实现看normalization-到底干了什么/cv_batchnorm.png","post":"clxvu4k9t0007314k6r0fefgg","slug":"cv_batchnorm.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/从实现看normalization-到底干了什么/cv_layernorm.jpeg","post":"clxvu4k9t0007314k6r0fefgg","slug":"cv_layernorm.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/从实现看normalization-到底干了什么/norm_in_nlp.png","post":"clxvu4k9t0007314k6r0fefgg","slug":"norm_in_nlp.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/RoPE的远距离衰减/1.png","post":"clxvu4k9v000d314kfap0aerf","slug":"1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/RoPE的远距离衰减/2.png","post":"clxvu4k9v000d314kfap0aerf","slug":"2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/RoPE的远距离衰减/3.png","post":"clxvu4k9v000d314kfap0aerf","slug":"3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/RoPE的远距离衰减/4.png","post":"clxvu4k9v000d314kfap0aerf","slug":"4.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/RoPE的远距离衰减/5.png","post":"clxvu4k9v000d314kfap0aerf","slug":"5.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi.png","post":"clxvu4k9r0003314kgrlbb9js","slug":"meta_pi.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_explanation.png","post":"clxvu4k9r0003314kgrlbb9js","slug":"meta_pi_explanation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_nosft.png","post":"clxvu4k9r0003314kgrlbb9js","slug":"meta_pi_nosft.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_rope_ext.png","post":"clxvu4k9r0003314kgrlbb9js","slug":"meta_rope_ext.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/mix_precision_fp16.png","post":"clxvu4k9r0003314kgrlbb9js","slug":"mix_precision_fp16.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/rope_matrix.png","post":"clxvu4k9r0003314kgrlbb9js","slug":"rope_matrix.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/大模型偏好对齐-IPO/curve.png","post":"clxvu4k9w000k314kedlv11uf","slug":"curve.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/大模型算法题-7/lora.png","post":"clxvu4k9w000l314kboo50hi0","slug":"lora.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/3.png","post":"clxvu4k9v000g314kg07s11is","slug":"3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/4.png","post":"clxvu4k9v000g314kg07s11is","slug":"4.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/5.png","post":"clxvu4k9v000g314kg07s11is","slug":"5.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/6.png","post":"clxvu4k9v000g314kg07s11is","slug":"6.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/ditto_1.png","post":"clxvu4k9v000g314kg07s11is","slug":"ditto_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/关于LLM的重复生成现象/ditto_2.png","post":"clxvu4k9v000g314kg07s11is","slug":"ditto_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/data1.png","post":"clxvu4k9w000o314k8lspc9l4","slug":"data1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/data2.png","post":"clxvu4k9w000o314k8lspc9l4","slug":"data2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/evaluation.png","post":"clxvu4k9w000o314k8lspc9l4","slug":"evaluation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/model_param.png","post":"clxvu4k9w000o314k8lspc9l4","slug":"model_param.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/mtbench.png","post":"clxvu4k9w000o314k8lspc9l4","slug":"mtbench.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/成本10w刀的JetMoE/structure.png","post":"clxvu4k9w000o314k8lspc9l4","slug":"structure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/downstream_dataset.png","post":"clxvu4k9w000h314k4r62a89s","slug":"downstream_dataset.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/downstream_dataset_num.png","post":"clxvu4k9w000h314k4r62a89s","slug":"downstream_dataset_num.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/eng_data.png","post":"clxvu4k9w000h314k4r62a89s","slug":"eng_data.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp1_compute.png","post":"clxvu4k9w000h314k4r62a89s","slug":"exp1_compute.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp1_param.png","post":"clxvu4k9w000h314k4r62a89s","slug":"exp1_param.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp1_plot.png","post":"clxvu4k9w000h314k4r62a89s","slug":"exp1_plot.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp2_param.png","post":"clxvu4k9w000h314k4r62a89s","slug":"exp2_param.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp2_plot.png","post":"clxvu4k9w000h314k4r62a89s","slug":"exp2_plot.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/exp3_plot.png","post":"clxvu4k9w000h314k4r62a89s","slug":"exp3_plot.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/从loss视角理解大模型涌现能力/metrics.png","post":"clxvu4k9w000h314k4r62a89s","slug":"metrics.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/100B.png","post":"clxvu4k9x000u314k0a0ca50k","slug":"100B.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/diff_dense.png","post":"clxvu4k9x000u314k0a0ca50k","slug":"diff_dense.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/exp_1.png","post":"clxvu4k9x000u314k0a0ca50k","slug":"exp_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/gate_dist.png","post":"clxvu4k9x000u314k0a0ca50k","slug":"gate_dist.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/lr_exp.png","post":"clxvu4k9x000u314k0a0ca50k","slug":"lr_exp.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/lr_result.png","post":"clxvu4k9x000u314k0a0ca50k","slug":"lr_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/normaization.png","post":"clxvu4k9x000u314k0a0ca50k","slug":"normaization.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/perf.png","post":"clxvu4k9x000u314k0a0ca50k","slug":"perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/昆仑万维开源SkyworkMoE模型/structure.png","post":"clxvu4k9x000u314k0a0ca50k","slug":"structure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/construct_tree.png","post":"clxvu4k9x000p314k00fi58q0","slug":"construct_tree.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/exp1.png","post":"clxvu4k9x000p314k00fi58q0","slug":"exp1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/intro.png","post":"clxvu4k9x000p314k00fi58q0","slug":"intro.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/speed.png","post":"clxvu4k9x000p314k00fi58q0","slug":"speed.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/threshold.png","post":"clxvu4k9x000p314k00fi58q0","slug":"threshold.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/tree_attention.png","post":"clxvu4k9x000p314k00fi58q0","slug":"tree_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/大模型推理加速-MEDUSA/tree_attention_exp.png","post":"clxvu4k9x000p314k00fi58q0","slug":"tree_attention_exp.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/dpo_loss_code.png","post":"clxvu4k9x000r314k99vv31xz","slug":"dpo_loss_code.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/gradient.png","post":"clxvu4k9x000r314k99vv31xz","slug":"gradient.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/intro.png","post":"clxvu4k9x000r314k99vv31xz","slug":"intro.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/result_1.png","post":"clxvu4k9x000r314k99vv31xz","slug":"result_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/result_2.png","post":"clxvu4k9x000r314k99vv31xz","slug":"result_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/result_3.png","post":"clxvu4k9x000r314k99vv31xz","slug":"result_3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-DPO/result_4.png","post":"clxvu4k9x000r314k99vv31xz","slug":"result_4.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/alpha.png","post":"clxvu4k9x000x314kbqercw0q","slug":"alpha.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/odpo_intro.png","post":"clxvu4k9x000x314kbqercw0q","slug":"odpo_intro.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/scaling_function.png","post":"clxvu4k9x000x314kbqercw0q","slug":"scaling_function.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/sentiment_control.png","post":"clxvu4k9x000x314kbqercw0q","slug":"sentiment_control.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/summarization.png","post":"clxvu4k9x000x314kbqercw0q","slug":"summarization.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-ODPO/toxicity_control.png","post":"clxvu4k9x000x314kbqercw0q","slug":"toxicity_control.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/ablation.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"ablation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/benchmark.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"benchmark.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/contingency_table.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"contingency_table.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/dpo_correlation.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"dpo_correlation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/gradient.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"gradient.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/hyperparameters.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"hyperparameters.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/intro.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"intro.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/ln.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"ln.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/ln_effect.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"ln_effect.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/main_results.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"main_results.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/margin_dist.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"margin_dist.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/reward_accuracy.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"reward_accuracy.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/reward_accuracy_compare.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"reward_accuracy_compare.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/simpo_contingency.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"simpo_contingency.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型偏好对齐-simPO/simpo_hyperparameters.png","post":"clxvu4k9y0010314kdv0ae4sd","slug":"simpo_hyperparameters.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/acce_alog.png","post":"clxvu4k9y0013314k8borgtqh","slug":"acce_alog.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/acce_draft_model_param.png","post":"clxvu4k9y0013314k8borgtqh","slug":"acce_draft_model_param.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/acce_k.png","post":"clxvu4k9y0013314k8borgtqh","slug":"acce_k.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_alpha.png","post":"clxvu4k9y0013314k8borgtqh","slug":"fi_alpha.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_choose_gamma.png","post":"clxvu4k9y0013314k8borgtqh","slug":"fi_choose_gamma.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_example.png","post":"clxvu4k9y0013314k8borgtqh","slug":"fi_example.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_expected_token_num.png","post":"clxvu4k9y0013314k8borgtqh","slug":"fi_expected_token_num.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_sd_algo.png","post":"clxvu4k9y0013314k8borgtqh","slug":"fi_sd_algo.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_speed_and_op.png","post":"clxvu4k9y0013314k8borgtqh","slug":"fi_speed_and_op.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_speed_and_op_table.png","post":"clxvu4k9y0013314k8borgtqh","slug":"fi_speed_and_op_table.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_t5_result.png","post":"clxvu4k9y0013314k8borgtqh","slug":"fi_t5_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/fi_walltime.png","post":"clxvu4k9y0013314k8borgtqh","slug":"fi_walltime.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/formula.png","post":"clxvu4k9y0013314k8borgtqh","slug":"formula.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理加速-投机解码基础/speculative_decoding.png","post":"clxvu4k9y0013314k8borgtqh","slug":"speculative_decoding.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/add_money.jpg","post":"clxvu4k9y0018314k0v8mbilw","slug":"add_money.jpg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_config.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"eng_config.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_data.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"eng_data.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_data_dist.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"eng_data_dist.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_needle_comp.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"eng_needle_comp.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_ppl.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"eng_ppl.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_sample.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"eng_sample.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/eng_tokens.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"eng_tokens.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_dataset.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"paraphrasing_dataset.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_dataset_dist.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"paraphrasing_dataset_dist.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_example.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"paraphrasing_example.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_intro.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"paraphrasing_intro.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_lost.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"paraphrasing_lost.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_perf.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"paraphrasing_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/paraphrasing_quality.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"paraphrasing_quality.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/pose_method.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"pose_method.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/pose_passkey.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"pose_passkey.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/解锁大模型长上下文能力/pose_ppl.png","post":"clxvu4k9y0018314k0v8mbilw","slug":"pose_ppl.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/big_bird_attention.png","post":"clxvu4k9z001c314kasf352ne","slug":"big_bird_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/dilated_conv.png","post":"clxvu4k9z001c314kasf352ne","slug":"dilated_conv.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/longformer_attention.png","post":"clxvu4k9z001c314kasf352ne","slug":"longformer_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_architechture.png","post":"clxvu4k9z001c314kasf352ne","slug":"mistral_architechture.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_large_performance.jpeg","post":"clxvu4k9z001c314kasf352ne","slug":"mistral_large_performance.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_perf.png","post":"clxvu4k9z001c314kasf352ne","slug":"mistral_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_swa.png","post":"clxvu4k9z001c314kasf352ne","slug":"mistral_swa.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/ms_invest_mistral.png","post":"clxvu4k9z001c314kasf352ne","slug":"ms_invest_mistral.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/prefill_and_chunking.png","post":"clxvu4k9z001c314kasf352ne","slug":"prefill_and_chunking.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/receptive_field_cnn.png","post":"clxvu4k9z001c314kasf352ne","slug":"receptive_field_cnn.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/rolling_buffer.png","post":"clxvu4k9z001c314kasf352ne","slug":"rolling_buffer.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/digimon.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"digimon.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_booksum.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"infini_attention_booksum.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_compare.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"infini_attention_compare.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_gating.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"infini_attention_gating.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_language_modeling.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"infini_attention_language_modeling.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_passkey.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"infini_attention_passkey.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_process.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"infini_attention_process.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/infini_attention_structure.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"infini_attention_structure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_ablation.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"lm_infinite_ablation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_attention_entropy.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"lm_infinite_attention_entropy.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_attention_logits_explode.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"lm_infinite_attention_logits_explode.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_design.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"lm_infinite_design.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_downstream.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"lm_infinite_downstream.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_middle_k.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"lm_infinite_middle_k.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_ppl_200m.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"lm_infinite_ppl_200m.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_ppl_figure.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"lm_infinite_ppl_figure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_starting_tokens.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"lm_infinite_starting_tokens.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/lm_infinite_starting_tokens_num.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"lm_infinite_starting_tokens_num.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/streamingllm_compare.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"streamingllm_compare.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/streamingllm_model_ppl.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"streamingllm_model_ppl.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/stremingllm_attention_sink.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"stremingllm_attention_sink.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/stremingllm_exp.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"stremingllm_exp.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/stremingllm_init_token_num.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"stremingllm_init_token_num.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/stremingllm_kv_cache.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"stremingllm_kv_cache.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/stremingllm_perf_4m.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"stremingllm_perf_4m.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/xl_attention.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"xl_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型推理窗口-从有限到无限大/xl_vanilla_sw.png","post":"clxvu4k9y0016314kg0dp28gv","slug":"xl_vanilla_sw.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/bn_algo.png","post":"clxvu4k9z001e314khbudet36","slug":"bn_algo.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/bn_and_ln.png","post":"clxvu4k9z001e314khbudet36","slug":"bn_and_ln.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/bn_ics.png","post":"clxvu4k9z001e314khbudet36","slug":"bn_ics.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/bn_ln_gn_in.png","post":"clxvu4k9z001e314khbudet36","slug":"bn_ln_gn_in.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/bs_bn.png","post":"clxvu4k9z001e314khbudet36","slug":"bs_bn.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/deepnorm.png","post":"clxvu4k9z001e314khbudet36","slug":"deepnorm.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/deepnorm_result.png","post":"clxvu4k9z001e314khbudet36","slug":"deepnorm_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/ellipse_1.png","post":"clxvu4k9z001e314khbudet36","slug":"ellipse_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/ellipse_2.png","post":"clxvu4k9z001e314khbudet36","slug":"ellipse_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/ics_define.png","post":"clxvu4k9z001e314khbudet36","slug":"ics_define.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/ics_measure.png","post":"clxvu4k9z001e314khbudet36","slug":"ics_measure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/lossfunc_surface.jpeg","post":"clxvu4k9z001e314khbudet36","slug":"lossfunc_surface.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/postnorm_prenorm.png","post":"clxvu4k9z001e314khbudet36","slug":"postnorm_prenorm.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/prmsnorm.png","post":"clxvu4k9z001e314khbudet36","slug":"prmsnorm.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/realformer.png","post":"clxvu4k9z001e314khbudet36","slug":"realformer.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/realformer_attention.png","post":"clxvu4k9z001e314khbudet36","slug":"realformer_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/rmsnorm.png","post":"clxvu4k9z001e314khbudet36","slug":"rmsnorm.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/rmsnorm_eff.png","post":"clxvu4k9z001e314khbudet36","slug":"rmsnorm_eff.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/sigmoid.png","post":"clxvu4k9z001e314khbudet36","slug":"sigmoid.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/关于Transformer的normalization/warmup_effect.png","post":"clxvu4k9z001e314khbudet36","slug":"warmup_effect.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/9B.png","post":"clxvu4k9z001g314k1is3ehig","slug":"9B.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/base_model_eval.png","post":"clxvu4k9z001g314k1is3ehig","slug":"base_model_eval.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/cover.png","post":"clxvu4k9z001g314k1is3ehig","slug":"cover.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/eval.png","post":"clxvu4k9z001g314k1is3ehig","slug":"eval.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/ict.png","post":"clxvu4k9z001g314k1is3ehig","slug":"ict.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/long_context_result.png","post":"clxvu4k9z001g314k1is3ehig","slug":"long_context_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/model.png","post":"clxvu4k9z001g314k1is3ehig","slug":"model.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/multimodal.png","post":"clxvu4k9z001g314k1is3ehig","slug":"multimodal.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/perf.png","post":"clxvu4k9z001g314k1is3ehig","slug":"perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/pretrain_data_dist.png","post":"clxvu4k9z001g314k1is3ehig","slug":"pretrain_data_dist.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/pretrain_data_pipeline.png","post":"clxvu4k9z001g314k1is3ehig","slug":"pretrain_data_pipeline.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/sft.png","post":"clxvu4k9z001g314k1is3ehig","slug":"sft.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi技术报告-划重点看细节/third_party.png","post":"clxvu4k9z001g314k1is3ehig","slug":"third_party.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型算法题-5/bfloat16.jpeg","post":"clxvu4ka9009r314k85ui4tt8","slug":"bfloat16.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型算法题-5/ntk_by_parts.png","post":"clxvu4ka9009r314k85ui4tt8","slug":"ntk_by_parts.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/大模型算法题-5/yarn.png","post":"clxvu4ka9009r314k85ui4tt8","slug":"yarn.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/GQA.png","post":"clxvu4ka9009w314k19n78cog","slug":"GQA.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/GQA_result_1.png","post":"clxvu4ka9009w314k19n78cog","slug":"GQA_result_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/MQA.png","post":"clxvu4ka9009w314k19n78cog","slug":"MQA.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/MQA.webp","post":"clxvu4ka9009w314k19n78cog","slug":"MQA.webp","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Markdown _ 让排版变 Nice.html","post":"clxvu4ka9009w314k19n78cog","slug":"Markdown _ 让排版变 Nice.html","modified":1,"renderable":1},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Scaled-dot-product-self-attention.pbm","post":"clxvu4ka9009w314k19n78cog","slug":"Scaled-dot-product-self-attention.pbm","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Scaled-dot-product-self-attention.png","post":"clxvu4ka9009w314k19n78cog","slug":"Scaled-dot-product-self-attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/attention_calculation.png","post":"clxvu4ka9009w314k19n78cog","slug":"attention_calculation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/cnn_heatmap.png","post":"clxvu4ka9009w314k19n78cog","slug":"cnn_heatmap.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/decoder.png","post":"clxvu4ka9009w314k19n78cog","slug":"decoder.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/encoder.png","post":"clxvu4ka9009w314k19n78cog","slug":"encoder.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/gpu_cache.png","post":"clxvu4ka9009w314k19n78cog","slug":"gpu_cache.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/lihongyi_self_attention.png","post":"clxvu4ka9009w314k19n78cog","slug":"lihongyi_self_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/llama2_qga.png","post":"clxvu4ka9009w314k19n78cog","slug":"llama2_qga.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/mqa_result_1.png","post":"clxvu4ka9009w314k19n78cog","slug":"mqa_result_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/mqa_result_3.png","post":"clxvu4ka9009w314k19n78cog","slug":"mqa_result_3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/multihead_attention.png","post":"clxvu4ka9009w314k19n78cog","slug":"multihead_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/seq2seq.png","post":"clxvu4ka9009w314k19n78cog","slug":"seq2seq.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/seq2seq_attention.png","post":"clxvu4ka9009w314k19n78cog","slug":"seq2seq_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/softmax.png","post":"clxvu4ka9009w314k19n78cog","slug":"softmax.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/sram_dram.png","post":"clxvu4ka9009w314k19n78cog","slug":"sram_dram.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/transformer_structure.png","post":"clxvu4ka9009w314k19n78cog","slug":"transformer_structure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/cover.jpeg","post":"clxvu4ka9009s314k3xq4e1sm","slug":"cover.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_infer_efficiency.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"dbrx_infer_efficiency.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_long_perf_1.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"dbrx_long_perf_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_long_perf_2.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"dbrx_long_perf_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_perf.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"dbrx_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_train_efficiency.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"dbrx_train_efficiency.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_vs_closed_models.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"dbrx_vs_closed_models.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/dbrx_vs_open_models.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"dbrx_vs_open_models.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_16b_perf_1.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_16b_perf_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_16b_perf_2.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_16b_perf_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_2b_less_expert.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_2b_less_expert.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_model_param.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_model_param.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_145b.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_moe_145b.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_ablation.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_moe_ablation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_comparison.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_moe_comparison.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_expert_specialization.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_moe_expert_specialization.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_less_activated_expert.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_moe_less_activated_expert.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_perf.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_moe_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_sft.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_moe_sft.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_structure.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_moe_structure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_upper_bound_13b.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_moe_upper_bound_13b.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/ds_moe_upper_bound_2b.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"ds_moe_upper_bound_2b.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_compare_gpt3.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"glam_compare_gpt3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_compare_gpt3_2.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"glam_compare_gpt3_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_family.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"glam_family.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_model.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"glam_model.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_perf.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"glam_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/glam_related_model.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"glam_related_model.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/gshard_algo_1.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"gshard_algo_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/gshard_model.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"gshard_model.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/gshard_moe_family.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"gshard_moe_family.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/gshard_perf.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"gshard_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/gshard_result.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"gshard_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/mistral_8_22b_code.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"mistral_8_22b_code.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/mistral_8_22b_multiling.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"mistral_8_22b_multiling.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/mistral_8_22b_reasoning.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"mistral_8_22b_reasoning.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/mistral_8_7b_active_perf.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"mistral_8_7b_active_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/mistral_8_7b_perf.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"mistral_8_7b_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/modular_connectionist.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"modular_connectionist.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/qwen1.5_moe_params.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"qwen1.5_moe_params.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/qwen1.5_moe_perf.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"qwen1.5_moe_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/qwen1.5_moe_tps.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"qwen1.5_moe_tps.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"rnn_moe.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe_137b.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"rnn_moe_137b.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe_hierarchical_gating.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"rnn_moe_hierarchical_gating.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe_load_function.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"rnn_moe_load_function.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe_perf.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"rnn_moe_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/rnn_moe_specilized.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"rnn_moe_specilized.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/softplus.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"softplus.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_capacity_factor.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_capacity_factor.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_capacity_factor_speed.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_capacity_factor_speed.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_encoder_specialization.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_encoder_specialization.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_models.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_models.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_more_add_bias.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_more_add_bias.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_more_add_noise.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_more_add_noise.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_more_dense_layer.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_more_dense_layer.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_multiling_specialization.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_multiling_specialization.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_perf.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_remove_multiplications.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_remove_multiplications.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_round_error.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_round_error.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/st_moe_z_loss_result.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"st_moe_z_loss_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_capacity_effect.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_capacity_effect.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_diff_expert_capacity.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_diff_expert_capacity.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_distill.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_distill.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_distill_diff_model.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_distill_diff_model.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_distill_sft.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_distill_sft.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_dropout.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_dropout.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_init.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_init.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_pretrain_result.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_pretrain_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_scaling_dense.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_scaling_dense.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_scaling_step.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_scaling_step.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_scaling_time.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_scaling_time.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_sft_result.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_sft_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/switch_transformer_structure.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"switch_transformer_structure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/vanilla_moe.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"vanilla_moe.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/vanilla_moe_result.png","post":"clxvu4ka9009s314k3xq4e1sm","slug":"vanilla_moe_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/混合专家MoE-基础篇/xiaomi_moe.jpg","post":"clxvu4ka9009s314k3xq4e1sm","slug":"xiaomi_moe.jpg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/1.png","post":"clxvu4ka9009u314keeth33n8","slug":"1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/10.png","post":"clxvu4ka9009u314keeth33n8","slug":"10.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/11.png","post":"clxvu4ka9009u314keeth33n8","slug":"11.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/12.png","post":"clxvu4ka9009u314keeth33n8","slug":"12.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/13.png","post":"clxvu4ka9009u314keeth33n8","slug":"13.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/14.png","post":"clxvu4ka9009u314keeth33n8","slug":"14.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/15.png","post":"clxvu4ka9009u314keeth33n8","slug":"15.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/16.png","post":"clxvu4ka9009u314keeth33n8","slug":"16.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/17.png","post":"clxvu4ka9009u314keeth33n8","slug":"17.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/18.png","post":"clxvu4ka9009u314keeth33n8","slug":"18.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/19.png","post":"clxvu4ka9009u314keeth33n8","slug":"19.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/2.png","post":"clxvu4ka9009u314keeth33n8","slug":"2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/20.png","post":"clxvu4ka9009u314keeth33n8","slug":"20.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/21.png","post":"clxvu4ka9009u314keeth33n8","slug":"21.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/22.png","post":"clxvu4ka9009u314keeth33n8","slug":"22.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/23.png","post":"clxvu4ka9009u314keeth33n8","slug":"23.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/24.png","post":"clxvu4ka9009u314keeth33n8","slug":"24.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/25.png","post":"clxvu4ka9009u314keeth33n8","slug":"25.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/26.png","post":"clxvu4ka9009u314keeth33n8","slug":"26.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/27.png","post":"clxvu4ka9009u314keeth33n8","slug":"27.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/28.png","post":"clxvu4ka9009u314keeth33n8","slug":"28.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/3.png","post":"clxvu4ka9009u314keeth33n8","slug":"3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/4.png","post":"clxvu4ka9009u314keeth33n8","slug":"4.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/5.png","post":"clxvu4ka9009u314keeth33n8","slug":"5.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/6.png","post":"clxvu4ka9009u314keeth33n8","slug":"6.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/7.png","post":"clxvu4ka9009u314keeth33n8","slug":"7.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/8.png","post":"clxvu4ka9009u314keeth33n8","slug":"8.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/大规模对话模型：ChatGPT下的银牌选手们/9.png","post":"clxvu4ka9009u314keeth33n8","slug":"9.png","modified":1,"renderable":0}],"PostCategory":[{"post_id":"clxvu4k9w000k314kedlv11uf","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka0001w314kfe0l8gki"},{"post_id":"clxvu4k9w000k314kedlv11uf","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka0001x314khdud8zkb"},{"post_id":"clxvu4k9w000k314kedlv11uf","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka00020314k1pijcn4f"},{"post_id":"clxvu4k9t0008314k1t7sfln7","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka00021314k8wf83c0g"},{"post_id":"clxvu4k9t0008314k1t7sfln7","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka10024314k8qv1722y"},{"post_id":"clxvu4k9t0008314k1t7sfln7","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka10025314kgi136vzk"},{"post_id":"clxvu4k9w000l314kboo50hi0","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka10028314k4xq57vxr"},{"post_id":"clxvu4k9w000l314kboo50hi0","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka1002a314kcs8m57o3"},{"post_id":"clxvu4k9w000l314kboo50hi0","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka1002e314kbu5b5dbn"},{"post_id":"clxvu4k9w000o314k8lspc9l4","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka1002g314kf2vmgs6b"},{"post_id":"clxvu4k9w000o314k8lspc9l4","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka1002j314k28nz87co"},{"post_id":"clxvu4k9w000o314k8lspc9l4","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka1002m314kbww5h7o7"},{"post_id":"clxvu4k9q0001314k8wto8dcn","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka1002p314k1tgjfbvj"},{"post_id":"clxvu4k9q0001314k8wto8dcn","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka1002s314kc48dbcyz"},{"post_id":"clxvu4k9q0001314k8wto8dcn","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka1002v314k2qew33j6"},{"post_id":"clxvu4k9x000p314k00fi58q0","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka1002y314k9k0r876l"},{"post_id":"clxvu4k9x000p314k00fi58q0","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka10031314k98jo95g3"},{"post_id":"clxvu4k9x000p314k00fi58q0","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka10034314kcs3h800i"},{"post_id":"clxvu4k9x000r314k99vv31xz","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka20037314keckcb82n"},{"post_id":"clxvu4k9x000r314k99vv31xz","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka20039314kg7hoa403"},{"post_id":"clxvu4k9x000r314k99vv31xz","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka2003b314k0t2k8iro"},{"post_id":"clxvu4k9t0009314k1qrlaw74","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka2003d314kcjlgg4ak"},{"post_id":"clxvu4k9t0009314k1qrlaw74","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka2003f314kc7am59xt"},{"post_id":"clxvu4k9t0009314k1qrlaw74","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka2003h314k7qlebrx4"},{"post_id":"clxvu4k9x000u314k0a0ca50k","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka2003k314kemtq4quj"},{"post_id":"clxvu4k9x000u314k0a0ca50k","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka2003n314kbmcy4bsj"},{"post_id":"clxvu4k9x000u314k0a0ca50k","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka2003q314k5nk49svd"},{"post_id":"clxvu4k9x000x314kbqercw0q","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka3003t314k00mz6cay"},{"post_id":"clxvu4k9x000x314kbqercw0q","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka3003w314k0d8153wh"},{"post_id":"clxvu4k9x000x314kbqercw0q","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka3003z314k89fx0pr2"},{"post_id":"clxvu4k9v000c314k2b930rha","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka30042314kgskh4afv"},{"post_id":"clxvu4k9v000c314k2b930rha","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka30044314kfupk4259"},{"post_id":"clxvu4k9v000c314k2b930rha","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka30048314ke2940idp"},{"post_id":"clxvu4k9y0010314kdv0ae4sd","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka3004a314k979aeaw9"},{"post_id":"clxvu4k9y0010314kdv0ae4sd","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka3004e314kgdy48t28"},{"post_id":"clxvu4k9y0010314kdv0ae4sd","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka3004g314k50ec6v5t"},{"post_id":"clxvu4k9y0013314k8borgtqh","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka3004k314k1d0gbn3m"},{"post_id":"clxvu4k9y0013314k8borgtqh","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka3004m314kgapf096a"},{"post_id":"clxvu4k9y0013314k8borgtqh","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka4004q314kby3a38kh"},{"post_id":"clxvu4k9r0003314kgrlbb9js","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka4004s314k1xs2ef4w"},{"post_id":"clxvu4k9r0003314kgrlbb9js","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka4004v314k4hnc4qta"},{"post_id":"clxvu4k9r0003314kgrlbb9js","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka4004y314ka6bda2z2"},{"post_id":"clxvu4k9y0016314kg0dp28gv","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka40051314k0rmk1h5j"},{"post_id":"clxvu4k9y0016314kg0dp28gv","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka40054314kacf045y1"},{"post_id":"clxvu4k9y0016314kg0dp28gv","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka40056314kamnmb0zy"},{"post_id":"clxvu4k9y0018314k0v8mbilw","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka40058314k8sz741p9"},{"post_id":"clxvu4k9y0018314k0v8mbilw","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka4005b314k7wqegd96"},{"post_id":"clxvu4k9y0018314k0v8mbilw","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka4005e314kf7rvc65g"},{"post_id":"clxvu4k9v000d314kfap0aerf","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka4005g314k9wp8bn5n"},{"post_id":"clxvu4k9v000d314kfap0aerf","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka5005j314k8vt404ie"},{"post_id":"clxvu4k9v000d314kfap0aerf","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka5005l314k37uhcz06"},{"post_id":"clxvu4k9z001a314k66ligkpn","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka5005n314k78dh2anh"},{"post_id":"clxvu4k9z001a314k66ligkpn","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka5005q314k1ztsgwzt"},{"post_id":"clxvu4k9z001a314k66ligkpn","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka5005s314k8j1ygn8u"},{"post_id":"clxvu4k9z001c314kasf352ne","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka5005v314kg1w95236"},{"post_id":"clxvu4k9z001c314kasf352ne","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka5005x314kei9z1osl"},{"post_id":"clxvu4k9z001c314kasf352ne","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka50060314k253z3egz"},{"post_id":"clxvu4k9v000g314kg07s11is","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka50062314k78lr9sp5"},{"post_id":"clxvu4k9v000g314kg07s11is","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka50065314k9ewp8mbg"},{"post_id":"clxvu4k9v000g314kg07s11is","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka50067314kch3qdi6l"},{"post_id":"clxvu4k9z001e314khbudet36","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka5006a314kdcl74s17"},{"post_id":"clxvu4k9z001e314khbudet36","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka5006c314kd9vx1eyj"},{"post_id":"clxvu4k9z001e314khbudet36","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka5006f314k17fz3e6q"},{"post_id":"clxvu4k9z001g314k1is3ehig","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka5006h314kcwpng6jz"},{"post_id":"clxvu4k9z001g314k1is3ehig","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka5006k314k1msl2jx0"},{"post_id":"clxvu4k9z001g314k1is3ehig","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka5006m314kasj47xd3"},{"post_id":"clxvu4k9t0007314k6r0fefgg","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka5006p314ke3p60itj"},{"post_id":"clxvu4k9t0007314k6r0fefgg","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka5006r314k6vclhwvs"},{"post_id":"clxvu4k9t0007314k6r0fefgg","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka5006u314k5a5lhxxg"},{"post_id":"clxvu4ka0001i314k07psb31l","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka5006w314k53hjaeqd"},{"post_id":"clxvu4ka0001i314k07psb31l","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka5006z314kht099mys"},{"post_id":"clxvu4ka0001i314k07psb31l","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka50071314k6q8n0b6a"},{"post_id":"clxvu4ka0001l314k8x1o3924","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka60074314k33096mx7"},{"post_id":"clxvu4ka0001l314k8x1o3924","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka60076314ke8ht8zj2"},{"post_id":"clxvu4ka0001l314k8x1o3924","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka60079314kc3lpb68l"},{"post_id":"clxvu4k9w000h314k4r62a89s","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4ka6007b314k9vwx1eb8"},{"post_id":"clxvu4k9w000h314k4r62a89s","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4ka6007e314k87dn3ihi"},{"post_id":"clxvu4k9w000h314k4r62a89s","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4ka6007g314k2ps77xj5"},{"post_id":"clxvu4ka9009r314k85ui4tt8","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4kaa009x314k8bk99xj6"},{"post_id":"clxvu4ka9009r314k85ui4tt8","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4kaa00a0314kfuv27ot8"},{"post_id":"clxvu4ka9009r314k85ui4tt8","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4kaa00a2314k23aucx67"},{"post_id":"clxvu4ka9009s314k3xq4e1sm","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4kaa00a5314k9hjx5oad"},{"post_id":"clxvu4ka9009s314k3xq4e1sm","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4kaa00a7314kfsrx1taq"},{"post_id":"clxvu4ka9009s314k3xq4e1sm","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4kaa00aa314kaufwfnnq"},{"post_id":"clxvu4ka9009u314keeth33n8","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4kaa00ab314kghrt7015"},{"post_id":"clxvu4ka9009u314keeth33n8","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4kaa00ad314khwcr16uz"},{"post_id":"clxvu4ka9009u314keeth33n8","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4kaa00ae314k97so17n6"},{"post_id":"clxvu4ka9009w314k19n78cog","category_id":"clxvu4k9s0004314k9z5n2qt7","_id":"clxvu4kaa00af314kel6j5oto"},{"post_id":"clxvu4ka9009w314k19n78cog","category_id":"clxvu4k9w000i314k1ig5fcwv","_id":"clxvu4kaa00ah314k0kix4uzx"},{"post_id":"clxvu4ka9009w314k19n78cog","category_id":"clxvu4ka0001o314k17huehv3","_id":"clxvu4kaa00ai314k1nvnd8ji"}],"PostTag":[{"post_id":"clxvu4k9q0001314k8wto8dcn","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4k9x000t314k4t98bm7f"},{"post_id":"clxvu4k9q0001314k8wto8dcn","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4k9x000w314kdl3u35id"},{"post_id":"clxvu4k9q0001314k8wto8dcn","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4k9y000z314kb47sei65"},{"post_id":"clxvu4k9q0001314k8wto8dcn","tag_id":"clxvu4k9w000j314k4o8g4qk1","_id":"clxvu4k9y0012314kgtob9tqe"},{"post_id":"clxvu4k9q0001314k8wto8dcn","tag_id":"clxvu4k9w000n314kcvr95sr5","_id":"clxvu4k9y0015314k6lvwazx8"},{"post_id":"clxvu4k9r0003314kgrlbb9js","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka0001k314k9giogdsl"},{"post_id":"clxvu4k9r0003314kgrlbb9js","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka0001m314k9ihcgcbq"},{"post_id":"clxvu4k9r0003314kgrlbb9js","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka0001p314kf1w89jmf"},{"post_id":"clxvu4k9r0003314kgrlbb9js","tag_id":"clxvu4k9z0019314kcr0f8mxn","_id":"clxvu4ka0001q314kcdo820ns"},{"post_id":"clxvu4k9r0003314kgrlbb9js","tag_id":"clxvu4k9z001d314k3eq31yhw","_id":"clxvu4ka0001s314k6qnc1wws"},{"post_id":"clxvu4k9t0007314k6r0fefgg","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka10029314k88athsm9"},{"post_id":"clxvu4k9t0007314k6r0fefgg","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka1002b314k8vq95qsj"},{"post_id":"clxvu4k9t0007314k6r0fefgg","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka1002f314k1kw360mi"},{"post_id":"clxvu4k9t0007314k6r0fefgg","tag_id":"clxvu4ka0001u314kazyq6igs","_id":"clxvu4ka1002h314ke3yme7ws"},{"post_id":"clxvu4k9t0007314k6r0fefgg","tag_id":"clxvu4ka0001y314kgmbt3buo","_id":"clxvu4ka1002l314k8ho4dzy9"},{"post_id":"clxvu4k9t0007314k6r0fefgg","tag_id":"clxvu4ka00022314kdo0r6qgu","_id":"clxvu4ka1002n314khrg11q0g"},{"post_id":"clxvu4k9t0008314k1t7sfln7","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka1002r314kccf32kq8"},{"post_id":"clxvu4k9t0008314k1t7sfln7","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka1002t314kdi3fbscl"},{"post_id":"clxvu4k9t0008314k1t7sfln7","tag_id":"clxvu4ka1002c314kgw6f13wu","_id":"clxvu4ka1002x314k5v7oc8dc"},{"post_id":"clxvu4k9t0009314k1qrlaw74","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka1002z314k61tb218k"},{"post_id":"clxvu4k9t0009314k1qrlaw74","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka10033314kchbq7n4g"},{"post_id":"clxvu4k9t0009314k1qrlaw74","tag_id":"clxvu4ka1002c314kgw6f13wu","_id":"clxvu4ka20035314k0iaa871u"},{"post_id":"clxvu4k9v000c314k2b930rha","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka2003i314kfs9677iu"},{"post_id":"clxvu4k9v000c314k2b930rha","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka2003l314k8joj1gg5"},{"post_id":"clxvu4k9v000c314k2b930rha","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka2003o314k1sx09pmz"},{"post_id":"clxvu4k9v000c314k2b930rha","tag_id":"clxvu4ka10032314k0ujsa22x","_id":"clxvu4ka3003r314k25yh4pve"},{"post_id":"clxvu4k9v000c314k2b930rha","tag_id":"clxvu4ka20038314k1jjehvoh","_id":"clxvu4ka3003u314k3tru5axn"},{"post_id":"clxvu4k9v000c314k2b930rha","tag_id":"clxvu4ka2003c314k2owx3jej","_id":"clxvu4ka3003x314k9rtle9zl"},{"post_id":"clxvu4k9v000d314kfap0aerf","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka30040314k88oidufe"},{"post_id":"clxvu4k9v000d314kfap0aerf","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka30043314k1pdrewzn"},{"post_id":"clxvu4k9v000d314kfap0aerf","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka30046314kdst80hwc"},{"post_id":"clxvu4k9v000d314kfap0aerf","tag_id":"clxvu4k9w000j314k4o8g4qk1","_id":"clxvu4ka30049314kcl9h0uys"},{"post_id":"clxvu4k9v000d314kfap0aerf","tag_id":"clxvu4k9w000n314kcvr95sr5","_id":"clxvu4ka3004c314k44ubcql5"},{"post_id":"clxvu4k9v000g314kg07s11is","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka3004f314k2hrmczg8"},{"post_id":"clxvu4k9v000g314kg07s11is","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka3004i314k55xl33lz"},{"post_id":"clxvu4k9v000g314kg07s11is","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka3004l314k3vbo397v"},{"post_id":"clxvu4k9v000g314kg07s11is","tag_id":"clxvu4ka3003y314k3mfx9alw","_id":"clxvu4ka3004n314kccya3t2b"},{"post_id":"clxvu4k9v000g314kg07s11is","tag_id":"clxvu4ka30045314kf68se9lb","_id":"clxvu4ka4004r314k0hgiep9k"},{"post_id":"clxvu4k9w000h314k4r62a89s","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka4004t314k3pjg7ei6"},{"post_id":"clxvu4k9w000h314k4r62a89s","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka4004x314k05dh0ait"},{"post_id":"clxvu4k9w000h314k4r62a89s","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka4004z314k71oref3m"},{"post_id":"clxvu4k9w000h314k4r62a89s","tag_id":"clxvu4ka3004b314kew62ejik","_id":"clxvu4ka40053314kg6xs7vt0"},{"post_id":"clxvu4k9w000k314kedlv11uf","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka40059314k0ato0ta5"},{"post_id":"clxvu4k9w000k314kedlv11uf","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka4005c314k9sn04awf"},{"post_id":"clxvu4k9w000k314kedlv11uf","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka4005f314k6k8xgox6"},{"post_id":"clxvu4k9w000k314kedlv11uf","tag_id":"clxvu4ka3004h314kgzv3e34i","_id":"clxvu4ka4005h314k0iwqh9sr"},{"post_id":"clxvu4k9w000k314kedlv11uf","tag_id":"clxvu4ka3004o314k5e9733o1","_id":"clxvu4ka5005k314kf9vqepin"},{"post_id":"clxvu4k9w000k314kedlv11uf","tag_id":"clxvu4ka4004w314kcr712lar","_id":"clxvu4ka5005m314kdutpfq96"},{"post_id":"clxvu4k9w000k314kedlv11uf","tag_id":"clxvu4ka40052314k2c3sg1p4","_id":"clxvu4ka5005p314k1jqneiwk"},{"post_id":"clxvu4k9w000l314kboo50hi0","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka5005r314kdxhu1i4y"},{"post_id":"clxvu4k9w000l314kboo50hi0","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka5005u314kc61sdp3i"},{"post_id":"clxvu4k9w000l314kboo50hi0","tag_id":"clxvu4ka1002c314kgw6f13wu","_id":"clxvu4ka5005w314kgatvdznb"},{"post_id":"clxvu4k9w000o314k8lspc9l4","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka5005z314k01vsctmu"},{"post_id":"clxvu4k9w000o314k8lspc9l4","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka50061314kgfnc1tfh"},{"post_id":"clxvu4k9w000o314k8lspc9l4","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka50064314kbjjnckj6"},{"post_id":"clxvu4k9w000o314k8lspc9l4","tag_id":"clxvu4ka4005d314k87v3fc1a","_id":"clxvu4ka50066314kg7nu6doy"},{"post_id":"clxvu4k9x000p314k00fi58q0","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka50069314k84u0a795"},{"post_id":"clxvu4k9x000p314k00fi58q0","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka5006b314kalqr59t5"},{"post_id":"clxvu4k9x000p314k00fi58q0","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka5006e314k35ameskh"},{"post_id":"clxvu4k9x000p314k00fi58q0","tag_id":"clxvu4ka4005i314k70hy6ok5","_id":"clxvu4ka5006g314kfdcohauv"},{"post_id":"clxvu4k9x000r314k99vv31xz","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka5006j314k8192cikf"},{"post_id":"clxvu4k9x000r314k99vv31xz","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka5006l314kbiod0gcm"},{"post_id":"clxvu4k9x000r314k99vv31xz","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka5006o314k9tju6s3w"},{"post_id":"clxvu4k9x000r314k99vv31xz","tag_id":"clxvu4ka3004h314kgzv3e34i","_id":"clxvu4ka5006q314kfojq7i75"},{"post_id":"clxvu4k9x000r314k99vv31xz","tag_id":"clxvu4ka3004o314k5e9733o1","_id":"clxvu4ka5006t314k52nkcmcx"},{"post_id":"clxvu4k9x000r314k99vv31xz","tag_id":"clxvu4ka4004w314kcr712lar","_id":"clxvu4ka5006v314kdw7m0dd9"},{"post_id":"clxvu4k9x000r314k99vv31xz","tag_id":"clxvu4ka40052314k2c3sg1p4","_id":"clxvu4ka5006y314kbd20hg7l"},{"post_id":"clxvu4k9x000u314k0a0ca50k","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka50070314k9ja190ht"},{"post_id":"clxvu4k9x000u314k0a0ca50k","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka50073314k0ol6f7bo"},{"post_id":"clxvu4k9x000u314k0a0ca50k","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka60075314khoe7770i"},{"post_id":"clxvu4k9x000u314k0a0ca50k","tag_id":"clxvu4ka4005d314k87v3fc1a","_id":"clxvu4ka60078314kgq578u8i"},{"post_id":"clxvu4k9x000x314kbqercw0q","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka6007a314kcn9g50cw"},{"post_id":"clxvu4k9x000x314kbqercw0q","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka6007d314k43iebv5v"},{"post_id":"clxvu4k9x000x314kbqercw0q","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka6007f314k2xwpglzf"},{"post_id":"clxvu4k9x000x314kbqercw0q","tag_id":"clxvu4ka3004h314kgzv3e34i","_id":"clxvu4ka6007i314k3r1re1s5"},{"post_id":"clxvu4k9x000x314kbqercw0q","tag_id":"clxvu4ka3004o314k5e9733o1","_id":"clxvu4ka6007j314kaw5qciug"},{"post_id":"clxvu4k9x000x314kbqercw0q","tag_id":"clxvu4ka4004w314kcr712lar","_id":"clxvu4ka6007l314k8skta8kz"},{"post_id":"clxvu4k9x000x314kbqercw0q","tag_id":"clxvu4ka40052314k2c3sg1p4","_id":"clxvu4ka6007m314k1xou701v"},{"post_id":"clxvu4k9y0010314kdv0ae4sd","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka6007o314k768iheil"},{"post_id":"clxvu4k9y0010314kdv0ae4sd","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka6007p314k5kbpaqgi"},{"post_id":"clxvu4k9y0010314kdv0ae4sd","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka6007r314kc18n7i5j"},{"post_id":"clxvu4k9y0010314kdv0ae4sd","tag_id":"clxvu4ka3004h314kgzv3e34i","_id":"clxvu4ka6007s314k7ui8cw6u"},{"post_id":"clxvu4k9y0010314kdv0ae4sd","tag_id":"clxvu4ka3004o314k5e9733o1","_id":"clxvu4ka6007u314kbc8x8jz2"},{"post_id":"clxvu4k9y0010314kdv0ae4sd","tag_id":"clxvu4ka4004w314kcr712lar","_id":"clxvu4ka6007v314k90wtcwsj"},{"post_id":"clxvu4k9y0010314kdv0ae4sd","tag_id":"clxvu4ka40052314k2c3sg1p4","_id":"clxvu4ka6007x314k5m289kmw"},{"post_id":"clxvu4k9y0013314k8borgtqh","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka6007y314kduopai2w"},{"post_id":"clxvu4k9y0013314k8borgtqh","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka6007z314kaw2b57c1"},{"post_id":"clxvu4k9y0013314k8borgtqh","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka60081314k87tr376i"},{"post_id":"clxvu4k9y0013314k8borgtqh","tag_id":"clxvu4ka4005i314k70hy6ok5","_id":"clxvu4ka60082314keum5aboa"},{"post_id":"clxvu4k9y0016314kg0dp28gv","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka60084314kfn081o9n"},{"post_id":"clxvu4k9y0016314kg0dp28gv","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka60085314ke1gef65t"},{"post_id":"clxvu4k9y0016314kg0dp28gv","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka60087314kchfff15n"},{"post_id":"clxvu4k9y0016314kg0dp28gv","tag_id":"clxvu4ka6007k314k81owhi6o","_id":"clxvu4ka60088314kg7zg654h"},{"post_id":"clxvu4k9y0016314kg0dp28gv","tag_id":"clxvu4k9z0019314kcr0f8mxn","_id":"clxvu4ka7008a314kgq6c2ksx"},{"post_id":"clxvu4k9y0016314kg0dp28gv","tag_id":"clxvu4ka2003c314k2owx3jej","_id":"clxvu4ka7008b314k085ngooq"},{"post_id":"clxvu4k9y0016314kg0dp28gv","tag_id":"clxvu4ka3004o314k5e9733o1","_id":"clxvu4ka7008d314keq468hys"},{"post_id":"clxvu4k9y0016314kg0dp28gv","tag_id":"clxvu4ka6007w314kgd11ho1f","_id":"clxvu4ka7008e314k9szecty6"},{"post_id":"clxvu4k9y0018314k0v8mbilw","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka7008g314kf4tydhiv"},{"post_id":"clxvu4k9y0018314k0v8mbilw","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka7008h314k3r6vgzv3"},{"post_id":"clxvu4k9y0018314k0v8mbilw","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka7008j314k8qj0dj70"},{"post_id":"clxvu4k9y0018314k0v8mbilw","tag_id":"clxvu4k9z0019314kcr0f8mxn","_id":"clxvu4ka7008k314k8fgb94q7"},{"post_id":"clxvu4k9y0018314k0v8mbilw","tag_id":"clxvu4ka2003c314k2owx3jej","_id":"clxvu4ka7008m314kaech8zrb"},{"post_id":"clxvu4k9y0018314k0v8mbilw","tag_id":"clxvu4ka3004o314k5e9733o1","_id":"clxvu4ka7008n314k5b586qyk"},{"post_id":"clxvu4k9y0018314k0v8mbilw","tag_id":"clxvu4ka6007w314kgd11ho1f","_id":"clxvu4ka7008p314k5jwd13c3"},{"post_id":"clxvu4k9z001a314k66ligkpn","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka7008q314ke0146jqj"},{"post_id":"clxvu4k9z001a314k66ligkpn","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka7008s314kbpcu4ucd"},{"post_id":"clxvu4k9z001a314k66ligkpn","tag_id":"clxvu4ka1002c314kgw6f13wu","_id":"clxvu4ka7008t314kg0l49uo4"},{"post_id":"clxvu4k9z001c314kasf352ne","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka7008v314k3scv1ckc"},{"post_id":"clxvu4k9z001c314kasf352ne","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka7008w314k62d6a466"},{"post_id":"clxvu4k9z001c314kasf352ne","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka7008x314k15o33pna"},{"post_id":"clxvu4k9z001c314kasf352ne","tag_id":"clxvu4ka6007w314kgd11ho1f","_id":"clxvu4ka7008z314kccy24kbq"},{"post_id":"clxvu4k9z001c314kasf352ne","tag_id":"clxvu4ka7008i314kd2u0bqac","_id":"clxvu4ka70090314kbshd0r1l"},{"post_id":"clxvu4k9z001c314kasf352ne","tag_id":"clxvu4ka7008l314k78gc7hnf","_id":"clxvu4ka70092314k5840hmor"},{"post_id":"clxvu4k9z001e314khbudet36","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka70094314k5w0sed03"},{"post_id":"clxvu4k9z001e314khbudet36","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka70095314keofj8c4q"},{"post_id":"clxvu4k9z001e314khbudet36","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka70097314kd1no1ako"},{"post_id":"clxvu4k9z001e314khbudet36","tag_id":"clxvu4ka0001u314kazyq6igs","_id":"clxvu4ka80098314kd9p5dnqd"},{"post_id":"clxvu4k9z001e314khbudet36","tag_id":"clxvu4ka7008r314kbfxd36ox","_id":"clxvu4ka8009a314kf8aeaps1"},{"post_id":"clxvu4k9z001e314khbudet36","tag_id":"clxvu4ka7008u314k2kmg50vi","_id":"clxvu4ka8009b314kaxmffhwc"},{"post_id":"clxvu4k9z001e314khbudet36","tag_id":"clxvu4ka0001y314kgmbt3buo","_id":"clxvu4ka8009d314k38x6b555"},{"post_id":"clxvu4k9z001e314khbudet36","tag_id":"clxvu4ka00022314kdo0r6qgu","_id":"clxvu4ka8009e314k8cr1hhgm"},{"post_id":"clxvu4k9z001g314k1is3ehig","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka8009f314k3rmm54fx"},{"post_id":"clxvu4k9z001g314k1is3ehig","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka8009g314kd8lhcfyq"},{"post_id":"clxvu4k9z001g314k1is3ehig","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4ka8009h314k8td86qcz"},{"post_id":"clxvu4k9z001g314k1is3ehig","tag_id":"clxvu4ka10032314k0ujsa22x","_id":"clxvu4ka8009i314k8hyq6k9g"},{"post_id":"clxvu4k9z001g314k1is3ehig","tag_id":"clxvu4ka70096314kfz2w6gve","_id":"clxvu4ka8009j314k3k26b8pq"},{"post_id":"clxvu4k9z001g314k1is3ehig","tag_id":"clxvu4k9z0019314kcr0f8mxn","_id":"clxvu4ka8009k314kat62fza0"},{"post_id":"clxvu4ka0001i314k07psb31l","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka8009l314k5w0e6999"},{"post_id":"clxvu4ka0001i314k07psb31l","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka8009m314k4gqegaik"},{"post_id":"clxvu4ka0001i314k07psb31l","tag_id":"clxvu4ka1002c314kgw6f13wu","_id":"clxvu4ka8009n314keph4baf9"},{"post_id":"clxvu4ka0001l314k8x1o3924","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka8009o314kd4102bdk"},{"post_id":"clxvu4ka0001l314k8x1o3924","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka8009p314k5wizdg59"},{"post_id":"clxvu4ka0001l314k8x1o3924","tag_id":"clxvu4ka1002c314kgw6f13wu","_id":"clxvu4ka8009q314khmz2h04l"},{"post_id":"clxvu4ka9009r314k85ui4tt8","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4ka9009t314kbsyhebiv"},{"post_id":"clxvu4ka9009r314k85ui4tt8","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4ka9009v314kgkuw8tai"},{"post_id":"clxvu4ka9009r314k85ui4tt8","tag_id":"clxvu4ka1002c314kgw6f13wu","_id":"clxvu4kaa009z314k0lai3jo4"},{"post_id":"clxvu4ka9009s314k3xq4e1sm","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4kaa00a1314k5doo51sh"},{"post_id":"clxvu4ka9009s314k3xq4e1sm","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4kaa00a3314k953jgxmz"},{"post_id":"clxvu4ka9009s314k3xq4e1sm","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4kaa00a6314kae7l63mw"},{"post_id":"clxvu4ka9009s314k3xq4e1sm","tag_id":"clxvu4ka4005d314k87v3fc1a","_id":"clxvu4kaa00a8314k5bbq1hjh"},{"post_id":"clxvu4ka9009u314keeth33n8","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4kaa00al314khraj3wmp"},{"post_id":"clxvu4ka9009u314keeth33n8","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4kaa00am314k03a8g8mx"},{"post_id":"clxvu4ka9009u314keeth33n8","tag_id":"clxvu4kaa009y314k12c9eeng","_id":"clxvu4kaa00an314k6p8w6sep"},{"post_id":"clxvu4ka9009u314keeth33n8","tag_id":"clxvu4kaa00a4314k0rokdvo5","_id":"clxvu4kaa00ao314kf8ef1wr9"},{"post_id":"clxvu4ka9009u314keeth33n8","tag_id":"clxvu4kaa00a9314kaczieqth","_id":"clxvu4kaa00ap314kh5j13roo"},{"post_id":"clxvu4ka9009u314keeth33n8","tag_id":"clxvu4kaa00ac314kdzue9y7d","_id":"clxvu4kab00aq314kd2se4i9q"},{"post_id":"clxvu4ka9009u314keeth33n8","tag_id":"clxvu4kaa00ag314kb817ac6u","_id":"clxvu4kab00ar314k7o0m6666"},{"post_id":"clxvu4ka9009u314keeth33n8","tag_id":"clxvu4kaa00aj314k1gdx5ncp","_id":"clxvu4kab00as314k9x79d9d0"},{"post_id":"clxvu4ka9009w314k19n78cog","tag_id":"clxvu4k9s0005314kdyie7hq8","_id":"clxvu4kab00at314ked877knt"},{"post_id":"clxvu4ka9009w314k19n78cog","tag_id":"clxvu4k9t000b314kanlf2hre","_id":"clxvu4kab00au314k9d5c711d"},{"post_id":"clxvu4ka9009w314k19n78cog","tag_id":"clxvu4k9v000f314kha9aa0tz","_id":"clxvu4kab00av314k5b6777e6"},{"post_id":"clxvu4ka9009w314k19n78cog","tag_id":"clxvu4ka6007w314kgd11ho1f","_id":"clxvu4kab00aw314kgn7yc83h"},{"post_id":"clxvu4ka9009w314k19n78cog","tag_id":"clxvu4kaa00ak314k8b9q7s4t","_id":"clxvu4kab00ax314k42du4m4e"}],"Tag":[{"name":"NLP","_id":"clxvu4k9s0005314kdyie7hq8"},{"name":"LLM","_id":"clxvu4k9t000b314kanlf2hre"},{"name":"transformer","_id":"clxvu4k9v000f314kha9aa0tz"},{"name":"positional encoding","_id":"clxvu4k9w000j314k4o8g4qk1"},{"name":"RoPE","_id":"clxvu4k9w000n314kcvr95sr5"},{"name":"长上下文","_id":"clxvu4k9z0019314kcr0f8mxn"},{"name":"窗口外推","_id":"clxvu4k9z001d314k3eq31yhw"},{"name":"layernorm","_id":"clxvu4ka0001u314kazyq6igs"},{"name":"normalization","_id":"clxvu4ka0001y314kgmbt3buo"},{"name":"batchnorm","_id":"clxvu4ka00022314kdo0r6qgu"},{"name":"算法题","_id":"clxvu4ka1002c314kgw6f13wu"},{"name":"技术报告","_id":"clxvu4ka10032314k0ujsa22x"},{"name":"学习率","_id":"clxvu4ka20038314k1jjehvoh"},{"name":"预训练","_id":"clxvu4ka2003c314k2owx3jej"},{"name":"复读机","_id":"clxvu4ka3003y314k3mfx9alw"},{"name":"重复生成","_id":"clxvu4ka30045314kf68se9lb"},{"name":"涌现能力","_id":"clxvu4ka3004b314kew62ejik"},{"name":"强化学习","_id":"clxvu4ka3004h314kgzv3e34i"},{"name":"微调","_id":"clxvu4ka3004o314k5e9733o1"},{"name":"SFT","_id":"clxvu4ka4004w314kcr712lar"},{"name":"偏好对齐","_id":"clxvu4ka40052314k2c3sg1p4"},{"name":"MoE","_id":"clxvu4ka4005d314k87v3fc1a"},{"name":"推理加速","_id":"clxvu4ka4005i314k70hy6ok5"},{"name":"无限大","_id":"clxvu4ka6007k314k81owhi6o"},{"name":"attention","_id":"clxvu4ka6007w314kgd11ho1f"},{"name":"sliding window attention","_id":"clxvu4ka7008i314kd2u0bqac"},{"name":"sparse attention","_id":"clxvu4ka7008l314k78gc7hnf"},{"name":"post-norm","_id":"clxvu4ka7008r314kbfxd36ox"},{"name":"pre-norm","_id":"clxvu4ka7008u314k2kmg50vi"},{"name":"多模态","_id":"clxvu4ka70096314kfz2w6gve"},{"name":"ChatGPT","_id":"clxvu4kaa009y314k12c9eeng"},{"name":"Sparrow","_id":"clxvu4kaa00a4314k0rokdvo5"},{"name":"LaMDA","_id":"clxvu4kaa00a9314kaczieqth"},{"name":"GopherCite","_id":"clxvu4kaa00ac314kdzue9y7d"},{"name":"WebGPT","_id":"clxvu4kaa00ag314kb817ac6u"},{"name":"InstructGPT","_id":"clxvu4kaa00aj314k1gdx5ncp"},{"name":"KV Cache","_id":"clxvu4kaa00ak314k8b9q7s4t"}]}}