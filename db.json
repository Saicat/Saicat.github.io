{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","path":"css/noscript.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","path":"js/comments.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/config.js","path":"js/config.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","path":"js/pjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","path":"js/schedule.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","path":"js/third-party/addtoany.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","path":"js/third-party/tags/wavedrom.js","modified":1,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/images/qrcode.jpg","path":"images/qrcode.jpg","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-gpxpg3.png","path":"images/background/wallhaven-gpxpg3.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-p97q73.png","path":"images/background/wallhaven-p97q73.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-2ywymm.png","path":"images/background/wallhaven-2ywymm.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-x636oz.png","path":"images/background/wallhaven-x636oz.png","modified":1,"renderable":0},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","path":"images/avatar/20180303210737_XsJVr.jpeg","modified":1,"renderable":0},{"_id":"source/images/avatar/Picasso_Elephant.png","path":"images/avatar/Picasso_Elephant.png","modified":1,"renderable":0},{"_id":"source/images/avatar/shadow.png","path":"images/avatar/shadow.png","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","path":"images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","path":"images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","path":"images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/about.txt","path":"images/favicon/favicon_io/about.txt","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","path":"images/favicon/favicon_io/android-chrome-192x192.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","path":"images/favicon/favicon_io/apple-touch-icon.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","path":"images/favicon/favicon_io/favicon-16x16.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","path":"images/favicon/favicon_io/favicon-32x32.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon.ico","path":"images/favicon/favicon_io/favicon.ico","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","path":"images/favicon/favicon_io/android-chrome-512x512.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/site.webmanifest","path":"images/favicon/favicon_io/site.webmanifest","modified":1,"renderable":0}],"Cache":[{"_id":"node_modules/hexo-theme-next/.DS_Store","hash":"bef639ede199b5b3aea85285a9d03c82257c52aa","modified":1709026831096},{"_id":"node_modules/hexo-theme-next/_vendors.yml","hash":"4f6046ceb1470be9ff334ede20b73871c951d845","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/package.json","hash":"4b48877b223ec717e708540a2df03d64983c02ab","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/README.md","hash":"d6820f46d03a93bd6dc8b10f49f58aec82ad2b06","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/_config.yml","hash":"255c963c680da5da34c259c560dd8211b75188ca","modified":1708604632809},{"_id":"node_modules/hexo-theme-next/source/.DS_Store","hash":"20f7a9b9a682cc55305492b2e240489f6bf832e6","modified":1709026838425},{"_id":"node_modules/hexo-theme-next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/languages/ar.yml","hash":"7d0f39e8684284a04bb9808521c87fecda8bd131","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/languages/de.yml","hash":"79b37df731c29665dee6cd7c90d278e1edfb6e24","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/en.yml","hash":"ba0fd79a2b1d8db01a034180556061745965ff05","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/eo.yml","hash":"e34bb33ae827bf2f0727088599a73bc64bdad1b0","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/es.yml","hash":"dffc63ef42e1266b88e0acf08994fd17a9908d53","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/bn.yml","hash":"564bed75da6e05b11dce6164508f97a15e2fb6c2","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fr.yml","hash":"8ac44e58f71a38b7697a2f7f98a6971ed818cb5b","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/it.yml","hash":"16d716ecfd748def2f6486ef5a82d0ab7ceb4890","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fa.yml","hash":"f3ffc444599f4ac92d62e9ed00a1490ebc277d70","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ja.yml","hash":"543222bfc516aab6c33e8534f807972ecb8943a9","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ko.yml","hash":"d345a303310c8a5f4836c3683f3580f861ebd1b4","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/nl.yml","hash":"3cb3687696635ec71b4ca40c5fc43b56acc8843e","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/pt-BR.yml","hash":"76b8576ce228d540a16b1f0af5af2cce20923194","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/pt.yml","hash":"70de366e10ea584ba039d40d6b35ac97f93454ad","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/ru.yml","hash":"c6d8de0ff7d8148d09993257cfd3b7aca755696c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/si.yml","hash":"2d712eedf3f60d04d36c3108cf5a12e2a52e875c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/th.yml","hash":"6829e998b39f8f143e20b276bb1f62d95a29de58","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/tk.yml","hash":"511726054873f6f8d7ce0d2e803f6731de0ddbe7","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/id.yml","hash":"929df147f4f17d638b07de5fe52ca13e2549ab1c","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/tr.yml","hash":"a57e4ed089b893a95f5e1ecff17ce625165f4d46","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/uk.yml","hash":"ff537047b4b4c3ca9a7b64fa7f428a9942751eeb","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/zh-HK.yml","hash":"88ea50eeb9097ab4a87a44981a102d8594feb064","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/vi.yml","hash":"7ebcba5e1128784195e4681dffc9d34c4e873fec","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/zh-CN.yml","hash":"741d7efe0262c9cdc2c648014b55599665d90f6b","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/layout/_layout.njk","hash":"fc0a45112f2dcfc2642404e8934ea32a793c3bd7","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/languages/zh-TW.yml","hash":"4695c87d6b81b3a23d16ad6513d9eaa925f8d8ad","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/post.njk","hash":"0bfce9f133f501a9a4837257e3b862b3bbca15be","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/page.njk","hash":"b0660b2af0ac7d3fda14ca4d9f2c9e79ef06c6f9","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/source/css/_mixins.styl","hash":"83647a6207333b9609ba90b0946b3fa9548e6381","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/layout/.DS_Store","hash":"91040d8017183ac4f7319d2d695edb07ec1b09c8","modified":1707031368282},{"_id":"node_modules/hexo-theme-next/source/css/_colors.styl","hash":"3c6798c10cc220d83481cb3f3782e78558cee789","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","hash":"dadc81256afb127b77eac6763d5ee0ec9c77f0a3","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","hash":"921a58577f411cf4eb5cfd66db0a241f8f88578c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/scripts/filters/minify.js","hash":"447db39d17775b2bd18d8af9c9d65b7b8449f751","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/filters/locals.js","hash":"9eb5310664759931287dd28ea39165dfb67f12ed","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/events/index.js","hash":"bd9ea82376cd87df611ea3ae077875c7c595a3df","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/post.js","hash":"fdc8a0af90035e89c3fcb754a0eb189b8951a2bc","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/helpers/engine.js","hash":"d292b78485e8e8055712b0ed6de7cf559c5fbdcd","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-paginator.js","hash":"e86c764b546e4fbb87970cabc4135a56f9ef9fe1","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-config.js","hash":"ead37e9167b682f1fa34b5401c3050e18c7ee4a3","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/font.js","hash":"3394185a7f0393c16ce52c8028f90da3e9239c55","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/helpers/navigation.js","hash":"78107021101553c3d23e89290f7530b60cf4aa86","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-url.js","hash":"6281d47c1de98eb38f3aa0f6df29bbb19d412173","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-vendors.js","hash":"957241c28796ff352de7f4cffba7bb289b043586","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1706697684330},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/css/.DS_Store","hash":"6b12ac4edf32b2194ccd6b95c3f5930b07c7d56b","modified":1709026838423},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1706697684337},{"_id":"node_modules/hexo-theme-next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/group-pictures.js","hash":"9ed799c329abf830f623689d7e136991256a24ca","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/tags/index.js","hash":"1f6aba7820f1fb58b61969485148db21846e1aa9","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/tags/mermaid.js","hash":"4fb01ca650fa8b256b8d48f50dc1b18350bd3d6d","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/tags/wavedrom.js","hash":"b44dfeeb58b41945d469141787f3dbce4b117d08","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1706697684350},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/README.md","hash":"12a3e96581964a22b474cc739675d52ef93ff932","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/docs/ru/README.md","hash":"29c89a41b371f893e56c87ea61adabc444ec58cc","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/layout/_macro/post-collapse.njk","hash":"abda600685ee972e1f6b7a2dcc56f13e2daa6263","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/layout/_macro/sidebar.njk","hash":"547c62ab14d9e05d2d9116db9048a677fbe1fb6d","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_macro/post.njk","hash":"cbe208445e4d1df82ebd1761e1eaced3eab77fb3","modified":1706698899947},{"_id":"node_modules/hexo-theme-next/layout/_partials/pagination.njk","hash":"bc719473ed5948ab6859449d60b8d36cfc1542b4","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"12a6631617695504d5cf2a94b57d87bd331bef6f","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/layout/_partials/footer.njk","hash":"d77ec95cfee58b17807763dc2adb7946829cb316","modified":1706757600094},{"_id":"node_modules/hexo-theme-next/layout/_third-party/addtoany.njk","hash":"ef64c6bfb8540cd874701236b9be47db2496e98e","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/index.njk","hash":"dfd7cdd6ba89f8c3deabc27726c7a350cadafd11","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/.DS_Store","hash":"88e4de27e19e826f7296d295124581534c0c2c8b","modified":1707031373963},{"_id":"node_modules/hexo-theme-next/layout/_scripts/index.njk","hash":"6668878a0f9a1166c6a879755f54a08d942da870","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_scripts/vendors.njk","hash":"be80b9fe415a9a09d74c28e230995fd292dfc123","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_partials/widgets.njk","hash":"e7f988ecddb2159313699a00827a45eca5622bd4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/css/_common/.DS_Store","hash":"71c6bca6ae43dd79b3d75183550713e9ad0f9f8e","modified":1709197037676},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/config.js","hash":"9ec51eb61f7fee612ffc5252f489003a0fa301fc","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Muse.styl","hash":"e3be898f5ebcf435a26542653a9297ff2c71aeb0","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Mist.styl","hash":"a1418c9dc8c0f1a0ad4ded0f4627c45bf0db1a10","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/highlight.js","hash":"6aec7b2c38c50989a23bfaa0d560e75c7f553e12","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/navigation.js","hash":"dd3562686d95a50375e6fd32e717ccb0d99c1e3d","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/vendors.js","hash":"464db1e7182e5b9cdbd32e8b5368d5e683b1d9c7","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/utils.js","hash":"6853e5433e3eaa19ea43fa20b08d956ba4cec4ac","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/changyan.js","hash":"5798cfc8f63665031dd3e01debed051628cec319","modified":1706697684338},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/common.js","hash":"19a402a225c31edffc50f202a14e0d582d3db23e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqusjs.js","hash":"a600a98e7436edeb31e291abca359885567df3c9","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1706697684332},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Pisces.styl","hash":"48f4f277946a168d0db1ea02804e85c22ca2c7db","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_variables/base.styl","hash":"c4fc4e862d09221265ab1466085f057be2ad2e4d","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head-unique.njk","hash":"8da52a144060db1a0a088ccb2e6cc8376d1fce70","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/brand.njk","hash":"dd9c4c03e99dfde0dfb8edefcb2c933f2f560efc","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu-item.njk","hash":"41a8b0cc16f60fa085cb719d07216d86b6bc4bf8","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu.njk","hash":"ee6fc2f111572d3eeab0a2fecbb2d6b3e37ab26b","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head.njk","hash":"5388b157bba4a40b9312f4a45c6678974ccf0837","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/sub-menu.njk","hash":"06480d8ec5f0b87eafd47f082f07968d7282dd5c","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/breadcrumb.njk","hash":"89825e75cc45e9709fa6ba89883669eedaff6f46","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/schedule.njk","hash":"0f4bc8e257da60f77c0c1738607b2bde55810684","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-copyright.njk","hash":"bfff923526d6800218f08dba6ce0bbf5c17755fd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-followme.njk","hash":"c1e33b4889f75acc490af3c8bde0ec56c518ff41","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-related.njk","hash":"e0986db00a0201dd3c60570f964829c84ba5bc68","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-reward.njk","hash":"e8b8a7c41e9ec612d0c0c73419529d55d1c16256","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/comments.njk","hash":"d0c470b0f6690aa217e9ada848c5e2e73fb27c6f","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-share.njk","hash":"16696990e4ce65fc8db18c4635082a5d5d06ff07","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/index.njk","hash":"8f6f256ab3b351ffc80f1f3f1d9834e9a7cfac31","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/cloudflare.njk","hash":"a5b8297c2c383124dd6a56e256ecc0c0dcf489be","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/index.njk","hash":"f900306497b133e8b098bd9f4b96b93d1d96c185","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/plausible.njk","hash":"ef9f2bb7110507f1c4336800af9157d5fa9765bd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/matomo.njk","hash":"4e89648a8ec8194c5823064cbca39c938a799006","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"9dc00fcb0a05899f048eace9f9160b78956655d5","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/umami.njk","hash":"3343750682fbd8535e50f8129be3003ad26015b4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_partials/sidebar/site-overview.njk","hash":"78a1a8cac44de7e963ab4cd51c988442eb3e789a","modified":1707031409664},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/algolia-search.njk","hash":"24ed76e0c72a25ac152820c750a05826a706b6f4","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/katex.njk","hash":"1ebf658690468ea197bdd0416eb7cfa4bd0b083a","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/wavedrom.njk","hash":"02202bf563fb5eedde2ccad4d6c5b9109d30a703","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"a4bc501da0f22f7e420f0ca47e83988ce90b1368","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_menu.styl","hash":"fb550935d374e0bdf1097fce187337dc05cad3e1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_posts-expand.styl","hash":"485d23ccb42c0d0c8ead7ea8930dd3e06d79a285","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_header.styl","hash":"dafc6d23c80d6fe3e55a7711e94210d2479b629a","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_header.styl","hash":"ac2dc0ce9c775a83ef7132ae957b54539366ac9c","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_menu.styl","hash":"72dc825c50357402c342d62ab60fc0c478ab6bc1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sidebar.styl","hash":"91dbf3ca5c3a613d4e30618c120da535bf2d0336","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/base.styl","hash":"d0a7c99095f490b0d2ed6b1be43d435960798cec","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_layout.styl","hash":"26a0cba1eee5de45a45a5e14e17707f905390512","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top.styl","hash":"7664491542046df9a3887cf40a06e00c0b4086a9","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/pagination.styl","hash":"f4228c759db4a650c8d38745c2edd1dc83c45687","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/index.styl","hash":"2298e521253b3bf376a2412271bc2a7d305051f3","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_header.styl","hash":"3fbfab591f280e2e7f3b0265901c93bc4bd137ed","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_layout.styl","hash":"fa4fd8f76464e214fb7318f325b13c2b62f4b478","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_layout.styl","hash":"6569a6640f79d247a8235b3914772c0e2f99ead2","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_menu.styl","hash":"82cda756f5b7092df2eee6641b9786df71623bdb","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/toggles.styl","hash":"782ee1fc5e669d3ddbfeb82b73ad7fe561f1a4fb","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sidebar.styl","hash":"547c0b5cd5e7ea10d21863d13a6b16579a49396c","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Gemini/index.styl","hash":"9dfe853c901bdc52fc950bacdf15484dbb9bf140","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/mobile.styl","hash":"1dbf2c339adcd27026c3a2ded32ee91ce08cea26","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1706697684335},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1706697684333},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f634f94828620e88c3f5a8db56f7944f6ba232b0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/fold.styl","hash":"42a0b65491ad85438596b3fe0b7f23973e4cef34","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/index.styl","hash":"138f78147bc6bd6005f329ada34dc79b7625542d","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/index.styl","hash":"22cd37bd5df9972d5074710896aba4424ad5161c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7f8a7345e6537a62cd9e9a94c8f7065b541d9b04","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/tabs.styl","hash":"33dd6ad015dde65fd46f34961655442e8e82b52e","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/wavedrom.styl","hash":"af113411ad9cca7674177be36af8dd399680834d","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/categories.styl","hash":"b6e2eb1550a7845cb2adf86081a4ab6c7bde1e68","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/disqusjs.styl","hash":"877a537d5b95beb048142e4fdee6f17e6ef9c7bb","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/index.styl","hash":"54d12e2c5d9982f7b9e5b23be5133954a8514e9d","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/search.styl","hash":"e72799ce3f9b79753e365b2f8c8ef6c310668d4a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/note.styl","hash":"98d4c20aff0f0fcfe1824017fb06ab21ef0d218e","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/index.styl","hash":"098d4bd034e986fcf7e443eac4fc2193935461b7","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-collapse.styl","hash":"7369928305330c73ae0b3f063a681a8384d8fde4","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-followme.styl","hash":"1ecfd64507954810b07a9d21fb5305b5378feda0","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-body.styl","hash":"56d5b7ff73f466c9ae54f7204ae899281295d749","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-footer.styl","hash":"11497388f124bfbb4001495a67d3629a9f618405","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitalk.styl","hash":"8f094c4ac17e2ab45569b12d157747f9c7333c12","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-header.styl","hash":"1191f1bfa5c43e54be8e5b3cc0d802984e161747","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-widgets.styl","hash":"ebfba158a0a4af3d1dabcacbc58986664de52140","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-reward.styl","hash":"04cf4a69537fc14d3b8904f965d283356853847f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/footer/index.styl","hash":"4e967702cf4c637132346bc74ec8854426f1a68c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/index.styl","hash":"6e0d0796ef7fbbb62ffdfb448753a850de82c74f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/menu.styl","hash":"bbbc40b03cb299d2a6a568f329b2ce98e1cdc430","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/index.styl","hash":"da5e88f8debd5ac8d7af5c6ba6240df66104955f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b05908f04ef95f2d91e6eba89b12411c378d050f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"0847400d8579b0a2dd1bf662c78954c10adf2680","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"c6a27beb3f741211a14576026f3b4cfc44cc6407","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-meta.styl","hash":"a851e9d5aefcd027c95eeb323860b6da70f202d1","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"24752d145c6fb8f5344dca9c7b9640839c02e009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"46eece42510c2c89bb9209afb0262ad76a4b0b36","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"9a7c71560fbdc936ad4e736fe15063ea3e8a644b","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"c2e354a565c8c1b32bd0ceacc972b17982758b67","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1706697684374},{"_id":"source/categories/index.md","hash":"f5c920fbc09ea3d8edf250de7e31bcc6b3e765ae","modified":1706698717077},{"_id":"source/.DS_Store","hash":"ea4769cc264d17ee983394dc31d8d5cac0de7a50","modified":1710516092318},{"_id":"source/_posts/.DS_Store","hash":"f6c441d5db8a680fc3cf3bc13f8aea7f90886f6b","modified":1710516092315},{"_id":"source/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1706872557451},{"_id":"source/about/index.md","hash":"9294d008cc673abc2eaf740f101ebac560029267","modified":1706698701349},{"_id":"source/tags/index.md","hash":"e995ed2b8452b1906600b3853b920f13423098b7","modified":1706698644396},{"_id":"source/_data/styles.styl","hash":"f4bb55ef0972c829e3382d1bae1786b3ab5d54ef","modified":1707045638288},{"_id":"source/_posts/cs/.DS_Store","hash":"db1ba195a60412ec7a7d63e3f3ffaa7a874fe079","modified":1710516092319},{"_id":"source/images/.DS_Store","hash":"60fbc9bc8c2d88510803a394d229a0442cae1cb2","modified":1709016856181},{"_id":"source/_posts/cs/nlp/.DS_Store","hash":"852b2571c397715dd345e664be2511fd2cedd28f","modified":1710516092315},{"_id":"source/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1707548301740},{"_id":"source/images/background/.DS_Store","hash":"88f5d31d0db89adcf679f2a7fefc8947139a1c1f","modified":1709026807046},{"_id":"source/images/avatar/.DS_Store","hash":"c3fa37607ceb3f7ba411cf4203d2a333f773d921","modified":1707118756919},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1707030615190},{"_id":"source/images/favicon/.DS_Store","hash":"83ddccadffca5384db3dfc167728b7c7cacd9a87","modified":1707796842439},{"_id":"source/_posts/cs/nlp/2024/.DS_Store","hash":"844b88fcf49afe5de8acf941e83bcf545f0f45dc","modified":1710516100526},{"_id":"source/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1706844814000},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1707048498957},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention.md","hash":"8d504d5a027d6681b6dd1e6f60f8aa680c09c7ae","modified":1710563108847},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/02/.DS_Store","hash":"055b036ca6d94a43ddf5cc44ff50734997452711","modified":1709897296542},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/03/.DS_Store","hash":"3304d20f512bb2607b9b026ee250e81526e44ebf","modified":1710516100525},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/.DS_Store","hash":"93c7345df383234160af43ee62b335152dbfefc4","modified":1710560423245},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA.md","hash":"c5802e9f2c1eb743e74e4b89a0f4005a5c427b07","modified":1710425874411},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题.md","hash":"dd73e47b1cdb864f26d5780ac4fe08603bcc9b3c","modified":1710314618942},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE.md","hash":"1dae1211b15ab91c1b9f991209aa0a16fc6dbceb","modified":1709015970683},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1710560488146},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1710250364698},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1710321816411},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/.DS_Store","hash":"9d409e9dac238eb07cc6841f8e8d05eed83df842","modified":1710056957220},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1709973970557},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1709716894560},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1709716888116},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1709723125449},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1709780575011},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1709716876496},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1709821278308},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1709781776387},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/.DS_Store","hash":"2e33b8d145af72ec31cbad8c19fc2528fc2a909f","modified":1709014663934},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1708758408339},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1709188879237},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1707048511782},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1707048415396},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1710561804292},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1710255091449},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1709983486925},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1709199016415},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1708957811757},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1709262766062},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1709637863252},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1709971938995},{"_id":"source/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1707118741657},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1710560038203},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1710516051198},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1709975039443},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1708958930811},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1710516401027},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1709982190361},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1709638849226},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1709982952107},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1709970717456},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1709802508932},{"_id":"source/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1707045207190},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1709198077742},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1710252446580},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1710558943189},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1709986493059},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1709197025669},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1709206562025},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1709965340148},{"_id":"source/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1706779539112},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1709986303475},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1709986434286},{"_id":"source/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1707045618160},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Markdown _ 让排版变 Nice.html","hash":"c905c942579a520c7b3c788a00cdb9ae359d4a32","modified":1709897264700},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1710316114714},{"_id":"source/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1707045245660},{"_id":"source/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1706875075740},{"_id":"public/baidusitemap.xml","hash":"8d557095262039b16f7f207d0fd3787a85f0c004","modified":1710563124410},{"_id":"public/search.xml","hash":"e83d9ee1f40c259491f5d18045ea3d5618716867","modified":1710563124410},{"_id":"public/sitemap.xml","hash":"e86ad59922e90f5893bacde1e4a0a11b0dbbc5ef","modified":1710563124410},{"_id":"public/sitemap.txt","hash":"a5003abfdb8b49c59e4d1b2c564b6b76e81d923a","modified":1710563124410},{"_id":"public/tags/index.html","hash":"72e3e07c8042cf3a87a7d1a82ae67680947e5605","modified":1710563124410},{"_id":"public/categories/index.html","hash":"500460269d11f9e848b5f95828031beeb03c9351","modified":1710563124410},{"_id":"public/about/index.html","hash":"30248384c1ace60f26449759ed65effff0fb74ff","modified":1710563124410},{"_id":"public/c61d17e3.html","hash":"d92aa32d9afdc0b6fada51ea1be8c94080f560cf","modified":1710563124410},{"_id":"public/3dc22f96.html","hash":"9ebb738d51426ffec5dbb5faee7e5e18401e499e","modified":1710563124410},{"_id":"public/c4da56c0.html","hash":"35180d99766c1bd8c050011351f799da664ce100","modified":1710563124410},{"_id":"public/a051710f.html","hash":"3c886119178b1eb49607544dab43baa3f4879449","modified":1710563124410},{"_id":"public/archives/index.html","hash":"96a6c9dd0713ee5d6cd7b76de8121e0657b9cd69","modified":1710563124410},{"_id":"public/archives/2024/index.html","hash":"502748965f346982aa4d3c5f71975c308461f36d","modified":1710563124410},{"_id":"public/archives/2024/02/index.html","hash":"bef885c10a95c06d6ae62e3dbbb0077af5115610","modified":1710563124410},{"_id":"public/archives/2024/03/index.html","hash":"546aee1996e0170a9b538dff0e3e8fa367ce2d5f","modified":1710563124410},{"_id":"public/categories/CS/index.html","hash":"ba7b9f3f518f15559bfe25e4ac9e872d081f5474","modified":1710563124410},{"_id":"public/categories/CS/NLP/index.html","hash":"7a9db55e2403b4ae44c7fc056e3dbfc39aaaa8cf","modified":1710563124410},{"_id":"public/categories/CS/NLP/LLM/index.html","hash":"903b0d67b8fccd1381b85ca498d88ff4645f3137","modified":1710563124410},{"_id":"public/index.html","hash":"e5c436aeff39e60864552c73c691c47bae6512c1","modified":1710563124410},{"_id":"public/tags/NLP/index.html","hash":"9d9221ccc857123566640c405be50abb820c4337","modified":1710563124410},{"_id":"public/tags/LLM/index.html","hash":"5974eceea2842b5f210e529e0baa951808c234fe","modified":1710563124410},{"_id":"public/tags/transformer/index.html","hash":"270975f20a1b115d664e39f4205a14a7289a8eb6","modified":1710563124410},{"_id":"public/tags/长上下文/index.html","hash":"9554237804a77c34b45129317742adacd32da239","modified":1710563124410},{"_id":"public/tags/窗口外推/index.html","hash":"111cfd619a5f38e3fa4a44258904f9b90e1968c3","modified":1710563124410},{"_id":"public/tags/attention/index.html","hash":"84a2dc3f6e61232ec03040b0c75fb21860c4584c","modified":1710563124410},{"_id":"public/tags/sliding-window-attention/index.html","hash":"765360707b126689485e8a101b18a2462c1ad198","modified":1710563124410},{"_id":"public/tags/sparse-attention/index.html","hash":"48ac92d5ff6c7437f57694a0532418afc8d5bc44","modified":1710563124410},{"_id":"public/tags/positional-encoding/index.html","hash":"5649e84fa6852862598d15afda645ee99225a7cf","modified":1710563124410},{"_id":"public/tags/RoPE/index.html","hash":"bcb68167b2ddaa28eee689524e7de423bbd48280","modified":1710563124410},{"_id":"public/tags/KV-Cache/index.html","hash":"c79a6e692f28bad05ba5f1da37c25204349cb8da","modified":1710563124410},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1710563124410},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1710563124410},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1710563124410},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1710563124410},{"_id":"public/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1710563124410},{"_id":"public/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1710563124410},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1710563124410},{"_id":"public/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1710563124410},{"_id":"public/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1710563124410},{"_id":"public/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1710563124410},{"_id":"public/c4da56c0/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1710563124410},{"_id":"public/c61d17e3/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1710563124410},{"_id":"public/c61d17e3/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1710563124410},{"_id":"public/c61d17e3/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1710563124410},{"_id":"public/a051710f/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1710563124410},{"_id":"public/3dc22f96/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1710563124410},{"_id":"public/3dc22f96/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1710563124410},{"_id":"public/3dc22f96/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1710563124410},{"_id":"public/3dc22f96/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1710563124410},{"_id":"public/3dc22f96/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1710563124410},{"_id":"public/3dc22f96/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1710563124410},{"_id":"public/3dc22f96/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1710563124410},{"_id":"public/3dc22f96/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1710563124410},{"_id":"public/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1710563124410},{"_id":"public/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1710563124410},{"_id":"public/c4da56c0/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1710563124410},{"_id":"public/c4da56c0/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1710563124410},{"_id":"public/c61d17e3/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1710563124410},{"_id":"public/c61d17e3/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1710563124410},{"_id":"public/a051710f/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1710563124410},{"_id":"public/3dc22f96/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1710563124410},{"_id":"public/3dc22f96/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1710563124410},{"_id":"public/3dc22f96/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1710563124410},{"_id":"public/css/main.css","hash":"6aea217c0462e6970606601a9fe39183cf15614c","modified":1710563124410},{"_id":"public/css/noscript.css","hash":"4cd5301e478e0e0d4b176740ec314087ec5cb707","modified":1710563124410},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1710563124410},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1710563124410},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1710563124410},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1710563124410},{"_id":"public/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1710563124410},{"_id":"public/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1710563124410},{"_id":"public/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1710563124410},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1710563124410},{"_id":"public/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1710563124410},{"_id":"public/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1710563124410},{"_id":"public/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1710563124410},{"_id":"public/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1710563124410},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1710563124410},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1710563124410},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1710563124410},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1710563124410},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1710563124410},{"_id":"public/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1710563124410},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1710563124410},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1710563124410},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1710563124410},{"_id":"public/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1710563124410},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1710563124410},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1710563124410},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1710563124410},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1710563124410},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1710563124410},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1710563124410},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1710563124410},{"_id":"public/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1710563124410},{"_id":"public/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1710563124410},{"_id":"public/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1710563124410},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1710563124410},{"_id":"public/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1710563124410},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1710563124410},{"_id":"public/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1710563124410},{"_id":"public/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1710563124410},{"_id":"public/c61d17e3/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1710563124410},{"_id":"public/c61d17e3/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1710563124410},{"_id":"public/a051710f/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1710563124410},{"_id":"public/3dc22f96/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1710563124410},{"_id":"public/c61d17e3/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1710563124410},{"_id":"public/3dc22f96/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1710563124410},{"_id":"public/3dc22f96/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1710563124410},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1710563124410},{"_id":"public/3dc22f96/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1710563124410},{"_id":"public/3dc22f96/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1710563124410},{"_id":"public/3dc22f96/Markdown _ 让排版变 Nice.html","hash":"c905c942579a520c7b3c788a00cdb9ae359d4a32","modified":1710563124410},{"_id":"public/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1710563124410},{"_id":"public/c4da56c0/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1710563124410},{"_id":"public/c4da56c0/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1710563124410},{"_id":"public/c61d17e3/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1710563124410},{"_id":"public/c61d17e3/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1710563124410},{"_id":"public/3dc22f96/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1710563124410},{"_id":"public/c4da56c0/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1710563124410},{"_id":"public/3dc22f96/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1710563124410},{"_id":"public/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1710563124410},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1710563124410},{"_id":"public/3dc22f96/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1710563124410},{"_id":"public/c61d17e3/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1710563124410},{"_id":"public/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1710563124410},{"_id":"public/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1710563124410},{"_id":"public/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1710563124410}],"Category":[{"name":"CS","_id":"clttl4al00005rb4keyff2zjp"},{"name":"NLP","parent":"clttl4al00005rb4keyff2zjp","_id":"clttl4al3000drb4k4igca5dc"},{"name":"LLM","parent":"clttl4al3000drb4k4igca5dc","_id":"clttl4al3000orb4k08mp80z4"}],"Data":[{"_id":"styles","data":".post-toc .nav .nav-child {\n  display: block;\n}\n.post-toc ol {\n  font-size: 13px;\n}\nbody {\n  background: url(\"/images/background/wallhaven-p97q73.png\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-size: cover;\n  background-position: 50% 50%;\n}\n:root {\n  --content-bg-color: rgba(32,32,32,0.816);\n}\n"}],"Page":[{"title":"tags","date":"2024-01-31T10:50:02.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2024-01-31 18:50:02\ntype: \"tags\"\ncomments: false\n---\n","updated":"2024-01-31T10:57:24.396Z","path":"tags/index.html","layout":"page","_id":"clttl4akw0000rb4k57ly95hh","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"categories","date":"2024-01-31T10:57:57.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2024-01-31 18:57:57\ntype: \"categories\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:37.077Z","path":"categories/index.html","layout":"page","_id":"clttl4akx0001rb4k2ukpa4n6","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"about","date":"2024-01-31T10:57:44.000Z","type":"about","comments":0,"_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2024-01-31 18:57:44\ntype: \"about\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:21.349Z","path":"about/index.html","layout":"page","_id":"clttl4akz0003rb4kgomdbvw3","content":"\n","length":0,"excerpt":"","more":"\n"}],"Post":[{"title":"LLM长上下文的问题","abbrlink":"c4da56c0","date":"2024-02-28T07:19:28.000Z","_content":"\n最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。  \n\n跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：[博客](http://www.linsight.cn/a051710f.html) [知乎](https://zhuanlan.zhihu.com/p/684072868) [微信公众号](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n# 关于长上下文  \n\n2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k tokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。  \n\n（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）  \n\n差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。\n\n今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型  \n\n<center>\n\n| 模型 | 支持长度 |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20万汉字) |\n| Claude2 | 200k |  \n\n</center>\n\n大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。  \n\n为什么要那么长？  \n\n# 长上下文的需求  \n\n取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都>1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。  \n\n最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。  \n\n上面这个场景对应的是大模型的<big><u>**工具化**</u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。  \n\n另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented generation），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。  \n\n除了工具化的应用场景，还有一些<big><u>**个性化**</u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。  \n\n实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。  \n\n上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。\n\n# 模型怎么支持长上下文\n\n看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？  \n\n如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u>**位置编码**</u>，模型不能很好地处理。  \n\n## 直接训练\n\n既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？  \n\n这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。  \n\n1.训练数据  \n\n直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。  \n\n当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention mask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention mask，效果也挺好。  \n\n总的来说，就是【连续长文本】>【多个中等文本拼接】（也可用）  \n\n2.资源消耗  \n\n来简单看一下transformer在训练中所消耗的资源。  \n\n假设模型有 $l$ 层，词表大小为 $V$ ，hidden size为 $h$ ，batch size为 $b$ ，训练窗口长度为 $s$ ，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。  \n\n(1) 参数量\n\n模型总参数量 $\\Phi$  = 词向量参数量 + $l$ * decoder层参数量 = $Vh + l(12h^2 + 13h)$  \n\n可以看到参数量和窗口长度 $s$ 无关，模型确定了就是一个固定值。  \n\n(2) 计算量  \n\n一次前向计算量 = 输出分类头logits计算 + $l$ * 每层计算量 $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\n（这里的计算忽略了softmax，实际上softmax计算量也是和长度 $s$ 成平方关系）\n\n看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n可以看到，总计算量随着输入长度的增长是平方的。在 $s << h$ 的时候，基本还可以认为是线性的。目前大部分模型的 $h$ 是在1k到1w这个范围，基本上可以认为 $s$ 和 $sh$ 在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系  \n\n(3) 显存  \n\n训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。\n\n训练中，每个参数（$\\Phi$）有一个对应梯度（$\\Phi$），每个参数又对应优化器一个一阶动量和二阶动量（$2\\Phi$）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有 $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ 的参数占用。\n\n{% asset_img mix_precision_fp16.png 混合精度训练 %}  \n\n这部分跟输入长度没有直接关系。\n\n另外一个需要占用显存的部分是中间激活值。  \n\n保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。  \n\n对于attention层，输入时先要对 $x$ 做 $Q、K、V$ 投影，需要保存 $x$ 的中间值；计算权重的时候有 $Q、K$ 矩阵的相乘，需要保存 $Q、K$ 矩阵的值；做softmax的时候输入有 $QK^T$ 要保存；以此类推，则需要保存的所有中间激活值为 $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$ 。对于 $l$ 层的模型，就再乘以 $l$ 。  \n\n可以看到中间激活值随着 $s$ 增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch size，或者提升gradient accumulation的值，无论如何，都会增加<big><u>**训练成本**</u></big>。  \n\n小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。  \n\n现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。  \n\n而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。  \n\n## 线性插值 Position Interpolation\n\n23年6月，Meta在[《EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION》](https://arxiv.org/pdf/2306.15595.pdf)中就提出了针对RoPE的线性插值方法PI（Position Interpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。\n\n{% asset_img meta_pi.png PI效果 %}  \n{% asset_img LLM长上下文的问题/meta_pi.png PI效果 %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。\n\n看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。  \n\n论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差 $\\left|m-n \\right|$ 不太大的时候（<2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦 $\\left|m-n \\right|$ 超过这个区间，还是有可能出现很大的值。\n\n{% asset_img meta_rope_ext.png RoPE外推 %}  \n\n看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention score。而右边的图使用了插值的方式，就相对稳定。\n\n（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）\n\n而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。  \n\n{% asset_img meta_pi_nosft.png PI效果 %}  \n\n插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。\n\n{% asset_img meta_pi_explanation.png PI效果 %}  \n\n这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。  \n\n由于三角函数光滑的特性，我们可以重新定义attention score的计算，使得结果不要出现异常大的值，也就是 $\\tilde{a}(s)=a(Ls/L^{\\prime})$ ，$L$ 是原长度（也就是2048），$L^{\\prime}$ 是我们想要增大的长度（8k/16k/32k等）。\n\n更具体来说，就是对RoPE做一点修改  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n相当于位置 $m$ 的分辨率从1下降成了 ${L}/{L'}$。  \n\n（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）  \n\n然后使用几万到几十万条样本进行预训练，就可以了。\n\n（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）\n\n## NTK-Aware Interpolation \n\n线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware Interpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u>**非线性插值**</u>的方法，NTK-Aware Scaled RoPE。CodeLlama就是用这种方法把长度推广到1M。  \n\nNTK，就是Neural Tangent Kernel，神经正切核。具体是什么，让GLM4帮忙解答一下  \n\n>Neural Tangent Kernel (NTK) 是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK 的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。  \n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是 Neural Tangent Kernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。  \n具体来说，NTK 使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。  \nNTK 的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。\n\n这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？  \n\n它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。  \n\n回顾一下在RoPE中，对位置 $m$ 的输入向量进行“旋转”的矩阵长这样  \n\n{% asset_img rope_matrix.png RoPE旋转矩阵 %}  \n\n它把输入向量的元素划分成2个2个一组，共有 $d/2$ 组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于 $\\theta_j=10000^{-2j/d}$ ，可以看到， $j$ 越小越靠前的组旋转越快，$j$ 越大的旋转越慢。这里 $base=10000$ ， $base$ 越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。  \n\n不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。  \n\n怎么实现“高频外推，低频内插”？  \n\n先看回讲[RoPE](https://www.zhihu.com/people/us4ever)的时候，对于2维情况，有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n推广到高维的情况，则有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ ，$s=m-n$ 。  \n\n在这个公式下，线性插值相当于把  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n变成了  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $\\alpha=L'/L>1$ ，相当于把 $s$ 压缩了。  \n\n而NTK-Aware Scaled RoPE则是对 $\\theta_j$ 进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n相当于 $\\theta$ 乘了一个系数 $\\alpha^{\\frac{-2j}{d-2}}$ ，当 $j$ 比较小的时候， $\\alpha^{\\frac{-2j}{d-2}}$ 接近1，相当于直接进行了外推，而当 $j$ 比较大的时候（注意 $j$ 的取值是从0到 $d/2 - 1$），$\\alpha^{\\frac{-2j}{d-2}}$ 就接近 $\\alpha^{-1}$ ，这就和线性插值趋近了。\n\n引用来自[知乎一篇文章](https://zhuanlan.zhihu.com/p/645770522)的一个视角来理解NTK-Aware Interpolation  \n\n>有意思的解释一下，RoPE 的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的 RoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动 1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE 缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware RoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5 倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24 小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k 秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。  \n\n另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下[原文](https://kexue.fm/archives/9675)，也很巧妙。  \n\n在YaRN的[论文](https://arxiv.org/pdf/2309.00071.pdf)中，对NTK的优缺点作了点评  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那 $\\alpha=L'/L$ 就要选得比8更大一些，比如16。\n\n## NTK-by-parts\n\nNTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。  \n\n对于分量 $j$ ，RoPE嵌入的波长  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$ 代表旋转一周所需的长度。当 $j$ 比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。  \n\n这里观察到，当 $j$ 比较大时，波长就可能比 $L$ 要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如 $sin$ 只转了1/4圈，那值全都集中在0~1之间，-1~0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当 $j$ 比较小时，模型只能访问到相对位置信息。  \n\n此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是  \n\n- 如果维度 $j$ 的波长 $\\lambda_j$ 远小于上下文长度 ，就不插值只外推  \n- 如果波长 $\\lambda_j\\geq$ 上下文长度，就只插值不外推  \n- 中间的部分就同时存在两种，类似NTK-aware interpolation  \n\n引入一个比例 $r(j)=\\frac{L}{\\lambda_j}$ 来表示波长和上下文长度的关系。另外还需要两个阈值 $\\beta_1、\\beta_2$ 来区分以上三种情况。如果 $r(j)<\\beta_1$ ，就认为波长大，如果 $r(j)\\geq \\beta_2$ ，就认为波长小。方便起见，定义一个斜坡函数  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts插值可以定义为对 $\\theta_j$ 的一个操作  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n这里有两个超参 $\\beta_1、\\beta_2$ 要定，文中根据实验给出的推荐值是 $\\beta_1=1，\\beta_2=32$ ，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。   \n\n## Dynamically NTK Scaled RoPE  \n\n无论是线性插值还是NTK-Aware Interpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention score暴增的风险。另一方面，在解码过程中，当已解码的长度 $l$ 还没有达到训练长度 $L$ 时，就使用 $\\alpha$ 来修改base，也可能带来一些损失。Dynamically NTK Scaled RoPE是在NTK插值的基础上，把固定的系数改成动态的系数。  \n\n具体来说，就是  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n这样随着解码长度 $l$ 的增长，当 $l>L$ 之后 $\\alpha$ 从1逐渐增大， $l\\leq L$ 时则不需要改动。  \n\n有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。  \n\n## YaRN  \n\n上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。  \n\n当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度 $t>1$ 来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 $\\sqrt{t}$ 来扩展RoPE的长度。这样可以不必修改注意力的代码。  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\n通过对Llama 1和Llama 2的实验，文章提出了建议值$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。  \n\nYaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention score进行调整。  \n\nYaRN在微调以及无微调的情况下，效果都比上面的几种都要好。\n\n## logn  \n\nlogn指的是对attention计算中的缩放因子 $\\sqrt{d}$ 进行通过logn进行改进的一个方法，苏剑林在[博客](https://zhuanlan.zhihu.com/p/678755776)中进行了分析。大致的思路和YaRN中的缩放颇有些相似。  \n\n简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention score公式  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n可以看到，当 $L'>L$ 时，其效果和YaRN中的放缩是类似的。\n\n## 其他\n\n在扩展推理长度上，还有很多其他有效的工作，比如各种window attention，streaming LLM，LongLoRA，Focus Transformer等，还有数据、评测等更方面的分析，待逐个梳理。\n\n# 小结  \n\n较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降  \n\n- 推理时用到了没训练过的位置编码  \n- 推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏  \n\n这两个问题分别可以从位置编码和attention score的放缩来缓解。  \n\n线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。  \n\n# Reference  \n【1】分析transformer模型的参数量、计算量、中间激活、KV cache https://zhuanlan.zhihu.com/p/624740065  \n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n【3】Transformer升级之路：10、RoPE是一种β进制编码 https://kexue.fm/archives/9675  \n【4】YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n【5】详解基于调整RoPE旋转角度的大模型长度外推方法 https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符 https://cloud.tencent.com/developer/article/2330611  \n【8】Transformer升级之路：8、长度外推性与位置鲁棒性 https://spaces.ac.cn/archives/9444  \n【9】RoPE外推优化——支持192K上下文长度 https://zhuanlan.zhihu.com/p/678755776\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLM长上下文的问题.md","raw":"---\ntitle: LLM长上下文的问题\nabbrlink: c4da56c0\ndate: 2024-02-28 15:19:28\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - 长上下文\n  - 窗口外推\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。  \n\n跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：[博客](http://www.linsight.cn/a051710f.html) [知乎](https://zhuanlan.zhihu.com/p/684072868) [微信公众号](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n# 关于长上下文  \n\n2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k tokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。  \n\n（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）  \n\n差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。\n\n今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型  \n\n<center>\n\n| 模型 | 支持长度 |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20万汉字) |\n| Claude2 | 200k |  \n\n</center>\n\n大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。  \n\n为什么要那么长？  \n\n# 长上下文的需求  \n\n取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都>1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。  \n\n最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。  \n\n上面这个场景对应的是大模型的<big><u>**工具化**</u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。  \n\n另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented generation），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。  \n\n除了工具化的应用场景，还有一些<big><u>**个性化**</u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。  \n\n实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。  \n\n上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。\n\n# 模型怎么支持长上下文\n\n看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？  \n\n如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u>**位置编码**</u>，模型不能很好地处理。  \n\n## 直接训练\n\n既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？  \n\n这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。  \n\n1.训练数据  \n\n直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。  \n\n当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention mask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention mask，效果也挺好。  \n\n总的来说，就是【连续长文本】>【多个中等文本拼接】（也可用）  \n\n2.资源消耗  \n\n来简单看一下transformer在训练中所消耗的资源。  \n\n假设模型有 $l$ 层，词表大小为 $V$ ，hidden size为 $h$ ，batch size为 $b$ ，训练窗口长度为 $s$ ，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。  \n\n(1) 参数量\n\n模型总参数量 $\\Phi$  = 词向量参数量 + $l$ * decoder层参数量 = $Vh + l(12h^2 + 13h)$  \n\n可以看到参数量和窗口长度 $s$ 无关，模型确定了就是一个固定值。  \n\n(2) 计算量  \n\n一次前向计算量 = 输出分类头logits计算 + $l$ * 每层计算量 $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\n（这里的计算忽略了softmax，实际上softmax计算量也是和长度 $s$ 成平方关系）\n\n看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n可以看到，总计算量随着输入长度的增长是平方的。在 $s << h$ 的时候，基本还可以认为是线性的。目前大部分模型的 $h$ 是在1k到1w这个范围，基本上可以认为 $s$ 和 $sh$ 在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系  \n\n(3) 显存  \n\n训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。\n\n训练中，每个参数（$\\Phi$）有一个对应梯度（$\\Phi$），每个参数又对应优化器一个一阶动量和二阶动量（$2\\Phi$）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有 $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ 的参数占用。\n\n{% asset_img mix_precision_fp16.png 混合精度训练 %}  \n\n这部分跟输入长度没有直接关系。\n\n另外一个需要占用显存的部分是中间激活值。  \n\n保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。  \n\n对于attention层，输入时先要对 $x$ 做 $Q、K、V$ 投影，需要保存 $x$ 的中间值；计算权重的时候有 $Q、K$ 矩阵的相乘，需要保存 $Q、K$ 矩阵的值；做softmax的时候输入有 $QK^T$ 要保存；以此类推，则需要保存的所有中间激活值为 $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$ 。对于 $l$ 层的模型，就再乘以 $l$ 。  \n\n可以看到中间激活值随着 $s$ 增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch size，或者提升gradient accumulation的值，无论如何，都会增加<big><u>**训练成本**</u></big>。  \n\n小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。  \n\n现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。  \n\n而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。  \n\n## 线性插值 Position Interpolation\n\n23年6月，Meta在[《EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION》](https://arxiv.org/pdf/2306.15595.pdf)中就提出了针对RoPE的线性插值方法PI（Position Interpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。\n\n{% asset_img meta_pi.png PI效果 %}  \n{% asset_img LLM长上下文的问题/meta_pi.png PI效果 %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。\n\n看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。  \n\n论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差 $\\left|m-n \\right|$ 不太大的时候（<2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦 $\\left|m-n \\right|$ 超过这个区间，还是有可能出现很大的值。\n\n{% asset_img meta_rope_ext.png RoPE外推 %}  \n\n看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention score。而右边的图使用了插值的方式，就相对稳定。\n\n（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）\n\n而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。  \n\n{% asset_img meta_pi_nosft.png PI效果 %}  \n\n插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。\n\n{% asset_img meta_pi_explanation.png PI效果 %}  \n\n这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。  \n\n由于三角函数光滑的特性，我们可以重新定义attention score的计算，使得结果不要出现异常大的值，也就是 $\\tilde{a}(s)=a(Ls/L^{\\prime})$ ，$L$ 是原长度（也就是2048），$L^{\\prime}$ 是我们想要增大的长度（8k/16k/32k等）。\n\n更具体来说，就是对RoPE做一点修改  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n相当于位置 $m$ 的分辨率从1下降成了 ${L}/{L'}$。  \n\n（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）  \n\n然后使用几万到几十万条样本进行预训练，就可以了。\n\n（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）\n\n## NTK-Aware Interpolation \n\n线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware Interpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u>**非线性插值**</u>的方法，NTK-Aware Scaled RoPE。CodeLlama就是用这种方法把长度推广到1M。  \n\nNTK，就是Neural Tangent Kernel，神经正切核。具体是什么，让GLM4帮忙解答一下  \n\n>Neural Tangent Kernel (NTK) 是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK 的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。  \n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是 Neural Tangent Kernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。  \n具体来说，NTK 使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。  \nNTK 的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。\n\n这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？  \n\n它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。  \n\n回顾一下在RoPE中，对位置 $m$ 的输入向量进行“旋转”的矩阵长这样  \n\n{% asset_img rope_matrix.png RoPE旋转矩阵 %}  \n\n它把输入向量的元素划分成2个2个一组，共有 $d/2$ 组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于 $\\theta_j=10000^{-2j/d}$ ，可以看到， $j$ 越小越靠前的组旋转越快，$j$ 越大的旋转越慢。这里 $base=10000$ ， $base$ 越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。  \n\n不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。  \n\n怎么实现“高频外推，低频内插”？  \n\n先看回讲[RoPE](https://www.zhihu.com/people/us4ever)的时候，对于2维情况，有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n推广到高维的情况，则有  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ ，$s=m-n$ 。  \n\n在这个公式下，线性插值相当于把  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n变成了  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n其中 $\\alpha=L'/L>1$ ，相当于把 $s$ 压缩了。  \n\n而NTK-Aware Scaled RoPE则是对 $\\theta_j$ 进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n则有\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n相当于 $\\theta$ 乘了一个系数 $\\alpha^{\\frac{-2j}{d-2}}$ ，当 $j$ 比较小的时候， $\\alpha^{\\frac{-2j}{d-2}}$ 接近1，相当于直接进行了外推，而当 $j$ 比较大的时候（注意 $j$ 的取值是从0到 $d/2 - 1$），$\\alpha^{\\frac{-2j}{d-2}}$ 就接近 $\\alpha^{-1}$ ，这就和线性插值趋近了。\n\n引用来自[知乎一篇文章](https://zhuanlan.zhihu.com/p/645770522)的一个视角来理解NTK-Aware Interpolation  \n\n>有意思的解释一下，RoPE 的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的 RoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动 1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE 缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware RoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5 倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24 小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k 秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。  \n\n另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下[原文](https://kexue.fm/archives/9675)，也很巧妙。  \n\n在YaRN的[论文](https://arxiv.org/pdf/2309.00071.pdf)中，对NTK的优缺点作了点评  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那 $\\alpha=L'/L$ 就要选得比8更大一些，比如16。\n\n## NTK-by-parts\n\nNTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。  \n\n对于分量 $j$ ，RoPE嵌入的波长  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$ 代表旋转一周所需的长度。当 $j$ 比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。  \n\n这里观察到，当 $j$ 比较大时，波长就可能比 $L$ 要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如 $sin$ 只转了1/4圈，那值全都集中在0~1之间，-1~0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当 $j$ 比较小时，模型只能访问到相对位置信息。  \n\n此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是  \n\n- 如果维度 $j$ 的波长 $\\lambda_j$ 远小于上下文长度 ，就不插值只外推  \n- 如果波长 $\\lambda_j\\geq$ 上下文长度，就只插值不外推  \n- 中间的部分就同时存在两种，类似NTK-aware interpolation  \n\n引入一个比例 $r(j)=\\frac{L}{\\lambda_j}$ 来表示波长和上下文长度的关系。另外还需要两个阈值 $\\beta_1、\\beta_2$ 来区分以上三种情况。如果 $r(j)<\\beta_1$ ，就认为波长大，如果 $r(j)\\geq \\beta_2$ ，就认为波长小。方便起见，定义一个斜坡函数  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts插值可以定义为对 $\\theta_j$ 的一个操作  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n这里有两个超参 $\\beta_1、\\beta_2$ 要定，文中根据实验给出的推荐值是 $\\beta_1=1，\\beta_2=32$ ，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。   \n\n## Dynamically NTK Scaled RoPE  \n\n无论是线性插值还是NTK-Aware Interpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention score暴增的风险。另一方面，在解码过程中，当已解码的长度 $l$ 还没有达到训练长度 $L$ 时，就使用 $\\alpha$ 来修改base，也可能带来一些损失。Dynamically NTK Scaled RoPE是在NTK插值的基础上，把固定的系数改成动态的系数。  \n\n具体来说，就是  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n这样随着解码长度 $l$ 的增长，当 $l>L$ 之后 $\\alpha$ 从1逐渐增大， $l\\leq L$ 时则不需要改动。  \n\n有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。  \n\n## YaRN  \n\n上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。  \n\n当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度 $t>1$ 来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 $\\sqrt{t}$ 来扩展RoPE的长度。这样可以不必修改注意力的代码。  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\n通过对Llama 1和Llama 2的实验，文章提出了建议值$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。  \n\nYaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention score进行调整。  \n\nYaRN在微调以及无微调的情况下，效果都比上面的几种都要好。\n\n## logn  \n\nlogn指的是对attention计算中的缩放因子 $\\sqrt{d}$ 进行通过logn进行改进的一个方法，苏剑林在[博客](https://zhuanlan.zhihu.com/p/678755776)中进行了分析。大致的思路和YaRN中的缩放颇有些相似。  \n\n简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention score公式  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n可以看到，当 $L'>L$ 时，其效果和YaRN中的放缩是类似的。\n\n## 其他\n\n在扩展推理长度上，还有很多其他有效的工作，比如各种window attention，streaming LLM，LongLoRA，Focus Transformer等，还有数据、评测等更方面的分析，待逐个梳理。\n\n# 小结  \n\n较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降  \n\n- 推理时用到了没训练过的位置编码  \n- 推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏  \n\n这两个问题分别可以从位置编码和attention score的放缩来缓解。  \n\n线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。  \n\n# Reference  \n【1】分析transformer模型的参数量、计算量、中间激活、KV cache https://zhuanlan.zhihu.com/p/624740065  \n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n【3】Transformer升级之路：10、RoPE是一种β进制编码 https://kexue.fm/archives/9675  \n【4】YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n【5】详解基于调整RoPE旋转角度的大模型长度外推方法 https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符 https://cloud.tencent.com/developer/article/2330611  \n【8】Transformer升级之路：8、长度外推性与位置鲁棒性 https://spaces.ac.cn/archives/9444  \n【9】RoPE外推优化——支持192K上下文长度 https://zhuanlan.zhihu.com/p/678755776\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLM长上下文的问题","published":1,"updated":"2024-03-13T07:23:38.942Z","comments":1,"layout":"post","photos":[],"_id":"clttl4aky0002rb4ke9vgh881","content":"<p>最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。</p>\n<p>跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：<a href=\"http://www.linsight.cn/a051710f.html\">博客</a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\">知乎</a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\">微信公众号</a></p>\n<h1 id=\"关于长上下文\">关于长上下文</h1>\n<p>2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k\ntokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。</p>\n<p>（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）</p>\n<p>差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。</p>\n<p>今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\">模型</th>\n<th style=\"text-align: center;\">支持长度</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20万汉字)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。</p>\n<p>为什么要那么长？</p>\n<h1 id=\"长上下文的需求\">长上下文的需求</h1>\n<p>取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都&gt;1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。</p>\n<p>最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。</p>\n<p>上面这个场景对应的是大模型的<big><u><strong>工具化</strong></u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。</p>\n<p>另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented\ngeneration），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。</p>\n<p>除了工具化的应用场景，还有一些<big><u><strong>个性化</strong></u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。</p>\n<p>实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。</p>\n<p>上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。</p>\n<h1 id=\"模型怎么支持长上下文\">模型怎么支持长上下文</h1>\n<p>看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？</p>\n<p>如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u><strong>位置编码</strong></u>，模型不能很好地处理。</p>\n<h2 id=\"直接训练\">直接训练</h2>\n<p>既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？</p>\n<p>这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。</p>\n<p>1.训练数据</p>\n<p>直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。</p>\n<p>当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention\nmask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention\nmask，效果也挺好。</p>\n<p>总的来说，就是【连续长文本】&gt;【多个中等文本拼接】（也可用）</p>\n<p>2.资源消耗</p>\n<p>来简单看一下transformer在训练中所消耗的资源。</p>\n<p>假设模型有 <span class=\"math inline\">\\(l\\)</span> 层，词表大小为\n<span class=\"math inline\">\\(V\\)</span> ，hidden size为 <span class=\"math inline\">\\(h\\)</span> ，batch size为 <span class=\"math inline\">\\(b\\)</span> ，训练窗口长度为 <span class=\"math inline\">\\(s\\)</span>\n，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。</p>\n<ol type=\"1\">\n<li>参数量</li>\n</ol>\n<p>模型总参数量 <span class=\"math inline\">\\(\\Phi\\)</span> = 词向量参数量\n+ <span class=\"math inline\">\\(l\\)</span> * decoder层参数量 = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p>可以看到参数量和窗口长度 <span class=\"math inline\">\\(s\\)</span>\n无关，模型确定了就是一个固定值。</p>\n<ol start=\"2\" type=\"1\">\n<li>计算量</li>\n</ol>\n<p>一次前向计算量 = 输出分类头logits计算 + <span class=\"math inline\">\\(l\\)</span> * 每层计算量 <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>（这里的计算忽略了softmax，实际上softmax计算量也是和长度 <span class=\"math inline\">\\(s\\)</span> 成平方关系）</p>\n<p>看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，总计算量随着输入长度的增长是平方的。在 <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n的时候，基本还可以认为是线性的。目前大部分模型的 <span class=\"math inline\">\\(h\\)</span> 是在1k到1w这个范围，基本上可以认为\n<span class=\"math inline\">\\(s\\)</span> 和 <span class=\"math inline\">\\(sh\\)</span>\n在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系</p>\n<ol start=\"3\" type=\"1\">\n<li>显存</li>\n</ol>\n<p>训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。</p>\n<p>训练中，每个参数（<span class=\"math inline\">\\(\\Phi\\)</span>）有一个对应梯度（<span class=\"math inline\">\\(\\Phi\\)</span>），每个参数又对应优化器一个一阶动量和二阶动量（<span class=\"math inline\">\\(2\\Phi\\)</span>）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n的参数占用。</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"混合精度训练\">\n<p>这部分跟输入长度没有直接关系。</p>\n<p>另外一个需要占用显存的部分是中间激活值。</p>\n<p>保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。</p>\n<p>对于attention层，输入时先要对 <span class=\"math inline\">\\(x\\)</span>\n做 <span class=\"math inline\">\\(Q、K、V\\)</span> 投影，需要保存 <span class=\"math inline\">\\(x\\)</span> 的中间值；计算权重的时候有 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的相乘，需要保存 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的值；做softmax的时候输入有\n<span class=\"math inline\">\\(QK^T\\)</span>\n要保存；以此类推，则需要保存的所有中间激活值为 <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> 。对于\n<span class=\"math inline\">\\(l\\)</span> 层的模型，就再乘以 <span class=\"math inline\">\\(l\\)</span> 。</p>\n<p>可以看到中间激活值随着 <span class=\"math inline\">\\(s\\)</span>\n增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch\nsize，或者提升gradient\naccumulation的值，无论如何，都会增加<big><u><strong>训练成本</strong></u></big>。</p>\n<p>小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。</p>\n<p>现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。</p>\n<p>而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。</p>\n<h2 id=\"线性插值-position-interpolation\">线性插值 Position\nInterpolation</h2>\n<p>23年6月，Meta在<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">《EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION》</a>中就提出了针对RoPE的线性插值方法PI（Position\nInterpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI效果\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。</p>\n<p>看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。</p>\n<p>论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n不太大的时候（&lt;2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n超过这个区间，还是有可能出现很大的值。</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE外推\">\n<p>看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention\nscore。而右边的图使用了插值的方式，就相对稳定。</p>\n<p>（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）</p>\n<p>而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI效果\">\n<p>插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI效果\">\n<p>这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。</p>\n<p>由于三角函数光滑的特性，我们可以重新定义attention\nscore的计算，使得结果不要出现异常大的值，也就是 <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> ，<span class=\"math inline\">\\(L\\)</span> 是原长度（也就是2048），<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n是我们想要增大的长度（8k/16k/32k等）。</p>\n<p>更具体来说，就是对RoPE做一点修改</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于位置 <span class=\"math inline\">\\(m\\)</span> 的分辨率从1下降成了\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span>。</p>\n<p>（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）</p>\n<p>然后使用几万到几十万条样本进行预训练，就可以了。</p>\n<p>（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）</p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware\nInterpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u><strong>非线性插值</strong></u>的方法，NTK-Aware\nScaled RoPE。CodeLlama就是用这种方法把长度推广到1M。</p>\n<p>NTK，就是Neural Tangent\nKernel，神经正切核。具体是什么，让GLM4帮忙解答一下</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\n是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK\n的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。<br>\n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是\nNeural Tangent\nKernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。<br>\n具体来说，NTK\n使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。<br>\nNTK\n的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。</p>\n</blockquote>\n<p>这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？</p>\n<p>它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。</p>\n<p>回顾一下在RoPE中，对位置 <span class=\"math inline\">\\(m\\)</span>\n的输入向量进行“旋转”的矩阵长这样</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE旋转矩阵\">\n<p>它把输入向量的元素划分成2个2个一组，共有 <span class=\"math inline\">\\(d/2\\)</span>\n组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> ，可以看到，\n<span class=\"math inline\">\\(j\\)</span> 越小越靠前的组旋转越快，<span class=\"math inline\">\\(j\\)</span> 越大的旋转越慢。这里 <span class=\"math inline\">\\(base=10000\\)</span> ， <span class=\"math inline\">\\(base\\)</span>\n越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。</p>\n<p>不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。</p>\n<p>怎么实现“高频外推，低频内插”？</p>\n<p>先看回讲<a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>的时候，对于2维情况，有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>推广到高维的情况，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n，<span class=\"math inline\">\\(s=m-n\\)</span> 。</p>\n<p>在这个公式下，线性插值相当于把</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>变成了</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n，相当于把 <span class=\"math inline\">\\(s\\)</span> 压缩了。</p>\n<p>而NTK-Aware Scaled RoPE则是对 <span class=\"math inline\">\\(\\theta_j\\)</span>\n进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于 <span class=\"math inline\">\\(\\theta\\)</span> 乘了一个系数 <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> ，当 <span class=\"math inline\">\\(j\\)</span> 比较小的时候， <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n接近1，相当于直接进行了外推，而当 <span class=\"math inline\">\\(j\\)</span>\n比较大的时候（注意 <span class=\"math inline\">\\(j\\)</span> 的取值是从0到\n<span class=\"math inline\">\\(d/2 - 1\\)</span>），<span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> 就接近 <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> ，这就和线性插值趋近了。</p>\n<p>引用来自<a href=\"https://zhuanlan.zhihu.com/p/645770522\">知乎一篇文章</a>的一个视角来理解NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>有意思的解释一下，RoPE\n的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的\nRoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动\n1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE\n缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware\nRoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5\n倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24\n小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k\n秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。</p>\n</blockquote>\n<p>另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下<a href=\"https://kexue.fm/archives/9675\">原文</a>，也很巧妙。</p>\n<p>在YaRN的<a href=\"https://arxiv.org/pdf/2309.00071.pdf\">论文</a>中，对NTK的优缺点作了点评</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n就要选得比8更大一些，比如16。</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。</p>\n<p>对于分量 <span class=\"math inline\">\\(j\\)</span> ，RoPE嵌入的波长</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n代表旋转一周所需的长度。当 <span class=\"math inline\">\\(j\\)</span>\n比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。</p>\n<p>这里观察到，当 <span class=\"math inline\">\\(j\\)</span>\n比较大时，波长就可能比 <span class=\"math inline\">\\(L\\)</span>\n要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如\n<span class=\"math inline\">\\(sin\\)</span>\n只转了1/4圈，那值全都集中在0<sub>1之间，-1</sub>0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当\n<span class=\"math inline\">\\(j\\)</span>\n比较小时，模型只能访问到相对位置信息。</p>\n<p>此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是</p>\n<ul>\n<li>如果维度 <span class=\"math inline\">\\(j\\)</span> 的波长 <span class=\"math inline\">\\(\\lambda_j\\)</span> 远小于上下文长度\n，就不插值只外推<br>\n</li>\n<li>如果波长 <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n上下文长度，就只插值不外推<br>\n</li>\n<li>中间的部分就同时存在两种，类似NTK-aware interpolation</li>\n</ul>\n<p>引入一个比例 <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n来表示波长和上下文长度的关系。另外还需要两个阈值 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span> 来区分以上三种情况。如果\n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n，就认为波长大，如果 <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> ，就认为波长小。方便起见，定义一个斜坡函数</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts插值可以定义为对 <span class=\"math inline\">\\(\\theta_j\\)</span> 的一个操作</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这里有两个超参 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span>\n要定，文中根据实验给出的推荐值是 <span class=\"math inline\">\\(\\beta_1=1，\\beta_2=32\\)</span>\n，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>无论是线性插值还是NTK-Aware\nInterpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention\nscore暴增的风险。另一方面，在解码过程中，当已解码的长度 <span class=\"math inline\">\\(l\\)</span> 还没有达到训练长度 <span class=\"math inline\">\\(L\\)</span> 时，就使用 <span class=\"math inline\">\\(\\alpha\\)</span>\n来修改base，也可能带来一些损失。Dynamically NTK Scaled\nRoPE是在NTK插值的基础上，把固定的系数改成动态的系数。</p>\n<p>具体来说，就是</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这样随着解码长度 <span class=\"math inline\">\\(l\\)</span> 的增长，当\n<span class=\"math inline\">\\(l&gt;L\\)</span> 之后 <span class=\"math inline\">\\(\\alpha\\)</span> 从1逐渐增大， <span class=\"math inline\">\\(l\\leq L\\)</span> 时则不需要改动。</p>\n<p>有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。</p>\n<p>当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度\n<span class=\"math inline\">\\(t&gt;1\\)</span>\n来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\n来扩展RoPE的长度。这样可以不必修改注意力的代码。</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>通过对Llama 1和Llama 2的实验，文章提出了建议值<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。</p>\n<p>YaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention\nscore进行调整。</p>\n<p>YaRN在微调以及无微调的情况下，效果都比上面的几种都要好。</p>\n<h2 id=\"logn\">logn</h2>\n<p>logn指的是对attention计算中的缩放因子 <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\n进行通过logn进行改进的一个方法，苏剑林在<a href=\"https://zhuanlan.zhihu.com/p/678755776\">博客</a>中进行了分析。大致的思路和YaRN中的缩放颇有些相似。</p>\n<p>简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention\nscore公式</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，当 <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\n时，其效果和YaRN中的放缩是类似的。</p>\n<h2 id=\"其他\">其他</h2>\n<p>在扩展推理长度上，还有很多其他有效的工作，比如各种window\nattention，streaming LLM，LongLoRA，Focus\nTransformer等，还有数据、评测等更方面的分析，待逐个梳理。</p>\n<h1 id=\"小结\">小结</h1>\n<p>较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降</p>\n<ul>\n<li>推理时用到了没训练过的位置编码<br>\n</li>\n<li>推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏</li>\n</ul>\n<p>这两个问题分别可以从位置编码和attention score的放缩来缓解。</p>\n<p>线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】分析transformer模型的参数量、计算量、中间激活、KV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n【3】Transformer升级之路：10、RoPE是一种β进制编码\nhttps://kexue.fm/archives/9675<br>\n【4】YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n【5】详解基于调整RoPE旋转角度的大模型长度外推方法\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符\nhttps://cloud.tencent.com/developer/article/2330611<br>\n【8】Transformer升级之路：8、长度外推性与位置鲁棒性\nhttps://spaces.ac.cn/archives/9444<br>\n【9】RoPE外推优化——支持192K上下文长度\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":12689,"excerpt":"","more":"<p>最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。</p>\n<p>跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：<a href=\"http://www.linsight.cn/a051710f.html\">博客</a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\">知乎</a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\">微信公众号</a></p>\n<h1 id=\"关于长上下文\">关于长上下文</h1>\n<p>2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k\ntokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。</p>\n<p>（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）</p>\n<p>差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。</p>\n<p>今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\">模型</th>\n<th style=\"text-align: center;\">支持长度</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20万汉字)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。</p>\n<p>为什么要那么长？</p>\n<h1 id=\"长上下文的需求\">长上下文的需求</h1>\n<p>取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都&gt;1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。</p>\n<p>最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。</p>\n<p>上面这个场景对应的是大模型的<big><u><strong>工具化</strong></u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。</p>\n<p>另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented\ngeneration），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。</p>\n<p>除了工具化的应用场景，还有一些<big><u><strong>个性化</strong></u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。</p>\n<p>实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。</p>\n<p>上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。</p>\n<h1 id=\"模型怎么支持长上下文\">模型怎么支持长上下文</h1>\n<p>看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？</p>\n<p>如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u><strong>位置编码</strong></u>，模型不能很好地处理。</p>\n<h2 id=\"直接训练\">直接训练</h2>\n<p>既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？</p>\n<p>这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。</p>\n<p>1.训练数据</p>\n<p>直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。</p>\n<p>当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention\nmask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention\nmask，效果也挺好。</p>\n<p>总的来说，就是【连续长文本】&gt;【多个中等文本拼接】（也可用）</p>\n<p>2.资源消耗</p>\n<p>来简单看一下transformer在训练中所消耗的资源。</p>\n<p>假设模型有 <span class=\"math inline\">\\(l\\)</span> 层，词表大小为\n<span class=\"math inline\">\\(V\\)</span> ，hidden size为 <span class=\"math inline\">\\(h\\)</span> ，batch size为 <span class=\"math inline\">\\(b\\)</span> ，训练窗口长度为 <span class=\"math inline\">\\(s\\)</span>\n，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。</p>\n<ol type=\"1\">\n<li>参数量</li>\n</ol>\n<p>模型总参数量 <span class=\"math inline\">\\(\\Phi\\)</span> = 词向量参数量\n+ <span class=\"math inline\">\\(l\\)</span> * decoder层参数量 = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p>可以看到参数量和窗口长度 <span class=\"math inline\">\\(s\\)</span>\n无关，模型确定了就是一个固定值。</p>\n<ol start=\"2\" type=\"1\">\n<li>计算量</li>\n</ol>\n<p>一次前向计算量 = 输出分类头logits计算 + <span class=\"math inline\">\\(l\\)</span> * 每层计算量 <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>（这里的计算忽略了softmax，实际上softmax计算量也是和长度 <span class=\"math inline\">\\(s\\)</span> 成平方关系）</p>\n<p>看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{计算量}{参数量}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，总计算量随着输入长度的增长是平方的。在 <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n的时候，基本还可以认为是线性的。目前大部分模型的 <span class=\"math inline\">\\(h\\)</span> 是在1k到1w这个范围，基本上可以认为\n<span class=\"math inline\">\\(s\\)</span> 和 <span class=\"math inline\">\\(sh\\)</span>\n在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系</p>\n<ol start=\"3\" type=\"1\">\n<li>显存</li>\n</ol>\n<p>训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。</p>\n<p>训练中，每个参数（<span class=\"math inline\">\\(\\Phi\\)</span>）有一个对应梯度（<span class=\"math inline\">\\(\\Phi\\)</span>），每个参数又对应优化器一个一阶动量和二阶动量（<span class=\"math inline\">\\(2\\Phi\\)</span>）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n的参数占用。</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"混合精度训练\">\n<p>这部分跟输入长度没有直接关系。</p>\n<p>另外一个需要占用显存的部分是中间激活值。</p>\n<p>保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。</p>\n<p>对于attention层，输入时先要对 <span class=\"math inline\">\\(x\\)</span>\n做 <span class=\"math inline\">\\(Q、K、V\\)</span> 投影，需要保存 <span class=\"math inline\">\\(x\\)</span> 的中间值；计算权重的时候有 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的相乘，需要保存 <span class=\"math inline\">\\(Q、K\\)</span> 矩阵的值；做softmax的时候输入有\n<span class=\"math inline\">\\(QK^T\\)</span>\n要保存；以此类推，则需要保存的所有中间激活值为 <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> 。对于\n<span class=\"math inline\">\\(l\\)</span> 层的模型，就再乘以 <span class=\"math inline\">\\(l\\)</span> 。</p>\n<p>可以看到中间激活值随着 <span class=\"math inline\">\\(s\\)</span>\n增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch\nsize，或者提升gradient\naccumulation的值，无论如何，都会增加<big><u><strong>训练成本</strong></u></big>。</p>\n<p>小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。</p>\n<p>现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。</p>\n<p>而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。</p>\n<h2 id=\"线性插值-position-interpolation\">线性插值 Position\nInterpolation</h2>\n<p>23年6月，Meta在<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">《EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION》</a>中就提出了针对RoPE的线性插值方法PI（Position\nInterpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI效果\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。</p>\n<p>看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。</p>\n<p>论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n不太大的时候（&lt;2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n超过这个区间，还是有可能出现很大的值。</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE外推\">\n<p>看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention\nscore。而右边的图使用了插值的方式，就相对稳定。</p>\n<p>（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）</p>\n<p>而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI效果\">\n<p>插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI效果\">\n<p>这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。</p>\n<p>由于三角函数光滑的特性，我们可以重新定义attention\nscore的计算，使得结果不要出现异常大的值，也就是 <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> ，<span class=\"math inline\">\\(L\\)</span> 是原长度（也就是2048），<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n是我们想要增大的长度（8k/16k/32k等）。</p>\n<p>更具体来说，就是对RoPE做一点修改</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于位置 <span class=\"math inline\">\\(m\\)</span> 的分辨率从1下降成了\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span>。</p>\n<p>（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）</p>\n<p>然后使用几万到几十万条样本进行预训练，就可以了。</p>\n<p>（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）</p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware\nInterpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u><strong>非线性插值</strong></u>的方法，NTK-Aware\nScaled RoPE。CodeLlama就是用这种方法把长度推广到1M。</p>\n<p>NTK，就是Neural Tangent\nKernel，神经正切核。具体是什么，让GLM4帮忙解答一下</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\n是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK\n的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。<br>\n在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是\nNeural Tangent\nKernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。<br>\n具体来说，NTK\n使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。<br>\nNTK\n的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。</p>\n</blockquote>\n<p>这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？</p>\n<p>它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。</p>\n<p>回顾一下在RoPE中，对位置 <span class=\"math inline\">\\(m\\)</span>\n的输入向量进行“旋转”的矩阵长这样</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE旋转矩阵\">\n<p>它把输入向量的元素划分成2个2个一组，共有 <span class=\"math inline\">\\(d/2\\)</span>\n组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> ，可以看到，\n<span class=\"math inline\">\\(j\\)</span> 越小越靠前的组旋转越快，<span class=\"math inline\">\\(j\\)</span> 越大的旋转越慢。这里 <span class=\"math inline\">\\(base=10000\\)</span> ， <span class=\"math inline\">\\(base\\)</span>\n越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。</p>\n<p>不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。</p>\n<p>怎么实现“高频外推，低频内插”？</p>\n<p>先看回讲<a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>的时候，对于2维情况，有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>推广到高维的情况，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n，<span class=\"math inline\">\\(s=m-n\\)</span> 。</p>\n<p>在这个公式下，线性插值相当于把</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>变成了</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n，相当于把 <span class=\"math inline\">\\(s\\)</span> 压缩了。</p>\n<p>而NTK-Aware Scaled RoPE则是对 <span class=\"math inline\">\\(\\theta_j\\)</span>\n进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>则有</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>相当于 <span class=\"math inline\">\\(\\theta\\)</span> 乘了一个系数 <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> ，当 <span class=\"math inline\">\\(j\\)</span> 比较小的时候， <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n接近1，相当于直接进行了外推，而当 <span class=\"math inline\">\\(j\\)</span>\n比较大的时候（注意 <span class=\"math inline\">\\(j\\)</span> 的取值是从0到\n<span class=\"math inline\">\\(d/2 - 1\\)</span>），<span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span> 就接近 <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> ，这就和线性插值趋近了。</p>\n<p>引用来自<a href=\"https://zhuanlan.zhihu.com/p/645770522\">知乎一篇文章</a>的一个视角来理解NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>有意思的解释一下，RoPE\n的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的\nRoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动\n1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE\n缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware\nRoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5\n倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24\n小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k\n秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。</p>\n</blockquote>\n<p>另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下<a href=\"https://kexue.fm/archives/9675\">原文</a>，也很巧妙。</p>\n<p>在YaRN的<a href=\"https://arxiv.org/pdf/2309.00071.pdf\">论文</a>中，对NTK的优缺点作了点评</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n就要选得比8更大一些，比如16。</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。</p>\n<p>对于分量 <span class=\"math inline\">\\(j\\)</span> ，RoPE嵌入的波长</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n代表旋转一周所需的长度。当 <span class=\"math inline\">\\(j\\)</span>\n比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。</p>\n<p>这里观察到，当 <span class=\"math inline\">\\(j\\)</span>\n比较大时，波长就可能比 <span class=\"math inline\">\\(L\\)</span>\n要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如\n<span class=\"math inline\">\\(sin\\)</span>\n只转了1/4圈，那值全都集中在0<sub>1之间，-1</sub>0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当\n<span class=\"math inline\">\\(j\\)</span>\n比较小时，模型只能访问到相对位置信息。</p>\n<p>此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是</p>\n<ul>\n<li>如果维度 <span class=\"math inline\">\\(j\\)</span> 的波长 <span class=\"math inline\">\\(\\lambda_j\\)</span> 远小于上下文长度\n，就不插值只外推<br>\n</li>\n<li>如果波长 <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n上下文长度，就只插值不外推<br>\n</li>\n<li>中间的部分就同时存在两种，类似NTK-aware interpolation</li>\n</ul>\n<p>引入一个比例 <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n来表示波长和上下文长度的关系。另外还需要两个阈值 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span> 来区分以上三种情况。如果\n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n，就认为波长大，如果 <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> ，就认为波长小。方便起见，定义一个斜坡函数</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts插值可以定义为对 <span class=\"math inline\">\\(\\theta_j\\)</span> 的一个操作</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这里有两个超参 <span class=\"math inline\">\\(\\beta_1、\\beta_2\\)</span>\n要定，文中根据实验给出的推荐值是 <span class=\"math inline\">\\(\\beta_1=1，\\beta_2=32\\)</span>\n，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>无论是线性插值还是NTK-Aware\nInterpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention\nscore暴增的风险。另一方面，在解码过程中，当已解码的长度 <span class=\"math inline\">\\(l\\)</span> 还没有达到训练长度 <span class=\"math inline\">\\(L\\)</span> 时，就使用 <span class=\"math inline\">\\(\\alpha\\)</span>\n来修改base，也可能带来一些损失。Dynamically NTK Scaled\nRoPE是在NTK插值的基础上，把固定的系数改成动态的系数。</p>\n<p>具体来说，就是</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>这样随着解码长度 <span class=\"math inline\">\\(l\\)</span> 的增长，当\n<span class=\"math inline\">\\(l&gt;L\\)</span> 之后 <span class=\"math inline\">\\(\\alpha\\)</span> 从1逐渐增大， <span class=\"math inline\">\\(l\\leq L\\)</span> 时则不需要改动。</p>\n<p>有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。</p>\n<p>当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度\n<span class=\"math inline\">\\(t&gt;1\\)</span>\n来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\n来扩展RoPE的长度。这样可以不必修改注意力的代码。</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>通过对Llama 1和Llama 2的实验，文章提出了建议值<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。</p>\n<p>YaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention\nscore进行调整。</p>\n<p>YaRN在微调以及无微调的情况下，效果都比上面的几种都要好。</p>\n<h2 id=\"logn\">logn</h2>\n<p>logn指的是对attention计算中的缩放因子 <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\n进行通过logn进行改进的一个方法，苏剑林在<a href=\"https://zhuanlan.zhihu.com/p/678755776\">博客</a>中进行了分析。大致的思路和YaRN中的缩放颇有些相似。</p>\n<p>简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention\nscore公式</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>可以看到，当 <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\n时，其效果和YaRN中的放缩是类似的。</p>\n<h2 id=\"其他\">其他</h2>\n<p>在扩展推理长度上，还有很多其他有效的工作，比如各种window\nattention，streaming LLM，LongLoRA，Focus\nTransformer等，还有数据、评测等更方面的分析，待逐个梳理。</p>\n<h1 id=\"小结\">小结</h1>\n<p>较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降</p>\n<ul>\n<li>推理时用到了没训练过的位置编码<br>\n</li>\n<li>推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏</li>\n</ul>\n<p>这两个问题分别可以从位置编码和attention score的放缩来缓解。</p>\n<p>线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】分析transformer模型的参数量、计算量、中间激活、KV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n【3】Transformer升级之路：10、RoPE是一种β进制编码\nhttps://kexue.fm/archives/9675<br>\n【4】YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n【5】详解基于调整RoPE旋转角度的大模型长度外推方法\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522\n【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符\nhttps://cloud.tencent.com/developer/article/2330611<br>\n【8】Transformer升级之路：8、长度外推性与位置鲁棒性\nhttps://spaces.ac.cn/archives/9444<br>\n【9】RoPE外推优化——支持192K上下文长度\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"稀疏注意力计算:sliding window attention","abbrlink":"c61d17e3","date":"2024-03-12T09:26:00.000Z","_content":"\n【本文已在同名微信公众号/知乎/个人博客同步上线】  \n\nLLM的长文本能力现在已经是各个大模型巨头的必争之地。  \n\n我们之前在[《LLM长上下文的问题》](http://www.linsight.cn/c4da56c0.html)简单介绍了目前把大模型理解和生成能力推广到32k+/128k+的主流方法，在[《理解Attention:从起源到MHA,MQA和GQA》](http://www.linsight.cn/3dc22f96.html)一文中也解析了MQA和GQA通过节省KV缓存的方式，支持模型在长上下文情况下推理加速的方案。  \n\n在这讲一下另一种（理论有损）提升注意力计算效率的方法：SWA（sliding window attention）。  \n\n一些效果受到广泛关注的模型，如Qwen系列和Mistral就使用了SWA。  \n\n关于Mistral：  \n\nMistral AI是法国一家AI独角兽公司，2023年5月才成立，但是在2023年9月和12月就分别推出了Mistral 7B和MoE模型Mistral 8x7B并开源。  \n\n2024年2月，微软也投资了它。  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n它在2024年2月发布的Mistral Large，支持多语言 & 32k的上下文长度，在MMLU上也是获得了直逼GPT4的效果  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\n（大家也因此对Mistral寄予了厚望，希望它能成为大模型行业的鲶鱼，激活一下OPENAI和META加速一下开源。）  \n\n# SWA\n\n虽然SWA的思路最早不是Mistral提出的，我们还是先以Mistral 7B为例来看下SWA的具体做法。  \n\n## Mistral 7B\n\n2023年10月，Mistral发布了Mistral 7B的[技术报告](https://arxiv.org/pdf/2310.06825.pdf)。其中开篇就说到，相比Llama，Mistral在结构上做了一些改动，除了GQA，另一个用于支持长文本下高效推理的改动就是SWA。  \n\n来看下Mistral 7B的模型结构参数  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistral使用了kv组数=8的GQA，intermediate size相比Llama2（11008）大一些，其他基本没有太大变化。  \n\n## 计算量和缓存\n\n对于原始的causal attention，其注意力矩阵是一个下三角矩阵，这样每个token都能看到自己和在自己前面的token。  \n\n这样随着输入长度 $s$ 增大，这个下三角矩阵中1的元素数量以 $s^2$ 的速度增长，带来的是计算量和所需的KV Cache以平方的速度增长。  \n\n（我们知道计算量/缓存和长度 $s$ 成平方关系，这里放一些更具体的推算细节，已经熟悉的朋友可以跳过）\n\n（1）计算量\n\n对于两个这样大小的矩阵相乘： $[m,n]\\times[n,p]$ ，输出矩阵大小为 $[m,p]$，共有 $m\\times p$ 个元素，每个元素需要 $n$ 次乘法和 $n$ 次加法，因此一次矩阵乘法有 $2mpn$ 个floating point operations（FLOPs）。  \n\n计算量上，按[《Training Compute-Optimal Large Language Models》](https://arxiv.org/pdf/2203.15556.pdf)的算法来。  \n\n对于一般MHA，输入长度为 $s$ ，层数为 $L$ ，模型hidden size为 $d_{model}$ ，每个头的维度为 $d_{q}$ ， 头的数量为 $n_{q}$（这里假设有 $d_{model} = n_{q}\\times d_{q}$ ），各个operation的FLOPs如下  \n\n<center>\n\n| Operation | FLOPs（MHA） |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $2\\times s^2\\times h_{model}$ |\n| Attention: Softmax | $3\\times n_{q}\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $2\\times s^2\\times h_{model}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax项中，对一个 $[1,s]$ 的向量做softmax，计算量为 $3s$ （一个 $s$ 是算每个元素的exp，一个 $s$ 是求和算分母，一个 $s$ 是算除法），而对 $[s,s]$ 的矩阵做softmax，则计算量为  $3s^2$ ，每个头都要计算一遍，因此再乘以 $n_{q}$ 。\n\n（这里忽略了其他一些operation，比如scaling，dropout等，有兴趣的朋友可以自己推算一下）\n\n顺便算下对于Mistral 7B这样使用了GQA的情况。  \n\n其实只有第一项的KV有变化，其他都没变。假设kv头的数量为 $n_{kv}$，则有\n\n<center>\n\n| Operation | FLOPs（GQA） |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\n从上面的推算可以看到QK logits、Softmax和Reduction三项是和长度 $s$ 成平方关系的，其他则是线性关系。  \n\n（2）缓存\n\nKV Cache需要缓存的参数量为  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n如果使用的是半精度浮点数，那么总共所需的空间就是\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n对于Mistral 7B，在输入长度为16k的情况下，所需的KV_Cache约为2G。  \n\n看来虽然用了GQA，但是在长文本（16k+）的情况下计算量和显存还是颇有压力。\n\n## SWA思路\n\n看来要提升attention计算效率，需要想办法减小上面推算中的 $s$ ，但是怎么在减小 $s$ 的同时，还能保持模型长上下文的理解和生成能力呢？\n\n来看一下，CNN中的感受野  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n如上图，假设模型有3层，每层卷积核大小为 $3\\times 3$ （实际上CNN里卷积操作就是一个sliding window）。  \n\n那对于layer 3，每一个像素能看到layer 2中的一个 $3\\times 3$ 的区域，layer 2中其他较远的像素就看到不了。  \n\n但我们再往前推，layer 2里的每个像素也可以看到layer 1中的一个 $3\\times 3$ 区域，那么layer 2中的 $3\\times 3$ 区域就可以看到layer 1中一个 $5\\times 5$ 的区域，相当于layer 3中一个像素可以<u>**间接**</u>看到一个 $5\\times 5$ 的输入。  \n\n以此类推，如果我们再增加一层layer 4，那么layer 4中一个像素就能获取输入层（layer 1） 一个 $7\\times 7$ 区域的信息。  \n\n虽然每层只能多看周围一格的信息，但是只要我们层数够多，理论上靠近输出端的层想看多远就能看多远。  \n\n值得注意的一点是，我们一般认为模型低层部分提取比较基础的特征，而高层会提取高级的语义特征。  \n\n在CNN里，前几层提取的可能更多是关于简单的边界、颜色、形状等基础特征，而后面的层则提取较复杂的语义特征，比如在分类任务中会是和分类类别相关的花纹、物体大小、风格等特征。  \n\n如果我们把模型设计成，最后一层的一个像素刚好要到第一层才能接收到全局信息（在其它层都只能看到局部），那对于图像边缘的语义特征识别能力可能会受到一些限制。  \n\n具体来说，假设我们做猫和狗的图像分类任务，如果这个时候决定性的特征出现在图像最边缘几个像素里，那这种情况下的错误率会比特征出现在图像中间时要高。  \n\n而对于语言模型，一般情况下，越远距离的信息，对当前位置的重要性越低，因此只要我们的窗口大小不要太过极限小，问题应该都还不大。\n\n看下Mistral的SWA具体是怎么做的  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\n左边是正常的causal attention，每个位置能看到自己和前面的位置，attention mask是个下三角矩阵。  \n\n中间则是SWA的attention mask，这里的窗口大小为3。包括自己在内，每个位置只能往前看3个输入。  \n\n同CNN的感受野一样，随着层数的堆叠，模型理论上能处理的最远距离也逐层线性递增。只是LLM里递增的方向是单向的，只能往前。  \n\nMistral 7B使用了4096的窗口大小，模型层数为32，则最终输出的”感受野“为 $4096\\times 32=131,072$ 达到131k的长度。  \n\n前面我们推算了attention的计算量，其中QK logits、Softmax和Reduction三项是和长度 $s$ 成平方关系。在使用了SWA之后，理论上，这几个operation仅使用4k的计算量，就能获得131k的上下文效果。当输入长度为131k时，除去已经缓存部分的数值，新的输入计算量相差 $32\\times 32=1024$ 倍。  \n\n而缓存和上下文长度 $s$ 成线性关系，当上下文长度为131k时，最大也能节省 $31/32$ 的显存。  \n\n即SWA在上下文长度在4k以下时，和普通causal attention一样；当上下文长度超过4k时，则相对节省资源，长度越大，节省的比例越高。\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\n实际使用中，Mistral通过把SWA实现在FlashAttention和xFormers中，对于16k的上下文长度，获得了2倍的速度提升。  \n\n## 和KV Cache的配合实现\n\n在不使用sliding window的情况下，随着自回归推理的进行，KV Cache是只增不减的。  \n\n而在使用SWA的情况下，超出窗口长度的kv就可以不用再缓存了，因此使用一个轮转替换的策略。  \n\n比如窗口大小 $W=4$ ，则当第5个token需要缓存是，直接替换掉第1个token，这样就可以保持kv缓存有一个最大值（为窗口大小），而不会无限增长。  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\n这样便于我们估计硬件设备所能支持的throughput，也不会因为少量超长的case而造成堵塞，在工程上有利于提高硬件利用率，降低成本。  \n\n## 长Prompt的分块\n\n更近一步，考虑到我们使用RAG或者funciton call的时候，都会使用比较长的，固定的prompt来知道模型的行为。  \n\n比如GPT4就被诱导说出它接收到的长system prompt（当然未必真的就是OPENAI用的）  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: 【{message idx}†{link text}】.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\n除了预先计算好system prompt的kv值，并保存在缓存中方便每次用户输入使用外，如果system prompt很长（比sliding window大），还可以通过对system prompt的kv值进行切分来进一步优化计算。\n\n比如窗口大小 $W=4$，system prompt大小为9时，就可以把system prompt的kv缓存切成 [4,4,1] 三块。  \n\n第一块由于和当前的输入距离超过了一个window的大小，所以是完全看不见的，对应的attention mask全为0，因此可以完全忽略。  \n\n第二块的attention mask则是一个上三角矩阵，当前的输入需要用到这部分信息。  \n\n第三块是一个下三角矩阵（的左边部分），包含了当前的输入在内。  \n\n在推理的时候，我们只需要用到第二块和第三块的内容，这就节省了缓存的操作。  \n\n而且无论prompt有多长，只要我们按窗口大小分块，一定只会用到最后两块。  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\n（实际上现在推理框架基本上都有FlashAttention/PagedAttention等技术加持，能够进一步节省资源，提高效率，这个后续再开一篇讲）\n\nMistral 7B整体的效果上的效果相比Llama是有优势的，部分任务甚至超过了Llama 34B。  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral认为大语言模型压缩知识的能力实际超过我们的认知，7B这个规模的效果还有提升空间。  \n\n# Sparse Attention\n\nSWA实际上是一种sparse attention，而sparse attention也有许多工作做了深入探索。  \n\n这里简单说一小部分，有机会再完整梳理一遍sparse attention的理论和实践。  \n\n## Longformer\n\n前面提到，Mistral并不是第一个使用SWA的。  \n\n2020年，[《Longformer: The Long-Document Transformer》](https://arxiv.org/pdf/2004.05150.pdf)就提出包含SWA在内的一系列sparse attention的做法。  \n\n从文章名字就看到出来，Longformer主要目的也是为了解决长上下文的问题。  \n\n{% asset_img longformer_attention.png longformer %}  \n\n上图中的（b）就是SWA，只是用在Bert中的时候它是双向的。  \n\n在SWA的基础上，还可以进行空洞滑窗（dilated sliding window），在不增加计算量的情况下，提升感受野。这也是从空洞卷积（下图）来的灵感了。    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\n还可以更进一步优化attention。无论是SWA还是dilated sliding window，每个位置都只能看到局部的信息。  \n\n但是实际上有些位置就是对全局信息有很高的需求。  \n\n在Bert中，[CLS] token就常常作为分类token或者相似度向量使用，这种情况下就需要它能获取整个上下文的完整信息。  \n\n而在GPT中，instruction，或者说prompt的部分也对全局信息有更高要求，因为我们希望在整个对话过程中，模型都能遵循我们给出的规则。  \n\n对于这些token，我们让它可以看到其他所有位置，使用完整的global attention，而其他位置则使用sliding window，如（d）中所示。  \n\n## Big Bird\n\n无独有偶，同样在2020年，和Longformer差不多在同一时期，也有另外一个通过sparse attention来优化长文本效果的工作，[《Big Bird: Transformers for Longer Sequences》](https://arxiv.org/abs/2007.14062)。  \n\n其中sliding window和global attention结合的思路和Longformer相似。Big Bird还额外加入了一个random attention的做法。  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n上图中 $r=2$ 即每个位置使用2个随机注意力。\n\n# 小结  \n\nSWA在优化长上下文的计算效率上有明显的收益。而在模型效果上，目前基本没有看到不可接受的损失。对长上下文有需求的业务，值得探索。  \n\n除了SWA，sparse attention还有许多其他探索。目前来看，这些做法都有一定的理论基础，效果也不错。但是阻碍这些方案大规模使用的一个原因就是<big>**工程实现**</big>，比如如何高效计算global + local attention，在flash attention中能够支持random attention，这都是要考虑的内容。\n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n【1】Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n【2】Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n【3】Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n【4】GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n【5】Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","source":"_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention.md","raw":"---\ntitle: '稀疏注意力计算:sliding window attention'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - sliding window attention\n  - sparse attention\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: c61d17e3\ndate: 2024-03-12 17:26:00\n---\n\n【本文已在同名微信公众号/知乎/个人博客同步上线】  \n\nLLM的长文本能力现在已经是各个大模型巨头的必争之地。  \n\n我们之前在[《LLM长上下文的问题》](http://www.linsight.cn/c4da56c0.html)简单介绍了目前把大模型理解和生成能力推广到32k+/128k+的主流方法，在[《理解Attention:从起源到MHA,MQA和GQA》](http://www.linsight.cn/3dc22f96.html)一文中也解析了MQA和GQA通过节省KV缓存的方式，支持模型在长上下文情况下推理加速的方案。  \n\n在这讲一下另一种（理论有损）提升注意力计算效率的方法：SWA（sliding window attention）。  \n\n一些效果受到广泛关注的模型，如Qwen系列和Mistral就使用了SWA。  \n\n关于Mistral：  \n\nMistral AI是法国一家AI独角兽公司，2023年5月才成立，但是在2023年9月和12月就分别推出了Mistral 7B和MoE模型Mistral 8x7B并开源。  \n\n2024年2月，微软也投资了它。  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n它在2024年2月发布的Mistral Large，支持多语言 & 32k的上下文长度，在MMLU上也是获得了直逼GPT4的效果  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\n（大家也因此对Mistral寄予了厚望，希望它能成为大模型行业的鲶鱼，激活一下OPENAI和META加速一下开源。）  \n\n# SWA\n\n虽然SWA的思路最早不是Mistral提出的，我们还是先以Mistral 7B为例来看下SWA的具体做法。  \n\n## Mistral 7B\n\n2023年10月，Mistral发布了Mistral 7B的[技术报告](https://arxiv.org/pdf/2310.06825.pdf)。其中开篇就说到，相比Llama，Mistral在结构上做了一些改动，除了GQA，另一个用于支持长文本下高效推理的改动就是SWA。  \n\n来看下Mistral 7B的模型结构参数  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistral使用了kv组数=8的GQA，intermediate size相比Llama2（11008）大一些，其他基本没有太大变化。  \n\n## 计算量和缓存\n\n对于原始的causal attention，其注意力矩阵是一个下三角矩阵，这样每个token都能看到自己和在自己前面的token。  \n\n这样随着输入长度 $s$ 增大，这个下三角矩阵中1的元素数量以 $s^2$ 的速度增长，带来的是计算量和所需的KV Cache以平方的速度增长。  \n\n（我们知道计算量/缓存和长度 $s$ 成平方关系，这里放一些更具体的推算细节，已经熟悉的朋友可以跳过）\n\n（1）计算量\n\n对于两个这样大小的矩阵相乘： $[m,n]\\times[n,p]$ ，输出矩阵大小为 $[m,p]$，共有 $m\\times p$ 个元素，每个元素需要 $n$ 次乘法和 $n$ 次加法，因此一次矩阵乘法有 $2mpn$ 个floating point operations（FLOPs）。  \n\n计算量上，按[《Training Compute-Optimal Large Language Models》](https://arxiv.org/pdf/2203.15556.pdf)的算法来。  \n\n对于一般MHA，输入长度为 $s$ ，层数为 $L$ ，模型hidden size为 $d_{model}$ ，每个头的维度为 $d_{q}$ ， 头的数量为 $n_{q}$（这里假设有 $d_{model} = n_{q}\\times d_{q}$ ），各个operation的FLOPs如下  \n\n<center>\n\n| Operation | FLOPs（MHA） |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $2\\times s^2\\times h_{model}$ |\n| Attention: Softmax | $3\\times n_{q}\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $2\\times s^2\\times h_{model}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax项中，对一个 $[1,s]$ 的向量做softmax，计算量为 $3s$ （一个 $s$ 是算每个元素的exp，一个 $s$ 是求和算分母，一个 $s$ 是算除法），而对 $[s,s]$ 的矩阵做softmax，则计算量为  $3s^2$ ，每个头都要计算一遍，因此再乘以 $n_{q}$ 。\n\n（这里忽略了其他一些operation，比如scaling，dropout等，有兴趣的朋友可以自己推算一下）\n\n顺便算下对于Mistral 7B这样使用了GQA的情况。  \n\n其实只有第一项的KV有变化，其他都没变。假设kv头的数量为 $n_{kv}$，则有\n\n<center>\n\n| Operation | FLOPs（GQA） |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\n从上面的推算可以看到QK logits、Softmax和Reduction三项是和长度 $s$ 成平方关系的，其他则是线性关系。  \n\n（2）缓存\n\nKV Cache需要缓存的参数量为  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n如果使用的是半精度浮点数，那么总共所需的空间就是\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n对于Mistral 7B，在输入长度为16k的情况下，所需的KV_Cache约为2G。  \n\n看来虽然用了GQA，但是在长文本（16k+）的情况下计算量和显存还是颇有压力。\n\n## SWA思路\n\n看来要提升attention计算效率，需要想办法减小上面推算中的 $s$ ，但是怎么在减小 $s$ 的同时，还能保持模型长上下文的理解和生成能力呢？\n\n来看一下，CNN中的感受野  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n如上图，假设模型有3层，每层卷积核大小为 $3\\times 3$ （实际上CNN里卷积操作就是一个sliding window）。  \n\n那对于layer 3，每一个像素能看到layer 2中的一个 $3\\times 3$ 的区域，layer 2中其他较远的像素就看到不了。  \n\n但我们再往前推，layer 2里的每个像素也可以看到layer 1中的一个 $3\\times 3$ 区域，那么layer 2中的 $3\\times 3$ 区域就可以看到layer 1中一个 $5\\times 5$ 的区域，相当于layer 3中一个像素可以<u>**间接**</u>看到一个 $5\\times 5$ 的输入。  \n\n以此类推，如果我们再增加一层layer 4，那么layer 4中一个像素就能获取输入层（layer 1） 一个 $7\\times 7$ 区域的信息。  \n\n虽然每层只能多看周围一格的信息，但是只要我们层数够多，理论上靠近输出端的层想看多远就能看多远。  \n\n值得注意的一点是，我们一般认为模型低层部分提取比较基础的特征，而高层会提取高级的语义特征。  \n\n在CNN里，前几层提取的可能更多是关于简单的边界、颜色、形状等基础特征，而后面的层则提取较复杂的语义特征，比如在分类任务中会是和分类类别相关的花纹、物体大小、风格等特征。  \n\n如果我们把模型设计成，最后一层的一个像素刚好要到第一层才能接收到全局信息（在其它层都只能看到局部），那对于图像边缘的语义特征识别能力可能会受到一些限制。  \n\n具体来说，假设我们做猫和狗的图像分类任务，如果这个时候决定性的特征出现在图像最边缘几个像素里，那这种情况下的错误率会比特征出现在图像中间时要高。  \n\n而对于语言模型，一般情况下，越远距离的信息，对当前位置的重要性越低，因此只要我们的窗口大小不要太过极限小，问题应该都还不大。\n\n看下Mistral的SWA具体是怎么做的  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\n左边是正常的causal attention，每个位置能看到自己和前面的位置，attention mask是个下三角矩阵。  \n\n中间则是SWA的attention mask，这里的窗口大小为3。包括自己在内，每个位置只能往前看3个输入。  \n\n同CNN的感受野一样，随着层数的堆叠，模型理论上能处理的最远距离也逐层线性递增。只是LLM里递增的方向是单向的，只能往前。  \n\nMistral 7B使用了4096的窗口大小，模型层数为32，则最终输出的”感受野“为 $4096\\times 32=131,072$ 达到131k的长度。  \n\n前面我们推算了attention的计算量，其中QK logits、Softmax和Reduction三项是和长度 $s$ 成平方关系。在使用了SWA之后，理论上，这几个operation仅使用4k的计算量，就能获得131k的上下文效果。当输入长度为131k时，除去已经缓存部分的数值，新的输入计算量相差 $32\\times 32=1024$ 倍。  \n\n而缓存和上下文长度 $s$ 成线性关系，当上下文长度为131k时，最大也能节省 $31/32$ 的显存。  \n\n即SWA在上下文长度在4k以下时，和普通causal attention一样；当上下文长度超过4k时，则相对节省资源，长度越大，节省的比例越高。\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\n实际使用中，Mistral通过把SWA实现在FlashAttention和xFormers中，对于16k的上下文长度，获得了2倍的速度提升。  \n\n## 和KV Cache的配合实现\n\n在不使用sliding window的情况下，随着自回归推理的进行，KV Cache是只增不减的。  \n\n而在使用SWA的情况下，超出窗口长度的kv就可以不用再缓存了，因此使用一个轮转替换的策略。  \n\n比如窗口大小 $W=4$ ，则当第5个token需要缓存是，直接替换掉第1个token，这样就可以保持kv缓存有一个最大值（为窗口大小），而不会无限增长。  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\n这样便于我们估计硬件设备所能支持的throughput，也不会因为少量超长的case而造成堵塞，在工程上有利于提高硬件利用率，降低成本。  \n\n## 长Prompt的分块\n\n更近一步，考虑到我们使用RAG或者funciton call的时候，都会使用比较长的，固定的prompt来知道模型的行为。  \n\n比如GPT4就被诱导说出它接收到的长system prompt（当然未必真的就是OPENAI用的）  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: 【{message idx}†{link text}】.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\n除了预先计算好system prompt的kv值，并保存在缓存中方便每次用户输入使用外，如果system prompt很长（比sliding window大），还可以通过对system prompt的kv值进行切分来进一步优化计算。\n\n比如窗口大小 $W=4$，system prompt大小为9时，就可以把system prompt的kv缓存切成 [4,4,1] 三块。  \n\n第一块由于和当前的输入距离超过了一个window的大小，所以是完全看不见的，对应的attention mask全为0，因此可以完全忽略。  \n\n第二块的attention mask则是一个上三角矩阵，当前的输入需要用到这部分信息。  \n\n第三块是一个下三角矩阵（的左边部分），包含了当前的输入在内。  \n\n在推理的时候，我们只需要用到第二块和第三块的内容，这就节省了缓存的操作。  \n\n而且无论prompt有多长，只要我们按窗口大小分块，一定只会用到最后两块。  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\n（实际上现在推理框架基本上都有FlashAttention/PagedAttention等技术加持，能够进一步节省资源，提高效率，这个后续再开一篇讲）\n\nMistral 7B整体的效果上的效果相比Llama是有优势的，部分任务甚至超过了Llama 34B。  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral认为大语言模型压缩知识的能力实际超过我们的认知，7B这个规模的效果还有提升空间。  \n\n# Sparse Attention\n\nSWA实际上是一种sparse attention，而sparse attention也有许多工作做了深入探索。  \n\n这里简单说一小部分，有机会再完整梳理一遍sparse attention的理论和实践。  \n\n## Longformer\n\n前面提到，Mistral并不是第一个使用SWA的。  \n\n2020年，[《Longformer: The Long-Document Transformer》](https://arxiv.org/pdf/2004.05150.pdf)就提出包含SWA在内的一系列sparse attention的做法。  \n\n从文章名字就看到出来，Longformer主要目的也是为了解决长上下文的问题。  \n\n{% asset_img longformer_attention.png longformer %}  \n\n上图中的（b）就是SWA，只是用在Bert中的时候它是双向的。  \n\n在SWA的基础上，还可以进行空洞滑窗（dilated sliding window），在不增加计算量的情况下，提升感受野。这也是从空洞卷积（下图）来的灵感了。    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\n还可以更进一步优化attention。无论是SWA还是dilated sliding window，每个位置都只能看到局部的信息。  \n\n但是实际上有些位置就是对全局信息有很高的需求。  \n\n在Bert中，[CLS] token就常常作为分类token或者相似度向量使用，这种情况下就需要它能获取整个上下文的完整信息。  \n\n而在GPT中，instruction，或者说prompt的部分也对全局信息有更高要求，因为我们希望在整个对话过程中，模型都能遵循我们给出的规则。  \n\n对于这些token，我们让它可以看到其他所有位置，使用完整的global attention，而其他位置则使用sliding window，如（d）中所示。  \n\n## Big Bird\n\n无独有偶，同样在2020年，和Longformer差不多在同一时期，也有另外一个通过sparse attention来优化长文本效果的工作，[《Big Bird: Transformers for Longer Sequences》](https://arxiv.org/abs/2007.14062)。  \n\n其中sliding window和global attention结合的思路和Longformer相似。Big Bird还额外加入了一个random attention的做法。  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n上图中 $r=2$ 即每个位置使用2个随机注意力。\n\n# 小结  \n\nSWA在优化长上下文的计算效率上有明显的收益。而在模型效果上，目前基本没有看到不可接受的损失。对长上下文有需求的业务，值得探索。  \n\n除了SWA，sparse attention还有许多其他探索。目前来看，这些做法都有一定的理论基础，效果也不错。但是阻碍这些方案大规模使用的一个原因就是<big>**工程实现**</big>，比如如何高效计算global + local attention，在flash attention中能够支持random attention，这都是要考虑的内容。\n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n【1】Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n【2】Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n【3】Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n【4】GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n【5】Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","slug":"cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention","published":1,"updated":"2024-03-16T04:25:08.847Z","comments":1,"layout":"post","photos":[],"_id":"clttl4al00004rb4k186cc9s1","content":"<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>\n<p>LLM的长文本能力现在已经是各个大模型巨头的必争之地。</p>\n<p>我们之前在<a href=\"http://www.linsight.cn/c4da56c0.html\">《LLM长上下文的问题》</a>简单介绍了目前把大模型理解和生成能力推广到32k+/128k+的主流方法，在<a href=\"http://www.linsight.cn/3dc22f96.html\">《理解Attention:从起源到MHA,MQA和GQA》</a>一文中也解析了MQA和GQA通过节省KV缓存的方式，支持模型在长上下文情况下推理加速的方案。</p>\n<p>在这讲一下另一种（理论有损）提升注意力计算效率的方法：SWA（sliding\nwindow attention）。</p>\n<p>一些效果受到广泛关注的模型，如Qwen系列和Mistral就使用了SWA。</p>\n<p>关于Mistral：</p>\n<p>Mistral\nAI是法国一家AI独角兽公司，2023年5月才成立，但是在2023年9月和12月就分别推出了Mistral\n7B和MoE模型Mistral 8x7B并开源。</p>\n<p>2024年2月，微软也投资了它。</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>它在2024年2月发布的Mistral Large，支持多语言 &amp;\n32k的上下文长度，在MMLU上也是获得了直逼GPT4的效果</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>（大家也因此对Mistral寄予了厚望，希望它能成为大模型行业的鲶鱼，激活一下OPENAI和META加速一下开源。）</p>\n<h1 id=\"swa\">SWA</h1>\n<p>虽然SWA的思路最早不是Mistral提出的，我们还是先以Mistral\n7B为例来看下SWA的具体做法。</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>2023年10月，Mistral发布了Mistral 7B的<a href=\"https://arxiv.org/pdf/2310.06825.pdf\">技术报告</a>。其中开篇就说到，相比Llama，Mistral在结构上做了一些改动，除了GQA，另一个用于支持长文本下高效推理的改动就是SWA。</p>\n<p>来看下Mistral 7B的模型结构参数</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistral使用了kv组数=8的GQA，intermediate\nsize相比Llama2（11008）大一些，其他基本没有太大变化。</p>\n<h2 id=\"计算量和缓存\">计算量和缓存</h2>\n<p>对于原始的causal\nattention，其注意力矩阵是一个下三角矩阵，这样每个token都能看到自己和在自己前面的token。</p>\n<p>这样随着输入长度 <span class=\"math inline\">\\(s\\)</span>\n增大，这个下三角矩阵中1的元素数量以 <span class=\"math inline\">\\(s^2\\)</span> 的速度增长，带来的是计算量和所需的KV\nCache以平方的速度增长。</p>\n<p>（我们知道计算量/缓存和长度 <span class=\"math inline\">\\(s\\)</span>\n成平方关系，这里放一些更具体的推算细节，已经熟悉的朋友可以跳过）</p>\n<p>（1）计算量</p>\n<p>对于两个这样大小的矩阵相乘： <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span> ，输出矩阵大小为 <span class=\"math inline\">\\([m,p]\\)</span>，共有 <span class=\"math inline\">\\(m\\times p\\)</span> 个元素，每个元素需要 <span class=\"math inline\">\\(n\\)</span> 次乘法和 <span class=\"math inline\">\\(n\\)</span> 次加法，因此一次矩阵乘法有 <span class=\"math inline\">\\(2mpn\\)</span> 个floating point\noperations（FLOPs）。</p>\n<p>计算量上，按<a href=\"https://arxiv.org/pdf/2203.15556.pdf\">《Training\nCompute-Optimal Large Language Models》</a>的算法来。</p>\n<p>对于一般MHA，输入长度为 <span class=\"math inline\">\\(s\\)</span>\n，层数为 <span class=\"math inline\">\\(L\\)</span> ，模型hidden size为\n<span class=\"math inline\">\\(d_{model}\\)</span> ，每个头的维度为 <span class=\"math inline\">\\(d_{q}\\)</span> ， 头的数量为 <span class=\"math inline\">\\(n_{q}\\)</span>（这里假设有 <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\n），各个operation的FLOPs如下</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPs（MHA）</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns^2\\times h_{model}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(3\\times\nn_{q}\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns^2\\times h_{model}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax项中，对一个 <span class=\"math inline\">\\([1,s]\\)</span>\n的向量做softmax，计算量为 <span class=\"math inline\">\\(3s\\)</span> （一个\n<span class=\"math inline\">\\(s\\)</span> 是算每个元素的exp，一个 <span class=\"math inline\">\\(s\\)</span> 是求和算分母，一个 <span class=\"math inline\">\\(s\\)</span> 是算除法），而对 <span class=\"math inline\">\\([s,s]\\)</span> 的矩阵做softmax，则计算量为 <span class=\"math inline\">\\(3s^2\\)</span> ，每个头都要计算一遍，因此再乘以\n<span class=\"math inline\">\\(n_{q}\\)</span> 。</p>\n<p>（这里忽略了其他一些operation，比如scaling，dropout等，有兴趣的朋友可以自己推算一下）</p>\n<p>顺便算下对于Mistral 7B这样使用了GQA的情况。</p>\n<p>其实只有第一项的KV有变化，其他都没变。假设kv头的数量为 <span class=\"math inline\">\\(n_{kv}\\)</span>，则有</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPs（GQA）</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>从上面的推算可以看到QK logits、Softmax和Reduction三项是和长度 <span class=\"math inline\">\\(s\\)</span> 成平方关系的，其他则是线性关系。</p>\n<p>（2）缓存</p>\n<p>KV Cache需要缓存的参数量为</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>如果使用的是半精度浮点数，那么总共所需的空间就是</p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>对于Mistral 7B，在输入长度为16k的情况下，所需的KV_Cache约为2G。</p>\n<p>看来虽然用了GQA，但是在长文本（16k+）的情况下计算量和显存还是颇有压力。</p>\n<h2 id=\"swa思路\">SWA思路</h2>\n<p>看来要提升attention计算效率，需要想办法减小上面推算中的 <span class=\"math inline\">\\(s\\)</span> ，但是怎么在减小 <span class=\"math inline\">\\(s\\)</span>\n的同时，还能保持模型长上下文的理解和生成能力呢？</p>\n<p>来看一下，CNN中的感受野</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>如上图，假设模型有3层，每层卷积核大小为 <span class=\"math inline\">\\(3\\times 3\\)</span>\n（实际上CNN里卷积操作就是一个sliding window）。</p>\n<p>那对于layer 3，每一个像素能看到layer 2中的一个 <span class=\"math inline\">\\(3\\times 3\\)</span> 的区域，layer\n2中其他较远的像素就看到不了。</p>\n<p>但我们再往前推，layer 2里的每个像素也可以看到layer 1中的一个 <span class=\"math inline\">\\(3\\times 3\\)</span> 区域，那么layer 2中的 <span class=\"math inline\">\\(3\\times 3\\)</span> 区域就可以看到layer 1中一个\n<span class=\"math inline\">\\(5\\times 5\\)</span> 的区域，相当于layer\n3中一个像素可以<u><strong>间接</strong></u>看到一个 <span class=\"math inline\">\\(5\\times 5\\)</span> 的输入。</p>\n<p>以此类推，如果我们再增加一层layer 4，那么layer\n4中一个像素就能获取输入层（layer 1） 一个 <span class=\"math inline\">\\(7\\times 7\\)</span> 区域的信息。</p>\n<p>虽然每层只能多看周围一格的信息，但是只要我们层数够多，理论上靠近输出端的层想看多远就能看多远。</p>\n<p>值得注意的一点是，我们一般认为模型低层部分提取比较基础的特征，而高层会提取高级的语义特征。</p>\n<p>在CNN里，前几层提取的可能更多是关于简单的边界、颜色、形状等基础特征，而后面的层则提取较复杂的语义特征，比如在分类任务中会是和分类类别相关的花纹、物体大小、风格等特征。</p>\n<p>如果我们把模型设计成，最后一层的一个像素刚好要到第一层才能接收到全局信息（在其它层都只能看到局部），那对于图像边缘的语义特征识别能力可能会受到一些限制。</p>\n<p>具体来说，假设我们做猫和狗的图像分类任务，如果这个时候决定性的特征出现在图像最边缘几个像素里，那这种情况下的错误率会比特征出现在图像中间时要高。</p>\n<p>而对于语言模型，一般情况下，越远距离的信息，对当前位置的重要性越低，因此只要我们的窗口大小不要太过极限小，问题应该都还不大。</p>\n<p>看下Mistral的SWA具体是怎么做的</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>左边是正常的causal\nattention，每个位置能看到自己和前面的位置，attention\nmask是个下三角矩阵。</p>\n<p>中间则是SWA的attention\nmask，这里的窗口大小为3。包括自己在内，每个位置只能往前看3个输入。</p>\n<p>同CNN的感受野一样，随着层数的堆叠，模型理论上能处理的最远距离也逐层线性递增。只是LLM里递增的方向是单向的，只能往前。</p>\n<p>Mistral 7B使用了4096的窗口大小，模型层数为32，则最终输出的”感受野“为\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n达到131k的长度。</p>\n<p>前面我们推算了attention的计算量，其中QK\nlogits、Softmax和Reduction三项是和长度 <span class=\"math inline\">\\(s\\)</span>\n成平方关系。在使用了SWA之后，理论上，这几个operation仅使用4k的计算量，就能获得131k的上下文效果。当输入长度为131k时，除去已经缓存部分的数值，新的输入计算量相差\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> 倍。</p>\n<p>而缓存和上下文长度 <span class=\"math inline\">\\(s\\)</span>\n成线性关系，当上下文长度为131k时，最大也能节省 <span class=\"math inline\">\\(31/32\\)</span> 的显存。</p>\n<p>即SWA在上下文长度在4k以下时，和普通causal\nattention一样；当上下文长度超过4k时，则相对节省资源，长度越大，节省的比例越高。</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>实际使用中，Mistral通过把SWA实现在FlashAttention和xFormers中，对于16k的上下文长度，获得了2倍的速度提升。</p>\n<h2 id=\"和kv-cache的配合实现\">和KV Cache的配合实现</h2>\n<p>在不使用sliding window的情况下，随着自回归推理的进行，KV\nCache是只增不减的。</p>\n<p>而在使用SWA的情况下，超出窗口长度的kv就可以不用再缓存了，因此使用一个轮转替换的策略。</p>\n<p>比如窗口大小 <span class=\"math inline\">\\(W=4\\)</span>\n，则当第5个token需要缓存是，直接替换掉第1个token，这样就可以保持kv缓存有一个最大值（为窗口大小），而不会无限增长。</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>这样便于我们估计硬件设备所能支持的throughput，也不会因为少量超长的case而造成堵塞，在工程上有利于提高硬件利用率，降低成本。</p>\n<h2 id=\"长prompt的分块\">长Prompt的分块</h2>\n<p>更近一步，考虑到我们使用RAG或者funciton\ncall的时候，都会使用比较长的，固定的prompt来知道模型的行为。</p>\n<p>比如GPT4就被诱导说出它接收到的长system\nprompt（当然未必真的就是OPENAI用的）</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: 【{message idx}†{link text}】. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>除了预先计算好system\nprompt的kv值，并保存在缓存中方便每次用户输入使用外，如果system\nprompt很长（比sliding window大），还可以通过对system\nprompt的kv值进行切分来进一步优化计算。</p>\n<p>比如窗口大小 <span class=\"math inline\">\\(W=4\\)</span>，system\nprompt大小为9时，就可以把system prompt的kv缓存切成 [4,4,1] 三块。</p>\n<p>第一块由于和当前的输入距离超过了一个window的大小，所以是完全看不见的，对应的attention\nmask全为0，因此可以完全忽略。</p>\n<p>第二块的attention\nmask则是一个上三角矩阵，当前的输入需要用到这部分信息。</p>\n<p>第三块是一个下三角矩阵（的左边部分），包含了当前的输入在内。</p>\n<p>在推理的时候，我们只需要用到第二块和第三块的内容，这就节省了缓存的操作。</p>\n<p>而且无论prompt有多长，只要我们按窗口大小分块，一定只会用到最后两块。</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>（实际上现在推理框架基本上都有FlashAttention/PagedAttention等技术加持，能够进一步节省资源，提高效率，这个后续再开一篇讲）</p>\n<p>Mistral\n7B整体的效果上的效果相比Llama是有优势的，部分任务甚至超过了Llama\n34B。</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral认为大语言模型压缩知识的能力实际超过我们的认知，7B这个规模的效果还有提升空间。</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWA实际上是一种sparse attention，而sparse\nattention也有许多工作做了深入探索。</p>\n<p>这里简单说一小部分，有机会再完整梳理一遍sparse\nattention的理论和实践。</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>前面提到，Mistral并不是第一个使用SWA的。</p>\n<p>2020年，<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">《Longformer:\nThe Long-Document Transformer》</a>就提出包含SWA在内的一系列sparse\nattention的做法。</p>\n<p>从文章名字就看到出来，Longformer主要目的也是为了解决长上下文的问题。</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>上图中的（b）就是SWA，只是用在Bert中的时候它是双向的。</p>\n<p>在SWA的基础上，还可以进行空洞滑窗（dilated sliding\nwindow），在不增加计算量的情况下，提升感受野。这也是从空洞卷积（下图）来的灵感了。</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>还可以更进一步优化attention。无论是SWA还是dilated sliding\nwindow，每个位置都只能看到局部的信息。</p>\n<p>但是实际上有些位置就是对全局信息有很高的需求。</p>\n<p>在Bert中，[CLS]\ntoken就常常作为分类token或者相似度向量使用，这种情况下就需要它能获取整个上下文的完整信息。</p>\n<p>而在GPT中，instruction，或者说prompt的部分也对全局信息有更高要求，因为我们希望在整个对话过程中，模型都能遵循我们给出的规则。</p>\n<p>对于这些token，我们让它可以看到其他所有位置，使用完整的global\nattention，而其他位置则使用sliding window，如（d）中所示。</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>无独有偶，同样在2020年，和Longformer差不多在同一时期，也有另外一个通过sparse\nattention来优化长文本效果的工作，<a href=\"https://arxiv.org/abs/2007.14062\">《Big Bird: Transformers for\nLonger Sequences》</a>。</p>\n<p>其中sliding window和global attention结合的思路和Longformer相似。Big\nBird还额外加入了一个random attention的做法。</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p>上图中 <span class=\"math inline\">\\(r=2\\)</span>\n即每个位置使用2个随机注意力。</p>\n<h1 id=\"小结\">小结</h1>\n<p>SWA在优化长上下文的计算效率上有明显的收益。而在模型效果上，目前基本没有看到不可接受的损失。对长上下文有需求的业务，值得探索。</p>\n<p>除了SWA，sparse\nattention还有许多其他探索。目前来看，这些做法都有一定的理论基础，效果也不错。但是阻碍这些方案大规模使用的一个原因就是<big><strong>工程实现</strong></big>，比如如何高效计算global\n+ local attention，在flash attention中能够支持random\nattention，这都是要考虑的内容。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n【2】Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n【3】Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n【4】GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n【5】Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n","length":10769,"excerpt":"","more":"<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>\n<p>LLM的长文本能力现在已经是各个大模型巨头的必争之地。</p>\n<p>我们之前在<a href=\"http://www.linsight.cn/c4da56c0.html\">《LLM长上下文的问题》</a>简单介绍了目前把大模型理解和生成能力推广到32k+/128k+的主流方法，在<a href=\"http://www.linsight.cn/3dc22f96.html\">《理解Attention:从起源到MHA,MQA和GQA》</a>一文中也解析了MQA和GQA通过节省KV缓存的方式，支持模型在长上下文情况下推理加速的方案。</p>\n<p>在这讲一下另一种（理论有损）提升注意力计算效率的方法：SWA（sliding\nwindow attention）。</p>\n<p>一些效果受到广泛关注的模型，如Qwen系列和Mistral就使用了SWA。</p>\n<p>关于Mistral：</p>\n<p>Mistral\nAI是法国一家AI独角兽公司，2023年5月才成立，但是在2023年9月和12月就分别推出了Mistral\n7B和MoE模型Mistral 8x7B并开源。</p>\n<p>2024年2月，微软也投资了它。</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>它在2024年2月发布的Mistral Large，支持多语言 &amp;\n32k的上下文长度，在MMLU上也是获得了直逼GPT4的效果</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>（大家也因此对Mistral寄予了厚望，希望它能成为大模型行业的鲶鱼，激活一下OPENAI和META加速一下开源。）</p>\n<h1 id=\"swa\">SWA</h1>\n<p>虽然SWA的思路最早不是Mistral提出的，我们还是先以Mistral\n7B为例来看下SWA的具体做法。</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>2023年10月，Mistral发布了Mistral 7B的<a href=\"https://arxiv.org/pdf/2310.06825.pdf\">技术报告</a>。其中开篇就说到，相比Llama，Mistral在结构上做了一些改动，除了GQA，另一个用于支持长文本下高效推理的改动就是SWA。</p>\n<p>来看下Mistral 7B的模型结构参数</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistral使用了kv组数=8的GQA，intermediate\nsize相比Llama2（11008）大一些，其他基本没有太大变化。</p>\n<h2 id=\"计算量和缓存\">计算量和缓存</h2>\n<p>对于原始的causal\nattention，其注意力矩阵是一个下三角矩阵，这样每个token都能看到自己和在自己前面的token。</p>\n<p>这样随着输入长度 <span class=\"math inline\">\\(s\\)</span>\n增大，这个下三角矩阵中1的元素数量以 <span class=\"math inline\">\\(s^2\\)</span> 的速度增长，带来的是计算量和所需的KV\nCache以平方的速度增长。</p>\n<p>（我们知道计算量/缓存和长度 <span class=\"math inline\">\\(s\\)</span>\n成平方关系，这里放一些更具体的推算细节，已经熟悉的朋友可以跳过）</p>\n<p>（1）计算量</p>\n<p>对于两个这样大小的矩阵相乘： <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span> ，输出矩阵大小为 <span class=\"math inline\">\\([m,p]\\)</span>，共有 <span class=\"math inline\">\\(m\\times p\\)</span> 个元素，每个元素需要 <span class=\"math inline\">\\(n\\)</span> 次乘法和 <span class=\"math inline\">\\(n\\)</span> 次加法，因此一次矩阵乘法有 <span class=\"math inline\">\\(2mpn\\)</span> 个floating point\noperations（FLOPs）。</p>\n<p>计算量上，按<a href=\"https://arxiv.org/pdf/2203.15556.pdf\">《Training\nCompute-Optimal Large Language Models》</a>的算法来。</p>\n<p>对于一般MHA，输入长度为 <span class=\"math inline\">\\(s\\)</span>\n，层数为 <span class=\"math inline\">\\(L\\)</span> ，模型hidden size为\n<span class=\"math inline\">\\(d_{model}\\)</span> ，每个头的维度为 <span class=\"math inline\">\\(d_{q}\\)</span> ， 头的数量为 <span class=\"math inline\">\\(n_{q}\\)</span>（这里假设有 <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\n），各个operation的FLOPs如下</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPs（MHA）</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns^2\\times h_{model}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(3\\times\nn_{q}\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns^2\\times h_{model}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax项中，对一个 <span class=\"math inline\">\\([1,s]\\)</span>\n的向量做softmax，计算量为 <span class=\"math inline\">\\(3s\\)</span> （一个\n<span class=\"math inline\">\\(s\\)</span> 是算每个元素的exp，一个 <span class=\"math inline\">\\(s\\)</span> 是求和算分母，一个 <span class=\"math inline\">\\(s\\)</span> 是算除法），而对 <span class=\"math inline\">\\([s,s]\\)</span> 的矩阵做softmax，则计算量为 <span class=\"math inline\">\\(3s^2\\)</span> ，每个头都要计算一遍，因此再乘以\n<span class=\"math inline\">\\(n_{q}\\)</span> 。</p>\n<p>（这里忽略了其他一些operation，比如scaling，dropout等，有兴趣的朋友可以自己推算一下）</p>\n<p>顺便算下对于Mistral 7B这样使用了GQA的情况。</p>\n<p>其实只有第一项的KV有变化，其他都没变。假设kv头的数量为 <span class=\"math inline\">\\(n_{kv}\\)</span>，则有</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPs（GQA）</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>从上面的推算可以看到QK logits、Softmax和Reduction三项是和长度 <span class=\"math inline\">\\(s\\)</span> 成平方关系的，其他则是线性关系。</p>\n<p>（2）缓存</p>\n<p>KV Cache需要缓存的参数量为</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>如果使用的是半精度浮点数，那么总共所需的空间就是</p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>对于Mistral 7B，在输入长度为16k的情况下，所需的KV_Cache约为2G。</p>\n<p>看来虽然用了GQA，但是在长文本（16k+）的情况下计算量和显存还是颇有压力。</p>\n<h2 id=\"swa思路\">SWA思路</h2>\n<p>看来要提升attention计算效率，需要想办法减小上面推算中的 <span class=\"math inline\">\\(s\\)</span> ，但是怎么在减小 <span class=\"math inline\">\\(s\\)</span>\n的同时，还能保持模型长上下文的理解和生成能力呢？</p>\n<p>来看一下，CNN中的感受野</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>如上图，假设模型有3层，每层卷积核大小为 <span class=\"math inline\">\\(3\\times 3\\)</span>\n（实际上CNN里卷积操作就是一个sliding window）。</p>\n<p>那对于layer 3，每一个像素能看到layer 2中的一个 <span class=\"math inline\">\\(3\\times 3\\)</span> 的区域，layer\n2中其他较远的像素就看到不了。</p>\n<p>但我们再往前推，layer 2里的每个像素也可以看到layer 1中的一个 <span class=\"math inline\">\\(3\\times 3\\)</span> 区域，那么layer 2中的 <span class=\"math inline\">\\(3\\times 3\\)</span> 区域就可以看到layer 1中一个\n<span class=\"math inline\">\\(5\\times 5\\)</span> 的区域，相当于layer\n3中一个像素可以<u><strong>间接</strong></u>看到一个 <span class=\"math inline\">\\(5\\times 5\\)</span> 的输入。</p>\n<p>以此类推，如果我们再增加一层layer 4，那么layer\n4中一个像素就能获取输入层（layer 1） 一个 <span class=\"math inline\">\\(7\\times 7\\)</span> 区域的信息。</p>\n<p>虽然每层只能多看周围一格的信息，但是只要我们层数够多，理论上靠近输出端的层想看多远就能看多远。</p>\n<p>值得注意的一点是，我们一般认为模型低层部分提取比较基础的特征，而高层会提取高级的语义特征。</p>\n<p>在CNN里，前几层提取的可能更多是关于简单的边界、颜色、形状等基础特征，而后面的层则提取较复杂的语义特征，比如在分类任务中会是和分类类别相关的花纹、物体大小、风格等特征。</p>\n<p>如果我们把模型设计成，最后一层的一个像素刚好要到第一层才能接收到全局信息（在其它层都只能看到局部），那对于图像边缘的语义特征识别能力可能会受到一些限制。</p>\n<p>具体来说，假设我们做猫和狗的图像分类任务，如果这个时候决定性的特征出现在图像最边缘几个像素里，那这种情况下的错误率会比特征出现在图像中间时要高。</p>\n<p>而对于语言模型，一般情况下，越远距离的信息，对当前位置的重要性越低，因此只要我们的窗口大小不要太过极限小，问题应该都还不大。</p>\n<p>看下Mistral的SWA具体是怎么做的</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>左边是正常的causal\nattention，每个位置能看到自己和前面的位置，attention\nmask是个下三角矩阵。</p>\n<p>中间则是SWA的attention\nmask，这里的窗口大小为3。包括自己在内，每个位置只能往前看3个输入。</p>\n<p>同CNN的感受野一样，随着层数的堆叠，模型理论上能处理的最远距离也逐层线性递增。只是LLM里递增的方向是单向的，只能往前。</p>\n<p>Mistral 7B使用了4096的窗口大小，模型层数为32，则最终输出的”感受野“为\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n达到131k的长度。</p>\n<p>前面我们推算了attention的计算量，其中QK\nlogits、Softmax和Reduction三项是和长度 <span class=\"math inline\">\\(s\\)</span>\n成平方关系。在使用了SWA之后，理论上，这几个operation仅使用4k的计算量，就能获得131k的上下文效果。当输入长度为131k时，除去已经缓存部分的数值，新的输入计算量相差\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> 倍。</p>\n<p>而缓存和上下文长度 <span class=\"math inline\">\\(s\\)</span>\n成线性关系，当上下文长度为131k时，最大也能节省 <span class=\"math inline\">\\(31/32\\)</span> 的显存。</p>\n<p>即SWA在上下文长度在4k以下时，和普通causal\nattention一样；当上下文长度超过4k时，则相对节省资源，长度越大，节省的比例越高。</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>实际使用中，Mistral通过把SWA实现在FlashAttention和xFormers中，对于16k的上下文长度，获得了2倍的速度提升。</p>\n<h2 id=\"和kv-cache的配合实现\">和KV Cache的配合实现</h2>\n<p>在不使用sliding window的情况下，随着自回归推理的进行，KV\nCache是只增不减的。</p>\n<p>而在使用SWA的情况下，超出窗口长度的kv就可以不用再缓存了，因此使用一个轮转替换的策略。</p>\n<p>比如窗口大小 <span class=\"math inline\">\\(W=4\\)</span>\n，则当第5个token需要缓存是，直接替换掉第1个token，这样就可以保持kv缓存有一个最大值（为窗口大小），而不会无限增长。</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>这样便于我们估计硬件设备所能支持的throughput，也不会因为少量超长的case而造成堵塞，在工程上有利于提高硬件利用率，降低成本。</p>\n<h2 id=\"长prompt的分块\">长Prompt的分块</h2>\n<p>更近一步，考虑到我们使用RAG或者funciton\ncall的时候，都会使用比较长的，固定的prompt来知道模型的行为。</p>\n<p>比如GPT4就被诱导说出它接收到的长system\nprompt（当然未必真的就是OPENAI用的）</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: 【{message idx}†{link text}】. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>除了预先计算好system\nprompt的kv值，并保存在缓存中方便每次用户输入使用外，如果system\nprompt很长（比sliding window大），还可以通过对system\nprompt的kv值进行切分来进一步优化计算。</p>\n<p>比如窗口大小 <span class=\"math inline\">\\(W=4\\)</span>，system\nprompt大小为9时，就可以把system prompt的kv缓存切成 [4,4,1] 三块。</p>\n<p>第一块由于和当前的输入距离超过了一个window的大小，所以是完全看不见的，对应的attention\nmask全为0，因此可以完全忽略。</p>\n<p>第二块的attention\nmask则是一个上三角矩阵，当前的输入需要用到这部分信息。</p>\n<p>第三块是一个下三角矩阵（的左边部分），包含了当前的输入在内。</p>\n<p>在推理的时候，我们只需要用到第二块和第三块的内容，这就节省了缓存的操作。</p>\n<p>而且无论prompt有多长，只要我们按窗口大小分块，一定只会用到最后两块。</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>（实际上现在推理框架基本上都有FlashAttention/PagedAttention等技术加持，能够进一步节省资源，提高效率，这个后续再开一篇讲）</p>\n<p>Mistral\n7B整体的效果上的效果相比Llama是有优势的，部分任务甚至超过了Llama\n34B。</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral认为大语言模型压缩知识的能力实际超过我们的认知，7B这个规模的效果还有提升空间。</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWA实际上是一种sparse attention，而sparse\nattention也有许多工作做了深入探索。</p>\n<p>这里简单说一小部分，有机会再完整梳理一遍sparse\nattention的理论和实践。</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>前面提到，Mistral并不是第一个使用SWA的。</p>\n<p>2020年，<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">《Longformer:\nThe Long-Document Transformer》</a>就提出包含SWA在内的一系列sparse\nattention的做法。</p>\n<p>从文章名字就看到出来，Longformer主要目的也是为了解决长上下文的问题。</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>上图中的（b）就是SWA，只是用在Bert中的时候它是双向的。</p>\n<p>在SWA的基础上，还可以进行空洞滑窗（dilated sliding\nwindow），在不增加计算量的情况下，提升感受野。这也是从空洞卷积（下图）来的灵感了。</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>还可以更进一步优化attention。无论是SWA还是dilated sliding\nwindow，每个位置都只能看到局部的信息。</p>\n<p>但是实际上有些位置就是对全局信息有很高的需求。</p>\n<p>在Bert中，[CLS]\ntoken就常常作为分类token或者相似度向量使用，这种情况下就需要它能获取整个上下文的完整信息。</p>\n<p>而在GPT中，instruction，或者说prompt的部分也对全局信息有更高要求，因为我们希望在整个对话过程中，模型都能遵循我们给出的规则。</p>\n<p>对于这些token，我们让它可以看到其他所有位置，使用完整的global\nattention，而其他位置则使用sliding window，如（d）中所示。</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>无独有偶，同样在2020年，和Longformer差不多在同一时期，也有另外一个通过sparse\nattention来优化长文本效果的工作，<a href=\"https://arxiv.org/abs/2007.14062\">《Big Bird: Transformers for\nLonger Sequences》</a>。</p>\n<p>其中sliding window和global attention结合的思路和Longformer相似。Big\nBird还额外加入了一个random attention的做法。</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p>上图中 <span class=\"math inline\">\\(r=2\\)</span>\n即每个位置使用2个随机注意力。</p>\n<h1 id=\"小结\">小结</h1>\n<p>SWA在优化长上下文的计算效率上有明显的收益。而在模型效果上，目前基本没有看到不可接受的损失。对长上下文有需求的业务，值得探索。</p>\n<p>除了SWA，sparse\nattention还有许多其他探索。目前来看，这些做法都有一定的理论基础，效果也不错。但是阻碍这些方案大规模使用的一个原因就是<big><strong>工程实现</strong></big>，比如如何高效计算global\n+ local attention，在flash attention中能够支持random\nattention，这都是要考虑的内容。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n【2】Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n【3】Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n【4】GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n【5】Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n"},{"title":"理解LLM位置编码:RoPE","abbrlink":"a051710f","date":"2024-02-21T13:18:13.000Z","mathjax":true,"_content":"\n最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。\n\n# 关于RoPE\n\nRoPE（Rotary Position Embedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u>**以绝对位置编码形式实现的相对位置编码**</u></big>，兼顾了模型性能和效率。\n\n2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。\n\n苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。  \n\n# 以绝对位置编码的方式实现相对位置编码\n\n前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？\n\n先说原因：  \n\n在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。  \n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。  \n而使用相对位置编码则<u>**更容易外推**</u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。  \n但是传统相对位置编码的实现相对<u>**复杂**</u>，有些也会有<u>**计算效率低**</u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u>**线性注意力**</u>计算法模型中。  \n总结来说，就是绝对位置编码<u>**好实现**</u>，<u>**效率高**</u>，<u>**适用线性注意力**</u>，而相对位置编码<u>**易外推**</u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。  \n\n下面简单回顾一下绝对位置编码和相对位置编码。  \n\n（对位置编码比较熟悉的朋友可以直接跳到第3节。）  \n\n## 绝对位置编码\n\n先回顾一下带绝对位置编码的self-attention。  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$ 和 $x_j$ 分别是位置 $i$ 和 $j$ 的输入，$p$ 是对应位置的位置编码向量。  \n\n这里的位置编码$p$可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量 $x$ 和位置向量 $p$ 相加即可，相比attention中的softmax计算，element-wise addition操作的计算量非常小，是可以忽略不计的。\n\n大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把 $x + p$ 变成  $x * p$ 这样，效果上也是大差不差。\n\n## 相对位置编码\n\n在绝对位置编码中，可以在输入阶段就把 $x$ 和 $p$ 直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。  \n\n比如“我”这个词放在位置1时，形成一个 $e_1 = x_我 + p_1$ 这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量 $e_8 = x_我 + p_8$ 。两个向量 $e_1$ 和 $e_8$ 虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u>**耦合**</u>在一起共同构成了一个完整的输入。  \n\n直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。\n\n扯远了，现在回来看一下相对位置编码。把公式（1）中的 $q_{i}k_{j}^{T}$展开来  \n\n$$\\begin{align*}q_1k_j^\\top&=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n和位置相关的有 $p_iW_\\mathbb{Q}$ 和 $W_K^\\top p_j^\\top$ 两项。  \n\n### Google式\n\n在最早引入相对位置编码的Google的论文《Self-Attention with Relative Position Representations》中，把第一项 $p_iW_\\mathbb{Q}$ 去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置 $j$），把第二项 $W_K^\\top p_j^\\top$ 改成和位置 $i$、$j$ 都相关的位置向量 $R_{ij}^K$，于是在这个使用相对位置编码的attention计算中，<u>**不再是直接计算input projection的内积来获取权重**</u>，而变成  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ 是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n其中 $p_\\mathrm{K}$ 就是可训练的向量或者三角函数向量。 \n\n为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系**理应**更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到>256的长度。\n\n本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle的方法把 $p_{j}W_{\\mathrm{V}}$ 也改成了包含相对位置信息的向量\n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$ 和 $R_{ij}^K$ 相似，都是一个相对位置向量 + clip操作。\n\n### XLNET式\n\nXLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。  \n\n在公式（2）的基础上继续展开  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n把绝对位置相关的几个参数改成相对位置相关的参数，变成：\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n把 $p_i$ 变成了两个可训练的向量 $u$ 和 $\\nu$ ，把 $p_j$ 变成相对位置向量 $R_{i-j}^\\top$ 。  \n\n实际实现上可以把 $u$ 和 $\\nu$ 后面跟着的矩阵省掉了，去掉这个线性变化不影响 $u$ 和 $\\nu$ 的训练，变成\n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\n此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\n可以看到，Google式和XLNET式的相对位置编码在权重 $\\mathrm{a_{i,j}}$ 的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置 $i$ 、 $j$ 都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。\n\n当然，也有简单一点的实现，比如T5的方法。  \n\n### T5式\n\n公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置 $i$ 和位置 $j$ 的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\n（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）\n\n## 对比\n\n看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。  \n\n公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法\n\n从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。  \n\n总之在实现方式上和计算效率上，绝对位置编码具有一些优势。\n\n而在输入输出窗口外推方面，相对位置编码有着天然的优势。\n\n另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear attention方案中去，这个以后再展开讲（又挖了个坑）。  \n\n# RoPE的设计思路\n\n## 保持attention计算形式\n\n回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。\n\n先说设计思路：  \n\n首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端 = 内积 + softmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。  \n\n也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n其中 $q_m$ 是在位置 $m$ 的query向量，$k_n$ 是在位置 $n$ 的key向量，$f_q$ 和 $f_k$ 是分别针对这query和key向量的操作函数。  \n\n我们的任务就是要找到一组 $f_q$ 、 $f_k$ 和 $g$ ，使得公式（11）恒成立。  \n\n当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？  \n\n## 借用复数寻找组合\n\n式（11）中， $g$ 的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。\n\n这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量  \n\n{% asset_img complex_number.png 282 401 复数平面 %}\n\n现在考虑query和key向量都是2维的情况，那么可以代入复数的操作  \n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）  \n\n那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n其中 $\\boldsymbol{k}_n^*$ 是 $\\boldsymbol{k}_n$ 的共轭复数。  \n\n（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）\n\n共轭复数是这样的关系  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n先证明一下这个组合的正确性，是不是真的满足公式（11）。  \n\n（也可以先跳过证明，选择先相信这个组合）  \n\n回顾一下欧拉公式  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n因为现在我们讨论的是2维的情况，那2维向量 $q_m$ 可以用一个复数来表示  \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n那从复数角度来看，就有\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）  \n\n类似地，有\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n和\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n则有  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n用了三角函数和差公式\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n再看 $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n证毕。\n\n## “旋转”位置编码\n\n发现式（17）可以写成这样\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n同样地  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n如果从向量视角来看，则有  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n看式（22）和（23），可以看到等号右边都有  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n这正是一个二维平面的旋转矩阵。 $f_q$ 、 $f_k$ 的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。  \n\n这也是为什么叫做“旋转”位置编码。\n\n## 从2维推广到高维\n\n我们现在已经确认，对于2维的情况，经过 $f_q$ 、 $f_k$ 和 $g$ 这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？  \n\n答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$ 是的输入向量的维度，由于是两个两个一组，所以一共有 $d/2$ 组小旋转矩阵，这 $d/2$ 组矩阵为了区分，设计使用了不同的 $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n那么在实际操作的时候，给位置 $m$ 和位置 $n$ 的输入向量分别乘以 $R_m$ 和 $R_n$，再进行self-attention，就能获得仅使用相对位置信息编码的效果。  \n\n另外 $\\theta$ 是怎么来的呢？这里是参考了Google最初在《Attention is All You Need》中提出的，这里就先不展开了，可以看看论文原文。\n\n## 高效率实现\n\n式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\n只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。  \n\n另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。\n\n## 远程衰减的特性\n\n至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。  \n\n直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。\n\n回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个 $\\theta$ 的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。  \n\n证明过程这里就偷偷懒略过了，具体可以看[Roformer的论文](https://arxiv.org/abs/2104.09864)或者[苏神的博客](https://spaces.ac.cn/archives/8265)。  \n\n当 $d = 128$ 时，画出来的图像如下\n\n{% asset_img remote_attenuation.png 775 457 远程衰减 %}  \n\n# 小结  \n\n总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。\n\n# Reference\n【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130  \n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265  \n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n【4】十分钟读懂旋转编码（RoPE） https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLM位置编码RoPE.md","raw":"---\ntitle: 理解LLM位置编码:RoPE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - positional encoding\n  - RoPE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: a051710f\ndate: 2024-02-21 21:18:13\nmathjax: true\n---\n\n最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。\n\n# 关于RoPE\n\nRoPE（Rotary Position Embedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u>**以绝对位置编码形式实现的相对位置编码**</u></big>，兼顾了模型性能和效率。\n\n2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。\n\n苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。  \n\n# 以绝对位置编码的方式实现相对位置编码\n\n前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？\n\n先说原因：  \n\n在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。  \n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。  \n而使用相对位置编码则<u>**更容易外推**</u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。  \n但是传统相对位置编码的实现相对<u>**复杂**</u>，有些也会有<u>**计算效率低**</u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u>**线性注意力**</u>计算法模型中。  \n总结来说，就是绝对位置编码<u>**好实现**</u>，<u>**效率高**</u>，<u>**适用线性注意力**</u>，而相对位置编码<u>**易外推**</u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。  \n\n下面简单回顾一下绝对位置编码和相对位置编码。  \n\n（对位置编码比较熟悉的朋友可以直接跳到第3节。）  \n\n## 绝对位置编码\n\n先回顾一下带绝对位置编码的self-attention。  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$ 和 $x_j$ 分别是位置 $i$ 和 $j$ 的输入，$p$ 是对应位置的位置编码向量。  \n\n这里的位置编码$p$可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量 $x$ 和位置向量 $p$ 相加即可，相比attention中的softmax计算，element-wise addition操作的计算量非常小，是可以忽略不计的。\n\n大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把 $x + p$ 变成  $x * p$ 这样，效果上也是大差不差。\n\n## 相对位置编码\n\n在绝对位置编码中，可以在输入阶段就把 $x$ 和 $p$ 直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。  \n\n比如“我”这个词放在位置1时，形成一个 $e_1 = x_我 + p_1$ 这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量 $e_8 = x_我 + p_8$ 。两个向量 $e_1$ 和 $e_8$ 虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u>**耦合**</u>在一起共同构成了一个完整的输入。  \n\n直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。\n\n扯远了，现在回来看一下相对位置编码。把公式（1）中的 $q_{i}k_{j}^{T}$展开来  \n\n$$\\begin{align*}q_1k_j^\\top&=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n和位置相关的有 $p_iW_\\mathbb{Q}$ 和 $W_K^\\top p_j^\\top$ 两项。  \n\n### Google式\n\n在最早引入相对位置编码的Google的论文《Self-Attention with Relative Position Representations》中，把第一项 $p_iW_\\mathbb{Q}$ 去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置 $j$），把第二项 $W_K^\\top p_j^\\top$ 改成和位置 $i$、$j$ 都相关的位置向量 $R_{ij}^K$，于是在这个使用相对位置编码的attention计算中，<u>**不再是直接计算input projection的内积来获取权重**</u>，而变成  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ 是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n其中 $p_\\mathrm{K}$ 就是可训练的向量或者三角函数向量。 \n\n为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系**理应**更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到>256的长度。\n\n本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle的方法把 $p_{j}W_{\\mathrm{V}}$ 也改成了包含相对位置信息的向量\n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$ 和 $R_{ij}^K$ 相似，都是一个相对位置向量 + clip操作。\n\n### XLNET式\n\nXLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。  \n\n在公式（2）的基础上继续展开  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n把绝对位置相关的几个参数改成相对位置相关的参数，变成：\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n把 $p_i$ 变成了两个可训练的向量 $u$ 和 $\\nu$ ，把 $p_j$ 变成相对位置向量 $R_{i-j}^\\top$ 。  \n\n实际实现上可以把 $u$ 和 $\\nu$ 后面跟着的矩阵省掉了，去掉这个线性变化不影响 $u$ 和 $\\nu$ 的训练，变成\n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\n此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\n可以看到，Google式和XLNET式的相对位置编码在权重 $\\mathrm{a_{i,j}}$ 的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置 $i$ 、 $j$ 都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。\n\n当然，也有简单一点的实现，比如T5的方法。  \n\n### T5式\n\n公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置 $i$ 和位置 $j$ 的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\n（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）\n\n## 对比\n\n看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。  \n\n公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法\n\n从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。  \n\n总之在实现方式上和计算效率上，绝对位置编码具有一些优势。\n\n而在输入输出窗口外推方面，相对位置编码有着天然的优势。\n\n另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear attention方案中去，这个以后再展开讲（又挖了个坑）。  \n\n# RoPE的设计思路\n\n## 保持attention计算形式\n\n回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。\n\n先说设计思路：  \n\n首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端 = 内积 + softmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。  \n\n也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n其中 $q_m$ 是在位置 $m$ 的query向量，$k_n$ 是在位置 $n$ 的key向量，$f_q$ 和 $f_k$ 是分别针对这query和key向量的操作函数。  \n\n我们的任务就是要找到一组 $f_q$ 、 $f_k$ 和 $g$ ，使得公式（11）恒成立。  \n\n当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？  \n\n## 借用复数寻找组合\n\n式（11）中， $g$ 的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。\n\n这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量  \n\n{% asset_img complex_number.png 282 401 复数平面 %}\n\n现在考虑query和key向量都是2维的情况，那么可以代入复数的操作  \n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）  \n\n那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n其中 $\\boldsymbol{k}_n^*$ 是 $\\boldsymbol{k}_n$ 的共轭复数。  \n\n（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）\n\n共轭复数是这样的关系  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n先证明一下这个组合的正确性，是不是真的满足公式（11）。  \n\n（也可以先跳过证明，选择先相信这个组合）  \n\n回顾一下欧拉公式  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n因为现在我们讨论的是2维的情况，那2维向量 $q_m$ 可以用一个复数来表示  \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n那从复数角度来看，就有\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）  \n\n类似地，有\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n和\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n则有  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n用了三角函数和差公式\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n再看 $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n证毕。\n\n## “旋转”位置编码\n\n发现式（17）可以写成这样\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n同样地  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n如果从向量视角来看，则有  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n看式（22）和（23），可以看到等号右边都有  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n这正是一个二维平面的旋转矩阵。 $f_q$ 、 $f_k$ 的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。  \n\n这也是为什么叫做“旋转”位置编码。\n\n## 从2维推广到高维\n\n我们现在已经确认，对于2维的情况，经过 $f_q$ 、 $f_k$ 和 $g$ 这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？  \n\n答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$ 是的输入向量的维度，由于是两个两个一组，所以一共有 $d/2$ 组小旋转矩阵，这 $d/2$ 组矩阵为了区分，设计使用了不同的 $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n那么在实际操作的时候，给位置 $m$ 和位置 $n$ 的输入向量分别乘以 $R_m$ 和 $R_n$，再进行self-attention，就能获得仅使用相对位置信息编码的效果。  \n\n另外 $\\theta$ 是怎么来的呢？这里是参考了Google最初在《Attention is All You Need》中提出的，这里就先不展开了，可以看看论文原文。\n\n## 高效率实现\n\n式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\n只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。  \n\n另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。\n\n## 远程衰减的特性\n\n至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。  \n\n直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。\n\n回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个 $\\theta$ 的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。  \n\n证明过程这里就偷偷懒略过了，具体可以看[Roformer的论文](https://arxiv.org/abs/2104.09864)或者[苏神的博客](https://spaces.ac.cn/archives/8265)。  \n\n当 $d = 128$ 时，画出来的图像如下\n\n{% asset_img remote_attenuation.png 775 457 远程衰减 %}  \n\n# 小结  \n\n总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。\n\n# Reference\n【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130  \n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265  \n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n【4】十分钟读懂旋转编码（RoPE） https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLM位置编码RoPE","published":1,"updated":"2024-02-27T06:39:30.683Z","comments":1,"layout":"post","photos":[],"_id":"clttl4al10007rb4k4nyqhg3o","content":"<p>最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。</p>\n<h1 id=\"关于rope\">关于RoPE</h1>\n<p>RoPE（Rotary Position\nEmbedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u><strong>以绝对位置编码形式实现的相对位置编码</strong></u></big>，兼顾了模型性能和效率。</p>\n<p>2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。</p>\n<p>苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。</p>\n<h1 id=\"以绝对位置编码的方式实现相对位置编码\">以绝对位置编码的方式实现相对位置编码</h1>\n<p>前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？</p>\n<p>先说原因：</p>\n<p>在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。<br>\n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。<br>\n而使用相对位置编码则<u><strong>更容易外推</strong></u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。<br>\n但是传统相对位置编码的实现相对<u><strong>复杂</strong></u>，有些也会有<u><strong>计算效率低</strong></u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u><strong>线性注意力</strong></u>计算法模型中。<br>\n总结来说，就是绝对位置编码<u><strong>好实现</strong></u>，<u><strong>效率高</strong></u>，<u><strong>适用线性注意力</strong></u>，而相对位置编码<u><strong>易外推</strong></u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。</p>\n<p>下面简单回顾一下绝对位置编码和相对位置编码。</p>\n<p>（对位置编码比较熟悉的朋友可以直接跳到第3节。）</p>\n<h2 id=\"绝对位置编码\">绝对位置编码</h2>\n<p>先回顾一下带绝对位置编码的self-attention。</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span> 和 <span class=\"math inline\">\\(x_j\\)</span> 分别是位置 <span class=\"math inline\">\\(i\\)</span> 和 <span class=\"math inline\">\\(j\\)</span> 的输入，<span class=\"math inline\">\\(p\\)</span> 是对应位置的位置编码向量。</p>\n<p>这里的位置编码<span class=\"math inline\">\\(p\\)</span>可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量\n<span class=\"math inline\">\\(x\\)</span> 和位置向量 <span class=\"math inline\">\\(p\\)</span>\n相加即可，相比attention中的softmax计算，element-wise\naddition操作的计算量非常小，是可以忽略不计的。</p>\n<p>大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把\n<span class=\"math inline\">\\(x + p\\)</span> 变成 <span class=\"math inline\">\\(x * p\\)</span> 这样，效果上也是大差不差。</p>\n<h2 id=\"相对位置编码\">相对位置编码</h2>\n<p>在绝对位置编码中，可以在输入阶段就把 <span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(p\\)</span>\n直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。</p>\n<p>比如“我”这个词放在位置1时，形成一个 <span class=\"math inline\">\\(e_1 =\nx_我 + p_1\\)</span>\n这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量\n<span class=\"math inline\">\\(e_8 = x_我 + p_8\\)</span> 。两个向量 <span class=\"math inline\">\\(e_1\\)</span> 和 <span class=\"math inline\">\\(e_8\\)</span>\n虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u><strong>耦合</strong></u>在一起共同构成了一个完整的输入。</p>\n<p>直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。</p>\n<p>扯远了，现在回来看一下相对位置编码。把公式（1）中的 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span>展开来</p>\n<p><span class=\"math display\">\\[\\begin{align*}q_1k_j^\\top&amp;=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p>和位置相关的有 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n和 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 两项。</p>\n<h3 id=\"google式\">Google式</h3>\n<p>在最早引入相对位置编码的Google的论文《Self-Attention with Relative\nPosition Representations》中，把第一项 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置\n<span class=\"math inline\">\\(j\\)</span>），把第二项 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 改成和位置 <span class=\"math inline\">\\(i\\)</span>、<span class=\"math inline\">\\(j\\)</span>\n都相关的位置向量 <span class=\"math inline\">\\(R_{ij}^K\\)</span>，于是在这个使用相对位置编码的attention计算中，<u><strong>不再是直接计算input\nprojection的内积来获取权重</strong></u>，而变成</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\n是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n就是可训练的向量或者三角函数向量。</p>\n<p>为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系<strong>理应</strong>更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到&gt;256的长度。</p>\n<p>本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google的方法把 <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n也改成了包含相对位置信息的向量</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> 和\n<span class=\"math inline\">\\(R_{ij}^K\\)</span> 相似，都是一个相对位置向量\n+ clip操作。</p>\n<h3 id=\"xlnet式\">XLNET式</h3>\n<p>XLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。</p>\n<p>在公式（2）的基础上继续展开</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p>把绝对位置相关的几个参数改成相对位置相关的参数，变成：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p>把 <span class=\"math inline\">\\(p_i\\)</span> 变成了两个可训练的向量\n<span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> ，把 <span class=\"math inline\">\\(p_j\\)</span> 变成相对位置向量 <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> 。</p>\n<p>实际实现上可以把 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span>\n后面跟着的矩阵省掉了，去掉这个线性变化不影响 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> 的训练，变成</p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>可以看到，Google式和XLNET式的相对位置编码在权重 <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置\n<span class=\"math inline\">\\(i\\)</span> 、 <span class=\"math inline\">\\(j\\)</span>\n都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。</p>\n<p>当然，也有简单一点的实现，比如T5的方法。</p>\n<h3 id=\"t5式\">T5式</h3>\n<p>公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置\n<span class=\"math inline\">\\(i\\)</span> 和位置 <span class=\"math inline\">\\(j\\)</span>\n的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）</p>\n<h2 id=\"对比\">对比</h2>\n<p>看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。</p>\n<p>公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法</p>\n<p>从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。</p>\n<p>总之在实现方式上和计算效率上，绝对位置编码具有一些优势。</p>\n<p>而在输入输出窗口外推方面，相对位置编码有着天然的优势。</p>\n<p>另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear\nattention方案中去，这个以后再展开讲（又挖了个坑）。</p>\n<h1 id=\"rope的设计思路\">RoPE的设计思路</h1>\n<h2 id=\"保持attention计算形式\">保持attention计算形式</h2>\n<p>回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。</p>\n<p>先说设计思路：</p>\n<p>首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端\n= 内积 +\nsoftmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。</p>\n<p>也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是</p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(q_m\\)</span> 是在位置 <span class=\"math inline\">\\(m\\)</span> 的query向量，<span class=\"math inline\">\\(k_n\\)</span> 是在位置 <span class=\"math inline\">\\(n\\)</span> 的key向量，<span class=\"math inline\">\\(f_q\\)</span> 和 <span class=\"math inline\">\\(f_k\\)</span>\n是分别针对这query和key向量的操作函数。</p>\n<p>我们的任务就是要找到一组 <span class=\"math inline\">\\(f_q\\)</span> 、\n<span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span> ，使得公式（11）恒成立。</p>\n<p>当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？</p>\n<h2 id=\"借用复数寻找组合\">借用复数寻找组合</h2>\n<p>式（11）中， <span class=\"math inline\">\\(g\\)</span>\n的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。</p>\n<p>这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"复数平面\">\n<p>现在考虑query和key向量都是2维的情况，那么可以代入复数的操作<br>\n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）</p>\n<p>那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span> 是 <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> 的共轭复数。</p>\n<p>（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）</p>\n<p>共轭复数是这样的关系</p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>先证明一下这个组合的正确性，是不是真的满足公式（11）。</p>\n<p>（也可以先跳过证明，选择先相信这个组合）</p>\n<p>回顾一下欧拉公式</p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>因为现在我们讨论的是2维的情况，那2维向量 <span class=\"math inline\">\\(q_m\\)</span> 可以用一个复数来表示</p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p>那从复数角度来看，就有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）</p>\n<p>类似地，有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p>和</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p>则有<br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p>用了三角函数和差公式 <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p>再看 <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p>证毕。</p>\n<h2 id=\"旋转位置编码\">“旋转”位置编码</h2>\n<p>发现式（17）可以写成这样</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p>同样地</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p>如果从向量视角来看，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>看式（22）和（23），可以看到等号右边都有</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p>这正是一个二维平面的旋转矩阵。 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span>\n的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。</p>\n<p>这也是为什么叫做“旋转”位置编码。</p>\n<h2 id=\"从2维推广到高维\">从2维推广到高维</h2>\n<p>我们现在已经确认，对于2维的情况，经过 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span>\n这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？</p>\n<p>答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n是的输入向量的维度，由于是两个两个一组，所以一共有 <span class=\"math inline\">\\(d/2\\)</span> 组小旋转矩阵，这 <span class=\"math inline\">\\(d/2\\)</span> 组矩阵为了区分，设计使用了不同的\n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p>那么在实际操作的时候，给位置 <span class=\"math inline\">\\(m\\)</span>\n和位置 <span class=\"math inline\">\\(n\\)</span> 的输入向量分别乘以 <span class=\"math inline\">\\(R_m\\)</span> 和 <span class=\"math inline\">\\(R_n\\)</span>，再进行self-attention，就能获得仅使用相对位置信息编码的效果。</p>\n<p>另外 <span class=\"math inline\">\\(\\theta\\)</span>\n是怎么来的呢？这里是参考了Google最初在《Attention is All You\nNeed》中提出的，这里就先不展开了，可以看看论文原文。</p>\n<h2 id=\"高效率实现\">高效率实现</h2>\n<p>式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。</p>\n<p>另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。</p>\n<h2 id=\"远程衰减的特性\">远程衰减的特性</h2>\n<p>至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。</p>\n<p>直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。</p>\n<p>回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个\n<span class=\"math inline\">\\(\\theta\\)</span>\n的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。</p>\n<p>证明过程这里就偷偷懒略过了，具体可以看<a href=\"https://arxiv.org/abs/2104.09864\">Roformer的论文</a>或者<a href=\"https://spaces.ac.cn/archives/8265\">苏神的博客</a>。</p>\n<p>当 <span class=\"math inline\">\\(d = 128\\)</span>\n时，画出来的图像如下</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"远程衰减\">\n<h1 id=\"小结\">小结</h1>\n<p>总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130<br>\n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265<br>\n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n【4】十分钟读懂旋转编码（RoPE）\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":14264,"excerpt":"","more":"<p>最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。</p>\n<h1 id=\"关于rope\">关于RoPE</h1>\n<p>RoPE（Rotary Position\nEmbedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u><strong>以绝对位置编码形式实现的相对位置编码</strong></u></big>，兼顾了模型性能和效率。</p>\n<p>2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。</p>\n<p>苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。</p>\n<h1 id=\"以绝对位置编码的方式实现相对位置编码\">以绝对位置编码的方式实现相对位置编码</h1>\n<p>前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？</p>\n<p>先说原因：</p>\n<p>在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。<br>\n如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。<br>\n而使用相对位置编码则<u><strong>更容易外推</strong></u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。<br>\n但是传统相对位置编码的实现相对<u><strong>复杂</strong></u>，有些也会有<u><strong>计算效率低</strong></u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u><strong>线性注意力</strong></u>计算法模型中。<br>\n总结来说，就是绝对位置编码<u><strong>好实现</strong></u>，<u><strong>效率高</strong></u>，<u><strong>适用线性注意力</strong></u>，而相对位置编码<u><strong>易外推</strong></u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。</p>\n<p>下面简单回顾一下绝对位置编码和相对位置编码。</p>\n<p>（对位置编码比较熟悉的朋友可以直接跳到第3节。）</p>\n<h2 id=\"绝对位置编码\">绝对位置编码</h2>\n<p>先回顾一下带绝对位置编码的self-attention。</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span> 和 <span class=\"math inline\">\\(x_j\\)</span> 分别是位置 <span class=\"math inline\">\\(i\\)</span> 和 <span class=\"math inline\">\\(j\\)</span> 的输入，<span class=\"math inline\">\\(p\\)</span> 是对应位置的位置编码向量。</p>\n<p>这里的位置编码<span class=\"math inline\">\\(p\\)</span>可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量\n<span class=\"math inline\">\\(x\\)</span> 和位置向量 <span class=\"math inline\">\\(p\\)</span>\n相加即可，相比attention中的softmax计算，element-wise\naddition操作的计算量非常小，是可以忽略不计的。</p>\n<p>大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把\n<span class=\"math inline\">\\(x + p\\)</span> 变成 <span class=\"math inline\">\\(x * p\\)</span> 这样，效果上也是大差不差。</p>\n<h2 id=\"相对位置编码\">相对位置编码</h2>\n<p>在绝对位置编码中，可以在输入阶段就把 <span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(p\\)</span>\n直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。</p>\n<p>比如“我”这个词放在位置1时，形成一个 <span class=\"math inline\">\\(e_1 =\nx_我 + p_1\\)</span>\n这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量\n<span class=\"math inline\">\\(e_8 = x_我 + p_8\\)</span> 。两个向量 <span class=\"math inline\">\\(e_1\\)</span> 和 <span class=\"math inline\">\\(e_8\\)</span>\n虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u><strong>耦合</strong></u>在一起共同构成了一个完整的输入。</p>\n<p>直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。</p>\n<p>扯远了，现在回来看一下相对位置编码。把公式（1）中的 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span>展开来</p>\n<p><span class=\"math display\">\\[\\begin{align*}q_1k_j^\\top&amp;=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p>和位置相关的有 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n和 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 两项。</p>\n<h3 id=\"google式\">Google式</h3>\n<p>在最早引入相对位置编码的Google的论文《Self-Attention with Relative\nPosition Representations》中，把第一项 <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置\n<span class=\"math inline\">\\(j\\)</span>），把第二项 <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> 改成和位置 <span class=\"math inline\">\\(i\\)</span>、<span class=\"math inline\">\\(j\\)</span>\n都相关的位置向量 <span class=\"math inline\">\\(R_{ij}^K\\)</span>，于是在这个使用相对位置编码的attention计算中，<u><strong>不再是直接计算input\nprojection的内积来获取权重</strong></u>，而变成</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\n是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n就是可训练的向量或者三角函数向量。</p>\n<p>为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系<strong>理应</strong>更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到&gt;256的长度。</p>\n<p>本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google的方法把 <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n也改成了包含相对位置信息的向量</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> 和\n<span class=\"math inline\">\\(R_{ij}^K\\)</span> 相似，都是一个相对位置向量\n+ clip操作。</p>\n<h3 id=\"xlnet式\">XLNET式</h3>\n<p>XLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。</p>\n<p>在公式（2）的基础上继续展开</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p>把绝对位置相关的几个参数改成相对位置相关的参数，变成：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p>把 <span class=\"math inline\">\\(p_i\\)</span> 变成了两个可训练的向量\n<span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> ，把 <span class=\"math inline\">\\(p_j\\)</span> 变成相对位置向量 <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> 。</p>\n<p>实际实现上可以把 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span>\n后面跟着的矩阵省掉了，去掉这个线性变化不影响 <span class=\"math inline\">\\(u\\)</span> 和 <span class=\"math inline\">\\(\\nu\\)</span> 的训练，变成</p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>可以看到，Google式和XLNET式的相对位置编码在权重 <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置\n<span class=\"math inline\">\\(i\\)</span> 、 <span class=\"math inline\">\\(j\\)</span>\n都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。</p>\n<p>当然，也有简单一点的实现，比如T5的方法。</p>\n<h3 id=\"t5式\">T5式</h3>\n<p>公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置\n<span class=\"math inline\">\\(i\\)</span> 和位置 <span class=\"math inline\">\\(j\\)</span>\n的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）</p>\n<h2 id=\"对比\">对比</h2>\n<p>看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。</p>\n<p>公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法</p>\n<p>从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。</p>\n<p>总之在实现方式上和计算效率上，绝对位置编码具有一些优势。</p>\n<p>而在输入输出窗口外推方面，相对位置编码有着天然的优势。</p>\n<p>另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear\nattention方案中去，这个以后再展开讲（又挖了个坑）。</p>\n<h1 id=\"rope的设计思路\">RoPE的设计思路</h1>\n<h2 id=\"保持attention计算形式\">保持attention计算形式</h2>\n<p>回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。</p>\n<p>先说设计思路：</p>\n<p>首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端\n= 内积 +\nsoftmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。</p>\n<p>也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是</p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(q_m\\)</span> 是在位置 <span class=\"math inline\">\\(m\\)</span> 的query向量，<span class=\"math inline\">\\(k_n\\)</span> 是在位置 <span class=\"math inline\">\\(n\\)</span> 的key向量，<span class=\"math inline\">\\(f_q\\)</span> 和 <span class=\"math inline\">\\(f_k\\)</span>\n是分别针对这query和key向量的操作函数。</p>\n<p>我们的任务就是要找到一组 <span class=\"math inline\">\\(f_q\\)</span> 、\n<span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span> ，使得公式（11）恒成立。</p>\n<p>当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？</p>\n<h2 id=\"借用复数寻找组合\">借用复数寻找组合</h2>\n<p>式（11）中， <span class=\"math inline\">\\(g\\)</span>\n的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。</p>\n<p>这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"复数平面\">\n<p>现在考虑query和key向量都是2维的情况，那么可以代入复数的操作<br>\n（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）</p>\n<p>那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p>其中 <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span> 是 <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> 的共轭复数。</p>\n<p>（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）</p>\n<p>共轭复数是这样的关系</p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>先证明一下这个组合的正确性，是不是真的满足公式（11）。</p>\n<p>（也可以先跳过证明，选择先相信这个组合）</p>\n<p>回顾一下欧拉公式</p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>因为现在我们讨论的是2维的情况，那2维向量 <span class=\"math inline\">\\(q_m\\)</span> 可以用一个复数来表示</p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p>那从复数角度来看，就有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）</p>\n<p>类似地，有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p>和</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p>则有<br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p>用了三角函数和差公式 <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p>再看 <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p>证毕。</p>\n<h2 id=\"旋转位置编码\">“旋转”位置编码</h2>\n<p>发现式（17）可以写成这样</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p>同样地</p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p>如果从向量视角来看，则有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>看式（22）和（23），可以看到等号右边都有</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p>这正是一个二维平面的旋转矩阵。 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span>\n的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。</p>\n<p>这也是为什么叫做“旋转”位置编码。</p>\n<h2 id=\"从2维推广到高维\">从2维推广到高维</h2>\n<p>我们现在已经确认，对于2维的情况，经过 <span class=\"math inline\">\\(f_q\\)</span> 、 <span class=\"math inline\">\\(f_k\\)</span> 和 <span class=\"math inline\">\\(g\\)</span>\n这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？</p>\n<p>答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n是的输入向量的维度，由于是两个两个一组，所以一共有 <span class=\"math inline\">\\(d/2\\)</span> 组小旋转矩阵，这 <span class=\"math inline\">\\(d/2\\)</span> 组矩阵为了区分，设计使用了不同的\n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p>那么在实际操作的时候，给位置 <span class=\"math inline\">\\(m\\)</span>\n和位置 <span class=\"math inline\">\\(n\\)</span> 的输入向量分别乘以 <span class=\"math inline\">\\(R_m\\)</span> 和 <span class=\"math inline\">\\(R_n\\)</span>，再进行self-attention，就能获得仅使用相对位置信息编码的效果。</p>\n<p>另外 <span class=\"math inline\">\\(\\theta\\)</span>\n是怎么来的呢？这里是参考了Google最初在《Attention is All You\nNeed》中提出的，这里就先不展开了，可以看看论文原文。</p>\n<h2 id=\"高效率实现\">高效率实现</h2>\n<p>式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。</p>\n<p>另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。</p>\n<h2 id=\"远程衰减的特性\">远程衰减的特性</h2>\n<p>至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。</p>\n<p>直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。</p>\n<p>回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个\n<span class=\"math inline\">\\(\\theta\\)</span>\n的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。</p>\n<p>证明过程这里就偷偷懒略过了，具体可以看<a href=\"https://arxiv.org/abs/2104.09864\">Roformer的论文</a>或者<a href=\"https://spaces.ac.cn/archives/8265\">苏神的博客</a>。</p>\n<p>当 <span class=\"math inline\">\\(d = 128\\)</span>\n时，画出来的图像如下</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"远程衰减\">\n<h1 id=\"小结\">小结</h1>\n<p>总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。</p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130<br>\n【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265<br>\n【3】RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n【4】十分钟读懂旋转编码（RoPE）\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"理解Attention:从起源到MHA,MQA和GQA","abbrlink":"3dc22f96","date":"2024-03-05T10:49:38.000Z","_content":"\n【本文已在同名微信公众号/知乎/个人博客同步上线】  \n\nAttention模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head Attention）、MQA（Multi-Query Attention）和GQA（Grouped-Query Attention）这一路线的思路和做法被很多主流模型所采用，因此简单地梳理一些这几个变体的思路和做法，以及会涉及到的KV Cache相关内容。思路比较直白，但也有一些细节和原理值得思考。  \n\n当然针对Attention优化，也有很多其他优秀的方案和思路，如线性注意力、FlashAttention和Sliding Window Attention等，这些在后续再开篇梳理。  \n\n（应一些朋友的要求，会增加一些直观基础的内容，以及LLM应用的案例。也欢迎大家提出更多建议。）\n\n# 关于Attention：从RNN到Attention\n\n了解一个概念的诞生和演进，有助于我们更深入去理解它。我们先简单回顾下attention从起源到最初的实现。\n\n（熟悉attention的朋友可以跳过这一节）\n\n## 从RNN说起\n\n>Memory is attention through time. ~ Alex Graves 2020\n\n注意力机制最初起源是为了解决序列问题。回想在还没有Transformer的上一世代，使用RNN的Seq2Seq是这样的\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n（图来自[AI Summer](https://theaisummer.com/attention/)）  \n\n每个RNN cell接收两个输入，输出一个hidden state。比如在翻译任务中，RNN encoder把所有输入迭代地编码成context向量 $z$ ，然后由RNN decoder基于 $z$ 迭代地解码。一般来说，这里decoder的第一个输入是一个特殊token，如[start]，表示解码开始。  \n\n这样会有一个问题， $z$ 能编码的长度显然有限，而且由于模型结构问题，会更加关注靠近尾部的输入。这样如果关键信息出现在开头，就容易被忽略。  \n\n并且时间步骤上的传播由于有多次迭代相乘，梯度很容易就过小，导致梯度消失问题。  \n\n当然我们有LSMT和GRU等变体来增强长距离记忆的能力，也缓解了梯度问题，但这些方案还是没有产生质变的能力。  \n\n回到问题的核心，我们想要 $z$ 能够编码所有前面的内容，但是显然， $z$ 的生成方式天然会让它更容易注意到靠后的内容，而容易忽略靠前的输入。  \n\n一个直觉的想法就是，我们需要想个办法跳过 $z$ ，和前面的每个输入建立直接的联系。我们希望模型能够有机会学习到去“注意”关键的输入，不管这个输入是在前面还是后面。  \n\n实际上神经网络天生就具有“注意力”的天赋。  \n\n比如在CNN分类中，如果我们画出分类层前的heatmap，会是如下图这个样子  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\n可以看到，值比较高的地方是在猫的鼻子胡子嘴巴区域，次之是身上和头上的花纹。直观来说，就是模型主要通过脸部的特征和身上的花纹，来识别出这是一只猫。这就是CNN学习到的注意力，这样的特征是神经网络implicitly学到的。\n\n回归到Seq2Seq，我们怎么来实现注意力，并且让这种implicit的机制变得explicit：单独抽离出来并具备一定可控制性？  \n\n回想翻译场景，在RNN中，每一个时间步骤 $i$ 都会产生一个隐向量，$h_i$ 向量，我们把这些 $h_i$ 保存起来，在最后要生成新的输出的时候，我们让模型回头看一下之前的这每一个 $h_i$ ，再决定要生成什么内容。相比原来只利用最后一个hidden state，现在我们可以访问之前所有的中间状态，如果发现前面有关键信息，就可以直接用上了，而不用担心输入太长而被覆盖了。  \n\n那么问题又来了，我们怎么知道前面某一个中间状态对于当前的生成来说是否重要？如果我们不知道怎么定义是否重要，那我们就把这个问题交给模型自己解决好了 -- 通过网络参数来学习识别某个输入状态是否重要，学习是否要“注意”到它，要给予多少的“注意力”。  \n\n具体来说，我们定义在解码第 $i$ 个输出是，decoder当前隐状态 $y_{i-1}$ 和encoder的所有隐状态 $\\mathbf{h}$ 之间的一个score计算\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n其中\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n注意力网络通过 $\\mathbf{y}_{i-1}$ 和 $h_j$ 来计算一个值 $e_{ij}$，这里的注意力网络可以设计各种操作，比如对输入进行拼接再通过fc层进行计算等。  \n \n 这里 $e_{ij}$ 是一个标量，但它还不是一个可用的权重值，还需要通过一个函数，把attention net对各个encoder hidden state的输出值转成一个分布：softmax。  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\n最后通过加权计算，获得最终输入给decoder的隐变量。  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\n可以看到，这里的attention net的任务就是找到decoder上一个hidden state和encoder hidden state之间的“相关”关系，使得模型能够将更多的注意力放在对应的输入信息上。\n\n实际上，上面这种attention的计算方式并不是唯一的，attention的计算方式有许多种  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\n这些attention的一般形式可以写作 $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$ 。这里的 $s$ 就是decoder的hidden state（也就是前文的 $y$ ），$h$ 就是encoder的hidden state。  \n\n（当然从结果上看，是scaled dot-product attention经受住了历史的考验，成为了主流。）  \n\n## Transformer的attention\n\n从RNN attention到transformer attention，所做的事情就如论文题目所说：《Attention Is All You Need》，彻底抛弃RNN的在time step上的迭代计算，完全拥抱attention机制，只用最简单粗暴的方式同步计算出每个输入的hidden state，其他的就交给attention来解决。  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\n这里还是保留有encoder和decoder的结构，encoder中的attention都是self-attention，decoder则除了self-attention还有cross-attention。  \n\ntransformer结构下，attention的一般形式可以写作 $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$，这里有 $Q=W_{Q}Y，K=W_{K}X，V=W_{V}X$ 。对于cross-attention， $X$ 是encoder的hidden states，$Y$ 是decoder的hidden states，而对于self-attention，则有 $X=Y$。  \n\n具体到我们熟悉的scaled dot-product attention，使用softmax计算，有\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\n到这里，终于见到我们熟悉的attention计算。  \n\n用一张很直观的图来展示整个计算\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\n这里的「query」，「key」和「value」的名称也暗示了整个attention计算的思路。  \n\n类比到一个数据库查询+预测的例子。  \n\n假设我们现在有一个“文章-阅读量”数据库，记录了每篇文章在发布30天内的阅读量。每篇文章就是一个key，对应的阅读量就是value。  \n\n我们现在有一篇将要发布的文章，想要预测这篇文章在30天内的阅读量，那我们就把这篇新的文章，作为query，去和数据库里的文章（key）做一个相关性计算，取最相关的5篇文章。  \n\n假设top5篇文章的相关性分别是 $[8,4,4,2,2]$ ，对应阅读量是 $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$ 。 \n \n那我们把相关性得分归一化成和为1的概率值 $[0.4,0.2,0.2,0.1,0.1]$ ，那我们就可以预测新文章30天内的阅读量是 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ 。\n\n这个例子中，我们计算相关性就相当于transformer attention中的 $QK^T$ ，归一化就是softmax，然后通过加权求和取得最后的阅读量/特征向量。  \n\n对于self-attention， $Q、K、V$ 都来自输入 $X$，sequence自己计算自己每个token的之间的相关性。而对于cross-attention，decoder中的输出sequence就是上面这个例子中的“将要发布的文章”，通过把这篇新的文章和数据库中的文章做相关计算，我们得到了新的预测结果。  \n\n对于self-attention，由于 $Q、K、V$ 都来自输入 $X$ ，在计算 $QK^T$ 时，模型很容易关注到自身的位置上，也就是 $QK^T$ 对角线上的激活值会明显比较大。这样的情况其实不是很好，因为这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。后面讲的MHA对这个问题会有一些缓解作用。  \n\n顺着这样的思路梳理下来，会发现attention的大思路还是很好理解的。而计算上，怎么去获得更好的效果，就是接下来要分析的几个内容，MHA，MQA和GQA所关注的。\n\n代码上，实现也很容易，直接看[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)的代码\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## 关于scaling\n\nBTW，为什么计算中 $QK^T$ 之后还要除以 $\\sqrt{d}$ ？  \n\n简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。  \n\n{% asset_img softmax.png softmax %}  \n\n苏剑林的[博客](https://spaces.ac.cn/archives/8620)中也有详细分析，并提到如果不对attention值进行scaling，也可以通过在参数初始化是将方差除以一个 $\\sqrt{d}$ ，同样可以起到预防softmax饱和的效果。类似地，通过normalization也可以做到类似的效果。不过实现上在attention里做scaling还是比较稳定高效的。  \n\n# MHA\n\n只要理解了attention计算的细节，MHA（multi-head attention）其实就很好明白。\n\nMHA在2017年就随着《Attention Is All You Need》一起提出，主要干的就是一个事：把原来一个attention计算，拆成多个小份的attention，并行计算，分别得出结果，最后再合回原来的维度。  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\n假设原来模型的hidden size是 $d$ ，在MHA中，会把投影后的 $Q、K、V$ 在hidden state的维度上切成 $head_{num}$ 份，每个头的维度是 $d_{head}$ 。这 $head_{num}$ 组小 $Q、K、V$ 分别独立地进行attention计算，之后把得到的  $head_{num}$ 份维度 $d_{head}$ 的输出concat起来。  \n\n直接看这个amazing的图，很直观  \n\n{% asset_img multihead_attention.png MHA %}  \n\n操作是这么个操作，多头注意力相比单头有什么好处呢？  \n\n《Attention Is All You Need》文章中给出的说法是\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\n我们希望多个头能够在训练中学会注意到不同的内容。例如在翻译任务里，一些attention head可以关注语法特征，另一些attention head可以关注单词特性。这样模型就可以从不同角度来分析和理解输入信息，获得更好的效果了。  \n\n这有点类似CNN中，不同的卷积核来学习不同的信息。比如一个 $3\\times3\\times128$ 的卷积，有128个 $3\\times3$ 参数组，假设我们的输入是一个灰度图，其中一组 $3\\times3$ 的参数是这样的  \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n那么这是一个检测纵向边界的卷积，而另外一组参数长这样  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n这是一个检测横向边界的卷积。  \n\n这128组 $3\\times3$ 就是128个不同特征的检测器，就同MHA中多个头一样，从不同的子空间学到不同的内容，最后再放到一起融合使用。  \n\n但是这是我们expect模型能做到的事情，实际情况是否真的是这样？  \n\n知乎上这篇[文章](https://zhuanlan.zhihu.com/p/626820422)里对此做了一些实验和分析。简单来说就是（1）每个头确实学到东西有所不同，但大部分头之间的差异没有我们想的那么大（比如一个学句法，一个学词义这样明显的区分）（2）多个头的情况下，确实有少部分头可以比较好地捕捉到各种文本信息，而不会过分关注自身位置，一定程度缓解了上文提到的计算 $QK^T$ 之后对角线元素过大的问题。  \n\n我们可以把MHA的多个attention计算视为多个独立的小模型，那么最终整体的attention计算相当于把来自多个小模型的结果进行了融合，这样效果比较好也是比较符合直觉的。\n\n另外还有一个问题是，使用几个头比较好呢？  \n\n实际上这个问题比较难有确定性的答案，首先可以确定的是头的数量不是越多约好（毕竟头的数量多了，各个子空间小了，子空间能表达的内容就少了），具体多少要视模型规模，任务而定。另外[《Are Sixteen Heads Really Better than One?》](https://arxiv.org/pdf/1905.10650.pdf)中也指出MHA并不总是优于单头的情况。  \n\n目前可以看到的趋势是，模型越大（也就是hidden size越大），头数的增多越能带来平均效果上的收益（或者说允许注意力头增大而不影响子空间的学习能力）。目前LLM主流的头数视乎模型结构和规模，大致有12、16、24、48、96这样一些主流设置。这里面又有比较多的方向和工作，在此暂时不展开，挖个坑，以后专门开一篇讲。  \n\n最后看一下[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)中的MHA代码实现  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n（[transformers](https://github.com/huggingface/transformers)中的写法就更为成熟一点，不过里面兼容了比较多的功能，代码太长就不放上来了）\n\n# 解码中的KV Cache\n\n在讲MQA和GQA之前，还需要了解一点背景，那就是解码的计算问题，以及KV Cache的方案。  \n\n无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。  \n\n也就是，解码的时候，先根据当前输入 $\\text{input}_{i-1}$ ，生成下一个 $\\text{token}_{i}$ ，然后把新生成的 $\\text{token}_{i}$ 拼接在 $\\text{input}_{i-1}$ 后面，获得新的输入 $\\text{input}_{i}$ ，再用 $\\text{input}_{i}$ 生成 $\\text{token}_{i+1}$ ，依此迭代，直到生成结束。  \n\n比如我们输入“窗前明月光下一句是”，那么模型每次生成一个token，输入输出会是这样（方便起见，默认每个token都是一个字符）  \n\n```\nstep0: 输入=[BOS]窗前明月光下一句是；输出=疑\nstep1: 输入=[BOS]窗前明月光下一句是疑；输出=是\nstep2: 输入=[BOS]窗前明月光下一句是疑是；输出=地\nstep3: 输入=[BOS]窗前明月光下一句是疑是地；输出=上\nstep4: 输入=[BOS]窗前明月光下一句是疑是地上；输出=霜\nstep5: 输入=[BOS]窗前明月光下一句是疑是地上霜；输出=[EOS]\n```\n\n（其中[BOS]和[EOS]分别是起始符号和终止符号）  \n\n仔细想一下，我们在生成“疑”字的时候，用的是输入序列中“是”字的最后一层hidden state，通过最后的分类头预测出来的。以此类推，后面每生成一个字，使用的都是输入序列中最后一个字的输出。 \n\n我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个token）。那么下一个step的计算应该也包含了上一个step的计算。\n\n从公式上来看是这样的：\n\n回想一下我们attention的计算  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\n注意对于decoder的时候，由于mask attention的存在，每个输入只能看到自己和前面的内容，而看不到后面的内容\n\n假设我们当前输入的长度是3，预测第4个字，那每层attention所做的计算有\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n预测完第4个字，放到输入里，继续预测第5个字，每层attention所做的计算有\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n可以看到，在预测第5个字时，只有最后一步引入了新的计算，而 $o_{0}$ 到 $o_{2}$ 的计算和前面是完全重复的。  \n\n但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。  \n\n也就是说中间有很多我们用不到的计算，这样就造成了浪费。  \n\n而且随着生成的结果越来越多，输入的长度也越来越长。上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写高考作文，那可能就有800个step或者更多。这个情况下，step0被算了800次，step1被算了799次...\n\n有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？  \n\n答案就是KV Cache。利用缓存空间，把需要重复利用的中间计算结果存下来，减少重复计算。  \n\n而 $k$ 和 $v$ 就是要缓存的对象。  \n\n想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要踏踏实实地计算一遍。然后把 $k$ 、 $v$ 值缓存起来。\n\n则有\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center>↓</center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache的下标 $l$ 表示模型层数。\n\n在进行第二次预测，也就是预测第5个字的时候，在第 $l$ 层的时候，由于前面我们缓存了<u>**每层**</u>的 $k$ 、 $v$ 值，那本层就只需要算新的 $o_{3}$ ，而不用算 $o_{0}、o_{1}、o_{2}$ 。  \n\n因为第 $l$ 层的 $o_{0}、o_{1}、o_{2}$ 本来会经过FNN层之后进到 $l+1$ 层，再经过新的投影变换，成为 $l+1$ 层的 $k$ 、 $v$ 值，但是 $l+1$ 层的 $k$ 、 $v$ 值我们已经缓存过了！  \n\n然后我们把本次新增算出来的 $k$ 、 $v$ 值也存入缓存。\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center>↓</center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\n这样就节省了attention和FFN的很多重复计算。  \n\ntransformers中，生成的时候传入use_cache=True就会开启KV Cache。  \n\n也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # 过去所存的值\n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # 把当前新的key加入\n            value = torch.cat((past_value, value), dim=-2)  # 把当前新的value加入\n\n        if use_cache is True:\n            present = (key, value)  # 输出用于保存\n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\n总的来说，KV Cache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask attention的存在，使得前面的token可以不用关注后面的token）  \n\n但是，用了KV Cache之后也不是立刻万事大吉。  \n\n我们简单算一下，对于输入长度为 $s$ ，层数为 $L$ ，hidden size为 $d$ 的模型，需要缓存的参数量为  \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n如果使用的是半精度浮点数，那么总共所需的空间就是\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\n以Llama2 7B为例，有 $L=32$ ， $L=4096$ ，那么每个token所需的缓存空间就是524,288 bytes，约52K，当 $s=1024$ 时，则需要536,870,912 bytes，超过500M的空间。  \n\n这里考虑的还只是batch size=1的情况，如果batch size增大，这个值更是很容易就超过1G。  \n\n（MHA相比单头的情况，相当于只是把 $q、k、v$ 切成多份并行计算了，对于实际需要缓存的大小没有影响）\n\n看下现在主流的科学计算卡配置\n\n{% asset_img gpu_cache.png gpu cache %}  \n\n强如H100也只有50M的L2 Cache（L1 Cache的大小更是可以忽略不计），大概只能支持Llama2 7B总共100个token左右的输入。  \n\n想想我们现在用的LLM动辄34B/70B的规模，长度更是以千为基础单位，这样明显是不够用的。\n\n那么超出L2 Cache的部分只能走到显存中去了，但是HBM速度比L2 Cache慢多了。  \n\n{% asset_img sram_dram.png 储存空间与速度 %}  \n\n看来还需要进一步优化。  \n\n要保证模型的推理加速，要么增大Cache的大小，而且是需要一到两个数量级的增强，那这个只能靠黄老板了。  \n\n要么就是减少需要缓存的量。  \n\n# MQA\n\nMQA就是来减少缓存所需要的量的。\n\nGoogle在2019年就在《Fast Transformer Decoding: One Write-Head is All You Need》提出了MQA，不过那时候主要到的人不多，那是大家主要还是关注在用Bert把榜刷出新高上。  \n\nMQA的做法其实很简单。在MHA中，输入分别经过 $W_{Q}、W_{K}、W_{V}$ 的变换之后，都切成了n份（n=头数），维度也从 $d_{model}$ 降到了 $d_{head}$ ，分别进行attention计算再拼接。而MQA这里，在线性变换之后，只对 $Q$ 进行切分（和MHA一样），而 $K、V$ 则直接在线性变换的时候把维度降到了 $d_{head}$ （而不是切分变小），然后这n个Query头分别和同一份 $K、V$ 进行attention计算，之后把结果拼接起来。  \n\n简单来说，就是MHA中，每个注意力头的 $K、V$ 是不一样的，而MQA这里，每个注意力头的 $K、V$ 是一样的，值是共享的。而其他步骤都和MHA一样。  \n\n{% asset_img MQA.webp MQA %}  \n\n这样一来，需要缓存的 $K、V$ 值一下就从所有头变成一个头的量。  \n\n比如在Llama2 7B中用的是32个头，那用MQA后，1024个token需要缓存的量就变成1/32，536,870,912 bytes / 32 = 16,777,216 bytes，差不多是16M，这就能全塞进缓存中了。\n\n（实现上，就是改一下线性变换矩阵，然后把 $K、V$ 的处理从切分变成复制，就不再赘述。）  \n\n当然，由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点，但是并不多，且相比其他修改hidden size或者head num的做法效果都好。  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\n既然MQA对效果有点影响，MHA缓存又存不下，那GQA（Grouped-Query Attention）就提出了一个折中的办法，既能减少MQA效果的损失，又相比MHA需要更少的缓存。  \n\n（文章：《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》，2023年）\n\nGQA里， $Q$ 还是按原来MHA/MQA的做法不变。只使用一套共享的 $K、V$ 不是效果不好吗，那就还是多弄几套。但是不要太多，数量还是比 $Q$ 的头数少一些。这样相当于把 $Q$ 的多个头给分了group，同一个group内的 $Q$ 共享同一套 $K、V$ ，不同group的 $Q$ 所用的 $K、V$ 不同。  \n\nMHA可以认为是 $K、V$ 头数最大时的GQA，而MQA可以任务是 $K、V$ 头数最少时的GQA。  \n\n看论文里的图就很直观\n\n{% asset_img GQA.png GQA %}  \n\n效果怎么样呢？  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n看表中2/3/4行对比，GQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。文中提到，这里的MQA和GQA都是通过average pooling从MHA初始化而来，然后进行了少量的训练得到的。如果我们想要把之前用MHA训练的模型改造成GQA，也可以通过这样的方法，增加少量训练来实现。当然如果从一开始就加上，从零开始训练，也是没有问题的。  \n\nLlama2用的就是GQA，在tech report中也做了MHA、MQA、GQA的效果对比，可以看到效果确实很不错。  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n# 小结  \n\nMHA、MQA、GQA的实现其实并不复杂，效果也很好，理解上并没有太多困难。但是想要真正理解它们的出发点，还是需要深入每一个细节，去了解当时要解决的事什么问题。\n\n目前来看GQA是LLM比较好的方案，但未来肯定还会有针对不同方向的进一步优化方案，计算效率、推理速度、显存消耗这些方向都值得我们继续去探索优化。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n【1】The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n【2】Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n【3】Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n【4】https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n【5】GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n【6】How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n【7】A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n【8】https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n【9】浅谈Transformer的初始化、参数化与标准化 https://spaces.ac.cn/archives/8620  \n【10】https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n【11】https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n【12】Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n【13】This post is all you need（上卷）——层层剥开Transformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n【14】The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n【15】Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","source":"_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA.md","raw":"---\ntitle: '理解Attention:从起源到MHA,MQA和GQA'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - KV Cache\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 3dc22f96\ndate: 2024-03-05 18:49:38\n---\n\n【本文已在同名微信公众号/知乎/个人博客同步上线】  \n\nAttention模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head Attention）、MQA（Multi-Query Attention）和GQA（Grouped-Query Attention）这一路线的思路和做法被很多主流模型所采用，因此简单地梳理一些这几个变体的思路和做法，以及会涉及到的KV Cache相关内容。思路比较直白，但也有一些细节和原理值得思考。  \n\n当然针对Attention优化，也有很多其他优秀的方案和思路，如线性注意力、FlashAttention和Sliding Window Attention等，这些在后续再开篇梳理。  \n\n（应一些朋友的要求，会增加一些直观基础的内容，以及LLM应用的案例。也欢迎大家提出更多建议。）\n\n# 关于Attention：从RNN到Attention\n\n了解一个概念的诞生和演进，有助于我们更深入去理解它。我们先简单回顾下attention从起源到最初的实现。\n\n（熟悉attention的朋友可以跳过这一节）\n\n## 从RNN说起\n\n>Memory is attention through time. ~ Alex Graves 2020\n\n注意力机制最初起源是为了解决序列问题。回想在还没有Transformer的上一世代，使用RNN的Seq2Seq是这样的\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n（图来自[AI Summer](https://theaisummer.com/attention/)）  \n\n每个RNN cell接收两个输入，输出一个hidden state。比如在翻译任务中，RNN encoder把所有输入迭代地编码成context向量 $z$ ，然后由RNN decoder基于 $z$ 迭代地解码。一般来说，这里decoder的第一个输入是一个特殊token，如[start]，表示解码开始。  \n\n这样会有一个问题， $z$ 能编码的长度显然有限，而且由于模型结构问题，会更加关注靠近尾部的输入。这样如果关键信息出现在开头，就容易被忽略。  \n\n并且时间步骤上的传播由于有多次迭代相乘，梯度很容易就过小，导致梯度消失问题。  \n\n当然我们有LSMT和GRU等变体来增强长距离记忆的能力，也缓解了梯度问题，但这些方案还是没有产生质变的能力。  \n\n回到问题的核心，我们想要 $z$ 能够编码所有前面的内容，但是显然， $z$ 的生成方式天然会让它更容易注意到靠后的内容，而容易忽略靠前的输入。  \n\n一个直觉的想法就是，我们需要想个办法跳过 $z$ ，和前面的每个输入建立直接的联系。我们希望模型能够有机会学习到去“注意”关键的输入，不管这个输入是在前面还是后面。  \n\n实际上神经网络天生就具有“注意力”的天赋。  \n\n比如在CNN分类中，如果我们画出分类层前的heatmap，会是如下图这个样子  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\n可以看到，值比较高的地方是在猫的鼻子胡子嘴巴区域，次之是身上和头上的花纹。直观来说，就是模型主要通过脸部的特征和身上的花纹，来识别出这是一只猫。这就是CNN学习到的注意力，这样的特征是神经网络implicitly学到的。\n\n回归到Seq2Seq，我们怎么来实现注意力，并且让这种implicit的机制变得explicit：单独抽离出来并具备一定可控制性？  \n\n回想翻译场景，在RNN中，每一个时间步骤 $i$ 都会产生一个隐向量，$h_i$ 向量，我们把这些 $h_i$ 保存起来，在最后要生成新的输出的时候，我们让模型回头看一下之前的这每一个 $h_i$ ，再决定要生成什么内容。相比原来只利用最后一个hidden state，现在我们可以访问之前所有的中间状态，如果发现前面有关键信息，就可以直接用上了，而不用担心输入太长而被覆盖了。  \n\n那么问题又来了，我们怎么知道前面某一个中间状态对于当前的生成来说是否重要？如果我们不知道怎么定义是否重要，那我们就把这个问题交给模型自己解决好了 -- 通过网络参数来学习识别某个输入状态是否重要，学习是否要“注意”到它，要给予多少的“注意力”。  \n\n具体来说，我们定义在解码第 $i$ 个输出是，decoder当前隐状态 $y_{i-1}$ 和encoder的所有隐状态 $\\mathbf{h}$ 之间的一个score计算\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n其中\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n注意力网络通过 $\\mathbf{y}_{i-1}$ 和 $h_j$ 来计算一个值 $e_{ij}$，这里的注意力网络可以设计各种操作，比如对输入进行拼接再通过fc层进行计算等。  \n \n 这里 $e_{ij}$ 是一个标量，但它还不是一个可用的权重值，还需要通过一个函数，把attention net对各个encoder hidden state的输出值转成一个分布：softmax。  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\n最后通过加权计算，获得最终输入给decoder的隐变量。  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\n可以看到，这里的attention net的任务就是找到decoder上一个hidden state和encoder hidden state之间的“相关”关系，使得模型能够将更多的注意力放在对应的输入信息上。\n\n实际上，上面这种attention的计算方式并不是唯一的，attention的计算方式有许多种  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\n这些attention的一般形式可以写作 $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$ 。这里的 $s$ 就是decoder的hidden state（也就是前文的 $y$ ），$h$ 就是encoder的hidden state。  \n\n（当然从结果上看，是scaled dot-product attention经受住了历史的考验，成为了主流。）  \n\n## Transformer的attention\n\n从RNN attention到transformer attention，所做的事情就如论文题目所说：《Attention Is All You Need》，彻底抛弃RNN的在time step上的迭代计算，完全拥抱attention机制，只用最简单粗暴的方式同步计算出每个输入的hidden state，其他的就交给attention来解决。  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\n这里还是保留有encoder和decoder的结构，encoder中的attention都是self-attention，decoder则除了self-attention还有cross-attention。  \n\ntransformer结构下，attention的一般形式可以写作 $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$，这里有 $Q=W_{Q}Y，K=W_{K}X，V=W_{V}X$ 。对于cross-attention， $X$ 是encoder的hidden states，$Y$ 是decoder的hidden states，而对于self-attention，则有 $X=Y$。  \n\n具体到我们熟悉的scaled dot-product attention，使用softmax计算，有\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\n到这里，终于见到我们熟悉的attention计算。  \n\n用一张很直观的图来展示整个计算\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\n这里的「query」，「key」和「value」的名称也暗示了整个attention计算的思路。  \n\n类比到一个数据库查询+预测的例子。  \n\n假设我们现在有一个“文章-阅读量”数据库，记录了每篇文章在发布30天内的阅读量。每篇文章就是一个key，对应的阅读量就是value。  \n\n我们现在有一篇将要发布的文章，想要预测这篇文章在30天内的阅读量，那我们就把这篇新的文章，作为query，去和数据库里的文章（key）做一个相关性计算，取最相关的5篇文章。  \n\n假设top5篇文章的相关性分别是 $[8,4,4,2,2]$ ，对应阅读量是 $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$ 。 \n \n那我们把相关性得分归一化成和为1的概率值 $[0.4,0.2,0.2,0.1,0.1]$ ，那我们就可以预测新文章30天内的阅读量是 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ 。\n\n这个例子中，我们计算相关性就相当于transformer attention中的 $QK^T$ ，归一化就是softmax，然后通过加权求和取得最后的阅读量/特征向量。  \n\n对于self-attention， $Q、K、V$ 都来自输入 $X$，sequence自己计算自己每个token的之间的相关性。而对于cross-attention，decoder中的输出sequence就是上面这个例子中的“将要发布的文章”，通过把这篇新的文章和数据库中的文章做相关计算，我们得到了新的预测结果。  \n\n对于self-attention，由于 $Q、K、V$ 都来自输入 $X$ ，在计算 $QK^T$ 时，模型很容易关注到自身的位置上，也就是 $QK^T$ 对角线上的激活值会明显比较大。这样的情况其实不是很好，因为这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。后面讲的MHA对这个问题会有一些缓解作用。  \n\n顺着这样的思路梳理下来，会发现attention的大思路还是很好理解的。而计算上，怎么去获得更好的效果，就是接下来要分析的几个内容，MHA，MQA和GQA所关注的。\n\n代码上，实现也很容易，直接看[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)的代码\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## 关于scaling\n\nBTW，为什么计算中 $QK^T$ 之后还要除以 $\\sqrt{d}$ ？  \n\n简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。  \n\n{% asset_img softmax.png softmax %}  \n\n苏剑林的[博客](https://spaces.ac.cn/archives/8620)中也有详细分析，并提到如果不对attention值进行scaling，也可以通过在参数初始化是将方差除以一个 $\\sqrt{d}$ ，同样可以起到预防softmax饱和的效果。类似地，通过normalization也可以做到类似的效果。不过实现上在attention里做scaling还是比较稳定高效的。  \n\n# MHA\n\n只要理解了attention计算的细节，MHA（multi-head attention）其实就很好明白。\n\nMHA在2017年就随着《Attention Is All You Need》一起提出，主要干的就是一个事：把原来一个attention计算，拆成多个小份的attention，并行计算，分别得出结果，最后再合回原来的维度。  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\n假设原来模型的hidden size是 $d$ ，在MHA中，会把投影后的 $Q、K、V$ 在hidden state的维度上切成 $head_{num}$ 份，每个头的维度是 $d_{head}$ 。这 $head_{num}$ 组小 $Q、K、V$ 分别独立地进行attention计算，之后把得到的  $head_{num}$ 份维度 $d_{head}$ 的输出concat起来。  \n\n直接看这个amazing的图，很直观  \n\n{% asset_img multihead_attention.png MHA %}  \n\n操作是这么个操作，多头注意力相比单头有什么好处呢？  \n\n《Attention Is All You Need》文章中给出的说法是\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\n我们希望多个头能够在训练中学会注意到不同的内容。例如在翻译任务里，一些attention head可以关注语法特征，另一些attention head可以关注单词特性。这样模型就可以从不同角度来分析和理解输入信息，获得更好的效果了。  \n\n这有点类似CNN中，不同的卷积核来学习不同的信息。比如一个 $3\\times3\\times128$ 的卷积，有128个 $3\\times3$ 参数组，假设我们的输入是一个灰度图，其中一组 $3\\times3$ 的参数是这样的  \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n那么这是一个检测纵向边界的卷积，而另外一组参数长这样  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n这是一个检测横向边界的卷积。  \n\n这128组 $3\\times3$ 就是128个不同特征的检测器，就同MHA中多个头一样，从不同的子空间学到不同的内容，最后再放到一起融合使用。  \n\n但是这是我们expect模型能做到的事情，实际情况是否真的是这样？  \n\n知乎上这篇[文章](https://zhuanlan.zhihu.com/p/626820422)里对此做了一些实验和分析。简单来说就是（1）每个头确实学到东西有所不同，但大部分头之间的差异没有我们想的那么大（比如一个学句法，一个学词义这样明显的区分）（2）多个头的情况下，确实有少部分头可以比较好地捕捉到各种文本信息，而不会过分关注自身位置，一定程度缓解了上文提到的计算 $QK^T$ 之后对角线元素过大的问题。  \n\n我们可以把MHA的多个attention计算视为多个独立的小模型，那么最终整体的attention计算相当于把来自多个小模型的结果进行了融合，这样效果比较好也是比较符合直觉的。\n\n另外还有一个问题是，使用几个头比较好呢？  \n\n实际上这个问题比较难有确定性的答案，首先可以确定的是头的数量不是越多约好（毕竟头的数量多了，各个子空间小了，子空间能表达的内容就少了），具体多少要视模型规模，任务而定。另外[《Are Sixteen Heads Really Better than One?》](https://arxiv.org/pdf/1905.10650.pdf)中也指出MHA并不总是优于单头的情况。  \n\n目前可以看到的趋势是，模型越大（也就是hidden size越大），头数的增多越能带来平均效果上的收益（或者说允许注意力头增大而不影响子空间的学习能力）。目前LLM主流的头数视乎模型结构和规模，大致有12、16、24、48、96这样一些主流设置。这里面又有比较多的方向和工作，在此暂时不展开，挖个坑，以后专门开一篇讲。  \n\n最后看一下[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)中的MHA代码实现  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n（[transformers](https://github.com/huggingface/transformers)中的写法就更为成熟一点，不过里面兼容了比较多的功能，代码太长就不放上来了）\n\n# 解码中的KV Cache\n\n在讲MQA和GQA之前，还需要了解一点背景，那就是解码的计算问题，以及KV Cache的方案。  \n\n无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。  \n\n也就是，解码的时候，先根据当前输入 $\\text{input}_{i-1}$ ，生成下一个 $\\text{token}_{i}$ ，然后把新生成的 $\\text{token}_{i}$ 拼接在 $\\text{input}_{i-1}$ 后面，获得新的输入 $\\text{input}_{i}$ ，再用 $\\text{input}_{i}$ 生成 $\\text{token}_{i+1}$ ，依此迭代，直到生成结束。  \n\n比如我们输入“窗前明月光下一句是”，那么模型每次生成一个token，输入输出会是这样（方便起见，默认每个token都是一个字符）  \n\n```\nstep0: 输入=[BOS]窗前明月光下一句是；输出=疑\nstep1: 输入=[BOS]窗前明月光下一句是疑；输出=是\nstep2: 输入=[BOS]窗前明月光下一句是疑是；输出=地\nstep3: 输入=[BOS]窗前明月光下一句是疑是地；输出=上\nstep4: 输入=[BOS]窗前明月光下一句是疑是地上；输出=霜\nstep5: 输入=[BOS]窗前明月光下一句是疑是地上霜；输出=[EOS]\n```\n\n（其中[BOS]和[EOS]分别是起始符号和终止符号）  \n\n仔细想一下，我们在生成“疑”字的时候，用的是输入序列中“是”字的最后一层hidden state，通过最后的分类头预测出来的。以此类推，后面每生成一个字，使用的都是输入序列中最后一个字的输出。 \n\n我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个token）。那么下一个step的计算应该也包含了上一个step的计算。\n\n从公式上来看是这样的：\n\n回想一下我们attention的计算  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\n注意对于decoder的时候，由于mask attention的存在，每个输入只能看到自己和前面的内容，而看不到后面的内容\n\n假设我们当前输入的长度是3，预测第4个字，那每层attention所做的计算有\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n预测完第4个字，放到输入里，继续预测第5个字，每层attention所做的计算有\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n可以看到，在预测第5个字时，只有最后一步引入了新的计算，而 $o_{0}$ 到 $o_{2}$ 的计算和前面是完全重复的。  \n\n但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。  \n\n也就是说中间有很多我们用不到的计算，这样就造成了浪费。  \n\n而且随着生成的结果越来越多，输入的长度也越来越长。上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写高考作文，那可能就有800个step或者更多。这个情况下，step0被算了800次，step1被算了799次...\n\n有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？  \n\n答案就是KV Cache。利用缓存空间，把需要重复利用的中间计算结果存下来，减少重复计算。  \n\n而 $k$ 和 $v$ 就是要缓存的对象。  \n\n想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要踏踏实实地计算一遍。然后把 $k$ 、 $v$ 值缓存起来。\n\n则有\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center>↓</center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache的下标 $l$ 表示模型层数。\n\n在进行第二次预测，也就是预测第5个字的时候，在第 $l$ 层的时候，由于前面我们缓存了<u>**每层**</u>的 $k$ 、 $v$ 值，那本层就只需要算新的 $o_{3}$ ，而不用算 $o_{0}、o_{1}、o_{2}$ 。  \n\n因为第 $l$ 层的 $o_{0}、o_{1}、o_{2}$ 本来会经过FNN层之后进到 $l+1$ 层，再经过新的投影变换，成为 $l+1$ 层的 $k$ 、 $v$ 值，但是 $l+1$ 层的 $k$ 、 $v$ 值我们已经缓存过了！  \n\n然后我们把本次新增算出来的 $k$ 、 $v$ 值也存入缓存。\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center>↓</center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\n这样就节省了attention和FFN的很多重复计算。  \n\ntransformers中，生成的时候传入use_cache=True就会开启KV Cache。  \n\n也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # 过去所存的值\n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # 把当前新的key加入\n            value = torch.cat((past_value, value), dim=-2)  # 把当前新的value加入\n\n        if use_cache is True:\n            present = (key, value)  # 输出用于保存\n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\n总的来说，KV Cache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask attention的存在，使得前面的token可以不用关注后面的token）  \n\n但是，用了KV Cache之后也不是立刻万事大吉。  \n\n我们简单算一下，对于输入长度为 $s$ ，层数为 $L$ ，hidden size为 $d$ 的模型，需要缓存的参数量为  \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n如果使用的是半精度浮点数，那么总共所需的空间就是\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\n以Llama2 7B为例，有 $L=32$ ， $L=4096$ ，那么每个token所需的缓存空间就是524,288 bytes，约52K，当 $s=1024$ 时，则需要536,870,912 bytes，超过500M的空间。  \n\n这里考虑的还只是batch size=1的情况，如果batch size增大，这个值更是很容易就超过1G。  \n\n（MHA相比单头的情况，相当于只是把 $q、k、v$ 切成多份并行计算了，对于实际需要缓存的大小没有影响）\n\n看下现在主流的科学计算卡配置\n\n{% asset_img gpu_cache.png gpu cache %}  \n\n强如H100也只有50M的L2 Cache（L1 Cache的大小更是可以忽略不计），大概只能支持Llama2 7B总共100个token左右的输入。  \n\n想想我们现在用的LLM动辄34B/70B的规模，长度更是以千为基础单位，这样明显是不够用的。\n\n那么超出L2 Cache的部分只能走到显存中去了，但是HBM速度比L2 Cache慢多了。  \n\n{% asset_img sram_dram.png 储存空间与速度 %}  \n\n看来还需要进一步优化。  \n\n要保证模型的推理加速，要么增大Cache的大小，而且是需要一到两个数量级的增强，那这个只能靠黄老板了。  \n\n要么就是减少需要缓存的量。  \n\n# MQA\n\nMQA就是来减少缓存所需要的量的。\n\nGoogle在2019年就在《Fast Transformer Decoding: One Write-Head is All You Need》提出了MQA，不过那时候主要到的人不多，那是大家主要还是关注在用Bert把榜刷出新高上。  \n\nMQA的做法其实很简单。在MHA中，输入分别经过 $W_{Q}、W_{K}、W_{V}$ 的变换之后，都切成了n份（n=头数），维度也从 $d_{model}$ 降到了 $d_{head}$ ，分别进行attention计算再拼接。而MQA这里，在线性变换之后，只对 $Q$ 进行切分（和MHA一样），而 $K、V$ 则直接在线性变换的时候把维度降到了 $d_{head}$ （而不是切分变小），然后这n个Query头分别和同一份 $K、V$ 进行attention计算，之后把结果拼接起来。  \n\n简单来说，就是MHA中，每个注意力头的 $K、V$ 是不一样的，而MQA这里，每个注意力头的 $K、V$ 是一样的，值是共享的。而其他步骤都和MHA一样。  \n\n{% asset_img MQA.webp MQA %}  \n\n这样一来，需要缓存的 $K、V$ 值一下就从所有头变成一个头的量。  \n\n比如在Llama2 7B中用的是32个头，那用MQA后，1024个token需要缓存的量就变成1/32，536,870,912 bytes / 32 = 16,777,216 bytes，差不多是16M，这就能全塞进缓存中了。\n\n（实现上，就是改一下线性变换矩阵，然后把 $K、V$ 的处理从切分变成复制，就不再赘述。）  \n\n当然，由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点，但是并不多，且相比其他修改hidden size或者head num的做法效果都好。  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\n既然MQA对效果有点影响，MHA缓存又存不下，那GQA（Grouped-Query Attention）就提出了一个折中的办法，既能减少MQA效果的损失，又相比MHA需要更少的缓存。  \n\n（文章：《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》，2023年）\n\nGQA里， $Q$ 还是按原来MHA/MQA的做法不变。只使用一套共享的 $K、V$ 不是效果不好吗，那就还是多弄几套。但是不要太多，数量还是比 $Q$ 的头数少一些。这样相当于把 $Q$ 的多个头给分了group，同一个group内的 $Q$ 共享同一套 $K、V$ ，不同group的 $Q$ 所用的 $K、V$ 不同。  \n\nMHA可以认为是 $K、V$ 头数最大时的GQA，而MQA可以任务是 $K、V$ 头数最少时的GQA。  \n\n看论文里的图就很直观\n\n{% asset_img GQA.png GQA %}  \n\n效果怎么样呢？  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n看表中2/3/4行对比，GQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。文中提到，这里的MQA和GQA都是通过average pooling从MHA初始化而来，然后进行了少量的训练得到的。如果我们想要把之前用MHA训练的模型改造成GQA，也可以通过这样的方法，增加少量训练来实现。当然如果从一开始就加上，从零开始训练，也是没有问题的。  \n\nLlama2用的就是GQA，在tech report中也做了MHA、MQA、GQA的效果对比，可以看到效果确实很不错。  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n# 小结  \n\nMHA、MQA、GQA的实现其实并不复杂，效果也很好，理解上并没有太多困难。但是想要真正理解它们的出发点，还是需要深入每一个细节，去了解当时要解决的事什么问题。\n\n目前来看GQA是LLM比较好的方案，但未来肯定还会有针对不同方向的进一步优化方案，计算效率、推理速度、显存消耗这些方向都值得我们继续去探索优化。  \n\n***  \n\n读到这了，来一发点赞收藏关注吧~\n\n博客：[http://www.linsight.cn/](http://www.linsight.cn/)  \n知乎：[Linsight](https://www.zhihu.com/people/us4ever)  \n微信公众号：Linsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n【1】The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n【2】Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n【3】Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n【4】https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n【5】GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n【6】How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n【7】A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n【8】https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n【9】浅谈Transformer的初始化、参数化与标准化 https://spaces.ac.cn/archives/8620  \n【10】https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n【11】https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n【12】Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n【13】This post is all you need（上卷）——层层剥开Transformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n【14】The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n【15】Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","slug":"cs/nlp/2024/03/理解Attention-MHA-MQA和GQA","published":1,"updated":"2024-03-14T14:17:54.411Z","comments":1,"layout":"post","photos":[],"_id":"clttl4al10008rb4k23jp14zi","content":"<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>\n<p>Attention模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head\nAttention）、MQA（Multi-Query Attention）和GQA（Grouped-Query\nAttention）这一路线的思路和做法被很多主流模型所采用，因此简单地梳理一些这几个变体的思路和做法，以及会涉及到的KV\nCache相关内容。思路比较直白，但也有一些细节和原理值得思考。</p>\n<p>当然针对Attention优化，也有很多其他优秀的方案和思路，如线性注意力、FlashAttention和Sliding\nWindow Attention等，这些在后续再开篇梳理。</p>\n<p>（应一些朋友的要求，会增加一些直观基础的内容，以及LLM应用的案例。也欢迎大家提出更多建议。）</p>\n<h1 id=\"关于attention从rnn到attention\">关于Attention：从RNN到Attention</h1>\n<p>了解一个概念的诞生和演进，有助于我们更深入去理解它。我们先简单回顾下attention从起源到最初的实现。</p>\n<p>（熟悉attention的朋友可以跳过这一节）</p>\n<h2 id=\"从rnn说起\">从RNN说起</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>注意力机制最初起源是为了解决序列问题。回想在还没有Transformer的上一世代，使用RNN的Seq2Seq是这样的</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p>（图来自<a href=\"https://theaisummer.com/attention/\">AI\nSummer</a>）</p>\n<p>每个RNN cell接收两个输入，输出一个hidden state。比如在翻译任务中，RNN\nencoder把所有输入迭代地编码成context向量 <span class=\"math inline\">\\(z\\)</span> ，然后由RNN decoder基于 <span class=\"math inline\">\\(z\\)</span>\n迭代地解码。一般来说，这里decoder的第一个输入是一个特殊token，如[start]，表示解码开始。</p>\n<p>这样会有一个问题， <span class=\"math inline\">\\(z\\)</span>\n能编码的长度显然有限，而且由于模型结构问题，会更加关注靠近尾部的输入。这样如果关键信息出现在开头，就容易被忽略。</p>\n<p>并且时间步骤上的传播由于有多次迭代相乘，梯度很容易就过小，导致梯度消失问题。</p>\n<p>当然我们有LSMT和GRU等变体来增强长距离记忆的能力，也缓解了梯度问题，但这些方案还是没有产生质变的能力。</p>\n<p>回到问题的核心，我们想要 <span class=\"math inline\">\\(z\\)</span>\n能够编码所有前面的内容，但是显然， <span class=\"math inline\">\\(z\\)</span>\n的生成方式天然会让它更容易注意到靠后的内容，而容易忽略靠前的输入。</p>\n<p>一个直觉的想法就是，我们需要想个办法跳过 <span class=\"math inline\">\\(z\\)</span>\n，和前面的每个输入建立直接的联系。我们希望模型能够有机会学习到去“注意”关键的输入，不管这个输入是在前面还是后面。</p>\n<p>实际上神经网络天生就具有“注意力”的天赋。</p>\n<p>比如在CNN分类中，如果我们画出分类层前的heatmap，会是如下图这个样子</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>可以看到，值比较高的地方是在猫的鼻子胡子嘴巴区域，次之是身上和头上的花纹。直观来说，就是模型主要通过脸部的特征和身上的花纹，来识别出这是一只猫。这就是CNN学习到的注意力，这样的特征是神经网络implicitly学到的。</p>\n<p>回归到Seq2Seq，我们怎么来实现注意力，并且让这种implicit的机制变得explicit：单独抽离出来并具备一定可控制性？</p>\n<p>回想翻译场景，在RNN中，每一个时间步骤 <span class=\"math inline\">\\(i\\)</span> 都会产生一个隐向量，<span class=\"math inline\">\\(h_i\\)</span> 向量，我们把这些 <span class=\"math inline\">\\(h_i\\)</span>\n保存起来，在最后要生成新的输出的时候，我们让模型回头看一下之前的这每一个\n<span class=\"math inline\">\\(h_i\\)</span>\n，再决定要生成什么内容。相比原来只利用最后一个hidden\nstate，现在我们可以访问之前所有的中间状态，如果发现前面有关键信息，就可以直接用上了，而不用担心输入太长而被覆盖了。</p>\n<p>那么问题又来了，我们怎么知道前面某一个中间状态对于当前的生成来说是否重要？如果我们不知道怎么定义是否重要，那我们就把这个问题交给模型自己解决好了\n--\n通过网络参数来学习识别某个输入状态是否重要，学习是否要“注意”到它，要给予多少的“注意力”。</p>\n<p>具体来说，我们定义在解码第 <span class=\"math inline\">\\(i\\)</span>\n个输出是，decoder当前隐状态 <span class=\"math inline\">\\(y_{i-1}\\)</span>\n和encoder的所有隐状态 <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\n之间的一个score计算</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p>其中</p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p>注意力网络通过 <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n和 <span class=\"math inline\">\\(h_j\\)</span> 来计算一个值 <span class=\"math inline\">\\(e_{ij}\\)</span>，这里的注意力网络可以设计各种操作，比如对输入进行拼接再通过fc层进行计算等。</p>\n<p>这里 <span class=\"math inline\">\\(e_{ij}\\)</span>\n是一个标量，但它还不是一个可用的权重值，还需要通过一个函数，把attention\nnet对各个encoder hidden state的输出值转成一个分布：softmax。</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>最后通过加权计算，获得最终输入给decoder的隐变量。</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>可以看到，这里的attention net的任务就是找到decoder上一个hidden\nstate和encoder hidden\nstate之间的“相关”关系，使得模型能够将更多的注意力放在对应的输入信息上。</p>\n<p>实际上，上面这种attention的计算方式并不是唯一的，attention的计算方式有许多种</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>这些attention的一般形式可以写作 <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span> 。这里的 <span class=\"math inline\">\\(s\\)</span>\n就是decoder的hidden state（也就是前文的 <span class=\"math inline\">\\(y\\)</span> ），<span class=\"math inline\">\\(h\\)</span> 就是encoder的hidden state。</p>\n<p>（当然从结果上看，是scaled dot-product\nattention经受住了历史的考验，成为了主流。）</p>\n<h2 id=\"transformer的attention\">Transformer的attention</h2>\n<p>从RNN attention到transformer\nattention，所做的事情就如论文题目所说：《Attention Is All You\nNeed》，彻底抛弃RNN的在time\nstep上的迭代计算，完全拥抱attention机制，只用最简单粗暴的方式同步计算出每个输入的hidden\nstate，其他的就交给attention来解决。</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>这里还是保留有encoder和decoder的结构，encoder中的attention都是self-attention，decoder则除了self-attention还有cross-attention。</p>\n<p>transformer结构下，attention的一般形式可以写作 <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>，这里有\n<span class=\"math inline\">\\(Q=W_{Q}Y，K=W_{K}X，V=W_{V}X\\)</span>\n。对于cross-attention， <span class=\"math inline\">\\(X\\)</span>\n是encoder的hidden states，<span class=\"math inline\">\\(Y\\)</span>\n是decoder的hidden states，而对于self-attention，则有 <span class=\"math inline\">\\(X=Y\\)</span>。</p>\n<p>具体到我们熟悉的scaled dot-product attention，使用softmax计算，有</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>到这里，终于见到我们熟悉的attention计算。</p>\n<p>用一张很直观的图来展示整个计算</p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>这里的「query」，「key」和「value」的名称也暗示了整个attention计算的思路。</p>\n<p>类比到一个数据库查询+预测的例子。</p>\n<p>假设我们现在有一个“文章-阅读量”数据库，记录了每篇文章在发布30天内的阅读量。每篇文章就是一个key，对应的阅读量就是value。</p>\n<p>我们现在有一篇将要发布的文章，想要预测这篇文章在30天内的阅读量，那我们就把这篇新的文章，作为query，去和数据库里的文章（key）做一个相关性计算，取最相关的5篇文章。</p>\n<p>假设top5篇文章的相关性分别是 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span> ，对应阅读量是 <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n。</p>\n<p>那我们把相关性得分归一化成和为1的概率值 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n，那我们就可以预测新文章30天内的阅读量是 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n。</p>\n<p>这个例子中，我们计算相关性就相当于transformer attention中的 <span class=\"math inline\">\\(QK^T\\)</span>\n，归一化就是softmax，然后通过加权求和取得最后的阅读量/特征向量。</p>\n<p>对于self-attention， <span class=\"math inline\">\\(Q、K、V\\)</span>\n都来自输入 <span class=\"math inline\">\\(X\\)</span>，sequence自己计算自己每个token的之间的相关性。而对于cross-attention，decoder中的输出sequence就是上面这个例子中的“将要发布的文章”，通过把这篇新的文章和数据库中的文章做相关计算，我们得到了新的预测结果。</p>\n<p>对于self-attention，由于 <span class=\"math inline\">\\(Q、K、V\\)</span>\n都来自输入 <span class=\"math inline\">\\(X\\)</span> ，在计算 <span class=\"math inline\">\\(QK^T\\)</span>\n时，模型很容易关注到自身的位置上，也就是 <span class=\"math inline\">\\(QK^T\\)</span>\n对角线上的激活值会明显比较大。这样的情况其实不是很好，因为这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。后面讲的MHA对这个问题会有一些缓解作用。</p>\n<p>顺着这样的思路梳理下来，会发现attention的大思路还是很好理解的。而计算上，怎么去获得更好的效果，就是接下来要分析的几个内容，MHA，MQA和GQA所关注的。</p>\n<p>代码上，实现也很容易，直接看<a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a>的代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"关于scaling\">关于scaling</h2>\n<p>BTW，为什么计算中 <span class=\"math inline\">\\(QK^T\\)</span>\n之后还要除以 <span class=\"math inline\">\\(\\sqrt{d}\\)</span> ？</p>\n<p>简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p>苏剑林的<a href=\"https://spaces.ac.cn/archives/8620\">博客</a>中也有详细分析，并提到如果不对attention值进行scaling，也可以通过在参数初始化是将方差除以一个\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\n，同样可以起到预防softmax饱和的效果。类似地，通过normalization也可以做到类似的效果。不过实现上在attention里做scaling还是比较稳定高效的。</p>\n<h1 id=\"mha\">MHA</h1>\n<p>只要理解了attention计算的细节，MHA（multi-head\nattention）其实就很好明白。</p>\n<p>MHA在2017年就随着《Attention Is All You\nNeed》一起提出，主要干的就是一个事：把原来一个attention计算，拆成多个小份的attention，并行计算，分别得出结果，最后再合回原来的维度。</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>假设原来模型的hidden size是 <span class=\"math inline\">\\(d\\)</span>\n，在MHA中，会把投影后的 <span class=\"math inline\">\\(Q、K、V\\)</span>\n在hidden state的维度上切成 <span class=\"math inline\">\\(head_{num}\\)</span> 份，每个头的维度是 <span class=\"math inline\">\\(d_{head}\\)</span> 。这 <span class=\"math inline\">\\(head_{num}\\)</span> 组小 <span class=\"math inline\">\\(Q、K、V\\)</span>\n分别独立地进行attention计算，之后把得到的 <span class=\"math inline\">\\(head_{num}\\)</span> 份维度 <span class=\"math inline\">\\(d_{head}\\)</span> 的输出concat起来。</p>\n<p>直接看这个amazing的图，很直观</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p>操作是这么个操作，多头注意力相比单头有什么好处呢？</p>\n<p>《Attention Is All You Need》文章中给出的说法是</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>我们希望多个头能够在训练中学会注意到不同的内容。例如在翻译任务里，一些attention\nhead可以关注语法特征，另一些attention\nhead可以关注单词特性。这样模型就可以从不同角度来分析和理解输入信息，获得更好的效果了。</p>\n<p>这有点类似CNN中，不同的卷积核来学习不同的信息。比如一个 <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 的卷积，有128个 <span class=\"math inline\">\\(3\\times3\\)</span>\n参数组，假设我们的输入是一个灰度图，其中一组 <span class=\"math inline\">\\(3\\times3\\)</span> 的参数是这样的</p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p>那么这是一个检测纵向边界的卷积，而另外一组参数长这样</p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p>这是一个检测横向边界的卷积。</p>\n<p>这128组 <span class=\"math inline\">\\(3\\times3\\)</span>\n就是128个不同特征的检测器，就同MHA中多个头一样，从不同的子空间学到不同的内容，最后再放到一起融合使用。</p>\n<p>但是这是我们expect模型能做到的事情，实际情况是否真的是这样？</p>\n<p>知乎上这篇<a href=\"https://zhuanlan.zhihu.com/p/626820422\">文章</a>里对此做了一些实验和分析。简单来说就是（1）每个头确实学到东西有所不同，但大部分头之间的差异没有我们想的那么大（比如一个学句法，一个学词义这样明显的区分）（2）多个头的情况下，确实有少部分头可以比较好地捕捉到各种文本信息，而不会过分关注自身位置，一定程度缓解了上文提到的计算\n<span class=\"math inline\">\\(QK^T\\)</span> 之后对角线元素过大的问题。</p>\n<p>我们可以把MHA的多个attention计算视为多个独立的小模型，那么最终整体的attention计算相当于把来自多个小模型的结果进行了融合，这样效果比较好也是比较符合直觉的。</p>\n<p>另外还有一个问题是，使用几个头比较好呢？</p>\n<p>实际上这个问题比较难有确定性的答案，首先可以确定的是头的数量不是越多约好（毕竟头的数量多了，各个子空间小了，子空间能表达的内容就少了），具体多少要视模型规模，任务而定。另外<a href=\"https://arxiv.org/pdf/1905.10650.pdf\">《Are Sixteen Heads Really\nBetter than One?》</a>中也指出MHA并不总是优于单头的情况。</p>\n<p>目前可以看到的趋势是，模型越大（也就是hidden\nsize越大），头数的增多越能带来平均效果上的收益（或者说允许注意力头增大而不影响子空间的学习能力）。目前LLM主流的头数视乎模型结构和规模，大致有12、16、24、48、96这样一些主流设置。这里面又有比较多的方向和工作，在此暂时不展开，挖个坑，以后专门开一篇讲。</p>\n<p>最后看一下<a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>中的MHA代码实现</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p>（<a href=\"https://github.com/huggingface/transformers\">transformers</a>中的写法就更为成熟一点，不过里面兼容了比较多的功能，代码太长就不放上来了）</p>\n<h1 id=\"解码中的kv-cache\">解码中的KV Cache</h1>\n<p>在讲MQA和GQA之前，还需要了解一点背景，那就是解码的计算问题，以及KV\nCache的方案。</p>\n<p>无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。</p>\n<p>也就是，解码的时候，先根据当前输入 <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> ，生成下一个 <span class=\"math inline\">\\(\\text{token}_{i}\\)</span> ，然后把新生成的 <span class=\"math inline\">\\(\\text{token}_{i}\\)</span> 拼接在 <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> 后面，获得新的输入\n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span> ，再用 <span class=\"math inline\">\\(\\text{input}_{i}\\)</span> 生成 <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n，依此迭代，直到生成结束。</p>\n<p>比如我们输入“窗前明月光下一句是”，那么模型每次生成一个token，输入输出会是这样（方便起见，默认每个token都是一个字符）</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: 输入=[BOS]窗前明月光下一句是；输出=疑</span><br><span class=\"line\">step1: 输入=[BOS]窗前明月光下一句是疑；输出=是</span><br><span class=\"line\">step2: 输入=[BOS]窗前明月光下一句是疑是；输出=地</span><br><span class=\"line\">step3: 输入=[BOS]窗前明月光下一句是疑是地；输出=上</span><br><span class=\"line\">step4: 输入=[BOS]窗前明月光下一句是疑是地上；输出=霜</span><br><span class=\"line\">step5: 输入=[BOS]窗前明月光下一句是疑是地上霜；输出=[EOS]</span><br></pre></td></tr></table></figure>\n<p>（其中[BOS]和[EOS]分别是起始符号和终止符号）</p>\n<p>仔细想一下，我们在生成“疑”字的时候，用的是输入序列中“是”字的最后一层hidden\nstate，通过最后的分类头预测出来的。以此类推，后面每生成一个字，使用的都是输入序列中最后一个字的输出。</p>\n<p>我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个token）。那么下一个step的计算应该也包含了上一个step的计算。</p>\n<p>从公式上来看是这样的：</p>\n<p>回想一下我们attention的计算</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>注意对于decoder的时候，由于mask\nattention的存在，每个输入只能看到自己和前面的内容，而看不到后面的内容</p>\n<p>假设我们当前输入的长度是3，预测第4个字，那每层attention所做的计算有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>预测完第4个字，放到输入里，继续预测第5个字，每层attention所做的计算有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>可以看到，在预测第5个字时，只有最后一步引入了新的计算，而 <span class=\"math inline\">\\(o_{0}\\)</span> 到 <span class=\"math inline\">\\(o_{2}\\)</span> 的计算和前面是完全重复的。</p>\n<p>但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。</p>\n<p>也就是说中间有很多我们用不到的计算，这样就造成了浪费。</p>\n<p>而且随着生成的结果越来越多，输入的长度也越来越长。上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写高考作文，那可能就有800个step或者更多。这个情况下，step0被算了800次，step1被算了799次...</p>\n<p>有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？</p>\n<p>答案就是KV\nCache。利用缓存空间，把需要重复利用的中间计算结果存下来，减少重复计算。</p>\n<p>而 <span class=\"math inline\">\\(k\\)</span> 和 <span class=\"math inline\">\\(v\\)</span> 就是要缓存的对象。</p>\n<p>想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要踏踏实实地计算一遍。然后把\n<span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值缓存起来。</p>\n<p>则有</p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n↓\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache的下标 <span class=\"math inline\">\\(l\\)</span>\n表示模型层数。</p>\n<p>在进行第二次预测，也就是预测第5个字的时候，在第 <span class=\"math inline\">\\(l\\)</span>\n层的时候，由于前面我们缓存了<u><strong>每层</strong></u>的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值，那本层就只需要算新的 <span class=\"math inline\">\\(o_{3}\\)</span> ，而不用算 <span class=\"math inline\">\\(o_{0}、o_{1}、o_{2}\\)</span> 。</p>\n<p>因为第 <span class=\"math inline\">\\(l\\)</span> 层的 <span class=\"math inline\">\\(o_{0}、o_{1}、o_{2}\\)</span>\n本来会经过FNN层之后进到 <span class=\"math inline\">\\(l+1\\)</span>\n层，再经过新的投影变换，成为 <span class=\"math inline\">\\(l+1\\)</span>\n层的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值，但是 <span class=\"math inline\">\\(l+1\\)</span> 层的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值我们已经缓存过了！</p>\n<p>然后我们把本次新增算出来的 <span class=\"math inline\">\\(k\\)</span> 、\n<span class=\"math inline\">\\(v\\)</span> 值也存入缓存。</p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n↓\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>这样就节省了attention和FFN的很多重复计算。</p>\n<p>transformers中，生成的时候传入use_cache=True就会开启KV Cache。</p>\n<p>也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 过去所存的值</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># 把当前新的key加入</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># 把当前新的value加入</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># 输出用于保存</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>总的来说，KV\nCache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask\nattention的存在，使得前面的token可以不用关注后面的token）</p>\n<p>但是，用了KV Cache之后也不是立刻万事大吉。</p>\n<p>我们简单算一下，对于输入长度为 <span class=\"math inline\">\\(s\\)</span>\n，层数为 <span class=\"math inline\">\\(L\\)</span> ，hidden size为 <span class=\"math inline\">\\(d\\)</span> 的模型，需要缓存的参数量为</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p>如果使用的是半精度浮点数，那么总共所需的空间就是</p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>以Llama2 7B为例，有 <span class=\"math inline\">\\(L=32\\)</span> ，\n<span class=\"math inline\">\\(L=4096\\)</span>\n，那么每个token所需的缓存空间就是524,288 bytes，约52K，当 <span class=\"math inline\">\\(s=1024\\)</span> 时，则需要536,870,912\nbytes，超过500M的空间。</p>\n<p>这里考虑的还只是batch size=1的情况，如果batch\nsize增大，这个值更是很容易就超过1G。</p>\n<p>（MHA相比单头的情况，相当于只是把 <span class=\"math inline\">\\(q、k、v\\)</span>\n切成多份并行计算了，对于实际需要缓存的大小没有影响）</p>\n<p>看下现在主流的科学计算卡配置</p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>强如H100也只有50M的L2 Cache（L1\nCache的大小更是可以忽略不计），大概只能支持Llama2\n7B总共100个token左右的输入。</p>\n<p>想想我们现在用的LLM动辄34B/70B的规模，长度更是以千为基础单位，这样明显是不够用的。</p>\n<p>那么超出L2 Cache的部分只能走到显存中去了，但是HBM速度比L2\nCache慢多了。</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"储存空间与速度\">\n<p>看来还需要进一步优化。</p>\n<p>要保证模型的推理加速，要么增大Cache的大小，而且是需要一到两个数量级的增强，那这个只能靠黄老板了。</p>\n<p>要么就是减少需要缓存的量。</p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA就是来减少缓存所需要的量的。</p>\n<p>Google在2019年就在《Fast Transformer Decoding: One Write-Head is All\nYou\nNeed》提出了MQA，不过那时候主要到的人不多，那是大家主要还是关注在用Bert把榜刷出新高上。</p>\n<p>MQA的做法其实很简单。在MHA中，输入分别经过 <span class=\"math inline\">\\(W_{Q}、W_{K}、W_{V}\\)</span>\n的变换之后，都切成了n份（n=头数），维度也从 <span class=\"math inline\">\\(d_{model}\\)</span> 降到了 <span class=\"math inline\">\\(d_{head}\\)</span>\n，分别进行attention计算再拼接。而MQA这里，在线性变换之后，只对 <span class=\"math inline\">\\(Q\\)</span> 进行切分（和MHA一样），而 <span class=\"math inline\">\\(K、V\\)</span> 则直接在线性变换的时候把维度降到了\n<span class=\"math inline\">\\(d_{head}\\)</span>\n（而不是切分变小），然后这n个Query头分别和同一份 <span class=\"math inline\">\\(K、V\\)</span>\n进行attention计算，之后把结果拼接起来。</p>\n<p>简单来说，就是MHA中，每个注意力头的 <span class=\"math inline\">\\(K、V\\)</span>\n是不一样的，而MQA这里，每个注意力头的 <span class=\"math inline\">\\(K、V\\)</span>\n是一样的，值是共享的。而其他步骤都和MHA一样。</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p>这样一来，需要缓存的 <span class=\"math inline\">\\(K、V\\)</span>\n值一下就从所有头变成一个头的量。</p>\n<p>比如在Llama2\n7B中用的是32个头，那用MQA后，1024个token需要缓存的量就变成1/32，536,870,912\nbytes / 32 = 16,777,216 bytes，差不多是16M，这就能全塞进缓存中了。</p>\n<p>（实现上，就是改一下线性变换矩阵，然后把 <span class=\"math inline\">\\(K、V\\)</span>\n的处理从切分变成复制，就不再赘述。）</p>\n<p>当然，由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点，但是并不多，且相比其他修改hidden\nsize或者head num的做法效果都好。</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>既然MQA对效果有点影响，MHA缓存又存不下，那GQA（Grouped-Query\nAttention）就提出了一个折中的办法，既能减少MQA效果的损失，又相比MHA需要更少的缓存。</p>\n<p>（文章：《GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints》，2023年）</p>\n<p>GQA里， <span class=\"math inline\">\\(Q\\)</span>\n还是按原来MHA/MQA的做法不变。只使用一套共享的 <span class=\"math inline\">\\(K、V\\)</span>\n不是效果不好吗，那就还是多弄几套。但是不要太多，数量还是比 <span class=\"math inline\">\\(Q\\)</span> 的头数少一些。这样相当于把 <span class=\"math inline\">\\(Q\\)</span> 的多个头给分了group，同一个group内的\n<span class=\"math inline\">\\(Q\\)</span> 共享同一套 <span class=\"math inline\">\\(K、V\\)</span> ，不同group的 <span class=\"math inline\">\\(Q\\)</span> 所用的 <span class=\"math inline\">\\(K、V\\)</span> 不同。</p>\n<p>MHA可以认为是 <span class=\"math inline\">\\(K、V\\)</span>\n头数最大时的GQA，而MQA可以任务是 <span class=\"math inline\">\\(K、V\\)</span> 头数最少时的GQA。</p>\n<p>看论文里的图就很直观</p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p>效果怎么样呢？</p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>看表中2/3/4行对比，GQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。文中提到，这里的MQA和GQA都是通过average\npooling从MHA初始化而来，然后进行了少量的训练得到的。如果我们想要把之前用MHA训练的模型改造成GQA，也可以通过这样的方法，增加少量训练来实现。当然如果从一开始就加上，从零开始训练，也是没有问题的。</p>\n<p>Llama2用的就是GQA，在tech\nreport中也做了MHA、MQA、GQA的效果对比，可以看到效果确实很不错。</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"小结\">小结</h1>\n<p>MHA、MQA、GQA的实现其实并不复杂，效果也很好，理解上并没有太多困难。但是想要真正理解它们的出发点，还是需要深入每一个细节，去了解当时要解决的事什么问题。</p>\n<p>目前来看GQA是LLM比较好的方案，但未来肯定还会有针对不同方向的进一步优化方案，计算效率、推理速度、显存消耗这些方向都值得我们继续去探索优化。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n【2】Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n【3】Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n【4】https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n【5】GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n【6】How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n【7】A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n【8】https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n【9】浅谈Transformer的初始化、参数化与标准化\nhttps://spaces.ac.cn/archives/8620<br>\n【10】https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n【11】https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n【12】Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n【13】This post is all you need（上卷）——层层剥开Transformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n【14】The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n【15】Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n","length":16733,"excerpt":"","more":"<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>\n<p>Attention模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head\nAttention）、MQA（Multi-Query Attention）和GQA（Grouped-Query\nAttention）这一路线的思路和做法被很多主流模型所采用，因此简单地梳理一些这几个变体的思路和做法，以及会涉及到的KV\nCache相关内容。思路比较直白，但也有一些细节和原理值得思考。</p>\n<p>当然针对Attention优化，也有很多其他优秀的方案和思路，如线性注意力、FlashAttention和Sliding\nWindow Attention等，这些在后续再开篇梳理。</p>\n<p>（应一些朋友的要求，会增加一些直观基础的内容，以及LLM应用的案例。也欢迎大家提出更多建议。）</p>\n<h1 id=\"关于attention从rnn到attention\">关于Attention：从RNN到Attention</h1>\n<p>了解一个概念的诞生和演进，有助于我们更深入去理解它。我们先简单回顾下attention从起源到最初的实现。</p>\n<p>（熟悉attention的朋友可以跳过这一节）</p>\n<h2 id=\"从rnn说起\">从RNN说起</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>注意力机制最初起源是为了解决序列问题。回想在还没有Transformer的上一世代，使用RNN的Seq2Seq是这样的</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p>（图来自<a href=\"https://theaisummer.com/attention/\">AI\nSummer</a>）</p>\n<p>每个RNN cell接收两个输入，输出一个hidden state。比如在翻译任务中，RNN\nencoder把所有输入迭代地编码成context向量 <span class=\"math inline\">\\(z\\)</span> ，然后由RNN decoder基于 <span class=\"math inline\">\\(z\\)</span>\n迭代地解码。一般来说，这里decoder的第一个输入是一个特殊token，如[start]，表示解码开始。</p>\n<p>这样会有一个问题， <span class=\"math inline\">\\(z\\)</span>\n能编码的长度显然有限，而且由于模型结构问题，会更加关注靠近尾部的输入。这样如果关键信息出现在开头，就容易被忽略。</p>\n<p>并且时间步骤上的传播由于有多次迭代相乘，梯度很容易就过小，导致梯度消失问题。</p>\n<p>当然我们有LSMT和GRU等变体来增强长距离记忆的能力，也缓解了梯度问题，但这些方案还是没有产生质变的能力。</p>\n<p>回到问题的核心，我们想要 <span class=\"math inline\">\\(z\\)</span>\n能够编码所有前面的内容，但是显然， <span class=\"math inline\">\\(z\\)</span>\n的生成方式天然会让它更容易注意到靠后的内容，而容易忽略靠前的输入。</p>\n<p>一个直觉的想法就是，我们需要想个办法跳过 <span class=\"math inline\">\\(z\\)</span>\n，和前面的每个输入建立直接的联系。我们希望模型能够有机会学习到去“注意”关键的输入，不管这个输入是在前面还是后面。</p>\n<p>实际上神经网络天生就具有“注意力”的天赋。</p>\n<p>比如在CNN分类中，如果我们画出分类层前的heatmap，会是如下图这个样子</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>可以看到，值比较高的地方是在猫的鼻子胡子嘴巴区域，次之是身上和头上的花纹。直观来说，就是模型主要通过脸部的特征和身上的花纹，来识别出这是一只猫。这就是CNN学习到的注意力，这样的特征是神经网络implicitly学到的。</p>\n<p>回归到Seq2Seq，我们怎么来实现注意力，并且让这种implicit的机制变得explicit：单独抽离出来并具备一定可控制性？</p>\n<p>回想翻译场景，在RNN中，每一个时间步骤 <span class=\"math inline\">\\(i\\)</span> 都会产生一个隐向量，<span class=\"math inline\">\\(h_i\\)</span> 向量，我们把这些 <span class=\"math inline\">\\(h_i\\)</span>\n保存起来，在最后要生成新的输出的时候，我们让模型回头看一下之前的这每一个\n<span class=\"math inline\">\\(h_i\\)</span>\n，再决定要生成什么内容。相比原来只利用最后一个hidden\nstate，现在我们可以访问之前所有的中间状态，如果发现前面有关键信息，就可以直接用上了，而不用担心输入太长而被覆盖了。</p>\n<p>那么问题又来了，我们怎么知道前面某一个中间状态对于当前的生成来说是否重要？如果我们不知道怎么定义是否重要，那我们就把这个问题交给模型自己解决好了\n--\n通过网络参数来学习识别某个输入状态是否重要，学习是否要“注意”到它，要给予多少的“注意力”。</p>\n<p>具体来说，我们定义在解码第 <span class=\"math inline\">\\(i\\)</span>\n个输出是，decoder当前隐状态 <span class=\"math inline\">\\(y_{i-1}\\)</span>\n和encoder的所有隐状态 <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\n之间的一个score计算</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p>其中</p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p>注意力网络通过 <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n和 <span class=\"math inline\">\\(h_j\\)</span> 来计算一个值 <span class=\"math inline\">\\(e_{ij}\\)</span>，这里的注意力网络可以设计各种操作，比如对输入进行拼接再通过fc层进行计算等。</p>\n<p>这里 <span class=\"math inline\">\\(e_{ij}\\)</span>\n是一个标量，但它还不是一个可用的权重值，还需要通过一个函数，把attention\nnet对各个encoder hidden state的输出值转成一个分布：softmax。</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>最后通过加权计算，获得最终输入给decoder的隐变量。</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>可以看到，这里的attention net的任务就是找到decoder上一个hidden\nstate和encoder hidden\nstate之间的“相关”关系，使得模型能够将更多的注意力放在对应的输入信息上。</p>\n<p>实际上，上面这种attention的计算方式并不是唯一的，attention的计算方式有许多种</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>这些attention的一般形式可以写作 <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span> 。这里的 <span class=\"math inline\">\\(s\\)</span>\n就是decoder的hidden state（也就是前文的 <span class=\"math inline\">\\(y\\)</span> ），<span class=\"math inline\">\\(h\\)</span> 就是encoder的hidden state。</p>\n<p>（当然从结果上看，是scaled dot-product\nattention经受住了历史的考验，成为了主流。）</p>\n<h2 id=\"transformer的attention\">Transformer的attention</h2>\n<p>从RNN attention到transformer\nattention，所做的事情就如论文题目所说：《Attention Is All You\nNeed》，彻底抛弃RNN的在time\nstep上的迭代计算，完全拥抱attention机制，只用最简单粗暴的方式同步计算出每个输入的hidden\nstate，其他的就交给attention来解决。</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>这里还是保留有encoder和decoder的结构，encoder中的attention都是self-attention，decoder则除了self-attention还有cross-attention。</p>\n<p>transformer结构下，attention的一般形式可以写作 <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>，这里有\n<span class=\"math inline\">\\(Q=W_{Q}Y，K=W_{K}X，V=W_{V}X\\)</span>\n。对于cross-attention， <span class=\"math inline\">\\(X\\)</span>\n是encoder的hidden states，<span class=\"math inline\">\\(Y\\)</span>\n是decoder的hidden states，而对于self-attention，则有 <span class=\"math inline\">\\(X=Y\\)</span>。</p>\n<p>具体到我们熟悉的scaled dot-product attention，使用softmax计算，有</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>到这里，终于见到我们熟悉的attention计算。</p>\n<p>用一张很直观的图来展示整个计算</p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>这里的「query」，「key」和「value」的名称也暗示了整个attention计算的思路。</p>\n<p>类比到一个数据库查询+预测的例子。</p>\n<p>假设我们现在有一个“文章-阅读量”数据库，记录了每篇文章在发布30天内的阅读量。每篇文章就是一个key，对应的阅读量就是value。</p>\n<p>我们现在有一篇将要发布的文章，想要预测这篇文章在30天内的阅读量，那我们就把这篇新的文章，作为query，去和数据库里的文章（key）做一个相关性计算，取最相关的5篇文章。</p>\n<p>假设top5篇文章的相关性分别是 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span> ，对应阅读量是 <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n。</p>\n<p>那我们把相关性得分归一化成和为1的概率值 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n，那我们就可以预测新文章30天内的阅读量是 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n。</p>\n<p>这个例子中，我们计算相关性就相当于transformer attention中的 <span class=\"math inline\">\\(QK^T\\)</span>\n，归一化就是softmax，然后通过加权求和取得最后的阅读量/特征向量。</p>\n<p>对于self-attention， <span class=\"math inline\">\\(Q、K、V\\)</span>\n都来自输入 <span class=\"math inline\">\\(X\\)</span>，sequence自己计算自己每个token的之间的相关性。而对于cross-attention，decoder中的输出sequence就是上面这个例子中的“将要发布的文章”，通过把这篇新的文章和数据库中的文章做相关计算，我们得到了新的预测结果。</p>\n<p>对于self-attention，由于 <span class=\"math inline\">\\(Q、K、V\\)</span>\n都来自输入 <span class=\"math inline\">\\(X\\)</span> ，在计算 <span class=\"math inline\">\\(QK^T\\)</span>\n时，模型很容易关注到自身的位置上，也就是 <span class=\"math inline\">\\(QK^T\\)</span>\n对角线上的激活值会明显比较大。这样的情况其实不是很好，因为这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。后面讲的MHA对这个问题会有一些缓解作用。</p>\n<p>顺着这样的思路梳理下来，会发现attention的大思路还是很好理解的。而计算上，怎么去获得更好的效果，就是接下来要分析的几个内容，MHA，MQA和GQA所关注的。</p>\n<p>代码上，实现也很容易，直接看<a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a>的代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"关于scaling\">关于scaling</h2>\n<p>BTW，为什么计算中 <span class=\"math inline\">\\(QK^T\\)</span>\n之后还要除以 <span class=\"math inline\">\\(\\sqrt{d}\\)</span> ？</p>\n<p>简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p>苏剑林的<a href=\"https://spaces.ac.cn/archives/8620\">博客</a>中也有详细分析，并提到如果不对attention值进行scaling，也可以通过在参数初始化是将方差除以一个\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\n，同样可以起到预防softmax饱和的效果。类似地，通过normalization也可以做到类似的效果。不过实现上在attention里做scaling还是比较稳定高效的。</p>\n<h1 id=\"mha\">MHA</h1>\n<p>只要理解了attention计算的细节，MHA（multi-head\nattention）其实就很好明白。</p>\n<p>MHA在2017年就随着《Attention Is All You\nNeed》一起提出，主要干的就是一个事：把原来一个attention计算，拆成多个小份的attention，并行计算，分别得出结果，最后再合回原来的维度。</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>假设原来模型的hidden size是 <span class=\"math inline\">\\(d\\)</span>\n，在MHA中，会把投影后的 <span class=\"math inline\">\\(Q、K、V\\)</span>\n在hidden state的维度上切成 <span class=\"math inline\">\\(head_{num}\\)</span> 份，每个头的维度是 <span class=\"math inline\">\\(d_{head}\\)</span> 。这 <span class=\"math inline\">\\(head_{num}\\)</span> 组小 <span class=\"math inline\">\\(Q、K、V\\)</span>\n分别独立地进行attention计算，之后把得到的 <span class=\"math inline\">\\(head_{num}\\)</span> 份维度 <span class=\"math inline\">\\(d_{head}\\)</span> 的输出concat起来。</p>\n<p>直接看这个amazing的图，很直观</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p>操作是这么个操作，多头注意力相比单头有什么好处呢？</p>\n<p>《Attention Is All You Need》文章中给出的说法是</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>我们希望多个头能够在训练中学会注意到不同的内容。例如在翻译任务里，一些attention\nhead可以关注语法特征，另一些attention\nhead可以关注单词特性。这样模型就可以从不同角度来分析和理解输入信息，获得更好的效果了。</p>\n<p>这有点类似CNN中，不同的卷积核来学习不同的信息。比如一个 <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 的卷积，有128个 <span class=\"math inline\">\\(3\\times3\\)</span>\n参数组，假设我们的输入是一个灰度图，其中一组 <span class=\"math inline\">\\(3\\times3\\)</span> 的参数是这样的</p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p>那么这是一个检测纵向边界的卷积，而另外一组参数长这样</p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p>这是一个检测横向边界的卷积。</p>\n<p>这128组 <span class=\"math inline\">\\(3\\times3\\)</span>\n就是128个不同特征的检测器，就同MHA中多个头一样，从不同的子空间学到不同的内容，最后再放到一起融合使用。</p>\n<p>但是这是我们expect模型能做到的事情，实际情况是否真的是这样？</p>\n<p>知乎上这篇<a href=\"https://zhuanlan.zhihu.com/p/626820422\">文章</a>里对此做了一些实验和分析。简单来说就是（1）每个头确实学到东西有所不同，但大部分头之间的差异没有我们想的那么大（比如一个学句法，一个学词义这样明显的区分）（2）多个头的情况下，确实有少部分头可以比较好地捕捉到各种文本信息，而不会过分关注自身位置，一定程度缓解了上文提到的计算\n<span class=\"math inline\">\\(QK^T\\)</span> 之后对角线元素过大的问题。</p>\n<p>我们可以把MHA的多个attention计算视为多个独立的小模型，那么最终整体的attention计算相当于把来自多个小模型的结果进行了融合，这样效果比较好也是比较符合直觉的。</p>\n<p>另外还有一个问题是，使用几个头比较好呢？</p>\n<p>实际上这个问题比较难有确定性的答案，首先可以确定的是头的数量不是越多约好（毕竟头的数量多了，各个子空间小了，子空间能表达的内容就少了），具体多少要视模型规模，任务而定。另外<a href=\"https://arxiv.org/pdf/1905.10650.pdf\">《Are Sixteen Heads Really\nBetter than One?》</a>中也指出MHA并不总是优于单头的情况。</p>\n<p>目前可以看到的趋势是，模型越大（也就是hidden\nsize越大），头数的增多越能带来平均效果上的收益（或者说允许注意力头增大而不影响子空间的学习能力）。目前LLM主流的头数视乎模型结构和规模，大致有12、16、24、48、96这样一些主流设置。这里面又有比较多的方向和工作，在此暂时不展开，挖个坑，以后专门开一篇讲。</p>\n<p>最后看一下<a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>中的MHA代码实现</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p>（<a href=\"https://github.com/huggingface/transformers\">transformers</a>中的写法就更为成熟一点，不过里面兼容了比较多的功能，代码太长就不放上来了）</p>\n<h1 id=\"解码中的kv-cache\">解码中的KV Cache</h1>\n<p>在讲MQA和GQA之前，还需要了解一点背景，那就是解码的计算问题，以及KV\nCache的方案。</p>\n<p>无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。</p>\n<p>也就是，解码的时候，先根据当前输入 <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> ，生成下一个 <span class=\"math inline\">\\(\\text{token}_{i}\\)</span> ，然后把新生成的 <span class=\"math inline\">\\(\\text{token}_{i}\\)</span> 拼接在 <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> 后面，获得新的输入\n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span> ，再用 <span class=\"math inline\">\\(\\text{input}_{i}\\)</span> 生成 <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n，依此迭代，直到生成结束。</p>\n<p>比如我们输入“窗前明月光下一句是”，那么模型每次生成一个token，输入输出会是这样（方便起见，默认每个token都是一个字符）</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: 输入=[BOS]窗前明月光下一句是；输出=疑</span><br><span class=\"line\">step1: 输入=[BOS]窗前明月光下一句是疑；输出=是</span><br><span class=\"line\">step2: 输入=[BOS]窗前明月光下一句是疑是；输出=地</span><br><span class=\"line\">step3: 输入=[BOS]窗前明月光下一句是疑是地；输出=上</span><br><span class=\"line\">step4: 输入=[BOS]窗前明月光下一句是疑是地上；输出=霜</span><br><span class=\"line\">step5: 输入=[BOS]窗前明月光下一句是疑是地上霜；输出=[EOS]</span><br></pre></td></tr></table></figure>\n<p>（其中[BOS]和[EOS]分别是起始符号和终止符号）</p>\n<p>仔细想一下，我们在生成“疑”字的时候，用的是输入序列中“是”字的最后一层hidden\nstate，通过最后的分类头预测出来的。以此类推，后面每生成一个字，使用的都是输入序列中最后一个字的输出。</p>\n<p>我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个token）。那么下一个step的计算应该也包含了上一个step的计算。</p>\n<p>从公式上来看是这样的：</p>\n<p>回想一下我们attention的计算</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>注意对于decoder的时候，由于mask\nattention的存在，每个输入只能看到自己和前面的内容，而看不到后面的内容</p>\n<p>假设我们当前输入的长度是3，预测第4个字，那每层attention所做的计算有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>预测完第4个字，放到输入里，继续预测第5个字，每层attention所做的计算有</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>可以看到，在预测第5个字时，只有最后一步引入了新的计算，而 <span class=\"math inline\">\\(o_{0}\\)</span> 到 <span class=\"math inline\">\\(o_{2}\\)</span> 的计算和前面是完全重复的。</p>\n<p>但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。</p>\n<p>也就是说中间有很多我们用不到的计算，这样就造成了浪费。</p>\n<p>而且随着生成的结果越来越多，输入的长度也越来越长。上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写高考作文，那可能就有800个step或者更多。这个情况下，step0被算了800次，step1被算了799次...</p>\n<p>有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？</p>\n<p>答案就是KV\nCache。利用缓存空间，把需要重复利用的中间计算结果存下来，减少重复计算。</p>\n<p>而 <span class=\"math inline\">\\(k\\)</span> 和 <span class=\"math inline\">\\(v\\)</span> 就是要缓存的对象。</p>\n<p>想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要踏踏实实地计算一遍。然后把\n<span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值缓存起来。</p>\n<p>则有</p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n↓\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache的下标 <span class=\"math inline\">\\(l\\)</span>\n表示模型层数。</p>\n<p>在进行第二次预测，也就是预测第5个字的时候，在第 <span class=\"math inline\">\\(l\\)</span>\n层的时候，由于前面我们缓存了<u><strong>每层</strong></u>的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值，那本层就只需要算新的 <span class=\"math inline\">\\(o_{3}\\)</span> ，而不用算 <span class=\"math inline\">\\(o_{0}、o_{1}、o_{2}\\)</span> 。</p>\n<p>因为第 <span class=\"math inline\">\\(l\\)</span> 层的 <span class=\"math inline\">\\(o_{0}、o_{1}、o_{2}\\)</span>\n本来会经过FNN层之后进到 <span class=\"math inline\">\\(l+1\\)</span>\n层，再经过新的投影变换，成为 <span class=\"math inline\">\\(l+1\\)</span>\n层的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值，但是 <span class=\"math inline\">\\(l+1\\)</span> 层的 <span class=\"math inline\">\\(k\\)</span> 、 <span class=\"math inline\">\\(v\\)</span> 值我们已经缓存过了！</p>\n<p>然后我们把本次新增算出来的 <span class=\"math inline\">\\(k\\)</span> 、\n<span class=\"math inline\">\\(v\\)</span> 值也存入缓存。</p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n↓\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>这样就节省了attention和FFN的很多重复计算。</p>\n<p>transformers中，生成的时候传入use_cache=True就会开启KV Cache。</p>\n<p>也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 过去所存的值</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># 把当前新的key加入</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># 把当前新的value加入</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># 输出用于保存</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>总的来说，KV\nCache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask\nattention的存在，使得前面的token可以不用关注后面的token）</p>\n<p>但是，用了KV Cache之后也不是立刻万事大吉。</p>\n<p>我们简单算一下，对于输入长度为 <span class=\"math inline\">\\(s\\)</span>\n，层数为 <span class=\"math inline\">\\(L\\)</span> ，hidden size为 <span class=\"math inline\">\\(d\\)</span> 的模型，需要缓存的参数量为</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p>如果使用的是半精度浮点数，那么总共所需的空间就是</p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>以Llama2 7B为例，有 <span class=\"math inline\">\\(L=32\\)</span> ，\n<span class=\"math inline\">\\(L=4096\\)</span>\n，那么每个token所需的缓存空间就是524,288 bytes，约52K，当 <span class=\"math inline\">\\(s=1024\\)</span> 时，则需要536,870,912\nbytes，超过500M的空间。</p>\n<p>这里考虑的还只是batch size=1的情况，如果batch\nsize增大，这个值更是很容易就超过1G。</p>\n<p>（MHA相比单头的情况，相当于只是把 <span class=\"math inline\">\\(q、k、v\\)</span>\n切成多份并行计算了，对于实际需要缓存的大小没有影响）</p>\n<p>看下现在主流的科学计算卡配置</p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>强如H100也只有50M的L2 Cache（L1\nCache的大小更是可以忽略不计），大概只能支持Llama2\n7B总共100个token左右的输入。</p>\n<p>想想我们现在用的LLM动辄34B/70B的规模，长度更是以千为基础单位，这样明显是不够用的。</p>\n<p>那么超出L2 Cache的部分只能走到显存中去了，但是HBM速度比L2\nCache慢多了。</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"储存空间与速度\">\n<p>看来还需要进一步优化。</p>\n<p>要保证模型的推理加速，要么增大Cache的大小，而且是需要一到两个数量级的增强，那这个只能靠黄老板了。</p>\n<p>要么就是减少需要缓存的量。</p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA就是来减少缓存所需要的量的。</p>\n<p>Google在2019年就在《Fast Transformer Decoding: One Write-Head is All\nYou\nNeed》提出了MQA，不过那时候主要到的人不多，那是大家主要还是关注在用Bert把榜刷出新高上。</p>\n<p>MQA的做法其实很简单。在MHA中，输入分别经过 <span class=\"math inline\">\\(W_{Q}、W_{K}、W_{V}\\)</span>\n的变换之后，都切成了n份（n=头数），维度也从 <span class=\"math inline\">\\(d_{model}\\)</span> 降到了 <span class=\"math inline\">\\(d_{head}\\)</span>\n，分别进行attention计算再拼接。而MQA这里，在线性变换之后，只对 <span class=\"math inline\">\\(Q\\)</span> 进行切分（和MHA一样），而 <span class=\"math inline\">\\(K、V\\)</span> 则直接在线性变换的时候把维度降到了\n<span class=\"math inline\">\\(d_{head}\\)</span>\n（而不是切分变小），然后这n个Query头分别和同一份 <span class=\"math inline\">\\(K、V\\)</span>\n进行attention计算，之后把结果拼接起来。</p>\n<p>简单来说，就是MHA中，每个注意力头的 <span class=\"math inline\">\\(K、V\\)</span>\n是不一样的，而MQA这里，每个注意力头的 <span class=\"math inline\">\\(K、V\\)</span>\n是一样的，值是共享的。而其他步骤都和MHA一样。</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p>这样一来，需要缓存的 <span class=\"math inline\">\\(K、V\\)</span>\n值一下就从所有头变成一个头的量。</p>\n<p>比如在Llama2\n7B中用的是32个头，那用MQA后，1024个token需要缓存的量就变成1/32，536,870,912\nbytes / 32 = 16,777,216 bytes，差不多是16M，这就能全塞进缓存中了。</p>\n<p>（实现上，就是改一下线性变换矩阵，然后把 <span class=\"math inline\">\\(K、V\\)</span>\n的处理从切分变成复制，就不再赘述。）</p>\n<p>当然，由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点，但是并不多，且相比其他修改hidden\nsize或者head num的做法效果都好。</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>既然MQA对效果有点影响，MHA缓存又存不下，那GQA（Grouped-Query\nAttention）就提出了一个折中的办法，既能减少MQA效果的损失，又相比MHA需要更少的缓存。</p>\n<p>（文章：《GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints》，2023年）</p>\n<p>GQA里， <span class=\"math inline\">\\(Q\\)</span>\n还是按原来MHA/MQA的做法不变。只使用一套共享的 <span class=\"math inline\">\\(K、V\\)</span>\n不是效果不好吗，那就还是多弄几套。但是不要太多，数量还是比 <span class=\"math inline\">\\(Q\\)</span> 的头数少一些。这样相当于把 <span class=\"math inline\">\\(Q\\)</span> 的多个头给分了group，同一个group内的\n<span class=\"math inline\">\\(Q\\)</span> 共享同一套 <span class=\"math inline\">\\(K、V\\)</span> ，不同group的 <span class=\"math inline\">\\(Q\\)</span> 所用的 <span class=\"math inline\">\\(K、V\\)</span> 不同。</p>\n<p>MHA可以认为是 <span class=\"math inline\">\\(K、V\\)</span>\n头数最大时的GQA，而MQA可以任务是 <span class=\"math inline\">\\(K、V\\)</span> 头数最少时的GQA。</p>\n<p>看论文里的图就很直观</p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p>效果怎么样呢？</p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>看表中2/3/4行对比，GQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。文中提到，这里的MQA和GQA都是通过average\npooling从MHA初始化而来，然后进行了少量的训练得到的。如果我们想要把之前用MHA训练的模型改造成GQA，也可以通过这样的方法，增加少量训练来实现。当然如果从一开始就加上，从零开始训练，也是没有问题的。</p>\n<p>Llama2用的就是GQA，在tech\nreport中也做了MHA、MQA、GQA的效果对比，可以看到效果确实很不错。</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"小结\">小结</h1>\n<p>MHA、MQA、GQA的实现其实并不复杂，效果也很好，理解上并没有太多困难。但是想要真正理解它们的出发点，还是需要深入每一个细节，去了解当时要解决的事什么问题。</p>\n<p>目前来看GQA是LLM比较好的方案，但未来肯定还会有针对不同方向的进一步优化方案，计算效率、推理速度、显存消耗这些方向都值得我们继续去探索优化。</p>\n<hr>\n<p>读到这了，来一发点赞收藏关注吧~</p>\n<p>博客：<a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n知乎：<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\n微信公众号：Linsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>【1】The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n【2】Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n【3】Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n【4】https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n【5】GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n【6】How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n【7】A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n【8】https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n【9】浅谈Transformer的初始化、参数化与标准化\nhttps://spaces.ac.cn/archives/8620<br>\n【10】https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n【11】https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n【12】Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n【13】This post is all you need（上卷）——层层剥开Transformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n【14】The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n【15】Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n"}],"PostAsset":[{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi.png","post":"clttl4aky0002rb4ke9vgh881","slug":"meta_pi.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_explanation.png","post":"clttl4aky0002rb4ke9vgh881","slug":"meta_pi_explanation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_pi_nosft.png","post":"clttl4aky0002rb4ke9vgh881","slug":"meta_pi_nosft.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/meta_rope_ext.png","post":"clttl4aky0002rb4ke9vgh881","slug":"meta_rope_ext.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/mix_precision_fp16.png","post":"clttl4aky0002rb4ke9vgh881","slug":"mix_precision_fp16.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM长上下文的问题/rope_matrix.png","post":"clttl4aky0002rb4ke9vgh881","slug":"rope_matrix.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/big_bird_attention.png","post":"clttl4al00004rb4k186cc9s1","slug":"big_bird_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/dilated_conv.png","post":"clttl4al00004rb4k186cc9s1","slug":"dilated_conv.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/longformer_attention.png","post":"clttl4al00004rb4k186cc9s1","slug":"longformer_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_architechture.png","post":"clttl4al00004rb4k186cc9s1","slug":"mistral_architechture.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_large_performance.jpeg","post":"clttl4al00004rb4k186cc9s1","slug":"mistral_large_performance.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_perf.png","post":"clttl4al00004rb4k186cc9s1","slug":"mistral_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/mistral_swa.png","post":"clttl4al00004rb4k186cc9s1","slug":"mistral_swa.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/ms_invest_mistral.png","post":"clttl4al00004rb4k186cc9s1","slug":"ms_invest_mistral.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/prefill_and_chunking.png","post":"clttl4al00004rb4k186cc9s1","slug":"prefill_and_chunking.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/receptive_field_cnn.png","post":"clttl4al00004rb4k186cc9s1","slug":"receptive_field_cnn.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM注意力计算提效-sliding-window-attention/rolling_buffer.png","post":"clttl4al00004rb4k186cc9s1","slug":"rolling_buffer.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/complex_number.png","post":"clttl4al10007rb4k4nyqhg3o","slug":"complex_number.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/remote_attenuation.png","post":"clttl4al10007rb4k4nyqhg3o","slug":"remote_attenuation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM位置编码RoPE/rope.png","post":"clttl4al10007rb4k4nyqhg3o","slug":"rope.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/GQA.png","post":"clttl4al10008rb4k23jp14zi","slug":"GQA.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/GQA_result_1.png","post":"clttl4al10008rb4k23jp14zi","slug":"GQA_result_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/MQA.png","post":"clttl4al10008rb4k23jp14zi","slug":"MQA.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/MQA.webp","post":"clttl4al10008rb4k23jp14zi","slug":"MQA.webp","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Markdown _ 让排版变 Nice.html","post":"clttl4al10008rb4k23jp14zi","slug":"Markdown _ 让排版变 Nice.html","modified":1,"renderable":1},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Scaled-dot-product-self-attention.pbm","post":"clttl4al10008rb4k23jp14zi","slug":"Scaled-dot-product-self-attention.pbm","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/Scaled-dot-product-self-attention.png","post":"clttl4al10008rb4k23jp14zi","slug":"Scaled-dot-product-self-attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/attention_calculation.png","post":"clttl4al10008rb4k23jp14zi","slug":"attention_calculation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/cnn_heatmap.png","post":"clttl4al10008rb4k23jp14zi","slug":"cnn_heatmap.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/decoder.png","post":"clttl4al10008rb4k23jp14zi","slug":"decoder.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/encoder.png","post":"clttl4al10008rb4k23jp14zi","slug":"encoder.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/gpu_cache.png","post":"clttl4al10008rb4k23jp14zi","slug":"gpu_cache.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/lihongyi_self_attention.png","post":"clttl4al10008rb4k23jp14zi","slug":"lihongyi_self_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/llama2_qga.png","post":"clttl4al10008rb4k23jp14zi","slug":"llama2_qga.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/mqa_result_1.png","post":"clttl4al10008rb4k23jp14zi","slug":"mqa_result_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/mqa_result_3.png","post":"clttl4al10008rb4k23jp14zi","slug":"mqa_result_3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/multihead_attention.png","post":"clttl4al10008rb4k23jp14zi","slug":"multihead_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/seq2seq.png","post":"clttl4al10008rb4k23jp14zi","slug":"seq2seq.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/seq2seq_attention.png","post":"clttl4al10008rb4k23jp14zi","slug":"seq2seq_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/softmax.png","post":"clttl4al10008rb4k23jp14zi","slug":"softmax.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/sram_dram.png","post":"clttl4al10008rb4k23jp14zi","slug":"sram_dram.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/理解Attention-MHA-MQA和GQA/transformer_structure.png","post":"clttl4al10008rb4k23jp14zi","slug":"transformer_structure.png","modified":1,"renderable":0}],"PostCategory":[{"post_id":"clttl4al10008rb4k23jp14zi","category_id":"clttl4al00005rb4keyff2zjp","_id":"clttl4al4000vrb4k4ehec6zy"},{"post_id":"clttl4al10008rb4k23jp14zi","category_id":"clttl4al3000drb4k4igca5dc","_id":"clttl4al4000xrb4k1han027h"},{"post_id":"clttl4al10008rb4k23jp14zi","category_id":"clttl4al3000orb4k08mp80z4","_id":"clttl4al4000zrb4kazuj2buy"},{"post_id":"clttl4aky0002rb4ke9vgh881","category_id":"clttl4al00005rb4keyff2zjp","_id":"clttl4al40010rb4ka1cgdhwb"},{"post_id":"clttl4aky0002rb4ke9vgh881","category_id":"clttl4al3000drb4k4igca5dc","_id":"clttl4al40012rb4k5jqm12o4"},{"post_id":"clttl4aky0002rb4ke9vgh881","category_id":"clttl4al3000orb4k08mp80z4","_id":"clttl4al40013rb4k695c84al"},{"post_id":"clttl4al00004rb4k186cc9s1","category_id":"clttl4al00005rb4keyff2zjp","_id":"clttl4al40015rb4kb1b89u6n"},{"post_id":"clttl4al00004rb4k186cc9s1","category_id":"clttl4al3000drb4k4igca5dc","_id":"clttl4al40017rb4k9fsy46hj"},{"post_id":"clttl4al00004rb4k186cc9s1","category_id":"clttl4al3000orb4k08mp80z4","_id":"clttl4al5001arb4k75hx4mkh"},{"post_id":"clttl4al10007rb4k4nyqhg3o","category_id":"clttl4al00005rb4keyff2zjp","_id":"clttl4al5001crb4k8wh1c24q"},{"post_id":"clttl4al10007rb4k4nyqhg3o","category_id":"clttl4al3000drb4k4igca5dc","_id":"clttl4al5001frb4kfsjv5l6i"},{"post_id":"clttl4al10007rb4k4nyqhg3o","category_id":"clttl4al3000orb4k08mp80z4","_id":"clttl4al5001hrb4k7pukd43s"}],"PostTag":[{"post_id":"clttl4aky0002rb4ke9vgh881","tag_id":"clttl4al10006rb4khrlb7570","_id":"clttl4al3000jrb4kbwn1gc4m"},{"post_id":"clttl4aky0002rb4ke9vgh881","tag_id":"clttl4al2000arb4k5vgyg94u","_id":"clttl4al3000lrb4kcujqc4g9"},{"post_id":"clttl4aky0002rb4ke9vgh881","tag_id":"clttl4al3000crb4k6if1e5ap","_id":"clttl4al3000nrb4k6tca6o51"},{"post_id":"clttl4aky0002rb4ke9vgh881","tag_id":"clttl4al3000erb4k09en2ljt","_id":"clttl4al4000prb4kgd6hdamk"},{"post_id":"clttl4aky0002rb4ke9vgh881","tag_id":"clttl4al3000grb4k6zho56xt","_id":"clttl4al4000rrb4kd11j2ecx"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al10006rb4khrlb7570","_id":"clttl4al40016rb4ke01h8klc"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al2000arb4k5vgyg94u","_id":"clttl4al50018rb4k1mhj0m18"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al3000crb4k6if1e5ap","_id":"clttl4al5001brb4kg9lzhtur"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al4000trb4khbyc189k","_id":"clttl4al5001drb4k0lo533ar"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al4000wrb4kgrzdf2f0","_id":"clttl4al5001grb4kcchlb24f"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al40011rb4kfg5janah","_id":"clttl4al5001irb4kbxlxecye"},{"post_id":"clttl4al10007rb4k4nyqhg3o","tag_id":"clttl4al10006rb4khrlb7570","_id":"clttl4al5001mrb4k8f9j73nb"},{"post_id":"clttl4al10007rb4k4nyqhg3o","tag_id":"clttl4al2000arb4k5vgyg94u","_id":"clttl4al5001nrb4kd44rdm7o"},{"post_id":"clttl4al10007rb4k4nyqhg3o","tag_id":"clttl4al3000crb4k6if1e5ap","_id":"clttl4al5001prb4kee5t078o"},{"post_id":"clttl4al10007rb4k4nyqhg3o","tag_id":"clttl4al5001jrb4kemj5bf3d","_id":"clttl4al5001qrb4kgg2l8m8f"},{"post_id":"clttl4al10007rb4k4nyqhg3o","tag_id":"clttl4al5001krb4kfysjd6ai","_id":"clttl4al5001srb4kcark3gyf"},{"post_id":"clttl4al10008rb4k23jp14zi","tag_id":"clttl4al10006rb4khrlb7570","_id":"clttl4al5001urb4keo5836bf"},{"post_id":"clttl4al10008rb4k23jp14zi","tag_id":"clttl4al2000arb4k5vgyg94u","_id":"clttl4al5001vrb4kb5dv16t4"},{"post_id":"clttl4al10008rb4k23jp14zi","tag_id":"clttl4al3000crb4k6if1e5ap","_id":"clttl4al5001wrb4k45ofdfwj"},{"post_id":"clttl4al10008rb4k23jp14zi","tag_id":"clttl4al4000trb4khbyc189k","_id":"clttl4al5001xrb4k137z4uyw"},{"post_id":"clttl4al10008rb4k23jp14zi","tag_id":"clttl4al5001trb4k7zmf7np9","_id":"clttl4al5001yrb4k53xa45g7"}],"Tag":[{"name":"NLP","_id":"clttl4al10006rb4khrlb7570"},{"name":"LLM","_id":"clttl4al2000arb4k5vgyg94u"},{"name":"transformer","_id":"clttl4al3000crb4k6if1e5ap"},{"name":"长上下文","_id":"clttl4al3000erb4k09en2ljt"},{"name":"窗口外推","_id":"clttl4al3000grb4k6zho56xt"},{"name":"attention","_id":"clttl4al4000trb4khbyc189k"},{"name":"sliding window attention","_id":"clttl4al4000wrb4kgrzdf2f0"},{"name":"sparse attention","_id":"clttl4al40011rb4kfg5janah"},{"name":"positional encoding","_id":"clttl4al5001jrb4kemj5bf3d"},{"name":"RoPE","_id":"clttl4al5001krb4kfysjd6ai"},{"name":"KV Cache","_id":"clttl4al5001trb4k7zmf7np9"}]}}